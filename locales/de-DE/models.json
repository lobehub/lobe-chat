{
  "01-ai/Yi-1.5-34B-Chat-16K": {
    "description": "Yi-1.5 34B bietet mit umfangreichen Trainingsbeispielen überlegene Leistungen in der Branchenanwendung."
  },
  "01-ai/Yi-1.5-6B-Chat": {
    "description": "Yi-1.5-6B-Chat ist eine Variante der Yi-1.5-Serie und gehört zu den Open-Source-Chatmodellen. Yi-1.5 ist die verbesserte Version von Yi, die auf 500B hochwertigen Korpora kontinuierlich vortrainiert wurde und auf 3M diversifizierten Feinabstimmungsbeispielen feinabgestimmt wurde. Im Vergleich zu Yi zeigt Yi-1.5 stärkere Fähigkeiten in Codierung, Mathematik, Inferenz und Befolgung von Anweisungen, während es hervorragende Sprachverständnis-, Alltagswissen- und Leseverständnisfähigkeiten bewahrt. Das Modell bietet Versionen mit Kontextlängen von 4K, 16K und 32K, mit einer Gesamtanzahl von 3,6T Tokens im Vortraining."
  },
  "01-ai/Yi-1.5-9B-Chat-16K": {
    "description": "Yi-1.5 9B unterstützt 16K Tokens und bietet effiziente, flüssige Sprachgenerierungsfähigkeiten."
  },
  "01-ai/yi-1.5-34b-chat": {
    "description": "Yi 1.5, das neueste Open-Source-Fine-Tuning-Modell mit 34 Milliarden Parametern, unterstützt verschiedene Dialogszenarien mit hochwertigen Trainingsdaten, die auf menschliche Präferenzen abgestimmt sind."
  },
  "01-ai/yi-1.5-9b-chat": {
    "description": "Yi 1.5, das neueste Open-Source-Fine-Tuning-Modell mit 9 Milliarden Parametern, unterstützt verschiedene Dialogszenarien mit hochwertigen Trainingsdaten, die auf menschliche Präferenzen abgestimmt sind."
  },
  "360gpt-pro": {
    "description": "360GPT Pro ist ein wichtiger Bestandteil der 360 AI-Modellreihe und erfüllt mit seiner effizienten Textverarbeitungsfähigkeit vielfältige Anwendungen der natürlichen Sprache, unterstützt das Verständnis langer Texte und Mehrfachdialoge."
  },
  "360gpt-turbo": {
    "description": "360GPT Turbo bietet leistungsstarke Berechnungs- und Dialogfähigkeiten, mit hervorragendem semantischen Verständnis und Generierungseffizienz, und ist die ideale intelligente Assistentenlösung für Unternehmen und Entwickler."
  },
  "360gpt-turbo-responsibility-8k": {
    "description": "360GPT Turbo Responsibility 8K betont semantische Sicherheit und verantwortungsbewusste Ausrichtung, speziell für Anwendungen mit hohen Anforderungen an die Inhaltssicherheit konzipiert, um die Genauigkeit und Robustheit der Benutzererfahrung zu gewährleisten."
  },
  "360gpt2-o1": {
    "description": "360gpt2-o1 verwendet Baumsuche zur Konstruktion von Denkketten und führt einen Reflexionsmechanismus ein, der durch verstärkendes Lernen trainiert wird. Das Modell verfügt über die Fähigkeit zur Selbstreflexion und Fehlerkorrektur."
  },
  "360gpt2-pro": {
    "description": "360GPT2 Pro ist ein fortschrittliches Modell zur Verarbeitung natürlicher Sprache, das von der 360 Company entwickelt wurde und über außergewöhnliche Textgenerierungs- und Verständnisfähigkeiten verfügt, insbesondere im Bereich der Generierung und Kreativität, und in der Lage ist, komplexe Sprachumwandlungs- und Rollendarstellungsaufgaben zu bewältigen."
  },
  "360zhinao2-o1": {
    "description": "360zhinao2-o1 verwendet Baumsuche zur Konstruktion von Denkketten und führt einen Reflexionsmechanismus ein, der durch verstärkendes Lernen trainiert wird. Das Modell verfügt über die Fähigkeit zur Selbstreflexion und Fehlerkorrektur."
  },
  "4.0Ultra": {
    "description": "Spark4.0 Ultra ist die leistungsstärkste Version der Spark-Großmodellreihe, die die Online-Suchverbindung aktualisiert und die Fähigkeit zur Textverständnis und -zusammenfassung verbessert. Es ist eine umfassende Lösung zur Steigerung der Büroproduktivität und zur genauen Reaktion auf Anforderungen und ein führendes intelligentes Produkt in der Branche."
  },
  "Baichuan2-Turbo": {
    "description": "Verwendet Suchverbesserungstechnologie, um eine umfassende Verknüpfung zwischen großen Modellen und Fachwissen sowie Wissen aus dem gesamten Internet zu ermöglichen. Unterstützt das Hochladen von Dokumenten wie PDF, Word und die Eingabe von URLs, um Informationen zeitnah und umfassend zu erhalten, mit genauen und professionellen Ergebnissen."
  },
  "Baichuan3-Turbo": {
    "description": "Für häufige Unternehmensszenarien optimiert, mit erheblichen Leistungssteigerungen und einem hohen Preis-Leistungs-Verhältnis. Im Vergleich zum Baichuan2-Modell wurde die Inhaltserstellung um 20 %, die Wissensabfrage um 17 % und die Rollenspiel-Fähigkeit um 40 % verbessert. Die Gesamtleistung übertrifft die von GPT-3.5."
  },
  "Baichuan3-Turbo-128k": {
    "description": "Verfügt über ein 128K Ultra-Langkontextfenster, optimiert für häufige Unternehmensszenarien, mit erheblichen Leistungssteigerungen und einem hohen Preis-Leistungs-Verhältnis. Im Vergleich zum Baichuan2-Modell wurde die Inhaltserstellung um 20 %, die Wissensabfrage um 17 % und die Rollenspiel-Fähigkeit um 40 % verbessert. Die Gesamtleistung übertrifft die von GPT-3.5."
  },
  "Baichuan4": {
    "description": "Das Modell hat die höchste Fähigkeit im Inland und übertrifft ausländische Mainstream-Modelle in Aufgaben wie Wissensdatenbanken, langen Texten und kreativer Generierung. Es verfügt auch über branchenführende multimodale Fähigkeiten und zeigt in mehreren autoritativen Bewertungsbenchmarks hervorragende Leistungen."
  },
  "Baichuan4-Air": {
    "description": "Das Modell hat die höchste Leistungsfähigkeit im Inland und übertrifft ausländische Mainstream-Modelle in Aufgaben wie Wissensdatenbanken, langen Texten und kreativen Generierungen auf Chinesisch. Es verfügt auch über branchenführende multimodale Fähigkeiten und zeigt in mehreren anerkannten Bewertungsbenchmarks hervorragende Leistungen."
  },
  "Baichuan4-Turbo": {
    "description": "Das Modell hat die höchste Leistungsfähigkeit im Inland und übertrifft ausländische Mainstream-Modelle in Aufgaben wie Wissensdatenbanken, langen Texten und kreativen Generierungen auf Chinesisch. Es verfügt auch über branchenführende multimodale Fähigkeiten und zeigt in mehreren anerkannten Bewertungsbenchmarks hervorragende Leistungen."
  },
  "DeepSeek-R1": {
    "description": "Ein hochmodernes, effizientes LLM, das sich auf Schlussfolgerungen, Mathematik und Programmierung spezialisiert hat."
  },
  "DeepSeek-R1-Distill-Llama-70B": {
    "description": "DeepSeek R1 – das größere und intelligentere Modell im DeepSeek-Paket – wurde in die Llama 70B-Architektur destilliert. Basierend auf Benchmark-Tests und menschlicher Bewertung ist dieses Modell intelligenter als das ursprüngliche Llama 70B, insbesondere bei Aufgaben, die mathematische und faktische Genauigkeit erfordern."
  },
  "DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Das DeepSeek-R1-Distill-Modell basiert auf Qwen2.5-Math-1.5B und optimiert die Inferenzleistung durch verstärkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Maßstäbe für Multitasking."
  },
  "DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Das DeepSeek-R1-Distill-Modell basiert auf Qwen2.5-14B und optimiert die Inferenzleistung durch verstärkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Maßstäbe für Multitasking."
  },
  "DeepSeek-R1-Distill-Qwen-32B": {
    "description": "Die DeepSeek-R1-Serie optimiert die Inferenzleistung durch verstärkendes Lernen und Kaltstartdaten, das Open-Source-Modell setzt neue Maßstäbe für Multitasking und übertrifft das Niveau von OpenAI-o1-mini."
  },
  "DeepSeek-R1-Distill-Qwen-7B": {
    "description": "Das DeepSeek-R1-Distill-Modell basiert auf Qwen2.5-Math-7B und optimiert die Inferenzleistung durch verstärkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Maßstäbe für Multitasking."
  },
  "Doubao-1.5-vision-pro-32k": {
    "description": "Doubao-1.5-vision-pro ist das neueste Upgrade des multimodalen Großmodells, das die Erkennung von Bildern mit beliebiger Auflösung und extremen Seitenverhältnissen unterstützt und die Fähigkeiten zur visuellen Schlussfolgerung, Dokumentenerkennung, Detailverständnis und Befehlsbefolgung verbessert."
  },
  "Doubao-lite-128k": {
    "description": "Doubao-lite bietet eine extrem hohe Reaktionsgeschwindigkeit und ein hervorragendes Preis-Leistungs-Verhältnis und bietet den Kunden flexiblere Optionen für verschiedene Szenarien. Es unterstützt Schlussfolgerungen und Feinabstimmungen mit einem 128k-Kontextfenster."
  },
  "Doubao-lite-32k": {
    "description": "Doubao-lite bietet eine extrem hohe Reaktionsgeschwindigkeit und ein hervorragendes Preis-Leistungs-Verhältnis und bietet den Kunden flexiblere Optionen für verschiedene Szenarien. Es unterstützt Schlussfolgerungen und Feinabstimmungen mit einem 32k-Kontextfenster."
  },
  "Doubao-lite-4k": {
    "description": "Doubao-lite bietet eine extrem hohe Reaktionsgeschwindigkeit und ein hervorragendes Preis-Leistungs-Verhältnis und bietet den Kunden flexiblere Optionen für verschiedene Szenarien. Es unterstützt Schlussfolgerungen und Feinabstimmungen mit einem 4k-Kontextfenster."
  },
  "Doubao-pro-128k": {
    "description": "Das leistungsstärkste Hauptmodell, das sich zur Verarbeitung komplexer Aufgaben eignet und in Szenarien wie Referenzfragen, Zusammenfassungen, Kreativität, Textklassifizierung und Rollenspiel sehr gute Ergebnisse erzielt. Es unterstützt Schlussfolgerungen und Feinabstimmungen mit einem 128k-Kontextfenster."
  },
  "Doubao-pro-256k": {
    "description": "Das leistungsstärkste Hauptmodell, das sich gut für komplexe Aufgaben eignet und in Szenarien wie Referenzfragen, Zusammenfassungen, kreatives Schreiben, Textklassifizierung und Rollenspiel hervorragende Ergebnisse erzielt. Es unterstützt Schlussfolgerungen und Feinabstimmungen mit einem Kontextfenster von 256k."
  },
  "Doubao-pro-32k": {
    "description": "Das leistungsstärkste Hauptmodell, das sich zur Verarbeitung komplexer Aufgaben eignet und in Szenarien wie Referenzfragen, Zusammenfassungen, Kreativität, Textklassifizierung und Rollenspiel sehr gute Ergebnisse erzielt. Es unterstützt Schlussfolgerungen und Feinabstimmungen mit einem 32k-Kontextfenster."
  },
  "Doubao-pro-4k": {
    "description": "Das leistungsstärkste Hauptmodell, das sich zur Verarbeitung komplexer Aufgaben eignet und in Szenarien wie Referenzfragen, Zusammenfassungen, Kreativität, Textklassifizierung und Rollenspiel sehr gute Ergebnisse erzielt. Es unterstützt Schlussfolgerungen und Feinabstimmungen mit einem 4k-Kontextfenster."
  },
  "Doubao-vision-lite-32k": {
    "description": "Das Doubao-vision-Modell ist ein multimodales Großmodell, das von Doubao eingeführt wurde und über starke Fähigkeiten zur Bildverständnis und Schlussfolgerung sowie präzise Befehlsverständnisfähigkeiten verfügt. Das Modell zeigt starke Leistungen bei der Extraktion von Bildtextinformationen und bildbasierten Schlussfolgerungsaufgaben und kann in komplexeren und breiteren visuellen Frage-Antwort-Aufgaben eingesetzt werden."
  },
  "Doubao-vision-pro-32k": {
    "description": "Das Doubao-vision-Modell ist ein multimodales Großmodell, das von Doubao eingeführt wurde und über starke Fähigkeiten zur Bildverständnis und Schlussfolgerung sowie präzise Befehlsverständnisfähigkeiten verfügt. Das Modell zeigt starke Leistungen bei der Extraktion von Bildtextinformationen und bildbasierten Schlussfolgerungsaufgaben und kann in komplexeren und breiteren visuellen Frage-Antwort-Aufgaben eingesetzt werden."
  },
  "ERNIE-3.5-128K": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für großangelegte Sprachverarbeitung, das eine riesige Menge an chinesischen und englischen Texten abdeckt. Es verfügt über starke allgemeine Fähigkeiten und kann die meisten Anforderungen an Dialogfragen, kreative Generierung und Anwendungsfälle von Plugins erfüllen. Es unterstützt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ERNIE-3.5-8K": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für großangelegte Sprachverarbeitung, das eine riesige Menge an chinesischen und englischen Texten abdeckt. Es verfügt über starke allgemeine Fähigkeiten und kann die meisten Anforderungen an Dialogfragen, kreative Generierung und Anwendungsfälle von Plugins erfüllen. Es unterstützt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ERNIE-3.5-8K-Preview": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für großangelegte Sprachverarbeitung, das eine riesige Menge an chinesischen und englischen Texten abdeckt. Es verfügt über starke allgemeine Fähigkeiten und kann die meisten Anforderungen an Dialogfragen, kreative Generierung und Anwendungsfälle von Plugins erfüllen. Es unterstützt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ERNIE-4.0-8K-Latest": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für ultra-große Sprachverarbeitung, das im Vergleich zu ERNIE 3.5 eine umfassende Verbesserung der Modellfähigkeiten erreicht hat und sich breit für komplexe Aufgaben in verschiedenen Bereichen eignet; unterstützt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ERNIE-4.0-8K-Preview": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für ultra-große Sprachverarbeitung, das im Vergleich zu ERNIE 3.5 eine umfassende Verbesserung der Modellfähigkeiten erreicht hat und sich breit für komplexe Aufgaben in verschiedenen Bereichen eignet; unterstützt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ERNIE-4.0-Turbo-8K-Latest": {
    "description": "Baidus selbstentwickeltes Flaggschiff-Modell für großflächige Sprachverarbeitung, das in vielen komplexen Aufgaben hervorragende Ergebnisse zeigt und umfassend in verschiedenen Bereichen eingesetzt werden kann; unterstützt die automatische Anbindung an Baidu-Suchplugins, um die Aktualität von Antwortinformationen zu gewährleisten. Im Vergleich zu ERNIE 4.0 hat es eine bessere Leistung."
  },
  "ERNIE-4.0-Turbo-8K-Preview": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für ultra-große Sprachverarbeitung, das in der Gesamtleistung herausragend ist und sich breit für komplexe Aufgaben in verschiedenen Bereichen eignet; unterstützt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualität der Antwortinformationen zu gewährleisten. Im Vergleich zu ERNIE 4.0 bietet es eine bessere Leistungsfähigkeit."
  },
  "ERNIE-Character-8K": {
    "description": "Das von Baidu entwickelte Sprachmodell für vertikale Szenarien, das sich für Anwendungen wie Spiel-NPCs, Kundenservice-Dialoge und Rollenspiele eignet. Es hat einen klareren und konsistenteren Charakterstil, eine stärkere Befolgung von Anweisungen und eine bessere Inferenzleistung."
  },
  "ERNIE-Lite-Pro-128K": {
    "description": "Das von Baidu entwickelte leichte Sprachmodell, das hervorragende Modellleistung und Inferenzleistung kombiniert. Es bietet bessere Ergebnisse als ERNIE Lite und eignet sich für die Inferenznutzung auf AI-Beschleunigungskarten mit geringer Rechenleistung."
  },
  "ERNIE-Speed-128K": {
    "description": "Das neueste von Baidu im Jahr 2024 veröffentlichte hochleistungsfähige Sprachmodell, das überragende allgemeine Fähigkeiten bietet und sich als Basis-Modell für Feinabstimmungen eignet, um spezifische Szenarien besser zu bearbeiten, und bietet gleichzeitig hervorragende Inferenzleistung."
  },
  "ERNIE-Speed-Pro-128K": {
    "description": "Das neueste von Baidu im Jahr 2024 veröffentlichte hochleistungsfähige Sprachmodell, das überragende allgemeine Fähigkeiten bietet und bessere Ergebnisse als ERNIE Speed erzielt. Es eignet sich als Basis-Modell für Feinabstimmungen, um spezifische Szenarien besser zu bearbeiten, und bietet gleichzeitig hervorragende Inferenzleistung."
  },
  "Gryphe/MythoMax-L2-13b": {
    "description": "MythoMax-L2 (13B) ist ein innovatives Modell, das sich für Anwendungen in mehreren Bereichen und komplexe Aufgaben eignet."
  },
  "InternVL2-8B": {
    "description": "InternVL2-8B ist ein leistungsstarkes visuelles Sprachmodell, das multimodale Verarbeitung von Bildern und Text unterstützt und in der Lage ist, Bildinhalte präzise zu erkennen und relevante Beschreibungen oder Antworten zu generieren."
  },
  "InternVL2.5-26B": {
    "description": "InternVL2.5-26B ist ein leistungsstarkes visuelles Sprachmodell, das multimodale Verarbeitung von Bildern und Text unterstützt und in der Lage ist, Bildinhalte präzise zu erkennen und relevante Beschreibungen oder Antworten zu generieren."
  },
  "Llama-3.2-11B-Vision-Instruct": {
    "description": "Hervorragende Bildschlussfolgerungsfähigkeiten auf hochauflösenden Bildern, geeignet für Anwendungen im Bereich der visuellen Verständigung."
  },
  "Llama-3.2-90B-Vision-Instruct\t": {
    "description": "Fortgeschrittene Bildschlussfolgerungsfähigkeiten für Anwendungen im Bereich der visuellen Verständigung."
  },
  "LoRA/Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct ist eines der neuesten großen Sprachmodelle, die von Alibaba Cloud veröffentlicht wurden. Dieses 72B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterstützung und deckt über 29 Sprachen ab, einschließlich Chinesisch und Englisch. Es zeigt signifikante Verbesserungen in der Befolgung von Anweisungen, im Verständnis strukturierter Daten und in der Generierung strukturierter Ausgaben (insbesondere JSON)."
  },
  "LoRA/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct ist eines der neuesten großen Sprachmodelle, die von Alibaba Cloud veröffentlicht wurden. Dieses 7B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterstützung und deckt über 29 Sprachen ab, einschließlich Chinesisch und Englisch. Es zeigt signifikante Verbesserungen in der Befolgung von Anweisungen, im Verständnis strukturierter Daten und in der Generierung strukturierter Ausgaben (insbesondere JSON)."
  },
  "Meta-Llama-3.1-405B-Instruct": {
    "description": "Das auf Anweisungen optimierte Textmodell Llama 3.1 wurde für mehrsprachige Dialoganwendungen optimiert und zeigt in vielen verfügbaren Open-Source- und geschlossenen Chat-Modellen in gängigen Branchenbenchmarks hervorragende Leistungen."
  },
  "Meta-Llama-3.1-70B-Instruct": {
    "description": "Das auf Anweisungen optimierte Textmodell Llama 3.1 wurde für mehrsprachige Dialoganwendungen optimiert und zeigt in vielen verfügbaren Open-Source- und geschlossenen Chat-Modellen in gängigen Branchenbenchmarks hervorragende Leistungen."
  },
  "Meta-Llama-3.1-8B-Instruct": {
    "description": "Das auf Anweisungen optimierte Textmodell Llama 3.1 wurde für mehrsprachige Dialoganwendungen optimiert und zeigt in vielen verfügbaren Open-Source- und geschlossenen Chat-Modellen in gängigen Branchenbenchmarks hervorragende Leistungen."
  },
  "Meta-Llama-3.2-1B-Instruct": {
    "description": "Ein fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverständnis, hervorragenden Schlussfolgerungsfähigkeiten und Textgenerierungsfähigkeiten."
  },
  "Meta-Llama-3.2-3B-Instruct": {
    "description": "Ein fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverständnis, hervorragenden Schlussfolgerungsfähigkeiten und Textgenerierungsfähigkeiten."
  },
  "Meta-Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das eine Leistung bietet, die mit einem 405B-Modell vergleichbar ist, und das zu extrem niedrigen Kosten. Es basiert auf der Transformer-Architektur und wurde durch überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF) in Bezug auf Nützlichkeit und Sicherheit verbessert. Die auf Anweisungen optimierte Version ist speziell für mehrsprachige Dialoge optimiert und übertrifft in mehreren Branchenbenchmarks viele verfügbare Open-Source- und geschlossene Chat-Modelle. Das Wissensdatum endet im Dezember 2023."
  },
  "MiniMax-Text-01": {
    "description": "In der MiniMax-01-Serie haben wir mutige Innovationen vorgenommen: Erstmals wurde die lineare Aufmerksamkeitsmechanismus in großem Maßstab implementiert, sodass die traditionelle Transformer-Architektur nicht mehr die einzige Wahl ist. Dieses Modell hat eine Parameteranzahl von bis zu 456 Milliarden, wobei eine Aktivierung 45,9 Milliarden beträgt. Die Gesamtleistung des Modells kann mit den besten Modellen im Ausland mithalten und kann gleichzeitig effizient den weltweit längsten Kontext von 4 Millionen Tokens verarbeiten, was 32-mal so viel wie GPT-4o und 20-mal so viel wie Claude-3.5-Sonnet ist."
  },
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) ist ein hochpräzises Anweisungsmodell, das für komplexe Berechnungen geeignet ist."
  },
  "OpenGVLab/InternVL2-26B": {
    "description": "InternVL2 zeigt herausragende Leistungen in verschiedenen visuellen Sprachaufgaben, einschließlich Dokumenten- und Diagrammverständnis, Szenentexterkennung, OCR, wissenschaftlicher und mathematischer Problemlösung."
  },
  "Phi-3-medium-128k-instruct": {
    "description": "Das gleiche Phi-3-medium-Modell, jedoch mit einer größeren Kontextgröße für RAG oder Few-Shot-Prompting."
  },
  "Phi-3-medium-4k-instruct": {
    "description": "Ein Modell mit 14 Milliarden Parametern, das eine bessere Qualität als Phi-3-mini bietet und sich auf qualitativ hochwertige, reasoning-dense Daten konzentriert."
  },
  "Phi-3-mini-128k-instruct": {
    "description": "Das gleiche Phi-3-mini-Modell, jedoch mit einer größeren Kontextgröße für RAG oder Few-Shot-Prompting."
  },
  "Phi-3-mini-4k-instruct": {
    "description": "Das kleinste Mitglied der Phi-3-Familie. Optimiert für Qualität und geringe Latenz."
  },
  "Phi-3-small-128k-instruct": {
    "description": "Das gleiche Phi-3-small-Modell, jedoch mit einer größeren Kontextgröße für RAG oder Few-Shot-Prompting."
  },
  "Phi-3-small-8k-instruct": {
    "description": "Ein Modell mit 7 Milliarden Parametern, das eine bessere Qualität als Phi-3-mini bietet und sich auf qualitativ hochwertige, reasoning-dense Daten konzentriert."
  },
  "Phi-3.5-mini-instruct": {
    "description": "Aktualisierte Version des Phi-3-mini-Modells."
  },
  "Phi-3.5-vision-instrust": {
    "description": "Aktualisierte Version des Phi-3-vision-Modells."
  },
  "Pro/OpenGVLab/InternVL2-8B": {
    "description": "InternVL2 zeigt herausragende Leistungen in verschiedenen visuellen Sprachaufgaben, einschließlich Dokumenten- und Diagrammverständnis, Szenentexterkennung, OCR, wissenschaftlicher und mathematischer Problemlösung."
  },
  "Pro/Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct ist das anweisungsfeinabgestimmte große Sprachmodell der Qwen2-Serie mit einer Parametergröße von 1,5B. Dieses Modell basiert auf der Transformer-Architektur und verwendet Technologien wie die SwiGLU-Aktivierungsfunktion, QKV-Offsets und gruppierte Abfrageaufmerksamkeit. Es zeigt hervorragende Leistungen in der Sprachverständnis, -generierung, Mehrsprachigkeit, Codierung, Mathematik und Inferenz in mehreren Benchmark-Tests und übertrifft die meisten Open-Source-Modelle. Im Vergleich zu Qwen1.5-1.8B-Chat zeigt Qwen2-1.5B-Instruct in Tests wie MMLU, HumanEval, GSM8K, C-Eval und IFEval signifikante Leistungsverbesserungen, obwohl die Parameteranzahl etwas geringer ist."
  },
  "Pro/Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-7B-Instruct ist das anweisungsfeinabgestimmte große Sprachmodell der Qwen2-Serie mit einer Parametergröße von 7B. Dieses Modell basiert auf der Transformer-Architektur und verwendet Technologien wie die SwiGLU-Aktivierungsfunktion, QKV-Offsets und gruppierte Abfrageaufmerksamkeit. Es kann große Eingaben verarbeiten. Das Modell zeigt hervorragende Leistungen in der Sprachverständnis, -generierung, Mehrsprachigkeit, Codierung, Mathematik und Inferenz in mehreren Benchmark-Tests und übertrifft die meisten Open-Source-Modelle und zeigt in bestimmten Aufgaben eine vergleichbare Wettbewerbsfähigkeit mit proprietären Modellen. Qwen2-7B-Instruct übertrifft Qwen1.5-7B-Chat in mehreren Bewertungen und zeigt signifikante Leistungsverbesserungen."
  },
  "Pro/Qwen/Qwen2-VL-7B-Instruct": {
    "description": "Qwen2-VL ist die neueste Iteration des Qwen-VL-Modells, das in visuellen Verständnis-Benchmarks erstklassige Leistungen erzielt."
  },
  "Pro/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct ist eines der neuesten großen Sprachmodelle, die von Alibaba Cloud veröffentlicht wurden. Dieses 7B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterstützung und deckt über 29 Sprachen ab, einschließlich Chinesisch und Englisch. Es zeigt signifikante Verbesserungen in der Befolgung von Anweisungen, im Verständnis strukturierter Daten und in der Generierung strukturierter Ausgaben (insbesondere JSON)."
  },
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct ist die neueste Version der von Alibaba Cloud veröffentlichten Reihe von code-spezifischen großen Sprachmodellen. Dieses Modell basiert auf Qwen2.5 und wurde mit 55 Billionen Tokens trainiert, um die Fähigkeiten zur Codegenerierung, Inferenz und Fehlerbehebung erheblich zu verbessern. Es verbessert nicht nur die Codierungsfähigkeiten, sondern bewahrt auch die Vorteile in Mathematik und allgemeinen Fähigkeiten. Das Modell bietet eine umfassendere Grundlage für praktische Anwendungen wie Code-Agenten."
  },
  "Pro/THUDM/glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat ist die Open-Source-Version des GLM-4-Modells, das von Zhizhu AI eingeführt wurde. Dieses Modell zeigt hervorragende Leistungen in den Bereichen Semantik, Mathematik, Inferenz, Code und Wissen. Neben der Unterstützung für mehrstufige Dialoge bietet GLM-4-9B-Chat auch fortgeschrittene Funktionen wie Web-Browsing, Code-Ausführung, benutzerdefinierte Tool-Aufrufe (Function Call) und langes Textverständnis. Das Modell unterstützt 26 Sprachen, darunter Chinesisch, Englisch, Japanisch, Koreanisch und Deutsch. In mehreren Benchmark-Tests zeigt GLM-4-9B-Chat hervorragende Leistungen, wie AlignBench-v2, MT-Bench, MMLU und C-Eval. Das Modell unterstützt eine maximale Kontextlänge von 128K und ist für akademische Forschung und kommerzielle Anwendungen geeignet."
  },
  "Pro/deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 ist ein durch verstärkendes Lernen (RL) gesteuertes Inferenzmodell, das Probleme mit Wiederholungen und Lesbarkeit im Modell löst. Vor dem RL führte DeepSeek-R1 Kaltstartdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in mathematischen, programmierbezogenen und Inferenzaufgaben eine vergleichbare Leistung zu OpenAI-o1 und verbessert die Gesamtleistung durch sorgfältig gestaltete Trainingsmethoden."
  },
  "Pro/deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 ist ein hybrides Experten (MoE) Sprachmodell mit 6710 Milliarden Parametern, das eine Multi-Head-Latente-Attention (MLA) und DeepSeekMoE-Architektur verwendet, kombiniert mit einer Lastenausgleichsstrategie ohne Hilfskosten, um die Inferenz- und Trainingseffizienz zu optimieren. Durch das Pre-Training auf 14,8 Billionen hochwertigen Tokens und anschließende überwachte Feinabstimmung und verstärktes Lernen übertrifft DeepSeek-V3 in der Leistung andere Open-Source-Modelle und nähert sich führenden geschlossenen Modellen."
  },
  "Pro/google/gemma-2-9b-it": {
    "description": "Gemma ist eines der leichtgewichtigen, hochmodernen offenen Modellserien, die von Google entwickelt wurden. Es handelt sich um ein großes Sprachmodell mit nur Decoder, das Englisch unterstützt und offene Gewichte, vortrainierte Varianten und anweisungsfeinabgestimmte Varianten bietet. Das Gemma-Modell eignet sich für verschiedene Textgenerierungsaufgaben, einschließlich Fragen und Antworten, Zusammenfassungen und Inferenz. Dieses 9B-Modell wurde mit 80 Billionen Tokens trainiert. Seine relativ kleine Größe ermöglicht es, in ressourcenbeschränkten Umgebungen wie Laptops, Desktop-Computern oder Ihrer eigenen Cloud-Infrastruktur bereitgestellt zu werden, wodurch mehr Menschen Zugang zu modernsten KI-Modellen erhalten und Innovationen gefördert werden."
  },
  "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "description": "Meta Llama 3.1 ist eine Familie von mehrsprachigen großen Sprachmodellen, die von Meta entwickelt wurden und vortrainierte sowie anweisungsfeinabgestimmte Varianten mit 8B, 70B und 405B Parametern umfasst. Dieses 8B-Anweisungsfeinabgestimmte Modell wurde für mehrsprachige Dialogszenarien optimiert und zeigt in mehreren Branchen-Benchmark-Tests hervorragende Leistungen. Das Modelltraining verwendete über 150 Billionen Tokens aus öffentlichen Daten und nutzte Techniken wie überwachte Feinabstimmung und verstärkendes Lernen mit menschlichem Feedback, um die Nützlichkeit und Sicherheit des Modells zu verbessern. Llama 3.1 unterstützt Text- und Codegenerierung, mit einem Wissensstichtag von Dezember 2023."
  },
  "QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview ist ein innovatives Modell für die Verarbeitung natürlicher Sprache, das komplexe Aufgaben der Dialoggenerierung und des Kontextverständnisses effizient bewältigen kann."
  },
  "Qwen/QVQ-72B-Preview": {
    "description": "QVQ-72B-Preview ist ein forschungsorientiertes Modell, das vom Qwen-Team entwickelt wurde und sich auf visuelle Inferenzfähigkeiten konzentriert. Es hat einzigartige Vorteile beim Verständnis komplexer Szenen und der Lösung visuell verwandter mathematischer Probleme."
  },
  "Qwen/QwQ-32B": {
    "description": "QwQ ist das Inferenzmodell der Qwen-Serie. Im Vergleich zu traditionellen, anweisungsoptimierten Modellen verfügt QwQ über Denk- und Schlussfolgerungsfähigkeiten, die eine signifikante Leistungssteigerung bei nachgelagerten Aufgaben ermöglichen, insbesondere bei der Lösung schwieriger Probleme. QwQ-32B ist ein mittelgroßes Inferenzmodell, das im Vergleich zu den fortschrittlichsten Inferenzmodellen (wie DeepSeek-R1, o1-mini) wettbewerbsfähige Leistungen erzielt. Dieses Modell verwendet Technologien wie RoPE, SwiGLU, RMSNorm und Attention QKV Bias und hat eine Netzwerkstruktur mit 64 Schichten und 40 Q-Attention-Köpfen (im GQA-Architektur sind es 8 KV)."
  },
  "Qwen/QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview ist das neueste experimentelle Forschungsmodell von Qwen, das sich auf die Verbesserung der KI-Inferenzfähigkeiten konzentriert. Durch die Erforschung komplexer Mechanismen wie Sprachmischung und rekursive Inferenz bietet es Hauptvorteile wie starke Analysefähigkeiten, mathematische und Programmierfähigkeiten. Gleichzeitig gibt es Herausforderungen wie Sprachwechsel, Inferenzzyklen, Sicherheitsüberlegungen und Unterschiede in anderen Fähigkeiten."
  },
  "Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct ist das anweisungsfeinabgestimmte große Sprachmodell der Qwen2-Serie mit einer Parametergröße von 1,5B. Dieses Modell basiert auf der Transformer-Architektur und verwendet Technologien wie die SwiGLU-Aktivierungsfunktion, QKV-Offsets und gruppierte Abfrageaufmerksamkeit. Es zeigt hervorragende Leistungen in der Sprachverständnis, -generierung, Mehrsprachigkeit, Codierung, Mathematik und Inferenz in mehreren Benchmark-Tests und übertrifft die meisten Open-Source-Modelle. Im Vergleich zu Qwen1.5-1.8B-Chat zeigt Qwen2-1.5B-Instruct in Tests wie MMLU, HumanEval, GSM8K, C-Eval und IFEval signifikante Leistungsverbesserungen, obwohl die Parameteranzahl etwas geringer ist."
  },
  "Qwen/Qwen2-72B-Instruct": {
    "description": "Qwen2 ist ein fortschrittliches allgemeines Sprachmodell, das eine Vielzahl von Anweisungsarten unterstützt."
  },
  "Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-72B-Instruct ist das anweisungsfeinabgestimmte große Sprachmodell der Qwen2-Serie mit einer Parametergröße von 72B. Dieses Modell basiert auf der Transformer-Architektur und verwendet Technologien wie die SwiGLU-Aktivierungsfunktion, QKV-Offsets und gruppierte Abfrageaufmerksamkeit. Es kann große Eingaben verarbeiten. Das Modell zeigt hervorragende Leistungen in der Sprachverständnis, -generierung, Mehrsprachigkeit, Codierung, Mathematik und Inferenz in mehreren Benchmark-Tests und übertrifft die meisten Open-Source-Modelle und zeigt in bestimmten Aufgaben eine vergleichbare Wettbewerbsfähigkeit mit proprietären Modellen."
  },
  "Qwen/Qwen2-VL-72B-Instruct": {
    "description": "Qwen2-VL ist die neueste Iteration des Qwen-VL-Modells, das in visuellen Verständnis-Benchmarks erstklassige Leistungen erzielt."
  },
  "Qwen/Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5 ist eine brandneue Serie von großen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5 ist eine brandneue Serie von großen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-72B-Instruct": {
    "description": "Ein großes Sprachmodell, das vom Alibaba Cloud Tongyi Qianwen-Team entwickelt wurde."
  },
  "Qwen/Qwen2.5-72B-Instruct-128K": {
    "description": "Qwen2.5 ist eine neue Serie großer Sprachmodelle mit stärkeren Verständnis- und Generierungsfähigkeiten."
  },
  "Qwen/Qwen2.5-72B-Instruct-Turbo": {
    "description": "Qwen2.5 ist eine neue Serie großer Sprachmodelle, die darauf abzielt, die Verarbeitung von instructiven Aufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5 ist eine brandneue Serie von großen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-7B-Instruct-Turbo": {
    "description": "Qwen2.5 ist eine neue Serie großer Sprachmodelle, die darauf abzielt, die Verarbeitung von instructiven Aufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder konzentriert sich auf das Programmieren."
  },
  "Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct ist die neueste Version der von Alibaba Cloud veröffentlichten Reihe von code-spezifischen großen Sprachmodellen. Dieses Modell basiert auf Qwen2.5 und wurde mit 55 Billionen Tokens trainiert, um die Fähigkeiten zur Codegenerierung, Inferenz und Fehlerbehebung erheblich zu verbessern. Es verbessert nicht nur die Codierungsfähigkeiten, sondern bewahrt auch die Vorteile in Mathematik und allgemeinen Fähigkeiten. Das Modell bietet eine umfassendere Grundlage für praktische Anwendungen wie Code-Agenten."
  },
  "Qwen2-72B-Instruct": {
    "description": "Qwen2 ist die neueste Reihe des Qwen-Modells, das 128k Kontext unterstützt. Im Vergleich zu den derzeit besten Open-Source-Modellen übertrifft Qwen2-72B in den Bereichen natürliche Sprachverständnis, Wissen, Code, Mathematik und Mehrsprachigkeit deutlich die führenden Modelle."
  },
  "Qwen2-7B-Instruct": {
    "description": "Qwen2 ist die neueste Reihe des Qwen-Modells, das in der Lage ist, die besten Open-Source-Modelle ähnlicher Größe oder sogar größerer Modelle zu übertreffen. Qwen2 7B hat in mehreren Bewertungen signifikante Vorteile erzielt, insbesondere im Bereich Code und Verständnis der chinesischen Sprache."
  },
  "Qwen2-VL-72B": {
    "description": "Qwen2-VL-72B ist ein leistungsstarkes visuelles Sprachmodell, das multimodale Verarbeitung von Bildern und Text unterstützt und in der Lage ist, Bildinhalte präzise zu erkennen und relevante Beschreibungen oder Antworten zu generieren."
  },
  "Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5-14B-Instruct ist ein großes Sprachmodell mit 14 Milliarden Parametern, das hervorragende Leistungen bietet, für chinesische und mehrsprachige Szenarien optimiert ist und Anwendungen wie intelligente Fragen und Antworten sowie Inhaltserstellung unterstützt."
  },
  "Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5-32B-Instruct ist ein großes Sprachmodell mit 32 Milliarden Parametern, das eine ausgewogene Leistung bietet, für chinesische und mehrsprachige Szenarien optimiert ist und Anwendungen wie intelligente Fragen und Antworten sowie Inhaltserstellung unterstützt."
  },
  "Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct unterstützt 16k Kontext und generiert lange Texte über 8K. Es unterstützt Funktionsaufrufe und nahtlose Interaktionen mit externen Systemen, was die Flexibilität und Skalierbarkeit erheblich verbessert. Das Wissen des Modells hat deutlich zugenommen, und die Codierungs- und mathematischen Fähigkeiten wurden erheblich verbessert, mit Unterstützung für über 29 Sprachen."
  },
  "Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct ist ein großes Sprachmodell mit 7 Milliarden Parametern, das Funktionsaufrufe unterstützt und nahtlos mit externen Systemen interagiert, was die Flexibilität und Skalierbarkeit erheblich erhöht. Es ist für chinesische und mehrsprachige Szenarien optimiert und unterstützt Anwendungen wie intelligente Fragen und Antworten sowie Inhaltserstellung."
  },
  "Qwen2.5-Coder-14B-Instruct": {
    "description": "Qwen2.5-Coder-14B-Instruct ist ein auf großflächigem Pre-Training basierendes Programmiermodell, das über starke Fähigkeiten zur Codeverstehung und -generierung verfügt und effizient verschiedene Programmieraufgaben bearbeiten kann. Es eignet sich besonders gut für intelligente Codeerstellung, automatisierte Skripterstellung und die Beantwortung von Programmierfragen."
  },
  "Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder-32B-Instruct ist ein großes Sprachmodell, das speziell für die Codegenerierung, das Verständnis von Code und effiziente Entwicklungsszenarien entwickelt wurde. Es verwendet eine branchenführende Parametergröße von 32B und kann vielfältige Programmieranforderungen erfüllen."
  },
  "SenseChat": {
    "description": "Basisversion des Modells (V4) mit 4K Kontextlänge, die über starke allgemeine Fähigkeiten verfügt."
  },
  "SenseChat-128K": {
    "description": "Basisversion des Modells (V4) mit 128K Kontextlänge, das in Aufgaben des Verständnisses und der Generierung langer Texte hervorragende Leistungen zeigt."
  },
  "SenseChat-32K": {
    "description": "Basisversion des Modells (V4) mit 32K Kontextlänge, flexibel einsetzbar in verschiedenen Szenarien."
  },
  "SenseChat-5": {
    "description": "Die neueste Modellversion (V5.5) mit 128K Kontextlänge hat signifikante Verbesserungen in den Bereichen mathematische Schlussfolgerungen, englische Konversation, Befolgen von Anweisungen und Verständnis langer Texte, vergleichbar mit GPT-4o."
  },
  "SenseChat-5-1202": {
    "description": "Dies ist die neueste Version basierend auf V5.5, die im Vergleich zur vorherigen Version signifikante Verbesserungen in den grundlegenden Fähigkeiten in Chinesisch und Englisch, im Chat, in Naturwissenschaften, in Geisteswissenschaften, im Schreiben, in mathematischer Logik und in der Wortanzahlkontrolle aufweist."
  },
  "SenseChat-5-Cantonese": {
    "description": "Mit 32K Kontextlänge übertrifft es GPT-4 im Verständnis von Konversationen auf Kantonesisch und kann in mehreren Bereichen wie Wissen, Schlussfolgerungen, Mathematik und Programmierung mit GPT-4 Turbo konkurrieren."
  },
  "SenseChat-Character": {
    "description": "Standardmodell mit 8K Kontextlänge und hoher Reaktionsgeschwindigkeit."
  },
  "SenseChat-Character-Pro": {
    "description": "Premium-Modell mit 32K Kontextlänge, das umfassende Verbesserungen in den Fähigkeiten bietet und sowohl chinesische als auch englische Konversationen unterstützt."
  },
  "SenseChat-Turbo": {
    "description": "Geeignet für schnelle Fragen und Antworten sowie Szenarien zur Feinabstimmung des Modells."
  },
  "SenseChat-Turbo-1202": {
    "description": "Dies ist das neueste leichte Modell, das über 90 % der Fähigkeiten des Vollmodells erreicht und die Kosten für die Inferenz erheblich senkt."
  },
  "SenseChat-Vision": {
    "description": "Das neueste Modell (V5.5) unterstützt die Eingabe mehrerer Bilder und optimiert umfassend die grundlegenden Fähigkeiten des Modells. Es hat signifikante Verbesserungen in der Erkennung von Objektattributen, räumlichen Beziehungen, Aktionsereignissen, Szenenverständnis, Emotionserkennung, logischem Wissen und Textverständnis und -generierung erreicht."
  },
  "Skylark2-lite-8k": {
    "description": "Das zweite Modell der Skylark-Reihe, das Skylark2-lite-Modell bietet eine hohe Reaktionsgeschwindigkeit und eignet sich für Szenarien mit hohen Echtzeitanforderungen, kostensensitiven Anforderungen und geringeren Genauigkeitsanforderungen, mit einer Kontextfensterlänge von 8k."
  },
  "Skylark2-pro-32k": {
    "description": "Das zweite Modell der Skylark-Reihe, die Skylark2-pro-Version hat eine hohe Modellgenauigkeit und eignet sich für komplexere Textgenerierungsszenarien, wie z. B. professionelle Texterstellung, Romankreation und hochwertige Übersetzungen, mit einer Kontextfensterlänge von 32k."
  },
  "Skylark2-pro-4k": {
    "description": "Das zweite Modell der Skylark-Reihe, die Skylark2-pro-Version hat eine hohe Modellgenauigkeit und eignet sich für komplexere Textgenerierungsszenarien, wie z. B. professionelle Texterstellung, Romankreation und hochwertige Übersetzungen, mit einer Kontextfensterlänge von 4k."
  },
  "Skylark2-pro-character-4k": {
    "description": "Das zweite Modell der Skylark-Reihe, das Skylark2-pro-character-Modell hat hervorragende Fähigkeiten im Rollenspiel und Chat, kann sich entsprechend den Anforderungen des Benutzers verkleiden und bietet natürliche und flüssige Dialoginhalte. Es eignet sich für den Aufbau von Chatbots, virtuellen Assistenten und Online-Kundensupport und bietet eine hohe Reaktionsgeschwindigkeit."
  },
  "Skylark2-pro-turbo-8k": {
    "description": "Das zweite Modell der Skylark-Reihe, das Skylark2-pro-turbo-8k bietet schnellere Schlussfolgerungen und niedrigere Kosten, mit einer Kontextfensterlänge von 8k."
  },
  "THUDM/chatglm3-6b": {
    "description": "ChatGLM3-6B ist das Open-Source-Modell der ChatGLM-Serie, das von Zhizhu AI entwickelt wurde. Dieses Modell bewahrt die hervorragenden Eigenschaften der Vorgängermodelle, wie flüssige Dialoge und niedrige Bereitstellungskosten, während es neue Funktionen einführt. Es verwendet vielfältigere Trainingsdaten, eine größere Anzahl an Trainingsschritten und eine sinnvollere Trainingsstrategie und zeigt hervorragende Leistungen unter den vortrainierten Modellen mit weniger als 10B. ChatGLM3-6B unterstützt mehrstufige Dialoge, Tool-Aufrufe, Code-Ausführung und Agentenaufgaben in komplexen Szenarien. Neben dem Dialogmodell wurden auch das Basis-Modell ChatGLM-6B-Base und das lange Textdialogmodell ChatGLM3-6B-32K als Open Source veröffentlicht. Dieses Modell ist vollständig für akademische Forschung geöffnet und erlaubt auch kostenlose kommerzielle Nutzung nach Registrierung."
  },
  "THUDM/glm-4-9b-chat": {
    "description": "GLM-4 9B ist die Open-Source-Version, die ein optimiertes Dialogerlebnis für Konversationsanwendungen bietet."
  },
  "TeleAI/TeleChat2": {
    "description": "Das TeleChat2-Modell ist ein generatives semantisches Großmodell, das von China Telecom von Grund auf neu entwickelt wurde und Funktionen wie Enzyklopädiefragen, Codegenerierung und lange Textgenerierung unterstützt. Es bietet Benutzern Beratungsdienste, ermöglicht Dialoginteraktionen mit Benutzern, beantwortet Fragen, unterstützt bei der Erstellung und hilft Benutzern effizient und bequem, Informationen, Wissen und Inspiration zu erhalten. Das Modell zeigt hervorragende Leistungen in den Bereichen Halluzinationsprobleme, lange Textgenerierung und logisches Verständnis."
  },
  "TeleAI/TeleMM": {
    "description": "Das TeleMM-Modell ist ein multimodales Großmodell, das von China Telecom entwickelt wurde und in der Lage ist, Texte, Bilder und andere Modalitäten zu verarbeiten. Es unterstützt Funktionen wie Bildverständnis und Diagrammanalyse und bietet Benutzern multimodale Verständnisdienste. Das Modell kann mit Benutzern multimodal interagieren, den Eingabeinhalt genau verstehen, Fragen beantworten, bei der Erstellung helfen und effizient multimodale Informationen und Inspirationsunterstützung bereitstellen. Es zeigt hervorragende Leistungen in multimodalen Aufgaben wie feinkörniger Wahrnehmung und logischem Schlussfolgern."
  },
  "Vendor-A/Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct ist eines der neuesten großen Sprachmodelle, die von Alibaba Cloud veröffentlicht wurden. Dieses 72B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterstützung und deckt über 29 Sprachen ab, einschließlich Chinesisch und Englisch. Es zeigt signifikante Verbesserungen in der Befolgung von Anweisungen, im Verständnis strukturierter Daten und in der Generierung strukturierter Ausgaben (insbesondere JSON)."
  },
  "Yi-34B-Chat": {
    "description": "Yi-1.5-34B hat die hervorragenden allgemeinen Sprachfähigkeiten des ursprünglichen Modells beibehalten und durch inkrementelles Training von 500 Milliarden hochwertigen Tokens die mathematische Logik und Codierungsfähigkeiten erheblich verbessert."
  },
  "abab5.5-chat": {
    "description": "Für produktivitätsorientierte Szenarien konzipiert, unterstützt es die Verarbeitung komplexer Aufgaben und die effiziente Textgenerierung, geeignet für professionelle Anwendungen."
  },
  "abab5.5s-chat": {
    "description": "Speziell für chinesische Charakterdialoge konzipiert, bietet es hochwertige chinesische Dialoggenerierung und ist für verschiedene Anwendungsszenarien geeignet."
  },
  "abab6.5g-chat": {
    "description": "Speziell für mehrsprachige Charakterdialoge konzipiert, unterstützt die hochwertige Dialoggenerierung in Englisch und anderen Sprachen."
  },
  "abab6.5s-chat": {
    "description": "Geeignet für eine Vielzahl von Aufgaben der natürlichen Sprachverarbeitung, einschließlich Textgenerierung und Dialogsystemen."
  },
  "abab6.5t-chat": {
    "description": "Für chinesische Charakterdialoge optimiert, bietet es flüssige und den chinesischen Ausdrucksgewohnheiten entsprechende Dialoggenerierung."
  },
  "accounts/fireworks/models/deepseek-r1": {
    "description": "DeepSeek-R1 ist ein hochmodernes großes Sprachmodell, das durch verstärktes Lernen und Optimierung mit Kaltstartdaten hervorragende Leistungen in Inferenz, Mathematik und Programmierung bietet."
  },
  "accounts/fireworks/models/deepseek-v3": {
    "description": "Ein leistungsstarkes Mixture-of-Experts (MoE) Sprachmodell von Deepseek mit insgesamt 671B Parametern, wobei 37B Parameter pro Token aktiviert werden."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct": {
    "description": "Das Llama 3 70B Instruct-Modell ist speziell für mehrsprachige Dialoge und natürliche Sprachverständnis optimiert und übertrifft die meisten Wettbewerbsmodelle."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct": {
    "description": "Das Llama 3 8B Instruct-Modell ist für Dialoge und mehrsprachige Aufgaben optimiert und bietet hervorragende und effiziente Leistungen."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct-hf": {
    "description": "Das Llama 3 8B Instruct-Modell (HF-Version) stimmt mit den offiziellen Ergebnissen überein und bietet hohe Konsistenz und plattformübergreifende Kompatibilität."
  },
  "accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "description": "Das Llama 3.1 405B Instruct-Modell verfügt über eine extrem große Anzahl von Parametern und eignet sich für komplexe Aufgaben und Anweisungsverfolgung in hochbelasteten Szenarien."
  },
  "accounts/fireworks/models/llama-v3p1-70b-instruct": {
    "description": "Das Llama 3.1 70B Instruct-Modell bietet hervorragende natürliche Sprachverständnis- und Generierungsfähigkeiten und ist die ideale Wahl für Dialog- und Analyseaufgaben."
  },
  "accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "description": "Das Llama 3.1 8B Instruct-Modell ist speziell für mehrsprachige Dialoge optimiert und kann die meisten Open-Source- und Closed-Source-Modelle in gängigen Branchenbenchmarks übertreffen."
  },
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct": {
    "description": "Meta's 11B Parameter instruct-Modell für Bildverarbeitung. Dieses Modell ist optimiert für visuelle Erkennung, Bildverarbeitung, Bildbeschreibung und die Beantwortung allgemeiner Fragen zu Bildern. Es kann visuelle Daten wie Diagramme und Grafiken verstehen und schließt die Lücke zwischen visuellen und sprachlichen Informationen, indem es textuelle Beschreibungen der Bilddetails generiert."
  },
  "accounts/fireworks/models/llama-v3p2-3b-instruct": {
    "description": "Llama 3.2 3B instruct-Modell ist ein leichtgewichtiges mehrsprachiges Modell, das von Meta veröffentlicht wurde. Dieses Modell zielt darauf ab, die Effizienz zu steigern und bietet im Vergleich zu größeren Modellen signifikante Verbesserungen in Bezug auf Latenz und Kosten. Anwendungsbeispiele für dieses Modell sind Abfragen und Aufforderungsneuschreibungen sowie Schreibassistenz."
  },
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct": {
    "description": "Meta's 90B Parameter instruct-Modell für Bildverarbeitung. Dieses Modell ist optimiert für visuelle Erkennung, Bildverarbeitung, Bildbeschreibung und die Beantwortung allgemeiner Fragen zu Bildern. Es kann visuelle Daten wie Diagramme und Grafiken verstehen und schließt die Lücke zwischen visuellen und sprachlichen Informationen, indem es textuelle Beschreibungen der Bilddetails generiert."
  },
  "accounts/fireworks/models/llama-v3p3-70b-instruct": {
    "description": "Llama 3.3 70B Instruct ist die aktualisierte Version von Llama 3.1 70B aus dem Dezember. Dieses Modell wurde auf der Grundlage von Llama 3.1 70B (veröffentlicht im Juli 2024) verbessert und bietet erweiterte Funktionen für Toolaufrufe, mehrsprachige Textunterstützung sowie mathematische und Programmierfähigkeiten. Das Modell erreicht branchenführende Leistungen in den Bereichen Inferenz, Mathematik und Befehlsbefolgung und bietet eine ähnliche Leistung wie 3.1 405B, während es gleichzeitig signifikante Vorteile in Bezug auf Geschwindigkeit und Kosten bietet."
  },
  "accounts/fireworks/models/mistral-small-24b-instruct-2501": {
    "description": "Ein 24B-Parameter-Modell mit fortschrittlichen Fähigkeiten, die mit größeren Modellen vergleichbar sind."
  },
  "accounts/fireworks/models/mixtral-8x22b-instruct": {
    "description": "Das Mixtral MoE 8x22B Instruct-Modell unterstützt durch seine große Anzahl an Parametern und Multi-Expert-Architektur die effiziente Verarbeitung komplexer Aufgaben."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct": {
    "description": "Das Mixtral MoE 8x7B Instruct-Modell bietet durch seine Multi-Expert-Architektur effiziente Anweisungsverfolgung und -ausführung."
  },
  "accounts/fireworks/models/mythomax-l2-13b": {
    "description": "Das MythoMax L2 13B-Modell kombiniert neuartige Kombinations-Technologien und ist besonders gut in Erzählungen und Rollenspielen."
  },
  "accounts/fireworks/models/phi-3-vision-128k-instruct": {
    "description": "Das Phi 3 Vision Instruct-Modell ist ein leichtgewichtiges multimodales Modell, das komplexe visuelle und textuelle Informationen verarbeiten kann und über starke Schlussfolgerungsfähigkeiten verfügt."
  },
  "accounts/fireworks/models/qwen-qwq-32b-preview": {
    "description": "Das QwQ-Modell ist ein experimentelles Forschungsmodell, das vom Qwen-Team entwickelt wurde und sich auf die Verbesserung der KI-Inferenzfähigkeiten konzentriert."
  },
  "accounts/fireworks/models/qwen2-vl-72b-instruct": {
    "description": "Die 72B-Version des Qwen-VL-Modells ist das neueste Ergebnis von Alibabas Iteration und repräsentiert fast ein Jahr an Innovation."
  },
  "accounts/fireworks/models/qwen2p5-72b-instruct": {
    "description": "Qwen2.5 ist eine Reihe von Sprachmodellen mit ausschließlich Decodern, die vom Alibaba Cloud Qwen-Team entwickelt wurde. Diese Modelle sind in verschiedenen Größen erhältlich, darunter 0.5B, 1.5B, 3B, 7B, 14B, 32B und 72B, mit Basis- und instruct-Varianten."
  },
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "description": "Qwen2.5 Coder 32B Instruct ist die neueste Version der von Alibaba Cloud veröffentlichten Reihe von code-spezifischen großen Sprachmodellen. Dieses Modell basiert auf Qwen2.5 und wurde mit 55 Billionen Tokens trainiert, um die Fähigkeiten zur Codegenerierung, Inferenz und Fehlerbehebung erheblich zu verbessern. Es verbessert nicht nur die Codierungsfähigkeiten, sondern bewahrt auch die Vorteile in Mathematik und allgemeinen Fähigkeiten. Das Modell bietet eine umfassendere Grundlage für praktische Anwendungen wie Code-Agenten."
  },
  "accounts/yi-01-ai/models/yi-large": {
    "description": "Das Yi-Large-Modell bietet hervorragende mehrsprachige Verarbeitungsfähigkeiten und kann für verschiedene Sprachgenerierungs- und Verständnisaufgaben eingesetzt werden."
  },
  "ai21-jamba-1.5-large": {
    "description": "Ein mehrsprachiges Modell mit 398 Milliarden Parametern (94 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und fundierte Generierung bietet."
  },
  "ai21-jamba-1.5-mini": {
    "description": "Ein mehrsprachiges Modell mit 52 Milliarden Parametern (12 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und fundierte Generierung bietet."
  },
  "anthropic.claude-3-5-sonnet-20240620-v1:0": {
    "description": "Claude 3.5 Sonnet hebt den Branchenstandard an, übertrifft die Konkurrenzmodelle und Claude 3 Opus und zeigt in umfassenden Bewertungen hervorragende Leistungen, während es die Geschwindigkeit und Kosten unserer mittleren Modelle beibehält."
  },
  "anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet setzt neue Maßstäbe in der Branche, übertrifft die Modelle der Konkurrenz und Claude 3 Opus, und zeigt in umfassenden Bewertungen hervorragende Leistungen, während es die Geschwindigkeit und Kosten unserer mittelgroßen Modelle beibehält."
  },
  "anthropic.claude-3-haiku-20240307-v1:0": {
    "description": "Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic und bietet nahezu sofortige Reaktionsgeschwindigkeiten. Es kann schnell einfache Anfragen und Anforderungen beantworten. Kunden werden in der Lage sein, nahtlose AI-Erlebnisse zu schaffen, die menschliche Interaktionen nachahmen. Claude 3 Haiku kann Bilder verarbeiten und Textausgaben zurückgeben, mit einem Kontextfenster von 200K."
  },
  "anthropic.claude-3-opus-20240229-v1:0": {
    "description": "Claude 3 Opus ist das leistungsstärkste AI-Modell von Anthropic mit fortschrittlicher Leistung bei hochkomplexen Aufgaben. Es kann offene Eingaben und unbekannte Szenarien verarbeiten und zeigt hervorragende Flüssigkeit und menschenähnliches Verständnis. Claude 3 Opus demonstriert die Grenzen der Möglichkeiten generativer AI. Claude 3 Opus kann Bilder verarbeiten und Textausgaben zurückgeben, mit einem Kontextfenster von 200K."
  },
  "anthropic.claude-3-sonnet-20240229-v1:0": {
    "description": "Anthropic's Claude 3 Sonnet erreicht ein ideales Gleichgewicht zwischen Intelligenz und Geschwindigkeit – besonders geeignet für Unternehmensarbeitslasten. Es bietet maximalen Nutzen zu einem Preis, der unter dem der Konkurrenz liegt, und wurde als zuverlässiges, langlebiges Hauptmodell für skalierbare AI-Implementierungen konzipiert. Claude 3 Sonnet kann Bilder verarbeiten und Textausgaben zurückgeben, mit einem Kontextfenster von 200K."
  },
  "anthropic.claude-instant-v1": {
    "description": "Ein schnelles, kostengünstiges und dennoch sehr leistungsfähiges Modell, das eine Reihe von Aufgaben bewältigen kann, darunter alltägliche Gespräche, Textanalysen, Zusammenfassungen und Dokumentenfragen."
  },
  "anthropic.claude-v2": {
    "description": "Anthropic zeigt in einer Vielzahl von Aufgaben, von komplexen Dialogen und kreativer Inhaltserstellung bis hin zu detaillierten Anweisungen, ein hohes Maß an Fähigkeiten."
  },
  "anthropic.claude-v2:1": {
    "description": "Die aktualisierte Version von Claude 2 bietet ein doppelt so großes Kontextfenster sowie Verbesserungen in der Zuverlässigkeit, der Halluzinationsrate und der evidenzbasierten Genauigkeit in langen Dokumenten und RAG-Kontexten."
  },
  "anthropic/claude-3-haiku": {
    "description": "Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic, das darauf ausgelegt ist, nahezu sofortige Antworten zu liefern. Es bietet schnelle und präzise zielgerichtete Leistungen."
  },
  "anthropic/claude-3-opus": {
    "description": "Claude 3 Opus ist das leistungsstärkste Modell von Anthropic zur Bearbeitung hochkomplexer Aufgaben. Es zeichnet sich durch hervorragende Leistung, Intelligenz, Flüssigkeit und Verständnis aus."
  },
  "anthropic/claude-3.5-haiku": {
    "description": "Claude 3.5 Haiku ist das schnellste nächste Generation Modell von Anthropic. Im Vergleich zu Claude 3 Haiku hat Claude 3.5 Haiku in allen Fähigkeiten Fortschritte gemacht und übertrifft in vielen intellektuellen Benchmark-Tests das größte Modell der vorherigen Generation, Claude 3 Opus."
  },
  "anthropic/claude-3.5-sonnet": {
    "description": "Claude 3.5 Sonnet bietet Fähigkeiten, die über Opus hinausgehen, und eine schnellere Geschwindigkeit als Sonnet, während es den gleichen Preis wie Sonnet beibehält. Sonnet ist besonders gut in Programmierung, Datenwissenschaft, visueller Verarbeitung und Agentenaufgaben."
  },
  "anthropic/claude-3.7-sonnet": {
    "description": "Claude 3.7 Sonnet ist das intelligenteste Modell von Anthropic bis heute und das erste hybride Inferenzmodell auf dem Markt. Claude 3.7 Sonnet kann nahezu sofortige Antworten oder verlängerte, schrittweise Überlegungen erzeugen, wobei die Benutzer diesen Prozess klar nachvollziehen können. Sonnet ist besonders gut in den Bereichen Programmierung, Datenwissenschaft, visuelle Verarbeitung und Agentenaufgaben."
  },
  "aya": {
    "description": "Aya 23 ist ein mehrsprachiges Modell von Cohere, das 23 Sprachen unterstützt und die Anwendung in einer Vielzahl von Sprachen erleichtert."
  },
  "aya:35b": {
    "description": "Aya 23 ist ein mehrsprachiges Modell von Cohere, das 23 Sprachen unterstützt und die Anwendung in einer Vielzahl von Sprachen erleichtert."
  },
  "baichuan/baichuan2-13b-chat": {
    "description": "Baichuan-13B ist ein Open-Source-Sprachmodell mit 13 Milliarden Parametern, das von Baichuan Intelligence entwickelt wurde und in autorisierten chinesischen und englischen Benchmarks die besten Ergebnisse in seiner Größenordnung erzielt hat."
  },
  "charglm-3": {
    "description": "CharGLM-3 ist für Rollenspiele und emotionale Begleitung konzipiert und unterstützt extrem lange Mehrfachgedächtnisse und personalisierte Dialoge, mit breiter Anwendung."
  },
  "chatgpt-4o-latest": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "claude-2.0": {
    "description": "Claude 2 bietet Unternehmen Fortschritte in kritischen Fähigkeiten, einschließlich branchenführenden 200K Token Kontext, erheblich reduzierter Häufigkeit von Modellillusionen, Systemaufforderungen und einer neuen Testfunktion: Werkzeugaufrufe."
  },
  "claude-2.1": {
    "description": "Claude 2 bietet Unternehmen Fortschritte in kritischen Fähigkeiten, einschließlich branchenführenden 200K Token Kontext, erheblich reduzierter Häufigkeit von Modellillusionen, Systemaufforderungen und einer neuen Testfunktion: Werkzeugaufrufe."
  },
  "claude-3-5-haiku-20241022": {
    "description": "Claude 3.5 Haiku ist das schnellste nächste Modell von Anthropic. Im Vergleich zu Claude 3 Haiku hat Claude 3.5 Haiku in allen Fähigkeiten Verbesserungen erzielt und übertrifft das vorherige größte Modell, Claude 3 Opus, in vielen intellektuellen Benchmark-Tests."
  },
  "claude-3-5-sonnet-20240620": {
    "description": "Claude 3.5 Sonnet bietet Fähigkeiten, die über Opus hinausgehen, und ist schneller als Sonnet, während es den gleichen Preis wie Sonnet beibehält. Sonnet ist besonders gut in Programmierung, Datenwissenschaft, visueller Verarbeitung und Agenturaufgaben."
  },
  "claude-3-5-sonnet-20241022": {
    "description": "Claude 3.5 Sonnet bietet überlegene Fähigkeiten im Vergleich zu Opus und schnellere Geschwindigkeiten als Sonnet, während es den gleichen Preis wie Sonnet beibehält. Sonnet ist besonders gut in den Bereichen Programmierung, Datenwissenschaft, visuelle Verarbeitung und Aufgabenübertragung."
  },
  "claude-3-7-sonnet-20250219": {
    "description": "Claude 3.7 Sonnet hebt den Branchenstandard an, übertrifft die Modelle der Konkurrenz und Claude 3 Opus, und zeigt in umfassenden Bewertungen hervorragende Leistungen, während es die Geschwindigkeit und Kosten unserer mittelgroßen Modelle beibehält."
  },
  "claude-3-haiku-20240307": {
    "description": "Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic, das darauf abzielt, nahezu sofortige Antworten zu liefern. Es bietet schnelle und präzise zielgerichtete Leistungen."
  },
  "claude-3-opus-20240229": {
    "description": "Claude 3 Opus ist das leistungsstärkste Modell von Anthropic für die Verarbeitung hochkomplexer Aufgaben. Es bietet herausragende Leistungen in Bezug auf Leistung, Intelligenz, Flüssigkeit und Verständnis."
  },
  "claude-3-sonnet-20240229": {
    "description": "Claude 3 Sonnet bietet eine ideale Balance zwischen Intelligenz und Geschwindigkeit für Unternehmensarbeitslasten. Es bietet maximalen Nutzen zu einem niedrigeren Preis, ist zuverlässig und für großflächige Bereitstellungen geeignet."
  },
  "codegeex-4": {
    "description": "CodeGeeX-4 ist ein leistungsstarker AI-Programmierassistent, der intelligente Fragen und Codevervollständigung in verschiedenen Programmiersprachen unterstützt und die Entwicklungseffizienz steigert."
  },
  "codegeex4-all-9b": {
    "description": "CodeGeeX4-ALL-9B ist ein mehrsprachiges Code-Generierungsmodell, das umfassende Funktionen unterstützt, darunter Code-Vervollständigung und -Generierung, Code-Interpreter, Websuche, Funktionsaufrufe und repository-weite Codefragen und -antworten, und deckt verschiedene Szenarien der Softwareentwicklung ab. Es ist das führende Code-Generierungsmodell mit weniger als 10B Parametern."
  },
  "codegemma": {
    "description": "CodeGemma ist ein leichtgewichtiges Sprachmodell, das speziell für verschiedene Programmieraufgaben entwickelt wurde und schnelle Iterationen und Integrationen unterstützt."
  },
  "codegemma:2b": {
    "description": "CodeGemma ist ein leichtgewichtiges Sprachmodell, das speziell für verschiedene Programmieraufgaben entwickelt wurde und schnelle Iterationen und Integrationen unterstützt."
  },
  "codellama": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codellama/CodeLlama-34b-Instruct-hf": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die für Entwicklerumgebungen geeignet ist."
  },
  "codellama:13b": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codellama:34b": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codellama:70b": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codeqwen": {
    "description": "CodeQwen1.5 ist ein großes Sprachmodell, das auf einer umfangreichen Code-Datenbasis trainiert wurde und speziell für die Lösung komplexer Programmieraufgaben entwickelt wurde."
  },
  "codestral": {
    "description": "Codestral ist das erste Code-Modell von Mistral AI und bietet hervorragende Unterstützung für Aufgaben der Codegenerierung."
  },
  "codestral-latest": {
    "description": "Codestral ist ein hochmodernes Generierungsmodell, das sich auf die Codegenerierung konzentriert und für Aufgaben wie das Ausfüllen von Zwischenräumen und die Codevervollständigung optimiert wurde."
  },
  "cognitivecomputations/dolphin-mixtral-8x22b": {
    "description": "Dolphin Mixtral 8x22B ist ein Modell, das für die Befolgung von Anweisungen, Dialoge und Programmierung entwickelt wurde."
  },
  "cohere-command-r": {
    "description": "Command R ist ein skalierbares generatives Modell, das auf RAG und Tool-Nutzung abzielt, um KI in Produktionsgröße für Unternehmen zu ermöglichen."
  },
  "cohere-command-r-plus": {
    "description": "Command R+ ist ein hochmodernes, RAG-optimiertes Modell, das für unternehmensgerechte Arbeitslasten konzipiert ist."
  },
  "command-r": {
    "description": "Command R ist ein LLM, das für Dialoge und Aufgaben mit langen Kontexten optimiert ist und sich besonders gut für dynamische Interaktionen und Wissensmanagement eignet."
  },
  "command-r-plus": {
    "description": "Command R+ ist ein leistungsstarkes großes Sprachmodell, das speziell für reale Unternehmensszenarien und komplexe Anwendungen entwickelt wurde."
  },
  "dall-e-2": {
    "description": "Zweite Generation des DALL·E-Modells, unterstützt realistischere und genauere Bildgenerierung, mit einer Auflösung, die viermal so hoch ist wie die der ersten Generation."
  },
  "dall-e-3": {
    "description": "Das neueste DALL·E-Modell, veröffentlicht im November 2023. Unterstützt realistischere und genauere Bildgenerierung mit verbesserter Detailgenauigkeit."
  },
  "databricks/dbrx-instruct": {
    "description": "DBRX Instruct bietet zuverlässige Anweisungsverarbeitungsfähigkeiten und unterstützt Anwendungen in verschiedenen Branchen."
  },
  "deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 ist ein durch verstärkendes Lernen (RL) gesteuertes Inferenzmodell, das die Probleme der Wiederholbarkeit und Lesbarkeit im Modell löst. Vor dem RL führte DeepSeek-R1 Kaltstartdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in mathematischen, programmierbezogenen und Inferenzaufgaben eine vergleichbare Leistung zu OpenAI-o1 und verbessert durch sorgfältig gestaltete Trainingsmethoden die Gesamteffizienz."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
    "description": "Das DeepSeek-R1-Distill-Modell optimiert die Inferenzleistung durch verstärkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Maßstäbe für Multitasking."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Llama-8B": {
    "description": "DeepSeek-R1-Distill-Llama-8B ist ein destilliertes Modell, das auf Llama-3.1-8B basiert. Dieses Modell wurde mit Beispielen, die von DeepSeek-R1 generiert wurden, feinabgestimmt und zeigt hervorragende Inferenzfähigkeiten. Es hat in mehreren Benchmark-Tests gut abgeschnitten, darunter eine Genauigkeit von 89,1 % in MATH-500, eine Bestehensquote von 50,4 % in AIME 2024 und eine Bewertung von 1205 in CodeForces, was starke mathematische und Programmierfähigkeiten für ein 8B-Modell demonstriert."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Das DeepSeek-R1-Distill-Modell optimiert die Inferenzleistung durch verstärkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Maßstäbe für Multitasking."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Das DeepSeek-R1-Distill-Modell optimiert die Inferenzleistung durch verstärkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Maßstäbe für Multitasking."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
    "description": "DeepSeek-R1-Distill-Qwen-32B ist ein Modell, das durch Wissensdestillation aus Qwen2.5-32B gewonnen wurde. Dieses Modell wurde mit 800.000 ausgewählten Beispielen, die von DeepSeek-R1 generiert wurden, feinabgestimmt und zeigt herausragende Leistungen in mehreren Bereichen wie Mathematik, Programmierung und Inferenz. Es hat in mehreren Benchmark-Tests, darunter AIME 2024, MATH-500 und GPQA Diamond, hervorragende Ergebnisse erzielt, wobei es in MATH-500 eine Genauigkeit von 94,3 % erreicht hat und damit starke mathematische Schlussfolgerungsfähigkeiten demonstriert."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B ist ein Modell, das durch Wissensdestillation aus Qwen2.5-Math-7B gewonnen wurde. Dieses Modell wurde mit 800.000 ausgewählten Beispielen, die von DeepSeek-R1 generiert wurden, feinabgestimmt und zeigt hervorragende Inferenzfähigkeiten. Es hat in mehreren Benchmark-Tests, darunter eine Genauigkeit von 92,8 % in MATH-500, eine Bestehensquote von 55,5 % in AIME 2024 und eine Bewertung von 1189 in CodeForces, was starke mathematische und Programmierfähigkeiten für ein 7B-Modell demonstriert."
  },
  "deepseek-ai/DeepSeek-V2.5": {
    "description": "DeepSeek V2.5 vereint die hervorragenden Merkmale früherer Versionen und verbessert die allgemeinen und kodierenden Fähigkeiten."
  },
  "deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 ist ein hybrides Expertenmodell (MoE) mit 6710 Milliarden Parametern, das eine Multi-Head-Latent-Attention (MLA) und die DeepSeekMoE-Architektur verwendet, kombiniert mit einer Lastenausgleichsstrategie ohne Hilfskosten, um die Inferenz- und Trainingseffizienz zu optimieren. Durch das Pre-Training auf 14,8 Billionen hochwertigen Tokens und anschließendes überwachten Feintuning und verstärkendes Lernen übertrifft DeepSeek-V3 in der Leistung andere Open-Source-Modelle und nähert sich führenden Closed-Source-Modellen."
  },
  "deepseek-ai/deepseek-llm-67b-chat": {
    "description": "DeepSeek 67B ist ein fortschrittliches Modell, das für komplexe Dialoge trainiert wurde."
  },
  "deepseek-ai/deepseek-r1": {
    "description": "Hochmodernes, effizientes LLM, das auf Schlussfolgern, Mathematik und Programmierung spezialisiert ist."
  },
  "deepseek-ai/deepseek-vl2": {
    "description": "DeepSeek-VL2 ist ein hybrides Expertenmodell (MoE) für visuelle Sprache, das auf DeepSeekMoE-27B basiert und eine spärliche Aktivierung der MoE-Architektur verwendet, um außergewöhnliche Leistungen bei der Aktivierung von nur 4,5 Milliarden Parametern zu erzielen. Dieses Modell zeigt hervorragende Leistungen in mehreren Aufgaben, darunter visuelle Fragenbeantwortung, optische Zeichenerkennung, Dokument-/Tabellen-/Diagrammverständnis und visuelle Lokalisierung."
  },
  "deepseek-chat": {
    "description": "Ein neues Open-Source-Modell, das allgemeine und Codefähigkeiten kombiniert. Es bewahrt nicht nur die allgemeinen Dialogfähigkeiten des ursprünglichen Chat-Modells und die leistungsstarken Codeverarbeitungsfähigkeiten des Coder-Modells, sondern stimmt auch besser mit menschlichen Präferenzen überein. Darüber hinaus hat DeepSeek-V2.5 in mehreren Bereichen wie Schreibaufgaben und Befolgung von Anweisungen erhebliche Verbesserungen erzielt."
  },
  "deepseek-coder-33B-instruct": {
    "description": "DeepSeek Coder 33B ist ein Code-Sprachmodell, das auf 20 Billionen Daten trainiert wurde, von denen 87 % Code und 13 % in Chinesisch und Englisch sind. Das Modell führt eine Fenstergröße von 16K und Aufgaben zur Lückenergänzung ein und bietet projektbezogene Code-Vervollständigung und Fragmentfüllfunktionen."
  },
  "deepseek-coder-v2": {
    "description": "DeepSeek Coder V2 ist ein Open-Source-Mischexperten-Code-Modell, das in Codeaufgaben hervorragende Leistungen erbringt und mit GPT4-Turbo vergleichbar ist."
  },
  "deepseek-coder-v2:236b": {
    "description": "DeepSeek Coder V2 ist ein Open-Source-Mischexperten-Code-Modell, das in Codeaufgaben hervorragende Leistungen erbringt und mit GPT4-Turbo vergleichbar ist."
  },
  "deepseek-r1": {
    "description": "DeepSeek-R1 ist ein durch verstärkendes Lernen (RL) gesteuertes Inferenzmodell, das die Probleme der Wiederholbarkeit und Lesbarkeit im Modell löst. Vor dem RL führte DeepSeek-R1 Kaltstartdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in mathematischen, programmierbezogenen und Inferenzaufgaben eine vergleichbare Leistung zu OpenAI-o1 und verbessert durch sorgfältig gestaltete Trainingsmethoden die Gesamteffizienz."
  },
  "deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 – das größere und intelligentere Modell im DeepSeek-Paket – wurde in die Llama 70B-Architektur destilliert. Basierend auf Benchmark-Tests und menschlicher Bewertung ist dieses Modell intelligenter als das ursprüngliche Llama 70B, insbesondere bei Aufgaben, die mathematische und faktische Genauigkeit erfordern."
  },
  "deepseek-r1-distill-llama-8b": {
    "description": "Das DeepSeek-R1-Distill Modell wurde durch Wissensdistillationstechniken entwickelt, indem Proben, die von DeepSeek-R1 generiert wurden, auf Qwen, Llama und andere Open-Source-Modelle feinabgestimmt wurden."
  },
  "deepseek-r1-distill-qwen-1.5b": {
    "description": "Das DeepSeek-R1-Distill Modell wurde durch Wissensdistillationstechniken entwickelt, indem Proben, die von DeepSeek-R1 generiert wurden, auf Qwen, Llama und andere Open-Source-Modelle feinabgestimmt wurden."
  },
  "deepseek-r1-distill-qwen-14b": {
    "description": "Das DeepSeek-R1-Distill Modell wurde durch Wissensdistillationstechniken entwickelt, indem Proben, die von DeepSeek-R1 generiert wurden, auf Qwen, Llama und andere Open-Source-Modelle feinabgestimmt wurden."
  },
  "deepseek-r1-distill-qwen-32b": {
    "description": "Das DeepSeek-R1-Distill Modell wurde durch Wissensdistillationstechniken entwickelt, indem Proben, die von DeepSeek-R1 generiert wurden, auf Qwen, Llama und andere Open-Source-Modelle feinabgestimmt wurden."
  },
  "deepseek-r1-distill-qwen-7b": {
    "description": "Das DeepSeek-R1-Distill Modell wurde durch Wissensdistillationstechniken entwickelt, indem Proben, die von DeepSeek-R1 generiert wurden, auf Qwen, Llama und andere Open-Source-Modelle feinabgestimmt wurden."
  },
  "deepseek-reasoner": {
    "description": "Das von DeepSeek entwickelte Inferenzmodell. Bevor das Modell die endgültige Antwort ausgibt, gibt es zunächst eine Denkprozesskette aus, um die Genauigkeit der endgültigen Antwort zu erhöhen."
  },
  "deepseek-v2": {
    "description": "DeepSeek V2 ist ein effizientes Mixture-of-Experts-Sprachmodell, das für wirtschaftliche Verarbeitungsanforderungen geeignet ist."
  },
  "deepseek-v2:236b": {
    "description": "DeepSeek V2 236B ist das Design-Code-Modell von DeepSeek und bietet starke Fähigkeiten zur Codegenerierung."
  },
  "deepseek-v3": {
    "description": "DeepSeek-V3 ist ein MoE-Modell, das von der Hangzhou DeepSeek Artificial Intelligence Technology Research Co., Ltd. entwickelt wurde. Es hat in mehreren Bewertungen herausragende Ergebnisse erzielt und belegt in den gängigen Rankings den ersten Platz unter den Open-Source-Modellen. Im Vergleich zum V2.5-Modell hat sich die Generierungsgeschwindigkeit um das Dreifache erhöht, was den Nutzern ein schnelleres und flüssigeres Nutzungserlebnis bietet."
  },
  "deepseek/deepseek-chat": {
    "description": "Ein neues Open-Source-Modell, das allgemeine und Codefähigkeiten vereint. Es behält nicht nur die allgemeinen Dialogfähigkeiten des ursprünglichen Chat-Modells und die leistungsstarken Codeverarbeitungsfähigkeiten des Coder-Modells bei, sondern stimmt auch besser mit menschlichen Vorlieben überein. Darüber hinaus hat DeepSeek-V2.5 in vielen Bereichen wie Schreibaufgaben und Befehlsbefolgung erhebliche Verbesserungen erzielt."
  },
  "deepseek/deepseek-r1": {
    "description": "DeepSeek-R1 hat die Schlussfolgerungsfähigkeiten des Modells erheblich verbessert, selbst bei nur wenigen gekennzeichneten Daten. Bevor das Modell die endgültige Antwort ausgibt, gibt es zunächst eine Denkprozesskette aus, um die Genauigkeit der endgültigen Antwort zu erhöhen."
  },
  "deepseek/deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 Distill Llama 70B ist ein großes Sprachmodell, das auf Llama3.3 70B basiert und durch Feinabstimmung mit den Ausgaben von DeepSeek R1 eine wettbewerbsfähige Leistung erreicht, die mit großen, fortschrittlichen Modellen vergleichbar ist."
  },
  "deepseek/deepseek-r1-distill-llama-8b": {
    "description": "DeepSeek R1 Distill Llama 8B ist ein distilliertes großes Sprachmodell, das auf Llama-3.1-8B-Instruct basiert und durch Training mit den Ausgaben von DeepSeek R1 erstellt wurde."
  },
  "deepseek/deepseek-r1-distill-qwen-14b": {
    "description": "DeepSeek R1 Distill Qwen 14B ist ein distilliertes großes Sprachmodell, das auf Qwen 2.5 14B basiert und durch Training mit den Ausgaben von DeepSeek R1 erstellt wurde. Dieses Modell hat in mehreren Benchmark-Tests OpenAI's o1-mini übertroffen und die neuesten technologischen Fortschritte bei dichten Modellen (state-of-the-art) erzielt. Hier sind einige Ergebnisse der Benchmark-Tests:\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nDas Modell zeigt durch Feinabstimmung mit den Ausgaben von DeepSeek R1 eine wettbewerbsfähige Leistung, die mit größeren, fortschrittlichen Modellen vergleichbar ist."
  },
  "deepseek/deepseek-r1-distill-qwen-32b": {
    "description": "DeepSeek R1 Distill Qwen 32B ist ein distilliertes großes Sprachmodell, das auf Qwen 2.5 32B basiert und durch Training mit den Ausgaben von DeepSeek R1 erstellt wurde. Dieses Modell hat in mehreren Benchmark-Tests OpenAI's o1-mini übertroffen und die neuesten technologischen Fortschritte bei dichten Modellen (state-of-the-art) erzielt. Hier sind einige Ergebnisse der Benchmark-Tests:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nDas Modell zeigt durch Feinabstimmung mit den Ausgaben von DeepSeek R1 eine wettbewerbsfähige Leistung, die mit größeren, fortschrittlichen Modellen vergleichbar ist."
  },
  "deepseek/deepseek-r1/community": {
    "description": "DeepSeek R1 ist das neueste Open-Source-Modell, das vom DeepSeek-Team veröffentlicht wurde und über eine äußerst leistungsstarke Inferenzleistung verfügt, insbesondere in den Bereichen Mathematik, Programmierung und logisches Denken, die mit dem OpenAI o1-Modell vergleichbar ist."
  },
  "deepseek/deepseek-r1:free": {
    "description": "DeepSeek-R1 hat die Schlussfolgerungsfähigkeiten des Modells erheblich verbessert, selbst bei nur wenigen gekennzeichneten Daten. Bevor das Modell die endgültige Antwort ausgibt, gibt es zunächst eine Denkprozesskette aus, um die Genauigkeit der endgültigen Antwort zu erhöhen."
  },
  "deepseek/deepseek-v3": {
    "description": "DeepSeek-V3 hat einen bedeutenden Durchbruch in der Inferenzgeschwindigkeit im Vergleich zu früheren Modellen erzielt. Es belegt den ersten Platz unter den Open-Source-Modellen und kann mit den weltweit fortschrittlichsten proprietären Modellen konkurrieren. DeepSeek-V3 verwendet die Multi-Head-Latent-Attention (MLA) und die DeepSeekMoE-Architektur, die in DeepSeek-V2 umfassend validiert wurden. Darüber hinaus hat DeepSeek-V3 eine unterstützende verlustfreie Strategie für die Lastenverteilung eingeführt und mehrere Zielvorgaben für das Training von Mehrfachvorhersagen festgelegt, um eine stärkere Leistung zu erzielen."
  },
  "deepseek/deepseek-v3/community": {
    "description": "DeepSeek-V3 hat einen bedeutenden Durchbruch in der Inferenzgeschwindigkeit im Vergleich zu früheren Modellen erzielt. Es belegt den ersten Platz unter den Open-Source-Modellen und kann mit den weltweit fortschrittlichsten proprietären Modellen konkurrieren. DeepSeek-V3 verwendet die Multi-Head-Latent-Attention (MLA) und die DeepSeekMoE-Architektur, die in DeepSeek-V2 umfassend validiert wurden. Darüber hinaus hat DeepSeek-V3 eine unterstützende verlustfreie Strategie für die Lastenverteilung eingeführt und mehrere Zielvorgaben für das Training von Mehrfachvorhersagen festgelegt, um eine stärkere Leistung zu erzielen."
  },
  "doubao-1.5-lite-32k": {
    "description": "Doubao-1.5-lite ist das neueste leichte Modell der nächsten Generation, das eine extrem schnelle Reaktionszeit bietet und sowohl in der Leistung als auch in der Latenz weltweit erstklassig ist."
  },
  "doubao-1.5-pro-256k": {
    "description": "Doubao-1.5-pro-256k ist die umfassend verbesserte Version von Doubao-1.5-Pro, die die Gesamtleistung um 10 % steigert. Es unterstützt Schlussfolgerungen mit einem Kontextfenster von 256k und eine maximale Ausgabelänge von 12k Tokens. Höhere Leistung, größeres Fenster und ein hervorragendes Preis-Leistungs-Verhältnis machen es für eine breitere Palette von Anwendungsszenarien geeignet."
  },
  "doubao-1.5-pro-32k": {
    "description": "Doubao-1.5-pro ist das neueste Hauptmodell der nächsten Generation, dessen Leistung umfassend verbessert wurde und das in den Bereichen Wissen, Code, Schlussfolgerungen usw. herausragende Leistungen zeigt."
  },
  "emohaa": {
    "description": "Emohaa ist ein psychologisches Modell mit professionellen Beratungsfähigkeiten, das den Nutzern hilft, emotionale Probleme zu verstehen."
  },
  "ernie-3.5-128k": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle deckt eine riesige Menge an chinesischen und englischen Korpora ab und bietet starke allgemeine Fähigkeiten, die die meisten Anforderungen an Dialogfragen, kreative Generierung und Plugin-Anwendungen erfüllen; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ernie-3.5-8k": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle deckt eine riesige Menge an chinesischen und englischen Korpora ab und bietet starke allgemeine Fähigkeiten, die die meisten Anforderungen an Dialogfragen, kreative Generierung und Plugin-Anwendungen erfüllen; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ernie-3.5-8k-preview": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle deckt eine riesige Menge an chinesischen und englischen Korpora ab und bietet starke allgemeine Fähigkeiten, die die meisten Anforderungen an Dialogfragen, kreative Generierung und Plugin-Anwendungen erfüllen; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ernie-4.0-8k-latest": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle hat im Vergleich zu ERNIE 3.5 eine umfassende Verbesserung der Modellfähigkeiten erreicht und ist weit verbreitet in komplexen Aufgabenbereichen anwendbar; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ernie-4.0-8k-preview": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle hat im Vergleich zu ERNIE 3.5 eine umfassende Verbesserung der Modellfähigkeiten erreicht und ist weit verbreitet in komplexen Aufgabenbereichen anwendbar; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten."
  },
  "ernie-4.0-turbo-128k": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle zeigt hervorragende Gesamtergebnisse und ist weit verbreitet in komplexen Aufgabenbereichen anwendbar; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten. Im Vergleich zu ERNIE 4.0 bietet es eine bessere Leistung."
  },
  "ernie-4.0-turbo-8k-latest": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle zeigt hervorragende Gesamtergebnisse und ist weit verbreitet in komplexen Aufgabenbereichen anwendbar; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten. Im Vergleich zu ERNIE 4.0 bietet es eine bessere Leistung."
  },
  "ernie-4.0-turbo-8k-preview": {
    "description": "Das von Baidu entwickelte Flaggschiff-Modell für große Sprachmodelle zeigt hervorragende Gesamtergebnisse und ist weit verbreitet in komplexen Aufgabenbereichen anwendbar; es unterstützt die automatische Anbindung an das Baidu-Suchplugin, um die Aktualität der Antwortinformationen zu gewährleisten. Im Vergleich zu ERNIE 4.0 bietet es eine bessere Leistung."
  },
  "ernie-char-8k": {
    "description": "Das von Baidu entwickelte große Sprachmodell für vertikale Szenarien eignet sich für Anwendungen wie NPCs in Spielen, Kundenservice-Dialoge und Rollenspiele, mit einem klareren und konsistenteren Charakterstil, einer stärkeren Befolgung von Anweisungen und besserer Inferenzleistung."
  },
  "ernie-char-fiction-8k": {
    "description": "Das von Baidu entwickelte große Sprachmodell für vertikale Szenarien eignet sich für Anwendungen wie NPCs in Spielen, Kundenservice-Dialoge und Rollenspiele, mit einem klareren und konsistenteren Charakterstil, einer stärkeren Befolgung von Anweisungen und besserer Inferenzleistung."
  },
  "ernie-lite-8k": {
    "description": "ERNIE Lite ist ein leichtgewichtiges großes Sprachmodell, das von Baidu entwickelt wurde und sowohl hervorragende Modellleistung als auch Inferenzleistung bietet, geeignet für die Verwendung mit AI-Beschleunigungskarten mit geringer Rechenleistung."
  },
  "ernie-lite-pro-128k": {
    "description": "Das von Baidu entwickelte leichtgewichtige große Sprachmodell bietet sowohl hervorragende Modellleistung als auch Inferenzleistung, die besser ist als die von ERNIE Lite, und ist geeignet für die Verwendung mit AI-Beschleunigungskarten mit geringer Rechenleistung."
  },
  "ernie-novel-8k": {
    "description": "Das von Baidu entwickelte allgemeine große Sprachmodell hat deutliche Vorteile in der Fähigkeit zur Fortsetzung von Romanen und kann auch in Szenarien wie Kurzdramen und Filmen eingesetzt werden."
  },
  "ernie-speed-128k": {
    "description": "Das neueste hochleistungsfähige große Sprachmodell von Baidu, das 2024 veröffentlicht wurde, bietet hervorragende allgemeine Fähigkeiten und eignet sich gut als Basismodell für Feinabstimmungen, um spezifische Szenarien besser zu bewältigen, während es auch hervorragende Inferenzleistungen bietet."
  },
  "ernie-speed-pro-128k": {
    "description": "Das neueste hochleistungsfähige große Sprachmodell von Baidu, das 2024 veröffentlicht wurde, bietet hervorragende allgemeine Fähigkeiten und ist besser als ERNIE Speed, geeignet als Basismodell für Feinabstimmungen, um spezifische Szenarien besser zu bewältigen, während es auch hervorragende Inferenzleistungen bietet."
  },
  "ernie-tiny-8k": {
    "description": "ERNIE Tiny ist ein hochleistungsfähiges großes Sprachmodell, dessen Bereitstellungs- und Feinabstimmungskosten die niedrigsten unter den Wenshin-Modellen sind."
  },
  "gemini-1.0-pro-001": {
    "description": "Gemini 1.0 Pro 001 (Tuning) bietet stabile und anpassbare Leistung und ist die ideale Wahl für Lösungen komplexer Aufgaben."
  },
  "gemini-1.0-pro-002": {
    "description": "Gemini 1.0 Pro 002 (Tuning) bietet hervorragende multimodale Unterstützung und konzentriert sich auf die effektive Lösung komplexer Aufgaben."
  },
  "gemini-1.0-pro-latest": {
    "description": "Gemini 1.0 Pro ist Googles leistungsstarkes KI-Modell, das für die Skalierung einer Vielzahl von Aufgaben konzipiert ist."
  },
  "gemini-1.5-flash": {
    "description": "Gemini 1.5 Flash ist Googles neuestes multimodales KI-Modell, das über eine schnelle Verarbeitungskapazität verfügt und Texte, Bilder und Videoeingaben unterstützt, um eine effiziente Skalierung für verschiedene Aufgaben zu ermöglichen."
  },
  "gemini-1.5-flash-001": {
    "description": "Gemini 1.5 Flash 001 ist ein effizientes multimodales Modell, das eine breite Anwendbarkeit unterstützt."
  },
  "gemini-1.5-flash-002": {
    "description": "Gemini 1.5 Flash 002 ist ein effizientes multimodales Modell, das eine breite Palette von Anwendungen unterstützt."
  },
  "gemini-1.5-flash-8b": {
    "description": "Gemini 1.5 Flash 8B ist ein leistungsstarkes multimodales Modell, das eine breite Palette von Anwendungen unterstützt."
  },
  "gemini-1.5-flash-8b-exp-0924": {
    "description": "Gemini 1.5 Flash 8B 0924 ist das neueste experimentelle Modell, das in Text- und multimodalen Anwendungsfällen erhebliche Leistungsverbesserungen aufweist."
  },
  "gemini-1.5-flash-exp-0827": {
    "description": "Gemini 1.5 Flash 0827 bietet optimierte multimodale Verarbeitungskapazitäten, die für verschiedene komplexe Aufgaben geeignet sind."
  },
  "gemini-1.5-flash-latest": {
    "description": "Gemini 1.5 Flash ist Googles neuestes multimodales KI-Modell, das über schnelle Verarbeitungsfähigkeiten verfügt und Text-, Bild- und Videoeingaben unterstützt, um eine effiziente Skalierung für verschiedene Aufgaben zu ermöglichen."
  },
  "gemini-1.5-pro-001": {
    "description": "Gemini 1.5 Pro 001 ist eine skalierbare multimodale KI-Lösung, die eine breite Palette komplexer Aufgaben unterstützt."
  },
  "gemini-1.5-pro-002": {
    "description": "Gemini 1.5 Pro 002 ist das neueste produktionsbereite Modell, das eine höhere Ausgabequalität bietet, insbesondere bei mathematischen, langen Kontexten und visuellen Aufgaben erhebliche Verbesserungen aufweist."
  },
  "gemini-1.5-pro-exp-0801": {
    "description": "Gemini 1.5 Pro 0801 bietet herausragende multimodale Verarbeitungskapazitäten und bringt größere Flexibilität in die Anwendungsentwicklung."
  },
  "gemini-1.5-pro-exp-0827": {
    "description": "Gemini 1.5 Pro 0827 kombiniert die neuesten Optimierungstechnologien, um eine effizientere multimodale Datenverarbeitung zu ermöglichen."
  },
  "gemini-1.5-pro-latest": {
    "description": "Gemini 1.5 Pro unterstützt bis zu 2 Millionen Tokens und ist die ideale Wahl für mittelgroße multimodale Modelle, die umfassende Unterstützung für komplexe Aufgaben bieten."
  },
  "gemini-2.0-flash": {
    "description": "Gemini 2.0 Flash bietet nächste Generation Funktionen und Verbesserungen, einschließlich außergewöhnlicher Geschwindigkeit, nativer Werkzeugnutzung, multimodaler Generierung und einem Kontextfenster von 1M Tokens."
  },
  "gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash bietet nächste Generation Funktionen und Verbesserungen, einschließlich außergewöhnlicher Geschwindigkeit, nativer Werkzeugnutzung, multimodaler Generierung und einem Kontextfenster von 1M Tokens."
  },
  "gemini-2.0-flash-exp": {
    "description": "Gemini 2.0 Flash-Modellvariante, die auf Kosteneffizienz und niedrige Latenz optimiert ist."
  },
  "gemini-2.0-flash-exp-image-generation": {
    "description": "Gemini 2.0 Flash Experimentmodell, das die Bildgenerierung unterstützt"
  },
  "gemini-2.0-flash-lite": {
    "description": "Gemini 2.0 Flash ist eine Modellvariante, die auf Kosteneffizienz und niedrige Latenz optimiert ist."
  },
  "gemini-2.0-flash-lite-001": {
    "description": "Gemini 2.0 Flash ist eine Modellvariante, die auf Kosteneffizienz und niedrige Latenz optimiert ist."
  },
  "gemini-2.0-flash-lite-preview-02-05": {
    "description": "Ein Gemini 2.0 Flash Modell, das auf Kosteneffizienz und niedrige Latenz optimiert wurde."
  },
  "gemini-2.0-flash-thinking-exp": {
    "description": "Gemini 2.0 Flash Exp ist Googles neuestes experimentelles multimodales KI-Modell mit der nächsten Generation von Funktionen, außergewöhnlicher Geschwindigkeit, nativer Tool-Nutzung und multimodaler Generierung."
  },
  "gemini-2.0-flash-thinking-exp-01-21": {
    "description": "Gemini 2.0 Flash Exp ist Googles neuestes experimentelles multimodales KI-Modell mit der nächsten Generation von Funktionen, außergewöhnlicher Geschwindigkeit, nativer Tool-Nutzung und multimodaler Generierung."
  },
  "gemini-2.0-pro-exp-02-05": {
    "description": "Gemini 2.0 Pro Experimental ist Googles neuestes experimentelles multimodales KI-Modell, das im Vergleich zu früheren Versionen eine gewisse Qualitätsverbesserung aufweist, insbesondere in Bezug auf Weltwissen, Code und lange Kontexte."
  },
  "gemma-7b-it": {
    "description": "Gemma 7B eignet sich für die Verarbeitung von mittelgroßen Aufgaben und bietet ein gutes Kosten-Nutzen-Verhältnis."
  },
  "gemma2": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."
  },
  "gemma2-9b-it": {
    "description": "Gemma 2 9B ist ein Modell, das für spezifische Aufgaben und die Integration von Werkzeugen optimiert wurde."
  },
  "gemma2:27b": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."
  },
  "gemma2:2b": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."
  },
  "generalv3": {
    "description": "Spark Pro ist ein hochleistungsfähiges großes Sprachmodell, das für professionelle Bereiche optimiert ist und sich auf Mathematik, Programmierung, Medizin, Bildung und andere Bereiche konzentriert, und unterstützt die Online-Suche sowie integrierte Plugins für Wetter, Datum usw. Das optimierte Modell zeigt hervorragende Leistungen und hohe Effizienz in komplexen Wissensabfragen, Sprachverständnis und hochrangiger Textgenerierung und ist die ideale Wahl für professionelle Anwendungsszenarien."
  },
  "generalv3.5": {
    "description": "Spark3.5 Max ist die umfassendste Version, die Online-Suche und zahlreiche integrierte Plugins unterstützt. Ihre umfassend optimierten Kernfähigkeiten sowie die Systemrolleneinstellungen und Funktionsaufrufmöglichkeiten ermöglichen eine außergewöhnliche Leistung in verschiedenen komplexen Anwendungsszenarien."
  },
  "glm-4": {
    "description": "GLM-4 ist die alte Flaggschiffversion, die im Januar 2024 veröffentlicht wurde und mittlerweile durch das leistungsstärkere GLM-4-0520 ersetzt wurde."
  },
  "glm-4-0520": {
    "description": "GLM-4-0520 ist die neueste Modellversion, die für hochkomplexe und vielfältige Aufgaben konzipiert wurde und hervorragende Leistungen zeigt."
  },
  "glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat zeigt in den Bereichen Semantik, Mathematik, Schlussfolgerungen, Code und Wissen eine hohe Leistung. Es verfügt auch über Funktionen wie Web-Browsing, Code-Ausführung, benutzerdefinierte Toolaufrufe und langes Textverständnis. Es unterstützt 26 Sprachen, darunter Japanisch, Koreanisch und Deutsch."
  },
  "glm-4-air": {
    "description": "GLM-4-Air ist eine kosteneffiziente Version, die in der Leistung nahe am GLM-4 liegt und schnelle Geschwindigkeiten zu einem erschwinglichen Preis bietet."
  },
  "glm-4-airx": {
    "description": "GLM-4-AirX bietet eine effiziente Version von GLM-4-Air mit einer Inferenzgeschwindigkeit von bis zu 2,6-fach."
  },
  "glm-4-alltools": {
    "description": "GLM-4-AllTools ist ein multifunktionales Agentenmodell, das optimiert wurde, um komplexe Anweisungsplanung und Werkzeugaufrufe zu unterstützen, wie z. B. Web-Browsing, Code-Interpretation und Textgenerierung, geeignet für die Ausführung mehrerer Aufgaben."
  },
  "glm-4-flash": {
    "description": "GLM-4-Flash ist die ideale Wahl für die Verarbeitung einfacher Aufgaben, mit der schnellsten Geschwindigkeit und dem besten Preis."
  },
  "glm-4-flashx": {
    "description": "GLM-4-FlashX ist eine verbesserte Version von Flash mit extrem schneller Inferenzgeschwindigkeit."
  },
  "glm-4-long": {
    "description": "GLM-4-Long unterstützt extrem lange Texteingaben und eignet sich für Gedächtnisaufgaben und die Verarbeitung großer Dokumente."
  },
  "glm-4-plus": {
    "description": "GLM-4-Plus ist das hochintelligente Flaggschiffmodell mit starken Fähigkeiten zur Verarbeitung langer Texte und komplexer Aufgaben, mit umfassenden Leistungsverbesserungen."
  },
  "glm-4v": {
    "description": "GLM-4V bietet starke Fähigkeiten zur Bildverständnis und -schlussfolgerung und unterstützt eine Vielzahl visueller Aufgaben."
  },
  "glm-4v-flash": {
    "description": "GLM-4V-Flash konzentriert sich auf die effiziente Verarbeitung einzelner Bilder und eignet sich für Szenarien der schnellen Bildanalyse, wie z. B. die Echtzeitanalyse von Bildern oder die Verarbeitung von Bilddaten in großen Mengen."
  },
  "glm-4v-plus": {
    "description": "GLM-4V-Plus hat die Fähigkeit, Videoinhalte und mehrere Bilder zu verstehen und eignet sich für multimodale Aufgaben."
  },
  "glm-zero-preview": {
    "description": "GLM-Zero-Preview verfügt über starke Fähigkeiten zur komplexen Schlussfolgerung und zeigt hervorragende Leistungen in den Bereichen logisches Denken, Mathematik und Programmierung."
  },
  "google/gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash bietet nächste Generation Funktionen und Verbesserungen, einschließlich außergewöhnlicher Geschwindigkeit, nativer Werkzeugnutzung, multimodaler Generierung und einem Kontextfenster von 1M Tokens."
  },
  "google/gemini-2.0-pro-exp-02-05:free": {
    "description": "Gemini 2.0 Pro Experimental ist Googles neuestes experimentelles multimodales KI-Modell, das im Vergleich zu früheren Versionen eine gewisse Qualitätsverbesserung aufweist, insbesondere in Bezug auf Weltwissen, Code und lange Kontexte."
  },
  "google/gemini-flash-1.5": {
    "description": "Gemini 1.5 Flash bietet optimierte multimodale Verarbeitungsfähigkeiten, die für verschiedene komplexe Aufgabenszenarien geeignet sind."
  },
  "google/gemini-pro-1.5": {
    "description": "Gemini 1.5 Pro kombiniert die neuesten Optimierungstechnologien und bietet eine effizientere Verarbeitung multimodaler Daten."
  },
  "google/gemma-2-27b": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexer Datenverarbeitung abdeckt."
  },
  "google/gemma-2-27b-it": {
    "description": "Gemma 2 setzt das Designkonzept von Leichtbau und Effizienz fort."
  },
  "google/gemma-2-2b-it": {
    "description": "Das leichtgewichtige Anweisungsoptimierungsmodell von Google."
  },
  "google/gemma-2-9b": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexer Datenverarbeitung abdeckt."
  },
  "google/gemma-2-9b-it": {
    "description": "Gemma 2 ist eine leichtgewichtige Open-Source-Textmodellreihe von Google."
  },
  "google/gemma-2-9b-it:free": {
    "description": "Gemma 2 ist eine leichtgewichtige Open-Source-Textmodellreihe von Google."
  },
  "google/gemma-2b-it": {
    "description": "Gemma Instruct (2B) bietet grundlegende Anweisungsverarbeitungsfähigkeiten und eignet sich für leichte Anwendungen."
  },
  "gpt-3.5-turbo": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-0125": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-1106": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-instruct": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-35-turbo": {
    "description": "GPT 3.5 Turbo ist ein effizientes Modell von OpenAI, das für Chat- und Textgenerierungsaufgaben geeignet ist und parallele Funktionsaufrufe unterstützt."
  },
  "gpt-35-turbo-16k": {
    "description": "GPT 3.5 Turbo 16k ist ein hochkapazitives Textgenerierungsmodell, das sich für komplexe Aufgaben eignet."
  },
  "gpt-4": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-0125-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-0613": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-1106-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-32k": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-32k-0613": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-turbo": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-turbo-2024-04-09": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-turbo-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-vision-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4.5-preview": {
    "description": "Die Forschungs-Vorschau von GPT-4.5, unserem bisher größten und leistungsstärksten GPT-Modell. Es verfügt über umfangreiches Weltwissen und kann die Absichten der Benutzer besser verstehen, was es in kreativen Aufgaben und autonomer Planung herausragend macht. GPT-4.5 akzeptiert Text- und Bild-Eingaben und generiert Textausgaben (einschließlich strukturierter Ausgaben). Es unterstützt wichtige Entwicklerfunktionen wie Funktionsaufrufe, Batch-APIs und Streaming-Ausgaben. In Aufgaben, die kreatives, offenes Denken und Dialog erfordern (wie Schreiben, Lernen oder das Erkunden neuer Ideen), zeigt GPT-4.5 besonders gute Leistungen. Der Wissensstand ist bis Oktober 2023."
  },
  "gpt-4o": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "gpt-4o-2024-05-13": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "gpt-4o-2024-08-06": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "gpt-4o-2024-11-20": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsbereiche, einschließlich Kundenservice, Bildung und technischen Support."
  },
  "gpt-4o-audio-preview": {
    "description": "GPT-4o Audio-Modell, unterstützt Audioeingabe und -ausgabe."
  },
  "gpt-4o-mini": {
    "description": "GPT-4o mini ist das neueste Modell von OpenAI, das nach GPT-4 Omni veröffentlicht wurde und sowohl Text- als auch Bildinput unterstützt. Als ihr fortschrittlichstes kleines Modell ist es viel günstiger als andere neueste Modelle und kostet über 60 % weniger als GPT-3.5 Turbo. Es behält die fortschrittliche Intelligenz bei und bietet gleichzeitig ein hervorragendes Preis-Leistungs-Verhältnis. GPT-4o mini erzielte 82 % im MMLU-Test und rangiert derzeit in den Chat-Präferenzen über GPT-4."
  },
  "gpt-4o-mini-realtime-preview": {
    "description": "Echtzeitversion von GPT-4o-mini, unterstützt Audio- und Texteingabe sowie -ausgabe in Echtzeit."
  },
  "gpt-4o-realtime-preview": {
    "description": "Echtzeitversion von GPT-4o, unterstützt Audio- und Texteingabe sowie -ausgabe in Echtzeit."
  },
  "gpt-4o-realtime-preview-2024-10-01": {
    "description": "Echtzeitversion von GPT-4o, unterstützt Audio- und Texteingabe sowie -ausgabe in Echtzeit."
  },
  "gpt-4o-realtime-preview-2024-12-17": {
    "description": "Echtzeitversion von GPT-4o, unterstützt Audio- und Texteingabe sowie -ausgabe in Echtzeit."
  },
  "grok-2-1212": {
    "description": "Dieses Modell hat Verbesserungen in Bezug auf Genauigkeit, Befolgung von Anweisungen und Mehrsprachigkeit erfahren."
  },
  "grok-2-vision-1212": {
    "description": "Dieses Modell hat Verbesserungen in Bezug auf Genauigkeit, Befolgung von Anweisungen und Mehrsprachigkeit erfahren."
  },
  "grok-beta": {
    "description": "Bietet eine Leistung, die mit Grok 2 vergleichbar ist, jedoch mit höherer Effizienz, Geschwindigkeit und Funktionalität."
  },
  "grok-vision-beta": {
    "description": "Das neueste Modell zur Bildverständnis, das eine Vielzahl von visuellen Informationen verarbeiten kann, einschließlich Dokumenten, Diagrammen, Screenshots und Fotos."
  },
  "gryphe/mythomax-l2-13b": {
    "description": "MythoMax l2 13B ist ein Sprachmodell, das Kreativität und Intelligenz kombiniert und mehrere führende Modelle integriert."
  },
  "hunyuan-code": {
    "description": "Das neueste Code-Generierungsmodell von Hunyuan, das auf einem Basismodell mit 200B hochwertigen Code-Daten trainiert wurde, hat ein halbes Jahr lang mit hochwertigen SFT-Daten trainiert, das Kontextfenster auf 8K erhöht und belegt in den automatischen Bewertungsmetriken für die fünf großen Programmiersprachen Spitzenplätze; in den zehn Aspekten der umfassenden Codeaufgabenbewertung für die fünf großen Sprachen liegt die Leistung in der ersten Reihe."
  },
  "hunyuan-functioncall": {
    "description": "Das neueste MOE-Architektur-FunctionCall-Modell von Hunyuan, das mit hochwertigen FunctionCall-Daten trainiert wurde, hat ein Kontextfenster von 32K und führt in mehreren Bewertungsmetriken."
  },
  "hunyuan-large": {
    "description": "Das Hunyuan-large Modell hat insgesamt etwa 389B Parameter, davon etwa 52B aktivierte Parameter, und ist das derzeit größte und leistungsstärkste Open-Source MoE-Modell mit Transformer-Architektur in der Branche."
  },
  "hunyuan-large-longcontext": {
    "description": "Besonders gut geeignet für lange Textaufgaben wie Dokumentenzusammenfassungen und Dokumentenfragen, verfügt es auch über die Fähigkeit, allgemeine Textgenerierungsaufgaben zu bearbeiten. Es zeigt hervorragende Leistungen bei der Analyse und Generierung von langen Texten und kann effektiv mit komplexen und detaillierten Anforderungen an die Verarbeitung von langen Inhalten umgehen."
  },
  "hunyuan-lite": {
    "description": "Aufgerüstet auf eine MOE-Struktur mit einem Kontextfenster von 256k, führt es in mehreren Bewertungssets in NLP, Code, Mathematik und Industrie zahlreiche Open-Source-Modelle an."
  },
  "hunyuan-lite-vision": {
    "description": "Das neueste 7B multimodale Modell von Hunyuan, mit einem Kontextfenster von 32K, unterstützt multimodale Dialoge in Chinesisch und Englisch, Objekterkennung in Bildern, Dokumenten- und Tabellenverständnis sowie multimodale Mathematik und übertrifft in mehreren Dimensionen die Bewertungskennzahlen von 7B Wettbewerbsmodellen."
  },
  "hunyuan-pro": {
    "description": "Ein MOE-32K-Modell für lange Texte mit einer Billion Parametern. Es erreicht in verschiedenen Benchmarks ein absolut führendes Niveau, hat komplexe Anweisungen und Schlussfolgerungen, verfügt über komplexe mathematische Fähigkeiten und unterstützt Funktionsaufrufe, mit Schwerpunkt auf Optimierung in den Bereichen mehrsprachige Übersetzung, Finanzrecht und Medizin."
  },
  "hunyuan-role": {
    "description": "Das neueste Rollenspielmodell von Hunyuan, das auf dem offiziellen feinabgestimmten Training von Hunyuan basiert, wurde mit einem Datensatz für Rollenspiel-Szenarien weiter trainiert und bietet in Rollenspiel-Szenarien bessere Grundeffekte."
  },
  "hunyuan-standard": {
    "description": "Verwendet eine verbesserte Routing-Strategie und mildert gleichzeitig die Probleme der Lastenverteilung und Expertenkonvergenz. Bei langen Texten erreicht der Needle-in-a-Haystack-Indikator 99,9%. MOE-32K bietet ein besseres Preis-Leistungs-Verhältnis und ermöglicht die Verarbeitung von langen Texteingaben bei ausgewogenem Effekt und Preis."
  },
  "hunyuan-standard-256K": {
    "description": "Verwendet eine verbesserte Routing-Strategie und mildert gleichzeitig die Probleme der Lastenverteilung und Expertenkonvergenz. Bei langen Texten erreicht der Needle-in-a-Haystack-Indikator 99,9%. MOE-256K bricht in Länge und Effektivität weiter durch und erweitert die eingabefähige Länge erheblich."
  },
  "hunyuan-standard-vision": {
    "description": "Das neueste multimodale Modell von Hunyuan, das mehrsprachige Antworten unterstützt und sowohl in Chinesisch als auch in Englisch ausgewogen ist."
  },
  "hunyuan-translation": {
    "description": "Unterstützt die Übersetzung zwischen Chinesisch und Englisch, Japanisch, Französisch, Portugiesisch, Spanisch, Türkisch, Russisch, Arabisch, Koreanisch, Italienisch, Deutsch, Vietnamesisch, Malaiisch und Indonesisch in 15 Sprachen. Basierend auf einem automatisierten Bewertungs-Framework COMET, das auf mehrsprachigen Übersetzungsbewertungsszenarien basiert, übertrifft es insgesamt die Übersetzungsfähigkeiten anderer Modelle ähnlicher Größe auf dem Markt."
  },
  "hunyuan-translation-lite": {
    "description": "Das Hunyuan-Übersetzungsmodell unterstützt die dialogbasierte Übersetzung in natürlicher Sprache; es unterstützt die Übersetzung zwischen Chinesisch und Englisch, Japanisch, Französisch, Portugiesisch, Spanisch, Türkisch, Russisch, Arabisch, Koreanisch, Italienisch, Deutsch, Vietnamesisch, Malaiisch und Indonesisch in 15 Sprachen."
  },
  "hunyuan-turbo": {
    "description": "Die Vorschauversion des neuen großen Sprachmodells von Hunyuan verwendet eine neuartige hybride Expertenmodellstruktur (MoE) und bietet im Vergleich zu Hunyuan-Pro eine schnellere Inferenz und bessere Leistung."
  },
  "hunyuan-turbo-20241120": {
    "description": "Hunyuan-turbo Version vom 20. November 2024, eine feste Version, die zwischen hunyuan-turbo und hunyuan-turbo-latest liegt."
  },
  "hunyuan-turbo-20241223": {
    "description": "Diese Version optimiert: Datenanweisungs-Skalierung, erhebliche Verbesserung der allgemeinen Generalisierungsfähigkeit des Modells; erhebliche Verbesserung der mathematischen, programmierbaren und logischen Denkfähigkeiten; Optimierung der Fähigkeiten im Textverständnis und der Wortverständnisfähigkeiten; Optimierung der Qualität der Inhaltserzeugung in der Texterstellung."
  },
  "hunyuan-turbo-latest": {
    "description": "Allgemeine Optimierung der Benutzererfahrung, einschließlich NLP-Verständnis, Texterstellung, Smalltalk, Wissensfragen, Übersetzung, Fachgebieten usw.; Verbesserung der Menschlichkeit, Optimierung der emotionalen Intelligenz des Modells; Verbesserung der Fähigkeit des Modells, bei unklaren Absichten aktiv Klarheit zu schaffen; Verbesserung der Bearbeitungsfähigkeit von Fragen zur Wort- und Satzanalyse; Verbesserung der Qualität und Interaktivität der Kreation; Verbesserung der Mehrfachinteraktionserfahrung."
  },
  "hunyuan-turbo-vision": {
    "description": "Das neue Flaggschiff-Modell der visuellen Sprache von Hunyuan, das eine brandneue Struktur des gemischten Expertenmodells (MoE) verwendet, bietet umfassende Verbesserungen in den Fähigkeiten zur grundlegenden Erkennung, Inhaltserstellung, Wissensfragen und Analyse sowie Schlussfolgerungen im Vergleich zum vorherigen Modell."
  },
  "hunyuan-vision": {
    "description": "Das neueste multimodale Modell von Hunyuan unterstützt die Eingabe von Bildern und Text zur Generierung von Textinhalten."
  },
  "internlm/internlm2_5-20b-chat": {
    "description": "Das innovative Open-Source-Modell InternLM2.5 hat durch eine große Anzahl von Parametern die Dialogintelligenz erhöht."
  },
  "internlm/internlm2_5-7b-chat": {
    "description": "InternLM2.5 bietet intelligente Dialoglösungen in mehreren Szenarien."
  },
  "internlm2-pro-chat": {
    "description": "Die ältere Modellversion, die wir weiterhin pflegen, bietet eine Auswahl an Modellparametern von 7B und 20B."
  },
  "internlm2.5-latest": {
    "description": "Unsere neueste Modellreihe mit herausragender Schlussfolgerungsleistung, die eine Kontextlänge von 1M unterstützt und über verbesserte Anweisungsbefolgung und Toolaufrufmöglichkeiten verfügt."
  },
  "internlm3-latest": {
    "description": "Unsere neueste Modellreihe bietet herausragende Inferenzleistungen und führt die Open-Source-Modelle in ihrer Gewichtsklasse an. Standardmäßig verweist sie auf unser neuestes veröffentlichtes InternLM3-Modell."
  },
  "jina-deepsearch-v1": {
    "description": "Die Tiefensuche kombiniert Websuche, Lesen und Schlussfolgern und ermöglicht umfassende Untersuchungen. Sie können es als einen Agenten betrachten, der Ihre Forschungsaufgaben übernimmt – er führt eine umfassende Suche durch und iteriert mehrfach, bevor er eine Antwort gibt. Dieser Prozess umfasst kontinuierliche Forschung, Schlussfolgerungen und die Lösung von Problemen aus verschiedenen Perspektiven. Dies unterscheidet sich grundlegend von den Standard-Großmodellen, die Antworten direkt aus vortrainierten Daten generieren, sowie von traditionellen RAG-Systemen, die auf einmaligen Oberflächensuchen basieren."
  },
  "kimi-latest": {
    "description": "Das Kimi intelligente Assistenzprodukt verwendet das neueste Kimi Großmodell, das möglicherweise noch instabile Funktionen enthält. Es unterstützt die Bildverarbeitung und wählt automatisch das Abrechnungsmodell 8k/32k/128k basierend auf der Länge des angeforderten Kontexts aus."
  },
  "learnlm-1.5-pro-experimental": {
    "description": "LearnLM ist ein experimentelles, aufgabenorientiertes Sprachmodell, das darauf trainiert wurde, den Prinzipien der Lernwissenschaft zu entsprechen und in Lehr- und Lernszenarien systematische Anweisungen zu befolgen, als Expertenmentor zu fungieren usw."
  },
  "lite": {
    "description": "Spark Lite ist ein leichtgewichtiges großes Sprachmodell mit extrem niedriger Latenz und effizienter Verarbeitung, das vollständig kostenlos und offen ist und Echtzeitsuchfunktionen unterstützt. Seine schnelle Reaktionsfähigkeit macht es besonders geeignet für Inferenzanwendungen und Modellanpassungen auf Geräten mit geringer Rechenleistung und bietet den Nutzern ein hervorragendes Kosten-Nutzen-Verhältnis sowie ein intelligentes Erlebnis, insbesondere in den Bereichen Wissensabfragen, Inhaltserstellung und Suchszenarien."
  },
  "llama-3.1-70b-versatile": {
    "description": "Llama 3.1 70B bietet leistungsstarke KI-Schlussfolgerungsfähigkeiten, die für komplexe Anwendungen geeignet sind und eine hohe Rechenverarbeitung bei gleichzeitiger Effizienz und Genauigkeit unterstützen."
  },
  "llama-3.1-8b-instant": {
    "description": "Llama 3.1 8B ist ein leistungsstarkes Modell, das schnelle Textgenerierungsfähigkeiten bietet und sich hervorragend für Anwendungen eignet, die große Effizienz und Kosteneffektivität erfordern."
  },
  "llama-3.2-11b-vision-instruct": {
    "description": "Überlegene Bildverarbeitungsfähigkeiten auf hochauflösenden Bildern, geeignet für visuelle Verständnisanwendungen."
  },
  "llama-3.2-11b-vision-preview": {
    "description": "Llama 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellen Fragen und Antworten und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "llama-3.2-90b-vision-instruct": {
    "description": "Erweiterte Bildverarbeitungsfähigkeiten für visuelle Verständnisagentenanwendungen."
  },
  "llama-3.2-90b-vision-preview": {
    "description": "Llama 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellen Fragen und Antworten und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "llama-3.3-70b-instruct": {
    "description": "Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das eine Leistung bietet, die mit einem 405B-Modell vergleichbar ist, und das zu extrem niedrigen Kosten. Es basiert auf der Transformer-Architektur und verbessert die Nützlichkeit und Sicherheit durch überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF). Die auf Anweisungen optimierte Version ist speziell für mehrsprachige Dialoge optimiert und übertrifft in mehreren Branchenbenchmarks viele Open-Source- und geschlossene Chat-Modelle. Das Wissensdatum endet im Dezember 2023."
  },
  "llama-3.3-70b-versatile": {
    "description": "Das Meta Llama 3.3 ist ein mehrsprachiges, großes Sprachmodell (LLM), das aus einem vortrainierten und anweisungsorientierten generativen Modell mit 70B (Text-Eingabe/Text-Ausgabe) besteht. Das anweisungsorientierte Modell von Llama 3.3 ist für mehrsprachige Dialoganwendungen optimiert und übertrifft viele verfügbare Open-Source- und Closed-Source-Chat-Modelle bei gängigen Branchenbenchmarks."
  },
  "llama3-70b-8192": {
    "description": "Meta Llama 3 70B bietet unvergleichliche Fähigkeiten zur Verarbeitung von Komplexität und ist maßgeschneidert für Projekte mit hohen Anforderungen."
  },
  "llama3-8b-8192": {
    "description": "Meta Llama 3 8B bietet hervorragende Schlussfolgerungsfähigkeiten und eignet sich für eine Vielzahl von Anwendungsanforderungen."
  },
  "llama3-groq-70b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 70B Tool Use bietet leistungsstarke Werkzeugaufruf-Fähigkeiten und unterstützt die effiziente Verarbeitung komplexer Aufgaben."
  },
  "llama3-groq-8b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 8B Tool Use ist ein Modell, das für die effiziente Nutzung von Werkzeugen optimiert ist und schnelle parallele Berechnungen unterstützt."
  },
  "llama3.1": {
    "description": "Llama 3.1 ist ein führendes Modell von Meta, das bis zu 405B Parameter unterstützt und in den Bereichen komplexe Dialoge, mehrsprachige Übersetzungen und Datenanalysen eingesetzt werden kann."
  },
  "llama3.1:405b": {
    "description": "Llama 3.1 ist ein führendes Modell von Meta, das bis zu 405B Parameter unterstützt und in den Bereichen komplexe Dialoge, mehrsprachige Übersetzungen und Datenanalysen eingesetzt werden kann."
  },
  "llama3.1:70b": {
    "description": "Llama 3.1 ist ein führendes Modell von Meta, das bis zu 405B Parameter unterstützt und in den Bereichen komplexe Dialoge, mehrsprachige Übersetzungen und Datenanalysen eingesetzt werden kann."
  },
  "llava": {
    "description": "LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und für starke visuelle und sprachliche Verständnisse sorgt."
  },
  "llava-v1.5-7b-4096-preview": {
    "description": "LLaVA 1.5 7B bietet integrierte visuelle Verarbeitungsfähigkeiten, um komplexe Ausgaben aus visuellen Informationen zu generieren."
  },
  "llava:13b": {
    "description": "LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und für starke visuelle und sprachliche Verständnisse sorgt."
  },
  "llava:34b": {
    "description": "LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und für starke visuelle und sprachliche Verständnisse sorgt."
  },
  "mathstral": {
    "description": "MathΣtral ist für wissenschaftliche Forschung und mathematische Schlussfolgerungen konzipiert und bietet effektive Rechenfähigkeiten und Ergebnisinterpretationen."
  },
  "max-32k": {
    "description": "Spark Max 32K bietet eine große Kontextverarbeitungsfähigkeit mit verbesserter Kontextverständnis und logischer Schlussfolgerungsfähigkeit und unterstützt Texteingaben von bis zu 32K Tokens, was es ideal für das Lesen langer Dokumente und private Wissensabfragen macht."
  },
  "meta-llama-3-70b-instruct": {
    "description": "Ein leistungsstarkes Modell mit 70 Milliarden Parametern, das in den Bereichen Schlussfolgerungen, Programmierung und breiten Sprachanwendungen herausragt."
  },
  "meta-llama-3-8b-instruct": {
    "description": "Ein vielseitiges Modell mit 8 Milliarden Parametern, das für Dialog- und Textgenerierungsaufgaben optimiert ist."
  },
  "meta-llama-3.1-405b-instruct": {
    "description": "Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele der verfügbaren Open-Source- und geschlossenen Chat-Modelle in gängigen Branchenbenchmarks."
  },
  "meta-llama-3.1-70b-instruct": {
    "description": "Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele der verfügbaren Open-Source- und geschlossenen Chat-Modelle in gängigen Branchenbenchmarks."
  },
  "meta-llama-3.1-8b-instruct": {
    "description": "Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele der verfügbaren Open-Source- und geschlossenen Chat-Modelle in gängigen Branchenbenchmarks."
  },
  "meta-llama/Llama-2-13b-chat-hf": {
    "description": "LLaMA-2 Chat (13B) bietet hervorragende Sprachverarbeitungsfähigkeiten und ein ausgezeichnetes Interaktionserlebnis."
  },
  "meta-llama/Llama-2-70b-hf": {
    "description": "LLaMA-2 bietet hervorragende Sprachverarbeitungsfähigkeiten und ein großartiges Interaktionserlebnis."
  },
  "meta-llama/Llama-3-70b-chat-hf": {
    "description": "LLaMA-3 Chat (70B) ist ein leistungsstarkes Chat-Modell, das komplexe Dialoganforderungen unterstützt."
  },
  "meta-llama/Llama-3-8b-chat-hf": {
    "description": "LLaMA-3 Chat (8B) bietet mehrsprachige Unterstützung und deckt ein breites Spektrum an Fachwissen ab."
  },
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bewältigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "meta-llama/Llama-3.2-3B-Instruct-Turbo": {
    "description": "LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bewältigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bewältigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "meta-llama/Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das zu extrem niedrigen Kosten eine Leistung bietet, die mit der eines 405B-Modells vergleichbar ist. Basierend auf der Transformer-Architektur und verbessert durch überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF) für Nützlichkeit und Sicherheit. Die optimierte Version für Anweisungen ist speziell für mehrsprachige Dialoge optimiert und übertrifft in mehreren Branchenbenchmarks viele Open-Source- und geschlossene Chat-Modelle. Wissensstichtag ist der 31. Dezember 2023."
  },
  "meta-llama/Llama-3.3-70B-Instruct-Turbo": {
    "description": "Das Meta Llama 3.3 mehrsprachige große Sprachmodell (LLM) ist ein vortrainiertes und anweisungsoptimiertes Generierungsmodell mit 70B (Textinput/Textoutput). Das anweisungsoptimierte reine Textmodell von Llama 3.3 wurde für mehrsprachige Dialoganwendungen optimiert und übertrifft viele verfügbare Open-Source- und geschlossene Chat-Modelle in gängigen Branchenbenchmarks."
  },
  "meta-llama/Llama-Vision-Free": {
    "description": "LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bewältigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
    "description": "Llama 3 70B Instruct Lite ist für Umgebungen geeignet, die hohe Leistung und niedrige Latenz erfordern."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": {
    "description": "Llama 3 70B Instruct Turbo bietet hervorragende Sprachverständnis- und Generierungsfähigkeiten und eignet sich für die anspruchsvollsten Rechenaufgaben."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite": {
    "description": "Llama 3 8B Instruct Lite ist für ressourcenbeschränkte Umgebungen geeignet und bietet eine hervorragende Balance zwischen Leistung und Effizienz."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": {
    "description": "Llama 3 8B Instruct Turbo ist ein leistungsstarkes großes Sprachmodell, das eine breite Palette von Anwendungsszenarien unterstützt."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "description": "LLaMA 3.1 405B ist ein leistungsstarkes Modell für Vortraining und Anweisungsanpassung."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
    "description": "Das 405B Llama 3.1 Turbo-Modell bietet eine enorme Kapazität zur Unterstützung von Kontexten für die Verarbeitung großer Datenmengen und zeigt herausragende Leistungen in groß angelegten KI-Anwendungen."
  },
  "meta-llama/Meta-Llama-3.1-70B": {
    "description": "Llama 3.1 ist das führende Modell von Meta, das bis zu 405B Parameter unterstützt und in komplexen Gesprächen, mehrsprachiger Übersetzung und Datenanalyse eingesetzt werden kann."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct": {
    "description": "LLaMA 3.1 70B bietet effiziente Dialogunterstützung in mehreren Sprachen."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
    "description": "Das Llama 3.1 70B-Modell wurde feinabgestimmt und eignet sich für hochbelastete Anwendungen, die auf FP8 quantisiert wurden, um eine effizientere Rechenleistung und Genauigkeit zu bieten und in komplexen Szenarien hervorragende Leistungen zu gewährleisten."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "description": "LLaMA 3.1 bietet Unterstützung für mehrere Sprachen und ist eines der führenden Generierungsmodelle der Branche."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
    "description": "Das Llama 3.1 8B-Modell verwendet FP8-Quantisierung und unterstützt bis zu 131.072 Kontextmarkierungen, es ist eines der besten Open-Source-Modelle, das sich für komplexe Aufgaben eignet und in vielen Branchenbenchmarks übertrifft."
  },
  "meta-llama/llama-3-70b-instruct": {
    "description": "Llama 3 70B Instruct ist optimiert für qualitativ hochwertige Dialogszenarien und zeigt hervorragende Leistungen in verschiedenen menschlichen Bewertungen."
  },
  "meta-llama/llama-3-8b-instruct": {
    "description": "Llama 3 8B Instruct optimiert qualitativ hochwertige Dialogszenarien und bietet bessere Leistungen als viele geschlossene Modelle."
  },
  "meta-llama/llama-3.1-70b-instruct": {
    "description": "Llama 3.1 70B Instruct ist speziell für qualitativ hochwertige Dialoge konzipiert und zeigt herausragende Leistungen in menschlichen Bewertungen, besonders geeignet für hochinteraktive Szenarien."
  },
  "meta-llama/llama-3.1-8b-instruct": {
    "description": "Llama 3.1 8B Instruct ist die neueste Version von Meta, optimiert für qualitativ hochwertige Dialogszenarien und übertrifft viele führende geschlossene Modelle."
  },
  "meta-llama/llama-3.1-8b-instruct:free": {
    "description": "LLaMA 3.1 bietet Unterstützung für mehrere Sprachen und gehört zu den führenden generativen Modellen der Branche."
  },
  "meta-llama/llama-3.2-11b-vision-instruct": {
    "description": "LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellem Fragen und Antworten und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "meta-llama/llama-3.2-3b-instruct": {
    "description": "meta-llama/llama-3.2-3b-instruct"
  },
  "meta-llama/llama-3.2-90b-vision-instruct": {
    "description": "LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellem Fragen und Antworten und überbrückt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."
  },
  "meta-llama/llama-3.3-70b-instruct": {
    "description": "Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das eine Leistung bietet, die mit einem 405B-Modell vergleichbar ist, und das zu extrem niedrigen Kosten. Es basiert auf der Transformer-Architektur und verbessert die Nützlichkeit und Sicherheit durch überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF). Die auf Anweisungen optimierte Version ist speziell für mehrsprachige Dialoge optimiert und übertrifft in mehreren Branchenbenchmarks viele Open-Source- und geschlossene Chat-Modelle. Das Wissensdatum endet im Dezember 2023."
  },
  "meta-llama/llama-3.3-70b-instruct:free": {
    "description": "Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das eine Leistung bietet, die mit einem 405B-Modell vergleichbar ist, und das zu extrem niedrigen Kosten. Es basiert auf der Transformer-Architektur und verbessert die Nützlichkeit und Sicherheit durch überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF). Die auf Anweisungen optimierte Version ist speziell für mehrsprachige Dialoge optimiert und übertrifft in mehreren Branchenbenchmarks viele Open-Source- und geschlossene Chat-Modelle. Das Wissensdatum endet im Dezember 2023."
  },
  "meta.llama3-1-405b-instruct-v1:0": {
    "description": "Meta Llama 3.1 405B Instruct ist das größte und leistungsstärkste Modell innerhalb des Llama 3.1 Instruct Modells. Es handelt sich um ein hochentwickeltes Modell für dialogbasierte Schlussfolgerungen und die Generierung synthetischer Daten, das auch als Grundlage für die professionelle kontinuierliche Vorab- und Feinabstimmung in bestimmten Bereichen verwendet werden kann. Die mehrsprachigen großen Sprachmodelle (LLMs) von Llama 3.1 sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, die in den Größen 8B, 70B und 405B (Text-Eingabe/Ausgabe) verfügbar sind. Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind speziell für mehrsprachige Dialoganwendungen optimiert und haben in gängigen Branchenbenchmarks viele verfügbare Open-Source-Chat-Modelle übertroffen. Llama 3.1 ist für kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich für assistentengleiche Chats, während die vortrainierten Modelle für verschiedene Aufgaben der natürlichen Sprachgenerierung angepasst werden können. Das Llama 3.1 Modell unterstützt auch die Nutzung seiner Ausgaben zur Verbesserung anderer Modelle, einschließlich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das auf einer optimierten Transformer-Architektur basiert. Die angepasste Version verwendet überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Präferenzen für Hilfsbereitschaft und Sicherheit zu entsprechen."
  },
  "meta.llama3-1-70b-instruct-v1:0": {
    "description": "Die aktualisierte Version von Meta Llama 3.1 70B Instruct umfasst eine erweiterte Kontextlänge von 128K, Mehrsprachigkeit und verbesserte Schlussfolgerungsfähigkeiten. Die von Llama 3.1 bereitgestellten mehrsprachigen großen Sprachmodelle (LLMs) sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, einschließlich Größen von 8B, 70B und 405B (Textinput/-output). Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele verfügbare Open-Source-Chat-Modelle in gängigen Branchenbenchmarks. Llama 3.1 ist für kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich für assistentengleiche Chats, während die vortrainierten Modelle für eine Vielzahl von Aufgaben der natürlichen Sprachgenerierung angepasst werden können. Llama 3.1-Modelle unterstützen auch die Nutzung ihrer Ausgaben zur Verbesserung anderer Modelle, einschließlich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das mit einer optimierten Transformer-Architektur entwickelt wurde. Die angepassten Versionen verwenden überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Präferenzen für Hilfsbereitschaft und Sicherheit zu entsprechen."
  },
  "meta.llama3-1-8b-instruct-v1:0": {
    "description": "Die aktualisierte Version von Meta Llama 3.1 8B Instruct umfasst eine erweiterte Kontextlänge von 128K, Mehrsprachigkeit und verbesserte Schlussfolgerungsfähigkeiten. Die von Llama 3.1 bereitgestellten mehrsprachigen großen Sprachmodelle (LLMs) sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, einschließlich Größen von 8B, 70B und 405B (Textinput/-output). Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele verfügbare Open-Source-Chat-Modelle in gängigen Branchenbenchmarks. Llama 3.1 ist für kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich für assistentengleiche Chats, während die vortrainierten Modelle für eine Vielzahl von Aufgaben der natürlichen Sprachgenerierung angepasst werden können. Llama 3.1-Modelle unterstützen auch die Nutzung ihrer Ausgaben zur Verbesserung anderer Modelle, einschließlich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das mit einer optimierten Transformer-Architektur entwickelt wurde. Die angepassten Versionen verwenden überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Präferenzen für Hilfsbereitschaft und Sicherheit zu entsprechen."
  },
  "meta.llama3-70b-instruct-v1:0": {
    "description": "Meta Llama 3 ist ein offenes großes Sprachmodell (LLM), das sich an Entwickler, Forscher und Unternehmen richtet und ihnen hilft, ihre Ideen für generative KI zu entwickeln, zu experimentieren und verantwortungsbewusst zu skalieren. Als Teil eines globalen Innovationssystems ist es besonders geeignet für die Erstellung von Inhalten, Dialog-KI, Sprachverständnis, Forschung und Unternehmensanwendungen."
  },
  "meta.llama3-8b-instruct-v1:0": {
    "description": "Meta Llama 3 ist ein offenes großes Sprachmodell (LLM), das sich an Entwickler, Forscher und Unternehmen richtet und ihnen hilft, ihre Ideen für generative KI zu entwickeln, zu experimentieren und verantwortungsbewusst zu skalieren. Als Teil eines globalen Innovationssystems ist es besonders geeignet für Umgebungen mit begrenzter Rechenleistung und Ressourcen, für Edge-Geräte und schnellere Trainingszeiten."
  },
  "meta/llama-3.1-405b-instruct": {
    "description": "Fortgeschrittenes LLM, das die Generierung synthetischer Daten, Wissensverdichtung und Schlussfolgerungen unterstützt, geeignet für Chatbots, Programmierung und spezifische Aufgaben."
  },
  "meta/llama-3.1-70b-instruct": {
    "description": "Ermöglicht komplexe Gespräche mit hervorragendem Kontextverständnis, Schlussfolgerungsfähigkeiten und Textgenerierungsfähigkeiten."
  },
  "meta/llama-3.1-8b-instruct": {
    "description": "Fortschrittliches, hochmodernes Modell mit Sprachverständnis, hervorragenden Schlussfolgerungsfähigkeiten und Textgenerierungsfähigkeiten."
  },
  "meta/llama-3.2-11b-vision-instruct": {
    "description": "Spitzenmäßiges visuelles Sprachmodell, das in der Lage ist, qualitativ hochwertige Schlussfolgerungen aus Bildern zu ziehen."
  },
  "meta/llama-3.2-1b-instruct": {
    "description": "Fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverständnis, hervorragenden Schlussfolgerungsfähigkeiten und Textgenerierungsfähigkeiten."
  },
  "meta/llama-3.2-3b-instruct": {
    "description": "Fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverständnis, hervorragenden Schlussfolgerungsfähigkeiten und Textgenerierungsfähigkeiten."
  },
  "meta/llama-3.2-90b-vision-instruct": {
    "description": "Spitzenmäßiges visuelles Sprachmodell, das in der Lage ist, qualitativ hochwertige Schlussfolgerungen aus Bildern zu ziehen."
  },
  "meta/llama-3.3-70b-instruct": {
    "description": "Fortschrittliches LLM, das auf Schlussfolgern, Mathematik, Allgemeinwissen und Funktionsaufrufen spezialisiert ist."
  },
  "microsoft/WizardLM-2-8x22B": {
    "description": "WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, Mehrsprachigkeit, Inferenz und intelligenten Assistenten besonders gut abschneidet."
  },
  "microsoft/wizardlm-2-8x22b": {
    "description": "WizardLM-2 8x22B ist das fortschrittlichste Wizard-Modell von Microsoft AI und zeigt äußerst wettbewerbsfähige Leistungen."
  },
  "minicpm-v": {
    "description": "MiniCPM-V ist das neue multimodale Großmodell von OpenBMB, das über hervorragende OCR-Erkennungs- und multimodale Verständnisfähigkeiten verfügt und eine Vielzahl von Anwendungsszenarien unterstützt."
  },
  "ministral-3b-latest": {
    "description": "Ministral 3B ist das weltbeste Edge-Modell von Mistral."
  },
  "ministral-8b-latest": {
    "description": "Ministral 8B ist das kosteneffizienteste Edge-Modell von Mistral."
  },
  "mistral": {
    "description": "Mistral ist ein 7B-Modell von Mistral AI, das sich für vielfältige Anforderungen an die Sprachverarbeitung eignet."
  },
  "mistral-large": {
    "description": "Mixtral Large ist das Flaggschiff-Modell von Mistral, das die Fähigkeiten zur Codegenerierung, Mathematik und Schlussfolgerungen kombiniert und ein Kontextfenster von 128k unterstützt."
  },
  "mistral-large-latest": {
    "description": "Mistral Large ist das Flaggschiff-Modell, das sich gut für mehrsprachige Aufgaben, komplexe Schlussfolgerungen und Codegenerierung eignet und die ideale Wahl für hochentwickelte Anwendungen ist."
  },
  "mistral-nemo": {
    "description": "Mistral Nemo wurde in Zusammenarbeit mit Mistral AI und NVIDIA entwickelt und ist ein leistungsstarkes 12B-Modell."
  },
  "mistral-small": {
    "description": "Mistral Small kann für jede sprachbasierte Aufgabe verwendet werden, die hohe Effizienz und geringe Latenz erfordert."
  },
  "mistral-small-latest": {
    "description": "Mistral Small ist eine kosteneffiziente, schnelle und zuverlässige Option für Anwendungsfälle wie Übersetzung, Zusammenfassung und Sentimentanalyse."
  },
  "mistralai/Mistral-7B-Instruct-v0.1": {
    "description": "Mistral (7B) Instruct ist bekannt für seine hohe Leistung und eignet sich für eine Vielzahl von Sprachaufgaben."
  },
  "mistralai/Mistral-7B-Instruct-v0.2": {
    "description": "Mistral 7B ist ein nach Bedarf feinabgestimmtes Modell, das optimierte Antworten auf Aufgaben bietet."
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "description": "Mistral (7B) Instruct v0.3 bietet effiziente Rechenleistung und natürliche Sprachverständnisfähigkeiten und eignet sich für eine Vielzahl von Anwendungen."
  },
  "mistralai/Mistral-7B-v0.1": {
    "description": "Mistral 7B ist ein kompaktes, aber leistungsstarkes Modell, das gut für Batch-Verarbeitung und einfache Aufgaben wie Klassifizierung und Textgenerierung geeignet ist und über gute Schlussfolgerungsfähigkeiten verfügt."
  },
  "mistralai/Mixtral-8x22B-Instruct-v0.1": {
    "description": "Mixtral-8x22B Instruct (141B) ist ein super großes Sprachmodell, das extrem hohe Verarbeitungsanforderungen unterstützt."
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "description": "Mixtral 8x7B ist ein vortrainiertes sparsames Mischmodell, das für allgemeine Textaufgaben verwendet wird."
  },
  "mistralai/Mixtral-8x7B-v0.1": {
    "description": "Mixtral 8x7B ist ein sparsames Expertenmodell, das mehrere Parameter nutzt, um die Schlussfolgerungsgeschwindigkeit zu erhöhen, und sich gut für mehrsprachige und Code-Generierungsaufgaben eignet."
  },
  "mistralai/mistral-7b-instruct": {
    "description": "Mistral 7B Instruct ist ein hochleistungsfähiges Branchenstandardmodell mit Geschwindigkeitsoptimierung und Unterstützung für lange Kontexte."
  },
  "mistralai/mistral-nemo": {
    "description": "Mistral Nemo ist ein 7,3B-Parameter-Modell mit Unterstützung für mehrere Sprachen und hoher Programmierleistung."
  },
  "mixtral": {
    "description": "Mixtral ist das Expertenmodell von Mistral AI, das über Open-Source-Gewichte verfügt und Unterstützung bei der Codegenerierung und Sprachverständnis bietet."
  },
  "mixtral-8x7b-32768": {
    "description": "Mixtral 8x7B bietet hochgradig fehlertolerante parallele Berechnungsfähigkeiten und eignet sich für komplexe Aufgaben."
  },
  "mixtral:8x22b": {
    "description": "Mixtral ist das Expertenmodell von Mistral AI, das über Open-Source-Gewichte verfügt und Unterstützung bei der Codegenerierung und Sprachverständnis bietet."
  },
  "moonshot-v1-128k": {
    "description": "Moonshot V1 128K ist ein Modell mit überragenden Fähigkeiten zur Verarbeitung von langen Kontexten, das für die Generierung von sehr langen Texten geeignet ist und die Anforderungen komplexer Generierungsaufgaben erfüllt. Es kann Inhalte mit bis zu 128.000 Tokens verarbeiten und eignet sich hervorragend für Anwendungen in der Forschung, Wissenschaft und der Erstellung großer Dokumente."
  },
  "moonshot-v1-128k-vision-preview": {
    "description": "Das Kimi-Visionsmodell (einschließlich moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview usw.) kann Bildinhalte verstehen, einschließlich Bildtext, Bildfarbe und Objektformen."
  },
  "moonshot-v1-32k": {
    "description": "Moonshot V1 32K bietet die Fähigkeit zur Verarbeitung von mittellangen Kontexten und kann 32.768 Tokens verarbeiten, was es besonders geeignet für die Generierung verschiedener langer Dokumente und komplexer Dialoge macht, die in den Bereichen Inhaltserstellung, Berichtsgenerierung und Dialogsysteme eingesetzt werden."
  },
  "moonshot-v1-32k-vision-preview": {
    "description": "Das Kimi-Visionsmodell (einschließlich moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview usw.) kann Bildinhalte verstehen, einschließlich Bildtext, Bildfarbe und Objektformen."
  },
  "moonshot-v1-8k": {
    "description": "Moonshot V1 8K ist für die Generierung von Kurztextaufgaben konzipiert und bietet eine effiziente Verarbeitungsleistung, die 8.192 Tokens verarbeiten kann. Es eignet sich hervorragend für kurze Dialoge, Notizen und schnelle Inhaltserstellung."
  },
  "moonshot-v1-8k-vision-preview": {
    "description": "Das Kimi-Visionsmodell (einschließlich moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview usw.) kann Bildinhalte verstehen, einschließlich Bildtext, Bildfarbe und Objektformen."
  },
  "moonshot-v1-auto": {
    "description": "Moonshot V1 Auto kann basierend auf der Anzahl der im aktuellen Kontext verwendeten Tokens das geeignete Modell auswählen."
  },
  "nousresearch/hermes-2-pro-llama-3-8b": {
    "description": "Hermes 2 Pro Llama 3 8B ist die aktualisierte Version von Nous Hermes 2 und enthält die neuesten intern entwickelten Datensätze."
  },
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
    "description": "Llama 3.1 Nemotron 70B ist ein von NVIDIA maßgeschneidertes großes Sprachmodell, das darauf abzielt, die Hilfsfähigkeit der von LLM generierten Antworten auf Benutzeranfragen zu verbessern. Dieses Modell hat in Benchmark-Tests wie Arena Hard, AlpacaEval 2 LC und GPT-4-Turbo MT-Bench hervorragende Leistungen gezeigt und belegt bis zum 1. Oktober 2024 den ersten Platz in allen drei automatischen Ausrichtungsbenchmarks. Das Modell wurde mit RLHF (insbesondere REINFORCE), Llama-3.1-Nemotron-70B-Reward und HelpSteer2-Preference-Prompts auf dem Llama-3.1-70B-Instruct-Modell trainiert."
  },
  "nvidia/llama-3.1-nemotron-51b-instruct": {
    "description": "Einzigartiges Sprachmodell, das unvergleichliche Genauigkeit und Effizienz bietet."
  },
  "nvidia/llama-3.1-nemotron-70b-instruct": {
    "description": "Llama-3.1-Nemotron-70B-Instruct ist ein von NVIDIA maßgeschneidertes großes Sprachmodell, das darauf abzielt, die Hilfsbereitschaft der von LLM generierten Antworten zu verbessern."
  },
  "o1": {
    "description": "Konzentriert sich auf fortgeschrittene Inferenz und die Lösung komplexer Probleme, einschließlich mathematischer und wissenschaftlicher Aufgaben. Besonders geeignet für Anwendungen, die ein tiefes Verständnis des Kontexts und die Abwicklung von Arbeitsabläufen erfordern."
  },
  "o1-mini": {
    "description": "o1-mini ist ein schnelles und kosteneffizientes Inferenzmodell, das für Programmier-, Mathematik- und Wissenschaftsanwendungen entwickelt wurde. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "o1-preview": {
    "description": "o1 ist OpenAIs neues Inferenzmodell, das für komplexe Aufgaben geeignet ist, die umfangreiches Allgemeinwissen erfordern. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "o3-mini": {
    "description": "o3-mini ist unser neuestes kompaktes Inferenzmodell, das bei den gleichen Kosten- und Verzögerungszielen wie o1-mini hohe Intelligenz bietet."
  },
  "open-codestral-mamba": {
    "description": "Codestral Mamba ist ein auf die Codegenerierung spezialisiertes Mamba 2-Sprachmodell, das starke Unterstützung für fortschrittliche Code- und Schlussfolgerungsaufgaben bietet."
  },
  "open-mistral-7b": {
    "description": "Mistral 7B ist ein kompaktes, aber leistungsstarkes Modell, das sich gut für Batch-Verarbeitung und einfache Aufgaben wie Klassifizierung und Textgenerierung eignet und über gute Schlussfolgerungsfähigkeiten verfügt."
  },
  "open-mistral-nemo": {
    "description": "Mistral Nemo ist ein 12B-Modell, das in Zusammenarbeit mit Nvidia entwickelt wurde und hervorragende Schlussfolgerungs- und Codierungsfähigkeiten bietet, die leicht zu integrieren und zu ersetzen sind."
  },
  "open-mixtral-8x22b": {
    "description": "Mixtral 8x22B ist ein größeres Expertenmodell, das sich auf komplexe Aufgaben konzentriert und hervorragende Schlussfolgerungsfähigkeiten sowie eine höhere Durchsatzrate bietet."
  },
  "open-mixtral-8x7b": {
    "description": "Mixtral 8x7B ist ein spärliches Expertenmodell, das mehrere Parameter nutzt, um die Schlussfolgerungsgeschwindigkeit zu erhöhen und sich für die Verarbeitung mehrsprachiger und Codegenerierungsaufgaben eignet."
  },
  "openai/gpt-4o": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technischem Support."
  },
  "openai/gpt-4o-mini": {
    "description": "GPT-4o mini ist das neueste Modell von OpenAI, das nach GPT-4 Omni veröffentlicht wurde und Text- und Bild-Eingaben unterstützt. Als ihr fortschrittlichstes kleines Modell ist es viel günstiger als andere neueste Modelle und über 60 % günstiger als GPT-3.5 Turbo. Es behält die fortschrittlichste Intelligenz bei und bietet gleichzeitig ein hervorragendes Preis-Leistungs-Verhältnis. GPT-4o mini erzielte 82 % im MMLU-Test und rangiert derzeit in den Chat-Präferenzen über GPT-4."
  },
  "openai/o1-mini": {
    "description": "o1-mini ist ein schnelles und kosteneffizientes Inferenzmodell, das für Programmier-, Mathematik- und Wissenschaftsanwendungen entwickelt wurde. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "openai/o1-preview": {
    "description": "o1 ist OpenAIs neues Inferenzmodell, das für komplexe Aufgaben geeignet ist, die umfangreiches Allgemeinwissen erfordern. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "openchat/openchat-7b": {
    "description": "OpenChat 7B ist eine Open-Source-Sprachmodellbibliothek, die mit der Strategie „C-RLFT (Conditional Reinforcement Learning Fine-Tuning)“ optimiert wurde."
  },
  "openrouter/auto": {
    "description": "Je nach Kontextlänge, Thema und Komplexität wird Ihre Anfrage an Llama 3 70B Instruct, Claude 3.5 Sonnet (selbstregulierend) oder GPT-4o gesendet."
  },
  "phi3": {
    "description": "Phi-3 ist ein leichtgewichtiges offenes Modell von Microsoft, das für effiziente Integration und großangelegte Wissensschlüsse geeignet ist."
  },
  "phi3:14b": {
    "description": "Phi-3 ist ein leichtgewichtiges offenes Modell von Microsoft, das für effiziente Integration und großangelegte Wissensschlüsse geeignet ist."
  },
  "pixtral-12b-2409": {
    "description": "Das Pixtral-Modell zeigt starke Fähigkeiten in Aufgaben wie Diagramm- und Bildverständnis, Dokumentenfragen, multimodale Schlussfolgerungen und Befolgung von Anweisungen. Es kann Bilder in natürlicher Auflösung und Seitenverhältnis aufnehmen und in einem langen Kontextfenster von bis zu 128K Tokens beliebig viele Bilder verarbeiten."
  },
  "pixtral-large-latest": {
    "description": "Pixtral Large ist ein Open-Source-Multimodalmodell mit 124 Milliarden Parametern, das auf Mistral Large 2 basiert. Dies ist unser zweites Modell in der multimodalen Familie und zeigt fortschrittliche Fähigkeiten im Bereich der Bildverständnis."
  },
  "pro-128k": {
    "description": "Spark Pro 128K verfügt über eine außergewöhnliche Kontextverarbeitungsfähigkeit und kann bis zu 128K Kontextinformationen verarbeiten, was es besonders geeignet für die Analyse langer Texte und die Verarbeitung langfristiger logischer Zusammenhänge macht. Es bietet in komplexen Textkommunikationen flüssige und konsistente Logik sowie vielfältige Unterstützung für Zitate."
  },
  "qvq-72b-preview": {
    "description": "Das QVQ-Modell ist ein experimentelles Forschungsmodell, das vom Qwen-Team entwickelt wurde und sich auf die Verbesserung der visuellen Schlussfolgerungsfähigkeiten konzentriert, insbesondere im Bereich der mathematischen Schlussfolgerungen."
  },
  "qwen-coder-plus-latest": {
    "description": "Tongyi Qianwen Code-Modell."
  },
  "qwen-coder-turbo-latest": {
    "description": "Das Tongyi Qianwen Code-Modell."
  },
  "qwen-long": {
    "description": "Qwen ist ein groß angelegtes Sprachmodell, das lange Textkontexte unterstützt und Dialogfunktionen für verschiedene Szenarien wie lange Dokumente und mehrere Dokumente bietet."
  },
  "qwen-math-plus-latest": {
    "description": "Das Tongyi Qianwen Mathematikmodell ist speziell für die Lösung von mathematischen Problemen konzipiert."
  },
  "qwen-math-turbo-latest": {
    "description": "Das Tongyi Qianwen Mathematikmodell ist speziell für die Lösung von mathematischen Problemen konzipiert."
  },
  "qwen-max": {
    "description": "Qwen Max ist ein großangelegtes Sprachmodell auf Billionenebene, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt und das API-Modell hinter der aktuellen Produktversion von Qwen 2.5 ist."
  },
  "qwen-max-latest": {
    "description": "Der Tongyi Qianwen ist ein Sprachmodell mit einem Umfang von mehreren Billionen, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt und die API-Modelle hinter der aktuellen Version 2.5 von Tongyi Qianwen darstellt."
  },
  "qwen-omni-turbo-latest": {
    "description": "Die Qwen-Omni-Serie unterstützt die Eingabe von Daten in verschiedenen Modalitäten, einschließlich Video, Audio, Bilder und Text, und gibt Audio und Text aus."
  },
  "qwen-plus": {
    "description": "Qwen Plus ist die verbesserte Version des großangelegten Sprachmodells, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt."
  },
  "qwen-plus-latest": {
    "description": "Der Tongyi Qianwen ist die erweiterte Version eines groß angelegten Sprachmodells, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt."
  },
  "qwen-turbo": {
    "description": "Qwen Turbo ist ein großangelegtes Sprachmodell, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt."
  },
  "qwen-turbo-latest": {
    "description": "Der Tongyi Qianwen ist ein groß angelegtes Sprachmodell, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt."
  },
  "qwen-vl-chat-v1": {
    "description": "Qwen VL unterstützt flexible Interaktionsmethoden, einschließlich Mehrbild-, Mehrfachfragen und kreativen Fähigkeiten."
  },
  "qwen-vl-max-latest": {
    "description": "Das Tongyi Qianwen Ultra-Scale Visuelle Sprachmodell. Im Vergleich zur verbesserten Version wurden die Fähigkeiten zur visuellen Schlussfolgerung und Befolgung von Anweisungen weiter gesteigert, was ein höheres Niveau an visueller Wahrnehmung und Kognition bietet."
  },
  "qwen-vl-ocr-latest": {
    "description": "Tongyi Qianwen OCR ist ein spezialisiertes Modell zur Textextraktion, das sich auf die Textextraktionsfähigkeiten von Dokumenten, Tabellen, Prüfungsfragen und handschriftlichen Texten konzentriert. Es kann verschiedene Schriftarten erkennen und unterstützt derzeit folgende Sprachen: Chinesisch, Englisch, Französisch, Japanisch, Koreanisch, Deutsch, Russisch, Italienisch, Vietnamesisch und Arabisch."
  },
  "qwen-vl-plus-latest": {
    "description": "Die verbesserte Version des Tongyi Qianwen, eines großangelegten visuellen Sprachmodells. Deutlich verbesserte Fähigkeiten zur Detailerkennung und Texterkennung, unterstützt Bildauflösungen von über einer Million Pixel und beliebige Seitenverhältnisse."
  },
  "qwen-vl-v1": {
    "description": "Initiiert mit dem Qwen-7B-Sprachmodell, fügt es ein Bildmodell hinzu, das für Bildeingaben mit einer Auflösung von 448 vortrainiert wurde."
  },
  "qwen/qwen-2-7b-instruct": {
    "description": "Qwen2 ist die brandneue Serie von großen Sprachmodellen von Qwen. Qwen2 7B ist ein transformerbasiertes Modell, das in den Bereichen Sprachverständnis, Mehrsprachigkeit, Programmierung, Mathematik und logisches Denken hervorragende Leistungen zeigt."
  },
  "qwen/qwen-2-7b-instruct:free": {
    "description": "Qwen2 ist eine neue Serie großer Sprachmodelle mit stärkeren Verständnis- und Generierungsfähigkeiten."
  },
  "qwen/qwen-2-vl-72b-instruct": {
    "description": "Qwen2-VL ist die neueste Iteration des Qwen-VL-Modells und hat in Benchmark-Tests zur visuellen Verständlichkeit eine fortschrittliche Leistung erreicht, einschließlich MathVista, DocVQA, RealWorldQA und MTVQA. Qwen2-VL kann über 20 Minuten Video verstehen und ermöglicht qualitativ hochwertige, videobasierte Fragen und Antworten, Dialoge und Inhaltserstellung. Es verfügt auch über komplexe Denk- und Entscheidungsfähigkeiten und kann mit mobilen Geräten, Robotern usw. integriert werden, um basierend auf visuellen Umgebungen und Textanweisungen automatisch zu agieren. Neben Englisch und Chinesisch unterstützt Qwen2-VL jetzt auch das Verständnis von Text in Bildern in verschiedenen Sprachen, einschließlich der meisten europäischen Sprachen, Japanisch, Koreanisch, Arabisch und Vietnamesisch."
  },
  "qwen/qwen-2.5-72b-instruct": {
    "description": "Qwen2.5-72B-Instruct ist eines der neuesten großen Sprachmodell-Serien, die von Alibaba Cloud veröffentlicht wurden. Dieses 72B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterstützung und deckt über 29 Sprachen ab, einschließlich Chinesisch und Englisch. Das Modell hat signifikante Verbesserungen in der Befolgung von Anweisungen, im Verständnis von strukturierten Daten und in der Generierung von strukturierten Ausgaben (insbesondere JSON) erzielt."
  },
  "qwen/qwen2.5-32b-instruct": {
    "description": "Qwen2.5-32B-Instruct ist eines der neuesten großen Sprachmodell-Serien, die von Alibaba Cloud veröffentlicht wurden. Dieses 32B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterstützung und deckt über 29 Sprachen ab, einschließlich Chinesisch und Englisch. Das Modell hat signifikante Verbesserungen in der Befolgung von Anweisungen, im Verständnis von strukturierten Daten und in der Generierung von strukturierten Ausgaben (insbesondere JSON) erzielt."
  },
  "qwen/qwen2.5-7b-instruct": {
    "description": "LLM, das auf Chinesisch und Englisch ausgerichtet ist und sich auf Sprache, Programmierung, Mathematik, Schlussfolgern und andere Bereiche konzentriert."
  },
  "qwen/qwen2.5-coder-32b-instruct": {
    "description": "Fortgeschrittenes LLM, das die Codegenerierung, Schlussfolgerungen und Korrekturen unterstützt und gängige Programmiersprachen abdeckt."
  },
  "qwen/qwen2.5-coder-7b-instruct": {
    "description": "Leistungsstarkes, mittelgroßes Codierungsmodell, das 32K Kontextlängen unterstützt und in der mehrsprachigen Programmierung versiert ist."
  },
  "qwen2": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "qwen2.5": {
    "description": "Qwen2.5 ist das neue, groß angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterstützung vielfältiger Anwendungsbedürfnisse bietet."
  },
  "qwen2.5-14b-instruct": {
    "description": "Das 14B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-14b-instruct-1m": {
    "description": "Tongyi Qianwen 2.5 ist ein Open-Source-Modell mit einer Größe von 72B."
  },
  "qwen2.5-32b-instruct": {
    "description": "Das 32B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-72b-instruct": {
    "description": "Das 72B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-7b-instruct": {
    "description": "Das 7B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-coder-1.5b-instruct": {
    "description": "Die Open-Source-Version des Qwen-Codemodells."
  },
  "qwen2.5-coder-32b-instruct": {
    "description": "Open-Source-Version des Tongyi Qianwen Code-Modells."
  },
  "qwen2.5-coder-7b-instruct": {
    "description": "Die Open-Source-Version des Tongyi Qianwen Code-Modells."
  },
  "qwen2.5-math-1.5b-instruct": {
    "description": "Das Qwen-Math-Modell verfügt über starke Fähigkeiten zur Lösung mathematischer Probleme."
  },
  "qwen2.5-math-72b-instruct": {
    "description": "Das Qwen-Math-Modell verfügt über starke Fähigkeiten zur Lösung mathematischer Probleme."
  },
  "qwen2.5-math-7b-instruct": {
    "description": "Das Qwen-Math-Modell verfügt über starke Fähigkeiten zur Lösung mathematischer Probleme."
  },
  "qwen2.5-vl-72b-instruct": {
    "description": "Verbesserte Befolgung von Anweisungen, Mathematik, Problemlösung und Programmierung, gesteigerte Erkennungsfähigkeiten für alle Arten von visuellen Elementen, Unterstützung für die präzise Lokalisierung visueller Elemente in verschiedenen Formaten, Verständnis von langen Videodateien (maximal 10 Minuten) und sekundengenauer Ereigniszeitpunktlokalisierung, Fähigkeit zur zeitlichen Einordnung und Geschwindigkeitsverständnis, Unterstützung für die Steuerung von OS- oder Mobile-Agenten basierend auf Analyse- und Lokalisierungsfähigkeiten, starke Fähigkeit zur Extraktion von Schlüsselinformationen und JSON-Format-Ausgabe. Diese Version ist die leistungsstärkste Version der 72B-Serie."
  },
  "qwen2.5-vl-7b-instruct": {
    "description": "Verbesserte Befolgung von Anweisungen, Mathematik, Problemlösung und Programmierung, gesteigerte Erkennungsfähigkeiten für alle Arten von visuellen Elementen, Unterstützung für die präzise Lokalisierung visueller Elemente in verschiedenen Formaten, Verständnis von langen Videodateien (maximal 10 Minuten) und sekundengenauer Ereigniszeitpunktlokalisierung, Fähigkeit zur zeitlichen Einordnung und Geschwindigkeitsverständnis, Unterstützung für die Steuerung von OS- oder Mobile-Agenten basierend auf Analyse- und Lokalisierungsfähigkeiten, starke Fähigkeit zur Extraktion von Schlüsselinformationen und JSON-Format-Ausgabe. Diese Version ist die leistungsstärkste Version der 72B-Serie."
  },
  "qwen2.5:0.5b": {
    "description": "Qwen2.5 ist das neue, groß angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterstützung vielfältiger Anwendungsbedürfnisse bietet."
  },
  "qwen2.5:1.5b": {
    "description": "Qwen2.5 ist das neue, groß angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterstützung vielfältiger Anwendungsbedürfnisse bietet."
  },
  "qwen2.5:72b": {
    "description": "Qwen2.5 ist das neue, groß angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterstützung vielfältiger Anwendungsbedürfnisse bietet."
  },
  "qwen2:0.5b": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "qwen2:1.5b": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "qwen2:72b": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "qwq": {
    "description": "QwQ ist ein experimentelles Forschungsmodell, das sich auf die Verbesserung der KI-Inferenzfähigkeiten konzentriert."
  },
  "qwq-32b": {
    "description": "Das QwQ-Inferenzmodell, das auf dem Qwen2.5-32B-Modell trainiert wurde, hat durch verstärktes Lernen die Inferenzfähigkeiten des Modells erheblich verbessert. Die Kernmetriken des Modells, wie mathematische Codes (AIME 24/25, LiveCodeBench) sowie einige allgemeine Metriken (IFEval, LiveBench usw.), erreichen das Niveau der DeepSeek-R1 Vollversion, wobei alle Metriken deutlich die ebenfalls auf Qwen2.5-32B basierende DeepSeek-R1-Distill-Qwen-32B übertreffen."
  },
  "qwq-32b-preview": {
    "description": "Das QwQ-Modell ist ein experimentelles Forschungsmodell, das vom Qwen-Team entwickelt wurde und sich auf die Verbesserung der KI-Inferenzfähigkeiten konzentriert."
  },
  "qwq-plus-latest": {
    "description": "Das QwQ-Inferenzmodell, das auf dem Qwen2.5-Modell trainiert wurde, hat durch verstärktes Lernen die Inferenzfähigkeiten des Modells erheblich verbessert. Die Kernmetriken des Modells, wie mathematische Codes (AIME 24/25, LiveCodeBench) sowie einige allgemeine Metriken (IFEval, LiveBench usw.), erreichen das Niveau der DeepSeek-R1 Vollversion."
  },
  "r1-1776": {
    "description": "R1-1776 ist eine Version des DeepSeek R1 Modells, die nachtrainiert wurde, um unverfälschte, unvoreingenommene Fakteninformationen bereitzustellen."
  },
  "solar-mini": {
    "description": "Solar Mini ist ein kompaktes LLM, das besser abschneidet als GPT-3.5 und über starke Mehrsprachigkeitsfähigkeiten verfügt. Es unterstützt Englisch und Koreanisch und bietet eine effiziente und kompakte Lösung."
  },
  "solar-mini-ja": {
    "description": "Solar Mini (Ja) erweitert die Fähigkeiten von Solar Mini und konzentriert sich auf Japanisch, während es gleichzeitig in der Nutzung von Englisch und Koreanisch hohe Effizienz und hervorragende Leistung beibehält."
  },
  "solar-pro": {
    "description": "Solar Pro ist ein hochintelligentes LLM, das von Upstage entwickelt wurde und sich auf die Befolgung von Anweisungen mit einer einzigen GPU konzentriert, mit einem IFEval-Score von über 80. Derzeit unterstützt es Englisch, die offizielle Version ist für November 2024 geplant und wird die Sprachunterstützung und Kontextlänge erweitern."
  },
  "sonar": {
    "description": "Ein leichtgewichtiges Suchprodukt, das auf kontextbezogener Suche basiert und schneller und günstiger ist als Sonar Pro."
  },
  "sonar-deep-research": {
    "description": "Deep Research führt umfassende Expertenforschung durch und fasst diese in zugänglichen, umsetzbaren Berichten zusammen."
  },
  "sonar-pro": {
    "description": "Ein fortschrittliches Suchprodukt, das kontextbezogene Suche unterstützt und erweiterte Abfragen sowie Nachverfolgung ermöglicht."
  },
  "sonar-reasoning": {
    "description": "Ein neues API-Produkt, das von DeepSeek-Inferenzmodellen unterstützt wird."
  },
  "sonar-reasoning-pro": {
    "description": "Ein neues API-Produkt, das von dem DeepSeek-Inferenzmodell unterstützt wird."
  },
  "step-1-128k": {
    "description": "Bietet ein ausgewogenes Verhältnis zwischen Leistung und Kosten, geeignet für allgemeine Szenarien."
  },
  "step-1-256k": {
    "description": "Verfügt über die Fähigkeit zur Verarbeitung ultra-langer Kontexte, besonders geeignet für die Analyse langer Dokumente."
  },
  "step-1-32k": {
    "description": "Unterstützt mittellange Dialoge und eignet sich für verschiedene Anwendungsszenarien."
  },
  "step-1-8k": {
    "description": "Kleinmodell, geeignet für leichte Aufgaben."
  },
  "step-1-flash": {
    "description": "Hochgeschwindigkeitsmodell, geeignet für Echtzeitdialoge."
  },
  "step-1.5v-mini": {
    "description": "Dieses Modell verfügt über starke Fähigkeiten zur Videoanalyse."
  },
  "step-1o-turbo-vision": {
    "description": "Dieses Modell verfügt über starke Fähigkeiten zur Bildverständnis und übertrifft 1o in den Bereichen Mathematik und Programmierung. Das Modell ist kleiner als 1o und bietet eine schnellere Ausgabegeschwindigkeit."
  },
  "step-1o-vision-32k": {
    "description": "Dieses Modell verfügt über starke Fähigkeiten zur Bildverständnis. Im Vergleich zu den Modellen der Schritt-1v-Serie bietet es eine verbesserte visuelle Leistung."
  },
  "step-1v-32k": {
    "description": "Unterstützt visuelle Eingaben und verbessert die multimodale Interaktionserfahrung."
  },
  "step-1v-8k": {
    "description": "Kleinvisualmodell, geeignet für grundlegende Text- und Bildaufgaben."
  },
  "step-2-16k": {
    "description": "Unterstützt groß angelegte Kontextinteraktionen und eignet sich für komplexe Dialogszenarien."
  },
  "step-2-mini": {
    "description": "Ein ultraschnelles Großmodell, das auf der neuen, selbstentwickelten Attention-Architektur MFA basiert. Es erreicht mit extrem niedrigen Kosten ähnliche Ergebnisse wie Schritt 1 und bietet gleichzeitig eine höhere Durchsatzrate und schnellere Reaktionszeiten. Es kann allgemeine Aufgaben bearbeiten und hat besondere Fähigkeiten im Bereich der Codierung."
  },
  "taichu_llm": {
    "description": "Das Zīdōng Taichu Sprachmodell verfügt über außergewöhnliche Sprachverständnisfähigkeiten sowie Fähigkeiten in Textgenerierung, Wissensabfrage, Programmierung, mathematischen Berechnungen, logischem Denken, Sentimentanalyse und Textzusammenfassung. Es kombiniert innovativ große Datenvortrainings mit reichhaltigem Wissen aus mehreren Quellen, verfeinert kontinuierlich die Algorithmen und absorbiert ständig neues Wissen aus umfangreichen Textdaten in Bezug auf Vokabular, Struktur, Grammatik und Semantik, um die Leistung des Modells kontinuierlich zu verbessern. Es bietet den Nutzern bequemere Informationen und Dienstleistungen sowie ein intelligenteres Erlebnis."
  },
  "taichu_vl": {
    "description": "Integriert Fähigkeiten wie Bildverständnis, Wissensübertragung und logische Attribution und zeigt herausragende Leistungen im Bereich der Bild-Text-Fragen."
  },
  "text-embedding-3-large": {
    "description": "Das leistungsstärkste Vektormodell, geeignet für englische und nicht-englische Aufgaben."
  },
  "text-embedding-3-small": {
    "description": "Effizientes und kostengünstiges neues Embedding-Modell, geeignet für Wissensabruf, RAG-Anwendungen und andere Szenarien."
  },
  "thudm/glm-4-9b-chat": {
    "description": "Die Open-Source-Version des neuesten vortrainierten Modells der GLM-4-Serie, das von Zhizhu AI veröffentlicht wurde."
  },
  "togethercomputer/StripedHyena-Nous-7B": {
    "description": "StripedHyena Nous (7B) bietet durch effiziente Strategien und Modellarchitekturen verbesserte Rechenfähigkeiten."
  },
  "tts-1": {
    "description": "Das neueste Text-zu-Sprache-Modell, optimiert für Geschwindigkeit in Echtzeitszenarien."
  },
  "tts-1-hd": {
    "description": "Das neueste Text-zu-Sprache-Modell, optimiert für Qualität."
  },
  "upstage/SOLAR-10.7B-Instruct-v1.0": {
    "description": "Upstage SOLAR Instruct v1 (11B) eignet sich für präzise Anweisungsaufgaben und bietet hervorragende Sprachverarbeitungsfähigkeiten."
  },
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet hebt den Branchenstandard an, übertrifft die Konkurrenzmodelle und Claude 3 Opus und zeigt in umfangreichen Bewertungen hervorragende Leistungen, während es die Geschwindigkeit und Kosten unserer mittelgroßen Modelle beibehält."
  },
  "us.anthropic.claude-3-7-sonnet-20250219-v1:0": {
    "description": "Claude 3.7 Sonett ist das schnellste nächste Modell von Anthropic. Im Vergleich zu Claude 3 Haiku hat Claude 3.7 Sonett in allen Fähigkeiten Verbesserungen erfahren und übertrifft in vielen intellektuellen Benchmark-Tests das größte Modell der vorherigen Generation, Claude 3 Opus."
  },
  "whisper-1": {
    "description": "Allgemeines Spracherkennungsmodell, unterstützt mehrsprachige Spracherkennung, Sprachübersetzung und Spracherkennung."
  },
  "wizardlm2": {
    "description": "WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, mehrsprachigen Anwendungen, Schlussfolgerungen und intelligenten Assistenten besonders gut abschneidet."
  },
  "wizardlm2:8x22b": {
    "description": "WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, mehrsprachigen Anwendungen, Schlussfolgerungen und intelligenten Assistenten besonders gut abschneidet."
  },
  "yi-large": {
    "description": "Das brandneue Modell mit einer Billion Parametern bietet außergewöhnliche Frage- und Textgenerierungsfähigkeiten."
  },
  "yi-large-fc": {
    "description": "Basierend auf dem yi-large-Modell unterstützt und verstärkt es die Fähigkeit zu Werkzeugaufrufen und eignet sich für verschiedene Geschäftsszenarien, die den Aufbau von Agenten oder Workflows erfordern."
  },
  "yi-large-preview": {
    "description": "Frühe Version, empfohlen wird die Verwendung von yi-large (neue Version)."
  },
  "yi-large-rag": {
    "description": "Ein fortgeschrittener Dienst, der auf dem leistungsstarken yi-large-Modell basiert und präzise Antworten durch die Kombination von Abruf- und Generierungstechnologien bietet, sowie Echtzeit-Informationsdienste aus dem gesamten Web."
  },
  "yi-large-turbo": {
    "description": "Hervorragendes Preis-Leistungs-Verhältnis und außergewöhnliche Leistung. Hochpräzise Feinabstimmung basierend auf Leistung, Schlussfolgerungsgeschwindigkeit und Kosten."
  },
  "yi-lightning": {
    "description": "Das neueste Hochleistungsmodell, das hochwertige Ausgaben gewährleistet und gleichzeitig die Schlussfolgerungsgeschwindigkeit erheblich verbessert."
  },
  "yi-lightning-lite": {
    "description": "Leichte Version, empfohlen wird die Verwendung von yi-lightning."
  },
  "yi-medium": {
    "description": "Mittelgroßes Modell mit verbesserten Feinabstimmungen, ausgewogene Fähigkeiten und gutes Preis-Leistungs-Verhältnis. Tiefgehende Optimierung der Anweisungsbefolgung."
  },
  "yi-medium-200k": {
    "description": "200K ultra-lange Kontextfenster bieten tiefes Verständnis und Generierungsfähigkeiten für lange Texte."
  },
  "yi-spark": {
    "description": "Klein und kompakt, ein leichtgewichtiges und schnelles Modell. Bietet verbesserte mathematische Berechnungs- und Programmierfähigkeiten."
  },
  "yi-vision": {
    "description": "Modell für komplexe visuelle Aufgaben, das hohe Leistungsfähigkeit bei der Bildverarbeitung und -analyse bietet."
  },
  "yi-vision-v2": {
    "description": "Ein Modell für komplexe visuelle Aufgaben, das leistungsstarke Verständnis- und Analysefähigkeiten auf der Grundlage mehrerer Bilder bietet."
  }
}
