{
  "360/deepseek-r1.description": "[360 Deployment Version] DeepSeek-R1 extensively utilizes reinforcement learning techniques in the post-training phase, significantly enhancing model inference capabilities with minimal labeled data. It performs comparably to OpenAI's o1 official version in tasks such as mathematics, coding, and natural language reasoning.",
  "360gpt-pro-trans.description": "A translation-specific model, finely tuned for optimal translation results.",
  "360gpt-pro.description": "360GPT Pro, as an important member of the 360 AI model series, meets diverse natural language application scenarios with efficient text processing capabilities, supporting long text understanding and multi-turn dialogue.",
  "360gpt-turbo-responsibility-8k.description": "360GPT Turbo Responsibility 8K emphasizes semantic safety and responsibility, designed specifically for applications with high content safety requirements, ensuring accuracy and robustness in user experience.",
  "360gpt-turbo.description": "360GPT Turbo offers powerful computation and dialogue capabilities, with excellent semantic understanding and generation efficiency, making it an ideal intelligent assistant solution for enterprises and developers.",
  "360gpt2-o1.description": "360gpt2-o1 builds a chain of thought using tree search and incorporates a reflection mechanism, trained with reinforcement learning, enabling the model to self-reflect and correct errors.",
  "360gpt2-pro.description": "360GPT2 Pro is an advanced natural language processing model launched by 360, featuring exceptional text generation and understanding capabilities, particularly excelling in generation and creative tasks, capable of handling complex language transformations and role-playing tasks.",
  "360zhinao2-o1.description": "360zhinao2-o1 uses tree search to build a chain of thought and introduces a reflection mechanism, utilizing reinforcement learning for training, enabling the model to possess self-reflection and error-correction capabilities.",
  "AnimeSharp.description": "AnimeSharp (also known as “4x-AnimeSharp”) is an open-source super-resolution model developed by Kim2091 based on the ESRGAN architecture, focusing on upscaling and sharpening anime-style images. It was renamed from “4x-TextSharpV1” in February 2022, originally also suitable for text images but significantly optimized for anime content.",
  "Baichuan2-Turbo.description": "Utilizes search enhancement technology to achieve comprehensive links between large models and domain knowledge, as well as knowledge from the entire web. Supports uploads of various documents such as PDF and Word, and URL input, providing timely and comprehensive information retrieval with accurate and professional output.",
  "Baichuan3-Turbo-128k.description": "Features a 128K ultra-long context window, optimized for high-frequency enterprise scenarios, significantly improving performance and cost-effectiveness. Compared to the Baichuan2 model, content creation improves by 20%, knowledge Q&A by 17%, and role-playing ability by 40%. Overall performance is superior to GPT-3.5.",
  "Baichuan3-Turbo.description": "Optimized for high-frequency enterprise scenarios, significantly improving performance and cost-effectiveness. Compared to the Baichuan2 model, content creation improves by 20%, knowledge Q&A by 17%, and role-playing ability by 40%. Overall performance is superior to GPT-3.5.",
  "Baichuan4-Air.description": "The leading model in the country, surpassing mainstream foreign models in Chinese tasks such as knowledge encyclopedias, long texts, and creative generation. It also possesses industry-leading multimodal capabilities, excelling in multiple authoritative evaluation benchmarks.",
  "Baichuan4-Turbo.description": "The leading model in the country, surpassing mainstream foreign models in Chinese tasks such as knowledge encyclopedias, long texts, and creative generation. It also possesses industry-leading multimodal capabilities, excelling in multiple authoritative evaluation benchmarks.",
  "Baichuan4.description": "The model is the best in the country, surpassing mainstream foreign models in Chinese tasks such as knowledge encyclopedias, long texts, and creative generation. It also boasts industry-leading multimodal capabilities, excelling in multiple authoritative evaluation benchmarks.",
  "ByteDance-Seed/Seed-OSS-36B-Instruct.description": "Seed-OSS is a series of open-source large language models developed by ByteDance's Seed team, designed specifically for powerful long-context processing, reasoning, agents, and general capabilities. The Seed-OSS-36B-Instruct in this series is an instruction-tuned model with 36 billion parameters, natively supporting ultra-long context lengths, enabling it to handle massive documents or complex codebases in a single pass. This model is specially optimized for reasoning, code generation, and agent tasks (such as tool usage), while maintaining balanced and excellent general capabilities. A key feature of this model is the \"Thinking Budget\" function, which allows users to flexibly adjust the reasoning length as needed, effectively improving reasoning efficiency in practical applications.",
  "DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek R1— the larger and smarter model in the DeepSeek suite— distilled into the Llama 70B architecture. Based on benchmark testing and human evaluation, this model is smarter than the original Llama 70B, particularly excelling in tasks requiring mathematical and factual accuracy.",
  "DeepSeek-R1-Distill-Qwen-14B.description": "The DeepSeek-R1 distillation model based on Qwen2.5-14B optimizes inference performance through reinforcement learning and cold-start data, refreshing the benchmark for open-source models across multiple tasks.",
  "DeepSeek-R1-Distill-Qwen-32B.description": "The DeepSeek-R1 series optimizes inference performance through reinforcement learning and cold-start data, refreshing the benchmark for open-source models across multiple tasks, surpassing the level of OpenAI-o1-mini.",
  "DeepSeek-R1-Distill-Qwen-7B.description": "The DeepSeek-R1 distillation model based on Qwen2.5-Math-7B optimizes inference performance through reinforcement learning and cold-start data, refreshing the benchmark for open-source models across multiple tasks.",
  "DeepSeek-R1.description": "A state-of-the-art efficient LLM, skilled in reasoning, mathematics, and programming.",
  "DeepSeek-V3-1.description": "DeepSeek V3.1: Next-generation reasoning model, enhancing complex reasoning and chain-of-thought capabilities, ideal for tasks requiring in-depth analysis.",
  "DeepSeek-V3-Fast.description": "Model provider: sophnet platform. DeepSeek V3 Fast is the high-TPS ultra-fast version of DeepSeek V3 0324, fully powered without quantization, featuring enhanced coding and mathematical capabilities for faster response!",
  "DeepSeek-V3.description": "DeepSeek-V3 is a MoE model developed in-house by Deep Seek Company. Its performance surpasses that of other open-source models such as Qwen2.5-72B and Llama-3.1-405B in multiple assessments, and it stands on par with the world's top proprietary models like GPT-4o and Claude-3.5-Sonnet.",
  "Doubao-lite-128k.description": "Doubao-lite offers ultra-fast response times and better cost-effectiveness, providing customers with more flexible options for different scenarios. Supports inference and fine-tuning with a 128k context window.",
  "Doubao-lite-32k.description": "Doubao-lite offers ultra-fast response times and better cost-effectiveness, providing customers with more flexible options for different scenarios. Supports inference and fine-tuning with a 32k context window.",
  "Doubao-lite-4k.description": "Doubao-lite offers ultra-fast response times and better cost-effectiveness, providing customers with more flexible options for different scenarios. Supports inference and fine-tuning with a 4k context window.",
  "Doubao-pro-128k.description": "The best-performing flagship model, suitable for handling complex tasks. It excels in scenarios such as reference Q&A, summarization, creative writing, text classification, and role-playing. Supports inference and fine-tuning with a 128k context window.",
  "Doubao-pro-32k.description": "The best-performing flagship model, suitable for handling complex tasks. It excels in scenarios such as reference Q&A, summarization, creative writing, text classification, and role-playing. Supports inference and fine-tuning with a 32k context window.",
  "Doubao-pro-4k.description": "The best-performing flagship model, suitable for handling complex tasks. It excels in scenarios such as reference Q&A, summarization, creative writing, text classification, and role-playing. Supports inference and fine-tuning with a 4k context window.",
  "DreamO.description": "DreamO is an open-source image customization generation model jointly developed by ByteDance and Peking University, designed to support multi-task image generation through a unified architecture. It employs an efficient compositional modeling approach to generate highly consistent and customized images based on multiple user-specified conditions such as identity, subject, style, and background.",
  "ERNIE-Character-8K.description": "Baidu's self-developed vertical scene large language model, suitable for applications such as game NPCs, customer service dialogues, and role-playing conversations, featuring more distinct and consistent character styles, stronger adherence to instructions, and superior inference performance.",
  "ERNIE-Lite-Pro-128K.description": "Baidu's self-developed lightweight large language model, balancing excellent model performance with inference efficiency, offering better results than ERNIE Lite, suitable for inference on low-power AI acceleration cards.",
  "ERNIE-Speed-128K.description": "Baidu's latest self-developed high-performance large language model released in 2024, with outstanding general capabilities, suitable as a base model for fine-tuning, effectively addressing specific scenario issues while also exhibiting excellent inference performance.",
  "ERNIE-Speed-Pro-128K.description": "Baidu's latest self-developed high-performance large language model released in 2024, with outstanding general capabilities, providing better results than ERNIE Speed, suitable as a base model for fine-tuning, effectively addressing specific scenario issues while also exhibiting excellent inference performance.",
  "Gryphe/MythoMax-L2-13b.description": "MythoMax-L2 (13B) is an innovative model suitable for multi-domain applications and complex tasks.",
  "HelloMeme.description": "HelloMeme is an AI tool that automatically generates memes, GIFs, or short videos based on the images or actions you provide. It requires no drawing or programming skills; simply prepare reference images, and it will help you create visually appealing, fun, and stylistically consistent content.",
  "HiDream-I1-Full.description": "HiDream-E1-Full is an open-source multimodal image editing large model launched by HiDream.ai, based on the advanced Diffusion Transformer architecture combined with powerful language understanding capabilities (embedded LLaMA 3.1-8B-Instruct). It supports image generation, style transfer, local editing, and content repainting through natural language instructions, demonstrating excellent vision-language comprehension and execution abilities.",
  "InstantCharacter.description": "InstantCharacter is a tuning-free personalized character generation model released by Tencent AI team in 2025, designed to achieve high-fidelity, cross-scene consistent character generation. The model supports character modeling based on a single reference image and can flexibly transfer the character to various styles, actions, and backgrounds.",
  "InternVL2-8B.description": "InternVL2-8B is a powerful visual language model that supports multimodal processing of images and text, capable of accurately recognizing image content and generating relevant descriptions or answers.",
  "Kolors.description": "Kolors is a text-to-image model developed by the Kuaishou Kolors team. Trained with billions of parameters, it excels in visual quality, Chinese semantic understanding, and text rendering.",
  "Kwai-Kolors/Kolors.description": "Kolors is a large-scale latent diffusion text-to-image generation model developed by the Kuaishou Kolors team. Trained on billions of text-image pairs, it demonstrates significant advantages in visual quality, complex semantic accuracy, and Chinese and English character rendering. It supports both Chinese and English inputs and performs exceptionally well in understanding and generating Chinese-specific content.",
  "Kwaipilot/KAT-Dev.description": "KAT-Dev (32B) is an open-source 32B parameter model specifically designed for software engineering tasks. It achieved a 62.4% resolution rate on the SWE-Bench Verified benchmark, ranking fifth among open-source models of all sizes. The model has been optimized through multiple stages, including intermediate training, supervised fine-tuning (SFT), and reinforcement learning (RL), aiming to provide robust support for complex programming tasks such as code completion, bug fixing, and code review.",
  "Meta-Llama-3-3-70B-Instruct.description": "Llama 3.3 70B: A versatile Transformer model suitable for conversational and generative tasks.",
  "Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.description": "Llama 4 Maverick: A large-scale model based on Mixture-of-Experts, offering an efficient expert activation strategy for superior inference performance.",
  "MiniMax-M1.description": "A newly developed inference model. World-leading: 80K chain-of-thought x 1M input, delivering performance on par with top-tier international models.",
  "MiniMax-M2-Stable.description": "Designed for efficient coding and agent workflows, offering higher concurrency and suitable for commercial use.",
  "MiniMax-M2.description": "Purpose-built for efficient coding and agent workflows.",
  "MiniMax-Text-01.description": "In the MiniMax-01 series of models, we have made bold innovations: for the first time, we have implemented a linear attention mechanism on a large scale, making the traditional Transformer architecture no longer the only option. This model has a parameter count of up to 456 billion, with a single activation of 45.9 billion. Its overall performance rivals that of top overseas models while efficiently handling the world's longest context of 4 million tokens, which is 32 times that of GPT-4o and 20 times that of Claude-3.5-Sonnet.",
  "MiniMaxAI/MiniMax-M1-80k.description": "MiniMax-M1 is a large-scale hybrid attention inference model with open-source weights, featuring 456 billion parameters, with approximately 45.9 billion parameters activated per token. The model natively supports ultra-long contexts of up to 1 million tokens and, through lightning attention mechanisms, reduces floating-point operations by 75% compared to DeepSeek R1 in tasks generating 100,000 tokens. Additionally, MiniMax-M1 employs a Mixture of Experts (MoE) architecture, combining the CISPO algorithm with an efficient reinforcement learning training design based on hybrid attention, achieving industry-leading performance in long-input inference and real-world software engineering scenarios.",
  "MiniMaxAI/MiniMax-M2.description": "MiniMax-M2 redefines efficiency for intelligent agents. It is a compact, fast, and cost-effective Mixture of Experts (MoE) model with 230 billion total parameters and 10 billion active parameters. Designed for top-tier performance in coding and agent tasks, it also maintains strong general intelligence. With only 10 billion active parameters, MiniMax-M2 delivers performance comparable to large-scale models, making it an ideal choice for high-efficiency applications.",
  "Moonshot-Kimi-K2-Instruct.description": "With a total of 1 trillion parameters and 32 billion activated parameters, this non-thinking model achieves top-tier performance in cutting-edge knowledge, mathematics, and coding, excelling in general agent tasks. It is carefully optimized for agent tasks, capable not only of answering questions but also taking actions. Ideal for improvisational, general chat, and agent experiences, it is a reflex-level model requiring no prolonged thinking.",
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO.description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) is a high-precision instruction model suitable for complex computations.",
  "OmniConsistency.description": "OmniConsistency enhances style consistency and generalization in image-to-image tasks by introducing large-scale Diffusion Transformers (DiTs) and paired stylized data, effectively preventing style degradation.",
  "Phi-3-medium-128k-instruct.description": "The same Phi-3-medium model, but with a larger context size for RAG or few-shot prompting.",
  "Phi-3-medium-4k-instruct.description": "A 14B parameter model that provides better quality than Phi-3-mini, focusing on high-quality, reasoning-dense data.",
  "Phi-3-mini-128k-instruct.description": "The same Phi-3-mini model, but with a larger context size for RAG or few-shot prompting.",
  "Phi-3-mini-4k-instruct.description": "The smallest member of the Phi-3 family, optimized for both quality and low latency.",
  "Phi-3-small-128k-instruct.description": "The same Phi-3-small model, but with a larger context size for RAG or few-shot prompting.",
  "Phi-3-small-8k-instruct.description": "A 7B parameter model that provides better quality than Phi-3-mini, focusing on high-quality, reasoning-dense data.",
  "Pro/Qwen/Qwen2-7B-Instruct.description": "Qwen2-7B-Instruct is an instruction-tuned large language model in the Qwen2 series, with a parameter size of 7B. This model is based on the Transformer architecture and employs techniques such as the SwiGLU activation function, attention QKV bias, and group query attention. It can handle large-scale inputs. The model excels in language understanding, generation, multilingual capabilities, coding, mathematics, and reasoning across multiple benchmark tests, surpassing most open-source models and demonstrating competitive performance comparable to proprietary models in certain tasks. Qwen2-7B-Instruct outperforms Qwen1.5-7B-Chat in multiple evaluations, showing significant performance improvements.",
  "Pro/THUDM/glm-4-9b-chat.description": "GLM-4-9B-Chat is the open-source version of the GLM-4 series pre-trained models launched by Zhipu AI. This model excels in semantics, mathematics, reasoning, code, and knowledge. In addition to supporting multi-turn dialogues, GLM-4-9B-Chat also features advanced capabilities such as web browsing, code execution, custom tool invocation (Function Call), and long-text reasoning. The model supports 26 languages, including Chinese, English, Japanese, Korean, and German. In multiple benchmark tests, GLM-4-9B-Chat has demonstrated excellent performance, such as in AlignBench-v2, MT-Bench, MMLU, and C-Eval. The model supports a maximum context length of 128K, making it suitable for academic research and commercial applications.",
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7B is a model derived from Qwen2.5-Math-7B through knowledge distillation. It was fine-tuned using 800,000 carefully selected samples generated by DeepSeek-R1, demonstrating exceptional reasoning capabilities. The model achieves outstanding performance across multiple benchmarks, including 92.8% accuracy on MATH-500, a 55.5% pass rate on AIME 2024, and a score of 1189 on CodeForces, showcasing strong mathematical and programming abilities for a 7B-scale model.",
  "Pro/deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1 is a reinforcement learning (RL) driven inference model that addresses issues of repetitiveness and readability in models. Prior to RL, DeepSeek-R1 introduced cold start data to further optimize inference performance. It performs comparably to OpenAI-o1 in mathematical, coding, and reasoning tasks, and enhances overall effectiveness through carefully designed training methods.",
  "Pro/deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3 is a mixed expert (MoE) language model with 671 billion parameters, utilizing multi-head latent attention (MLA) and the DeepSeekMoE architecture, combined with a load balancing strategy without auxiliary loss to optimize inference and training efficiency. Pre-trained on 14.8 trillion high-quality tokens and fine-tuned with supervision and reinforcement learning, DeepSeek-V3 outperforms other open-source models and approaches leading closed-source models.",
  "Pro/moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905 is the latest and most powerful version of Kimi K2. It is a top-tier Mixture of Experts (MoE) language model with a total of 1 trillion parameters and 32 billion activated parameters. Key features of this model include enhanced agent coding intelligence, demonstrating significant performance improvements in public benchmark tests and real-world agent coding tasks; and an improved frontend coding experience, with advancements in both aesthetics and practicality for frontend programming.",
  "Pro/moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinking Turbo is the turbocharged version of the Kimi K2 series, optimized for inference speed and throughput while retaining K2 Thinking’s multi-step reasoning and tool-calling capabilities. Built on a Mixture of Experts (MoE) architecture with approximately 1 trillion parameters, it natively supports a 256K context window and can reliably handle large-scale tool usage, making it ideal for production environments with high demands on latency and concurrency.",
  "QwQ-32B-Preview.description": "QwQ-32B-Preview is an innovative natural language processing model capable of efficiently handling complex dialogue generation and context understanding tasks.",
  "Qwen/QVQ-72B-Preview.description": "QVQ-72B-Preview is a research-oriented model developed by the Qwen team, focusing on visual reasoning capabilities, with unique advantages in understanding complex scenes and solving visually related mathematical problems.",
  "Qwen/QwQ-32B-Preview.description": "QwQ-32B-Preview is Qwen's latest experimental research model, focusing on enhancing AI reasoning capabilities. By exploring complex mechanisms such as language mixing and recursive reasoning, its main advantages include strong analytical reasoning, mathematical, and programming abilities. However, it also faces challenges such as language switching issues, reasoning loops, safety considerations, and differences in other capabilities.",
  "Qwen/QwQ-32B.description": "QwQ is the inference model of the Qwen series. Compared to traditional instruction-tuned models, QwQ possesses reasoning and cognitive abilities, achieving significantly enhanced performance in downstream tasks, especially in solving difficult problems. QwQ-32B is a medium-sized inference model that competes effectively against state-of-the-art inference models (such as DeepSeek-R1 and o1-mini). This model employs technologies such as RoPE, SwiGLU, RMSNorm, and Attention QKV bias, featuring a 64-layer network structure and 40 Q attention heads (with 8 KV heads in the GQA architecture).",
  "Qwen/Qwen-Image-Edit-2509.description": "Qwen-Image-Edit-2509 is the latest image editing version of Qwen-Image, released by Alibaba's Tongyi Qianwen team. Built upon the 20B-parameter Qwen-Image model, it has been further trained to extend its unique text rendering capabilities into the domain of image editing, enabling precise manipulation of text within images. Qwen-Image-Edit employs an innovative architecture that feeds the input image into both Qwen2.5-VL (for visual semantic control) and a VAE Encoder (for visual appearance control), enabling dual editing capabilities in both semantics and appearance. This allows for not only localized visual edits such as adding, removing, or modifying elements, but also high-level semantic edits like IP creation and style transfer that require semantic consistency. The model has demonstrated state-of-the-art (SOTA) performance across multiple public benchmarks, making it a powerful foundational model for image editing.",
  "Qwen/Qwen-Image.description": "Qwen-Image is a foundational image generation model developed by Alibaba's Tongyi Qianwen team, featuring 20 billion parameters. The model has made significant advancements in complex text rendering and precise image editing, excelling particularly at generating images with high-fidelity Chinese and English text. Qwen-Image can handle multi-line layouts and paragraph-level text while maintaining coherent typography and contextual harmony in generated images. Beyond its exceptional text rendering capabilities, the model supports a wide range of artistic styles—from photorealism to anime aesthetics—adapting flexibly to diverse creative needs. It also boasts powerful image editing and understanding capabilities, supporting advanced operations such as style transfer, object addition/removal, detail enhancement, text editing, and even human pose manipulation. Qwen-Image is designed to be a comprehensive foundational model for intelligent visual creation and processing, integrating language, layout, and imagery.",
  "Qwen/Qwen2-72B-Instruct.description": "Qwen2 is an advanced general-purpose language model that supports various types of instructions.",
  "Qwen/Qwen2-7B-Instruct.description": "Qwen2 is the latest series of the Qwen model, capable of outperforming optimal open-source models of similar size and even larger models. Qwen2 7B has achieved significant advantages in multiple evaluations, especially in coding and Chinese comprehension.",
  "Qwen/Qwen2-VL-72B-Instruct.description": "Qwen2-VL is the latest iteration of the Qwen-VL model, achieving state-of-the-art performance in visual understanding benchmarks.",
  "Qwen/Qwen3-14B.description": "Qwen3 is a next-generation model with significantly enhanced capabilities, achieving industry-leading levels in reasoning, general tasks, agent functions, and multilingual support, with a switchable thinking mode.",
  "Qwen/Qwen3-235B-A22B-Instruct-2507.description": "Qwen3-235B-A22B-Instruct-2507 is a flagship mixture-of-experts (MoE) large language model developed by Alibaba Cloud Tongyi Qianwen team within the Qwen3 series. It has 235 billion total parameters with 22 billion activated per inference. Released as an update to the non-thinking mode Qwen3-235B-A22B, it focuses on significant improvements in instruction following, logical reasoning, text comprehension, mathematics, science, programming, and tool usage. Additionally, it enhances coverage of multilingual long-tail knowledge and better aligns with user preferences in subjective and open-ended tasks to generate more helpful and higher-quality text.",
  "Qwen/Qwen3-235B-A22B-Thinking-2507.description": "Qwen3-235B-A22B-Thinking-2507 is a member of the Qwen3 large language model series developed by Alibaba Tongyi Qianwen team, specializing in complex reasoning tasks. Based on a mixture-of-experts (MoE) architecture with 235 billion total parameters and approximately 22 billion activated per token, it balances strong performance with computational efficiency. As a dedicated “thinking” model, it significantly improves performance in logic reasoning, mathematics, science, programming, and academic benchmarks requiring human expertise, ranking among the top open-source thinking models. It also enhances general capabilities such as instruction following, tool usage, and text generation, natively supports 256K long-context understanding, and is well-suited for scenarios requiring deep reasoning and long document processing.",
  "Qwen/Qwen3-235B-A22B.description": "Qwen3 is a next-generation model with significantly enhanced capabilities, achieving industry-leading levels in reasoning, general tasks, agent functions, and multilingual support, with a switchable thinking mode.",
  "Qwen/Qwen3-30B-A3B-Instruct-2507.description": "Qwen3-30B-A3B-Instruct-2507 is an updated version of the Qwen3-30B-A3B non-thinking mode. It is a Mixture of Experts (MoE) model with a total of 30.5 billion parameters and 3.3 billion active parameters. The model features key enhancements across multiple areas, including significant improvements in instruction following, logical reasoning, text comprehension, mathematics, science, coding, and tool usage. Additionally, it has made substantial progress in covering long-tail multilingual knowledge and better aligns with user preferences in subjective and open-ended tasks, enabling it to generate more helpful responses and higher-quality text. Furthermore, its long-text comprehension capability has been extended to 256K tokens. This model supports only the non-thinking mode and does not generate `<think></think>` tags in its output.",
  "Qwen/Qwen3-30B-A3B-Thinking-2507.description": "Qwen3-30B-A3B-Thinking-2507 is the latest “thinking” model in the Qwen3 series released by Alibaba’s Tongyi Qianwen team. As a mixture-of-experts (MoE) model with 30.5 billion total parameters and 3.3 billion active parameters, it is designed to improve capabilities for handling complex tasks. The model demonstrates significant performance gains on academic benchmarks requiring logical reasoning, mathematics, science, programming, and domain expertise. At the same time, its general abilities—such as instruction following, tool use, text generation, and alignment with human preferences—have been substantially enhanced. The model natively supports long-context understanding of 256K tokens and can scale up to 1 million tokens. This version is tailored for “thinking mode,” intended to solve highly complex problems through detailed step-by-step reasoning, and it also exhibits strong agent capabilities.",
  "Qwen/Qwen3-30B-A3B.description": "Qwen3 is a next-generation model with significantly enhanced capabilities, achieving industry-leading levels in reasoning, general tasks, agent functions, and multilingual support, with a switchable thinking mode.",
  "Qwen/Qwen3-32B.description": "Qwen3 is a next-generation model with significantly enhanced capabilities, achieving industry-leading levels in reasoning, general tasks, agent functions, and multilingual support, with a switchable thinking mode.",
  "Qwen/Qwen3-8B.description": "Qwen3 is a next-generation model with significantly enhanced capabilities, achieving industry-leading levels in reasoning, general tasks, agent functions, and multilingual support, with a switchable thinking mode.",
  "Qwen/Qwen3-Coder-30B-A3B-Instruct.description": "Qwen3-Coder-30B-A3B-Instruct is a code model in the Qwen3 series developed by Alibaba's Tongyi Qianwen team. As a streamlined and optimized model, it focuses on enhancing code-handling capabilities while maintaining high performance and efficiency. The model demonstrates notable advantages among open-source models on complex tasks such as agentic coding, automated browser operations, and tool invocation. It natively supports a long context of 256K tokens and can be extended up to 1M tokens, enabling better understanding and processing at the codebase level. Additionally, the model provides robust agentic coding support for platforms like Qwen Code and CLINE, and it employs a dedicated function-calling format.",
  "Qwen/Qwen3-Coder-480B-A35B-Instruct.description": "Qwen3-Coder-480B-A35B-Instruct, released by Alibaba, is the most agentic code model to date. It is a mixture-of-experts (MoE) model with 480 billion total parameters and 35 billion active parameters, striking a balance between efficiency and performance. The model natively supports a 256K (~260k) token context window and can be extended to 1,000,000 tokens through extrapolation methods such as YaRN, enabling it to handle large codebases and complex programming tasks. Qwen3-Coder is designed for agent-style coding workflows: it not only generates code but can autonomously interact with development tools and environments to solve complex programming problems. On multiple benchmarks for coding and agent tasks, this model achieves top-tier results among open-source models, with performance comparable to leading models like Claude Sonnet 4.",
  "Qwen/Qwen3-Next-80B-A3B-Instruct.description": "Qwen3-Next-80B-A3B-Instruct is the next-generation foundational model released by Alibaba's Tongyi Qianwen team. It is based on the brand-new Qwen3-Next architecture, designed to achieve ultimate training and inference efficiency. The model employs an innovative hybrid attention mechanism (Gated DeltaNet and Gated Attention), a highly sparse mixture-of-experts (MoE) structure, and multiple training stability optimizations. As a sparse model with a total of 80 billion parameters, it activates only about 3 billion parameters during inference, significantly reducing computational costs. When handling long-context tasks exceeding 32K tokens, its inference throughput is more than 10 times higher than the Qwen3-32B model. This model is an instruction-tuned version designed for general tasks and does not support the Thinking mode. In terms of performance, it is comparable to Tongyi Qianwen's flagship Qwen3-235B model on some benchmarks, especially demonstrating clear advantages in ultra-long context tasks.",
  "Qwen/Qwen3-Next-80B-A3B-Thinking.description": "Qwen3-Next-80B-A3B-Thinking is the next-generation foundational model released by Alibaba's Tongyi Qianwen team, specifically designed for complex reasoning tasks. It is based on the innovative Qwen3-Next architecture, which integrates a hybrid attention mechanism (Gated DeltaNet and Gated Attention) and a highly sparse mixture-of-experts (MoE) structure, aiming for ultimate training and inference efficiency. As a sparse model with a total of 80 billion parameters, it activates only about 3 billion parameters during inference, greatly reducing computational costs. When processing long-context tasks exceeding 32K tokens, its throughput is more than 10 times higher than the Qwen3-32B model. This \"Thinking\" version is optimized for executing challenging multi-step tasks such as mathematical proofs, code synthesis, logical analysis, and planning, and by default outputs the reasoning process in a structured \"chain-of-thought\" format. In terms of performance, it not only surpasses higher-cost models like Qwen3-32B-Thinking but also outperforms Gemini-2.5-Flash-Thinking on multiple benchmarks.",
  "Qwen/Qwen3-Omni-30B-A3B-Captioner.description": "Qwen3-Omni-30B-A3B-Captioner is a vision-language model (VLM) from Alibaba's Qwen3 series, developed by the Tongyi Qianwen team. It is specifically designed to generate high-quality, detailed, and accurate image captions. Built on a 30-billion-parameter Mixture of Experts (MoE) architecture, the model excels at understanding image content and converting it into natural, fluent textual descriptions. It demonstrates outstanding performance in capturing image details, scene understanding, object recognition, and relational reasoning, making it ideal for applications requiring precise image comprehension and caption generation.",
  "Qwen/Qwen3-Omni-30B-A3B-Instruct.description": "Qwen3-Omni-30B-A3B-Instruct is part of the latest Qwen3 series from Alibaba's Tongyi Qianwen team. This Mixture of Experts (MoE) model features 30 billion total parameters and 3 billion active parameters, offering powerful performance while reducing inference costs. Trained on high-quality, diverse, and multilingual data, it boasts strong general capabilities and supports full-modality input processing—including text, images, audio, and video—enabling it to understand and generate cross-modal content.",
  "Qwen/Qwen3-Omni-30B-A3B-Thinking.description": "Qwen3-Omni-30B-A3B-Thinking is the core 'Thinker' component of the Qwen3-Omni multimodal model. It is designed to handle complex chain-of-thought reasoning across multiple modalities, including text, audio, images, and video. Acting as the reasoning engine, it unifies all inputs into a shared representation space, enabling deep cross-modal understanding and sophisticated reasoning. Built on a Mixture of Experts (MoE) architecture with 30 billion total parameters and 3 billion active parameters, it balances powerful reasoning capabilities with computational efficiency.",
  "Qwen/Qwen3-VL-235B-A22B-Instruct.description": "Qwen3-VL-235B-A22B-Instruct is a large instruction-tuned model in the Qwen3-VL series. Based on a Mixture of Experts (MoE) architecture, it offers exceptional multimodal understanding and generation capabilities. With native support for 256K context length, it is well-suited for high-concurrency, production-grade multimodal services.",
  "Qwen/Qwen3-VL-235B-A22B-Thinking.description": "Qwen3-VL-235B-A22B-Thinking is the flagship reasoning model in the Qwen3-VL series. It is specially optimized for complex multimodal reasoning, long-context inference, and agent interaction, making it ideal for enterprise-level scenarios that demand deep reasoning and visual understanding.",
  "Qwen/Qwen3-VL-30B-A3B-Instruct.description": "Qwen3-VL-30B-A3B-Instruct is an instruction-tuned model in the Qwen3-VL series, featuring powerful vision-language understanding and generation capabilities. With native support for 256K context length, it is suitable for multimodal dialogue and image-conditioned generation tasks.",
  "Qwen/Qwen3-VL-30B-A3B-Thinking.description": "Qwen3-VL-30B-A3B-Thinking is the reasoning-enhanced version of Qwen3-VL. It is optimized for multimodal reasoning, image-to-code tasks, and complex visual understanding. Supporting 256K context length, it offers stronger chain-of-thought capabilities.",
  "Qwen/Qwen3-VL-32B-Instruct.description": "Qwen3-VL-32B-Instruct is a vision-language model developed by Alibaba's Tongyi Qianwen team, achieving state-of-the-art (SOTA) performance across multiple vision-language benchmarks. It supports high-resolution image inputs at the megapixel level and offers robust general visual understanding, multilingual OCR, fine-grained visual localization, and visual dialogue capabilities. As part of the Qwen3 series, it is equipped to handle complex multimodal tasks and supports advanced features such as tool invocation and prefix continuation.",
  "Qwen/Qwen3-VL-32B-Thinking.description": "Qwen3-VL-32B-Thinking is a specialized version of Alibaba's Qwen3 vision-language model, optimized for complex visual reasoning tasks. It features a built-in 'thinking mode' that enables the model to generate detailed intermediate reasoning steps before answering, significantly enhancing its performance on tasks requiring multi-step logic, planning, and complex inference. The model supports high-resolution image inputs at the megapixel level and offers strong general visual understanding, multilingual OCR, fine-grained visual localization, and visual dialogue capabilities, along with support for tool invocation and prefix continuation.",
  "Qwen/Qwen3-VL-8B-Instruct.description": "Qwen3-VL-8B-Instruct is a vision-language model from the Qwen3 series, built on Qwen3-8B-Instruct and trained on a large corpus of image-text data. It excels at general visual understanding, vision-centric dialogue, and multilingual text recognition within images. It is well-suited for tasks such as visual question answering, image captioning, multimodal instruction following, and tool invocation.",
  "Qwen/Qwen3-VL-8B-Thinking.description": "Qwen3-VL-8B-Thinking is the visual reasoning variant of the Qwen3 series, optimized for complex multi-step reasoning tasks. By default, it generates a step-by-step thinking chain before answering, enhancing reasoning accuracy. It is ideal for scenarios requiring in-depth reasoning, such as visual question answering and detailed analysis of image content.",
  "Qwen2-72B-Instruct.description": "Qwen2 is the latest series of the Qwen model, supporting 128k context. Compared to the current best open-source models, Qwen2-72B significantly surpasses leading models in natural language understanding, knowledge, coding, mathematics, and multilingual capabilities.",
  "Qwen2-7B-Instruct.description": "Qwen2 is the latest series of the Qwen model, capable of outperforming optimal open-source models of similar size and even larger models. Qwen2 7B has achieved significant advantages in multiple evaluations, especially in coding and Chinese comprehension.",
  "Qwen2-VL-72B.description": "Qwen2-VL-72B is a powerful visual language model that supports multimodal processing of images and text, capable of accurately recognizing image content and generating relevant descriptions or answers.",
  "Qwen3-235B-A22B-Instruct-2507-FP8.description": "Qwen3 235B A22B Instruct 2507: A model optimized for advanced reasoning and dialogue instructions, featuring a mixture-of-experts architecture to maintain inference efficiency at large scale.",
  "Qwen3-235B.description": "Qwen3-235B-A22B is a Mixture of Experts (MoE) model that introduces a \"Hybrid Reasoning Mode,\" allowing users to seamlessly switch between \"Thinking Mode\" and \"Non-Thinking Mode.\" It supports understanding and reasoning in 119 languages and dialects and possesses powerful tool invocation capabilities. In comprehensive benchmarks covering overall ability, coding and mathematics, multilingual proficiency, knowledge, and reasoning, it competes with leading large models on the market such as DeepSeek R1, OpenAI o1, o3-mini, Grok 3, and Google Gemini 2.5 Pro.",
  "Qwen3-32B.description": "Qwen3-32B is a dense model that introduces a \"Hybrid Reasoning Mode,\" enabling users to seamlessly switch between \"Thinking Mode\" and \"Non-Thinking Mode.\" Thanks to architectural improvements, increased training data, and more efficient training methods, its overall performance is comparable to that of Qwen2.5-72B.",
  "SenseChat-128K.description": "Basic version model (V4) with a context length of 128K, excelling in long text comprehension and generation tasks.",
  "SenseChat-32K.description": "Basic version model (V4) with a context length of 32K, flexibly applicable to various scenarios.",
  "SenseChat-5-1202.description": "Based on version V5.5, this latest release shows significant improvements over the previous version in foundational Chinese and English capabilities, chat, science knowledge, humanities knowledge, writing, mathematical logic, and word count control.",
  "SenseChat-5-Cantonese.description": "With a context length of 32K, it surpasses GPT-4 in Cantonese conversation comprehension and is competitive with GPT-4 Turbo in knowledge, reasoning, mathematics, and code writing across multiple domains.",
  "SenseChat-5-beta.description": "Partially outperforms SenseCat-5-1202",
  "SenseChat-5.description": "The latest version model (V5.5) with a context length of 128K shows significant improvements in mathematical reasoning, English conversation, instruction following, and long text comprehension, comparable to GPT-4o.",
  "SenseChat-Character-Pro.description": "Advanced version model with a context length of 32K, offering comprehensive capability enhancements and supporting both Chinese and English conversations.",
  "SenseChat-Character.description": "Standard version model with an 8K context length and high response speed.",
  "SenseChat-Turbo-1202.description": "This is the latest lightweight version model, achieving over 90% of the full model's capabilities while significantly reducing inference costs.",
  "SenseChat-Turbo.description": "Suitable for fast question answering and model fine-tuning scenarios.",
  "SenseChat-Vision.description": "The latest version model (V5.5) supports multi-image input and fully optimizes the model's basic capabilities, achieving significant improvements in object attribute recognition, spatial relationships, action event recognition, scene understanding, emotion recognition, logical reasoning, and text understanding and generation.",
  "SenseChat.description": "Basic version model (V4) with a context length of 4K, featuring strong general capabilities.",
  "SenseNova-V6-5-Pro.description": "With comprehensive updates to multimodal, language, and reasoning data, along with optimized training strategies, the new model achieves significant improvements in multimodal reasoning and generalized instruction-following capabilities. It supports a context window of up to 128K tokens and excels in specialized tasks such as OCR and cultural tourism IP recognition.",
  "SenseNova-V6-5-Turbo.description": "With comprehensive updates to multimodal, language, and reasoning data, along with optimized training strategies, the new model achieves significant improvements in multimodal reasoning and generalized instruction-following capabilities. It supports a context window of up to 128K tokens and excels in specialized tasks such as OCR and cultural tourism IP recognition.",
  "SenseNova-V6-Pro.description": "Achieves a native unification of image, text, and video capabilities, breaking through the limitations of traditional discrete multimodality, winning dual championships in the OpenCompass and SuperCLUE evaluations.",
  "SenseNova-V6-Reasoner.description": "Balances visual and linguistic deep reasoning, enabling slow thinking and profound inference, presenting a complete chain of thought process.",
  "SenseNova-V6-Turbo.description": "Achieves a native unification of image, text, and video capabilities, breaking through the limitations of traditional discrete multimodality, leading comprehensively in core dimensions such as multimodal foundational abilities and linguistic foundational abilities, excelling in both literature and science, and consistently ranking among the top tier in various assessments both domestically and internationally.",
  "Skylark2-lite-8k.description": "Skylark 2nd generation model, Skylark2-lite model is characterized by high response speed, suitable for high real-time requirements, cost-sensitive scenarios, and situations where model accuracy is less critical, with a context window length of 8k.",
  "Skylark2-pro-32k.description": "Skylark 2nd generation model, Skylark2-pro version has high model accuracy, suitable for more complex text generation scenarios such as professional field copy generation, novel writing, and high-quality translation, with a context window length of 32k.",
  "Skylark2-pro-4k.description": "Skylark 2nd generation model, Skylark2-pro model has high model accuracy, suitable for more complex text generation scenarios such as professional field copy generation, novel writing, and high-quality translation, with a context window length of 4k.",
  "Skylark2-pro-character-4k.description": "Skylark 2nd generation model, Skylark2-pro-character has excellent role-playing and chat capabilities, adept at engaging in conversations with users based on their prompt requests, showcasing distinct character styles and flowing dialogue, making it well-suited for building chatbots, virtual assistants, and online customer service, with high response speed.",
  "Skylark2-pro-turbo-8k.description": "Skylark 2nd generation model, Skylark2-pro-turbo-8k provides faster inference at a lower cost, with a context window length of 8k.",
  "THUDM/GLM-4-32B-0414.description": "GLM-4-32B-0414 is the next-generation open-source model in the GLM series, boasting 32 billion parameters. Its performance is comparable to OpenAI's GPT series and DeepSeek's V3/R1 series.",
  "THUDM/GLM-4-9B-0414.description": "GLM-4-9B-0414 is a small model in the GLM series, with 9 billion parameters. This model inherits the technical characteristics of the GLM-4-32B series while providing a more lightweight deployment option. Despite its smaller size, GLM-4-9B-0414 still demonstrates excellent capabilities in tasks such as code generation, web design, SVG graphics generation, and search-based writing.",
  "THUDM/GLM-Z1-32B-0414.description": "GLM-Z1-32B-0414 is a reasoning model with deep thinking capabilities. This model is developed based on GLM-4-32B-0414 through cold start and extended reinforcement learning, with further training in mathematics, coding, and logic tasks. Compared to the base model, GLM-Z1-32B-0414 significantly enhances mathematical abilities and the capacity to solve complex tasks.",
  "THUDM/GLM-Z1-9B-0414.description": "GLM-Z1-9B-0414 is a small model in the GLM series, with only 9 billion parameters, yet it demonstrates remarkable capabilities while maintaining the open-source tradition. Despite its smaller size, this model excels in mathematical reasoning and general tasks, leading the performance among similarly sized open-source models.",
  "THUDM/GLM-Z1-Rumination-32B-0414.description": "GLM-Z1-Rumination-32B-0414 is a deep reasoning model with reflective capabilities (comparable to OpenAI's Deep Research). Unlike typical deep thinking models, reflective models engage in longer periods of deep thought to tackle more open and complex problems.",
  "THUDM/glm-4-9b-chat.description": "GLM-4 9B is an open-source version that provides an optimized conversational experience for chat applications.",
  "Tongyi-Zhiwen/QwenLong-L1-32B.description": "QwenLong-L1-32B is the first large reasoning model (LRM) trained with reinforcement learning for long-context tasks, optimized specifically for long-text reasoning. It achieves stable transfer from short to long contexts through a progressive context expansion reinforcement learning framework. In seven long-context document QA benchmarks, QwenLong-L1-32B outperforms flagship models like OpenAI-o3-mini and Qwen3-235B-A22B, with performance comparable to Claude-3.7-Sonnet-Thinking. The model excels in complex tasks such as mathematical reasoning, logical reasoning, and multi-hop reasoning.",
  "Yi-34B-Chat.description": "Yi-1.5-34B significantly enhances mathematical logic and coding abilities by incrementally training on 500 billion high-quality tokens while maintaining the excellent general language capabilities of the original series.",
  "[\"01-ai/yi-1.5-34b-chat\"].description": "Zero One Everything, the latest open-source fine-tuned model with 34 billion parameters, supports various dialogue scenarios with high-quality training data aligned with human preferences.",
  "[\"01-ai/yi-1.5-9b-chat\"].description": "Zero One Everything, the latest open-source fine-tuned model with 9 billion parameters, supports various dialogue scenarios with high-quality training data aligned with human preferences.",
  "[\"4.0Ultra\"].description": "Spark4.0 Ultra is the most powerful version in the Spark large model series, enhancing text content understanding and summarization capabilities while upgrading online search links. It is a comprehensive solution for improving office productivity and accurately responding to demands, leading the industry as an intelligent product.",
  "[\"DeepSeek-R1-Distill-Qwen-1.5B\"].description": "The DeepSeek-R1 distillation model based on Qwen2.5-Math-1.5B optimizes inference performance through reinforcement learning and cold-start data, refreshing the benchmark for open-source models across multiple tasks.",
  "[\"DeepSeek-V3.1-Fast\"].description": "DeepSeek V3.1 Fast is the high-TPS, ultra-fast version of DeepSeek V3.1. Hybrid Thinking Mode: By changing the chat template, a single model can support both thinking and non-thinking modes simultaneously. Smarter Tool Invocation: Post-training optimization significantly improves the model's performance in tool usage and agent tasks.",
  "[\"DeepSeek-V3.1-Think\"].description": "DeepSeek-V3.1 - Thinking Mode; DeepSeek-V3.1 is a newly launched hybrid reasoning model by DeepSeek, supporting both thinking and non-thinking reasoning modes, with higher thinking efficiency compared to DeepSeek-R1-0528. Post-training optimization significantly enhances agent tool usage and agent task performance.",
  "[\"ERNIE-3.5-128K\"].description": "Baidu's self-developed flagship large-scale language model, covering a vast amount of Chinese and English corpus. It possesses strong general capabilities, meeting the requirements for most dialogue Q&A, creative generation, and plugin application scenarios; it supports automatic integration with Baidu's search plugin to ensure the timeliness of Q&A information.",
  "[\"ERNIE-3.5-8K\"].description": "Baidu's self-developed flagship large-scale language model, covering a vast amount of Chinese and English corpus. It possesses strong general capabilities, meeting the requirements for most dialogue Q&A, creative generation, and plugin application scenarios; it supports automatic integration with Baidu's search plugin to ensure the timeliness of Q&A information.",
  "[\"ERNIE-3.5-8K-Preview\"].description": "Baidu's self-developed flagship large-scale language model, covering a vast amount of Chinese and English corpus. It possesses strong general capabilities, meeting the requirements for most dialogue Q&A, creative generation, and plugin application scenarios; it supports automatic integration with Baidu's search plugin to ensure the timeliness of Q&A information.",
  "[\"ERNIE-4.0-8K-Latest\"].description": "Baidu's self-developed flagship ultra-large-scale language model, which has achieved a comprehensive upgrade in model capabilities compared to ERNIE 3.5, widely applicable to complex task scenarios across various fields; supports automatic integration with Baidu search plugins to ensure the timeliness of Q&A information.",
  "[\"ERNIE-4.0-8K-Preview\"].description": "Baidu's self-developed flagship ultra-large-scale language model, which has achieved a comprehensive upgrade in model capabilities compared to ERNIE 3.5, widely applicable to complex task scenarios across various fields; supports automatic integration with Baidu search plugins to ensure the timeliness of Q&A information.",
  "[\"ERNIE-4.0-Turbo-8K-Latest\"].description": "Baidu's self-developed flagship ultra-large-scale language model, demonstrating excellent overall performance, suitable for complex task scenarios across various fields; supports automatic integration with Baidu search plugins to ensure the timeliness of Q&A information. It offers better performance compared to ERNIE 4.0.",
  "[\"ERNIE-4.0-Turbo-8K-Preview\"].description": "Baidu's self-developed flagship ultra-large-scale language model, demonstrating excellent overall performance, widely applicable to complex task scenarios across various fields; supports automatic integration with Baidu search plugins to ensure the timeliness of Q&A information. It outperforms ERNIE 4.0 in performance.",
  "[\"FLUX-1.1-pro\"].description": "FLUX.1.1 Pro",
  "[\"FLUX.1-Kontext-dev\"].description": "FLUX.1-Kontext-dev is a multimodal image generation and editing model developed by Black Forest Labs based on the Rectified Flow Transformer architecture, featuring 12 billion parameters. It specializes in generating, reconstructing, enhancing, or editing images under given contextual conditions. The model combines the controllable generation advantages of diffusion models with the contextual modeling capabilities of Transformers, supporting high-quality image output and widely applicable to image restoration, completion, and visual scene reconstruction tasks.",
  "[\"FLUX.1-Kontext-pro\"].description": "FLUX.1 Kontext [pro]",
  "[\"FLUX.1-dev\"].description": "FLUX.1-dev is an open-source multimodal language model (MLLM) developed by Black Forest Labs, optimized for vision-and-language tasks by integrating image and text understanding and generation capabilities. Built upon advanced large language models such as Mistral-7B, it achieves vision-language collaborative processing and complex task reasoning through a carefully designed visual encoder and multi-stage instruction fine-tuning.",
  "[\"HunyuanDiT-v1.2-Diffusers-Distilled\"].description": "hunyuandit-v1.2-distilled is a lightweight text-to-image model optimized through distillation, capable of rapidly generating high-quality images, especially suitable for low-resource environments and real-time generation tasks.",
  "[\"InternVL2.5-26B\"].description": "InternVL2.5-26B is a powerful visual language model that supports multimodal processing of images and text, capable of accurately recognizing image content and generating relevant descriptions or answers.",
  "[\"Llama-3.2-11B-Vision-Instruct\"].description": "Exhibits outstanding image reasoning capabilities on high-resolution images, suitable for visual understanding applications.",
  "[\"Llama-3.2-90B-Vision-Instruct\\t\"].description": "Advanced image reasoning capabilities suitable for visual understanding agent applications.",
  "[\"Meta-Llama-3.1-405B-Instruct\"].description": "Llama 3.1 instruction-tuned text model optimized for multilingual dialogue use cases, performing excellently on common industry benchmarks among many available open-source and closed chat models.",
  "[\"Meta-Llama-3.1-70B-Instruct\"].description": "Llama 3.1 instruction-tuned text model optimized for multilingual dialogue use cases, performing excellently on common industry benchmarks among many available open-source and closed chat models.",
  "[\"Meta-Llama-3.1-8B-Instruct\"].description": "Llama 3.1 instruction-tuned text model optimized for multilingual dialogue use cases, performing excellently on common industry benchmarks among many available open-source and closed chat models.",
  "[\"Meta-Llama-3.2-1B-Instruct\"].description": "An advanced cutting-edge small language model with language understanding, excellent reasoning capabilities, and text generation abilities.",
  "[\"Meta-Llama-3.2-3B-Instruct\"].description": "An advanced cutting-edge small language model with language understanding, excellent reasoning capabilities, and text generation abilities.",
  "[\"Meta-Llama-3.3-70B-Instruct\"].description": "Llama 3.3 is the most advanced multilingual open-source large language model in the Llama series, offering performance comparable to a 405B model at a very low cost. Based on the Transformer architecture, it enhances usability and safety through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Its instruction-tuned version is optimized for multilingual dialogue and outperforms many open-source and closed chat models on various industry benchmarks. Knowledge cutoff date is December 2023.",
  "[\"Phi-3.5-mini-instruct\"].description": "An updated version of the Phi-3-mini model.",
  "[\"Phi-3.5-vision-instrust\"].description": "An updated version of the Phi-3-vision model.",
  "[\"Pro/Qwen/Qwen2.5-7B-Instruct\"].description": "Qwen2.5-7B-Instruct is one of the latest large language models released by Alibaba Cloud. This 7B model shows significant improvements in coding and mathematics. It also provides multilingual support, covering over 29 languages, including Chinese and English. The model has made notable advancements in instruction following, understanding structured data, and generating structured outputs, especially JSON.",
  "[\"Pro/Qwen/Qwen2.5-Coder-7B-Instruct\"].description": "Qwen2.5-Coder-7B-Instruct is the latest version in Alibaba Cloud's series of code-specific large language models. This model significantly enhances code generation, reasoning, and repair capabilities based on Qwen2.5, trained on 55 trillion tokens. It not only improves coding abilities but also maintains advantages in mathematics and general capabilities, providing a more comprehensive foundation for practical applications such as code agents.",
  "[\"Pro/Qwen/Qwen2.5-VL-7B-Instruct\"].description": "Qwen2.5-VL is the newest addition to the Qwen series, featuring enhanced visual comprehension capabilities. It can analyze text, charts, and layouts within images, comprehend long videos while capturing events. The model supports reasoning, tool manipulation, multi-format object localization, and structured output generation. It incorporates optimized dynamic resolution and frame rate training for video understanding, along with improved efficiency in its visual encoder.",
  "[\"Pro/THUDM/GLM-4.1V-9B-Thinking\"].description": "GLM-4.1V-9B-Thinking is an open-source vision-language model (VLM) jointly released by Zhipu AI and Tsinghua University's KEG Lab, designed specifically for handling complex multimodal cognitive tasks. Based on the GLM-4-9B-0414 foundation model, it significantly enhances cross-modal reasoning ability and stability by introducing the Chain-of-Thought reasoning mechanism and employing reinforcement learning strategies.",
  "[\"Pro/deepseek-ai/DeepSeek-V3.1-Terminus\"].description": "DeepSeek-V3.1-Terminus is an updated version of the V3.1 model released by DeepSeek, positioned as a hybrid agent large language model. This update focuses on fixing user-reported issues and improving stability while maintaining the model's original capabilities. It significantly enhances language consistency, reducing the mixing of Chinese and English and the occurrence of abnormal characters. The model integrates both \"Thinking Mode\" and \"Non-thinking Mode,\" allowing users to switch flexibly between chat templates to suit different tasks. As a key optimization, V3.1-Terminus improves the performance of the Code Agent and Search Agent, making tool invocation and multi-step complex task execution more reliable.",
  "[\"Pro/deepseek-ai/DeepSeek-V3.2-Exp\"].description": "DeepSeek-V3.2-Exp is an experimental version released by DeepSeek as an intermediate step toward the next-generation architecture. Building on V3.1-Terminus, it introduces the DeepSeek Sparse Attention (DSA) mechanism to enhance training and inference efficiency for long-context scenarios. It features targeted optimizations for tool use, long-document comprehension, and multi-step reasoning. V3.2-Exp serves as a bridge between research and production, ideal for users seeking higher inference efficiency in high-context-budget applications.",
  "[\"Qwen/Qwen2.5-14B-Instruct\"].description": "Qwen2.5 is a brand new series of large language models designed to optimize the handling of instruction-based tasks.",
  "[\"Qwen/Qwen2.5-32B-Instruct\"].description": "Qwen2.5 is a brand new series of large language models designed to optimize the handling of instruction-based tasks.",
  "[\"Qwen/Qwen2.5-72B-Instruct\"].description": "A large language model developed by the Alibaba Cloud Tongyi Qianwen team",
  "[\"Qwen/Qwen2.5-72B-Instruct-128K\"].description": "Qwen2.5-72B-Instruct is one of Alibaba Cloud’s latest large language model series. This 72B model brings significant improvements in coding and mathematics, supports up to 128K input tokens, and can generate long outputs (8K+). It supports over 29 languages and shows notable gains in instruction following, structured data understanding, and structured outputs (especially JSON).",
  "[\"Qwen/Qwen2.5-72B-Instruct-Turbo\"].description": "Qwen2.5 is a new large language model series designed to optimize instruction-based task processing.",
  "[\"Qwen/Qwen2.5-7B-Instruct\"].description": "Qwen2.5 is a brand new series of large language models designed to optimize the handling of instruction-based tasks.",
  "[\"Qwen/Qwen2.5-7B-Instruct-Turbo\"].description": "Qwen2.5 is a new large language model series designed to optimize instruction-based task processing.",
  "[\"Qwen/Qwen2.5-Coder-32B-Instruct\"].description": "Qwen2.5-Coder focuses on code writing.",
  "[\"Qwen/Qwen2.5-Coder-7B-Instruct\"].description": "Qwen2.5-Coder-7B-Instruct is the latest version in Alibaba Cloud's series of code-specific large language models. This model significantly enhances code generation, reasoning, and repair capabilities based on Qwen2.5, trained on 55 trillion tokens. It not only improves coding abilities but also maintains advantages in mathematics and general capabilities, providing a more comprehensive foundation for practical applications such as code agents.",
  "[\"Qwen/Qwen2.5-VL-32B-Instruct\"].description": "Qwen2.5-VL-32B-Instruct is a multimodal large language model developed by the Tongyi Qianwen team, representing part of the Qwen2.5-VL series. This model excels not only in recognizing common objects but also in analyzing text, charts, icons, graphics, and layouts within images. It functions as a visual agent capable of reasoning and dynamically manipulating tools, with the ability to operate computers and mobile devices. Additionally, the model can precisely locate objects in images and generate structured outputs for documents like invoices and tables. Compared to its predecessor Qwen2-VL, this version demonstrates enhanced mathematical and problem-solving capabilities through reinforcement learning, while also exhibiting more human-preferred response styles.",
  "[\"Qwen/Qwen2.5-VL-72B-Instruct\"].description": "Qwen2.5-VL is the vision-language model in the Qwen2.5 series. This model demonstrates significant improvements across multiple dimensions: enhanced visual comprehension capable of recognizing common objects, analyzing text, charts, and layouts; serving as a visual agent that can reason and dynamically guide tool usage; supporting understanding of long videos exceeding 1 hour while capturing key events; able to precisely locate objects in images by generating bounding boxes or points; and capable of producing structured outputs particularly suitable for scanned data like invoices and forms.",
  "[\"Qwen2.5-14B-Instruct\"].description": "Qwen2.5-14B-Instruct is a large language model with 14 billion parameters, delivering excellent performance, optimized for Chinese and multilingual scenarios, and supporting applications such as intelligent Q&A and content generation.",
  "[\"Qwen2.5-32B-Instruct\"].description": "Qwen2.5-32B-Instruct is a large language model with 32 billion parameters, offering balanced performance, optimized for Chinese and multilingual scenarios, and supporting applications such as intelligent Q&A and content generation.",
  "[\"Qwen2.5-72B-Instruct\"].description": "Qwen2.5-72B-Instruct supports 16k context and generates long texts exceeding 8K. It enables seamless interaction with external systems through function calls, greatly enhancing flexibility and scalability. The model's knowledge has significantly increased, and its coding and mathematical abilities have been greatly improved, with multilingual support for over 29 languages.",
  "[\"Qwen2.5-7B-Instruct\"].description": "Qwen2.5-7B-Instruct is a large language model with 7 billion parameters, supporting function calls and seamless interaction with external systems, greatly enhancing flexibility and scalability. It is optimized for Chinese and multilingual scenarios, supporting applications such as intelligent Q&A and content generation.",
  "[\"Qwen2.5-Coder-14B-Instruct\"].description": "Qwen2.5-Coder-14B-Instruct is a large-scale pre-trained programming instruction model with strong code understanding and generation capabilities, efficiently handling various programming tasks, particularly suited for intelligent code writing, automated script generation, and programming problem-solving.",
  "[\"Qwen2.5-Coder-32B-Instruct\"].description": "Qwen2.5-Coder-32B-Instruct is a large language model specifically designed for code generation, code understanding, and efficient development scenarios, featuring an industry-leading 32 billion parameters to meet diverse programming needs.",
  "[\"THUDM/GLM-4.1V-9B-Thinking\"].description": "GLM-4.1V-9B-Thinking is an open-source vision-language model (VLM) jointly released by Zhipu AI and Tsinghua University's KEG Lab, designed specifically for handling complex multimodal cognitive tasks. Based on the GLM-4-9B-0414 foundation model, it significantly enhances cross-modal reasoning ability and stability by introducing the Chain-of-Thought reasoning mechanism and employing reinforcement learning strategies.",
  "[\"abab5.5-chat\"].description": "Targeted at productivity scenarios, supporting complex task processing and efficient text generation, suitable for professional applications.",
  "[\"abab5.5s-chat\"].description": "Designed for Chinese persona dialogue scenarios, providing high-quality Chinese dialogue generation capabilities, suitable for various application contexts.",
  "[\"abab6.5g-chat\"].description": "Designed for multilingual persona dialogue, supporting high-quality dialogue generation in English and other languages.",
  "[\"abab6.5s-chat\"].description": "Suitable for a wide range of natural language processing tasks, including text generation and dialogue systems.",
  "[\"abab6.5t-chat\"].description": "Optimized for Chinese persona dialogue scenarios, providing smooth dialogue generation that aligns with Chinese expression habits.",
  "[\"ai21-jamba-1.5-large\"].description": "A 398B parameter (94B active) multilingual model, offering a 256K long context window, function calling, structured output, and grounded generation.",
  "[\"ai21-jamba-1.5-mini\"].description": "A 52B parameter (12B active) multilingual model, offering a 256K long context window, function calling, structured output, and grounded generation.",
  "[\"ai21-labs/AI21-Jamba-1.5-Large\"].description": "A 398B parameter (94B active) multilingual model providing a 256K long context window, function calling, structured output, and fact-based generation.",
  "[\"ai21-labs/AI21-Jamba-1.5-Mini\"].description": "A 52B parameter (12B active) multilingual model offering a 256K long context window, function calling, structured output, and fact-based generation.",
  "[\"anthropic.claude-3-5-sonnet-20240620-v1:0\"].description": "Claude 3.5 Sonnet raises the industry standard, outperforming competitor models and Claude 3 Opus, excelling in a wide range of evaluations while maintaining the speed and cost of our mid-tier models.",
  "[\"anthropic.claude-3-5-sonnet-20241022-v2:0\"].description": "Claude 3.5 Sonnet raises the industry standard, outperforming competing models and Claude 3 Opus, excelling in extensive evaluations while maintaining the speed and cost of our mid-tier models.",
  "[\"anthropic.claude-3-haiku-20240307-v1:0\"].description": "Claude 3 Haiku is Anthropic's fastest and most compact model, providing near-instantaneous response times. It can quickly answer simple queries and requests. Customers will be able to build seamless AI experiences that mimic human interaction. Claude 3 Haiku can process images and return text output, with a context window of 200K.",
  "[\"anthropic.claude-3-opus-20240229-v1:0\"].description": "Claude 3 Opus is Anthropic's most powerful AI model, featuring state-of-the-art performance on highly complex tasks. It can handle open-ended prompts and unseen scenarios, demonstrating exceptional fluency and human-like understanding. Claude 3 Opus showcases the forefront of generative AI possibilities. Claude 3 Opus can process images and return text output, with a context window of 200K.",
  "[\"anthropic.claude-3-sonnet-20240229-v1:0\"].description": "Anthropic's Claude 3 Sonnet strikes an ideal balance between intelligence and speed—especially suited for enterprise workloads. It offers maximum utility at a price lower than competitors and is designed to be a reliable, durable workhorse for scalable AI deployments. Claude 3 Sonnet can process images and return text output, with a context window of 200K.",
  "[\"anthropic.claude-instant-v1\"].description": "A fast, economical, yet still highly capable model that can handle a range of tasks, including everyday conversations, text analysis, summarization, and document Q&A.",
  "[\"anthropic.claude-v2\"].description": "Anthropic's model demonstrates high capability across a wide range of tasks, from complex conversations and creative content generation to detailed instruction following.",
  "[\"anthropic.claude-v2:1\"].description": "An updated version of Claude 2, featuring double the context window and improvements in reliability, hallucination rates, and evidence-based accuracy in long documents and RAG contexts.",
  "[\"anthropic/claude-3.5-haiku\"].description": "Claude 3.5 Haiku offers enhanced speed, coding accuracy, and tool usage. Ideal for scenarios requiring high performance in speed and tool interaction.",
  "[\"anthropic/claude-3.5-sonnet\"].description": "Claude 3.5 Sonnet is a fast and efficient model in the Sonnet family, offering improved coding and reasoning performance. Some versions will gradually be replaced by models like Sonnet 3.7.",
  "[\"anthropic/claude-3.7-sonnet\"].description": "Claude 3.7 Sonnet is an upgraded model in the Sonnet series, delivering stronger reasoning and coding capabilities, suitable for complex enterprise-level tasks.",
  "[\"anthropic/claude-haiku-4.5\"].description": "Claude Haiku 4.5 is a high-performance, low-latency model from Anthropic that maintains high accuracy.",
  "[\"anthropic/claude-opus-4.1\"].description": "Opus 4.1 is a premium model from Anthropic, optimized for programming, complex reasoning, and sustained tasks.",
  "[\"anthropic/claude-opus-4.5\"].description": "Claude Opus 4.5 is Anthropic's flagship model, combining exceptional intelligence with scalable performance. It is ideal for complex tasks that demand top-tier response quality and reasoning capabilities.",
  "[\"anthropic/claude-sonnet-4.5\"].description": "Claude Sonnet 4.5 is Anthropic’s latest hybrid reasoning model, optimized for complex reasoning and coding tasks.",
  "[\"baidu/ERNIE-4.5-300B-A47B\"].description": "ERNIE-4.5-300B-A47B is a large language model developed by Baidu based on a Mixture of Experts (MoE) architecture. The model has a total of 300 billion parameters, but only activates 47 billion parameters per token during inference, balancing powerful performance with computational efficiency. As a core model in the ERNIE 4.5 series, it demonstrates outstanding capabilities in text understanding, generation, reasoning, and programming tasks. The model employs an innovative multimodal heterogeneous MoE pretraining method, jointly training text and visual modalities to effectively enhance overall capabilities, especially excelling in instruction following and world knowledge retention.",
  "[\"baidu/ernie-5.0-thinking-preview\"].description": "ERNIE 5.0 Thinking Preview is Baidu’s next-generation native multimodal Wenxin model, excelling in multimodal understanding, instruction following, content creation, factual Q&A, and tool usage.",
  "[\"black-forest-labs/flux-1.1-pro\"].description": "FLUX 1.1 Pro - A faster and enhanced version of FLUX Pro, delivering exceptional image quality and prompt adherence.",
  "[\"claude-2.0\"].description": "Claude 2 provides advancements in key capabilities for enterprises, including industry-leading 200K token context, significantly reducing the occurrence of model hallucinations, system prompts, and a new testing feature: tool invocation.",
  "[\"claude-2.1\"].description": "Claude 2 provides advancements in key capabilities for enterprises, including industry-leading 200K token context, significantly reducing the occurrence of model hallucinations, system prompts, and a new testing feature: tool invocation.",
  "[\"cogito-2.1:671b\"].description": "Cogito v2.1 671B is a U.S.-based open-source large language model available for free commercial use. It offers top-tier performance, high token inference efficiency, 128k long context, and robust general capabilities.",
  "[\"cohere/embed-v4.0\"].description": "A model that enables classification or embedding transformation of text, images, or mixed content.",
  "[\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"].description": "The DeepSeek-R1 distillation model optimizes inference performance through reinforcement learning and cold-start data, refreshing the benchmark for open-source models across multiple tasks.",
  "[\"deepseek-ai/DeepSeek-V2.5\"].description": "DeepSeek V2.5 combines the excellent features of previous versions, enhancing general and coding capabilities.",
  "[\"deepseek-ai/DeepSeek-V3.1\"].description": "The DeepSeek V3.1 model features a hybrid reasoning architecture that supports both reasoning and non-reasoning modes.",
  "[\"deepseek-ai/DeepSeek-V3.1-Terminus\"].description": "DeepSeek-V3.1-Terminus is an updated version of the V3.1 model released by DeepSeek, positioned as a hybrid agent large language model. This update focuses on fixing user-reported issues and improving stability while maintaining the model's original capabilities. It significantly enhances language consistency, reducing the mixing of Chinese and English and the occurrence of abnormal characters. The model integrates both \"Thinking Mode\" and \"Non-thinking Mode,\" allowing users to switch flexibly between chat templates to suit different tasks. As a key optimization, V3.1-Terminus improves the performance of the Code Agent and Search Agent, making tool invocation and multi-step complex task execution more reliable.",
  "[\"deepseek-ai/DeepSeek-V3.2-Exp\"].description": "DeepSeek-V3.2-Exp is an experimental version released by DeepSeek as an intermediate step toward the next-generation architecture. Building on V3.1-Terminus, it introduces the DeepSeek Sparse Attention (DSA) mechanism to enhance training and inference efficiency for long-context scenarios. It features targeted optimizations for tool use, long-document comprehension, and multi-step reasoning. V3.2-Exp serves as a bridge between research and production, ideal for users seeking higher inference efficiency in high-context-budget applications.",
  "[\"deepseek-ai/deepseek-v3.1\"].description": "DeepSeek V3.1: The next-generation reasoning model that enhances complex reasoning and chain-of-thought capabilities, suitable for tasks requiring in-depth analysis.",
  "[\"deepseek-ai/deepseek-v3.1-terminus\"].description": "DeepSeek V3.1: A next-generation reasoning model designed to enhance complex reasoning and chain-of-thought capabilities, ideal for tasks requiring in-depth analysis.",
  "[\"deepseek-r1-distill-qwen-1.5b\"].description": "DeepSeek R1 Distill Qwen 1.5B, an ultra-lightweight R1 distilled model designed for extremely low-resource environments.",
  "[\"deepseek-v3.1\"].description": "DeepSeek-V3.1 is a newly launched hybrid reasoning model by DeepSeek, supporting two reasoning modes: thinking and non-thinking. It offers higher thinking efficiency compared to DeepSeek-R1-0528. With post-training optimization, the use of Agent tools and agent task performance have been significantly enhanced. It supports a 128k context window and an output length of up to 64k tokens.",
  "[\"deepseek-v3.1-terminus\"].description": "DeepSeek-V3.1-Terminus is an optimized large language model developed by DeepSeek, specifically tailored for terminal devices.",
  "[\"deepseek-v3.1-think-250821\"].description": "DeepSeek V3.1 Think 250821, a deep reasoning model aligned with the Terminus version, suitable for high-performance inference scenarios.",
  "[\"deepseek-v3.1:671b\"].description": "DeepSeek V3.1: The next-generation reasoning model that enhances complex reasoning and chain-of-thought capabilities, suitable for tasks requiring in-depth analysis.",
  "[\"deepseek-v3.2-exp\"].description": "deepseek-v3.2-exp introduces a sparse attention mechanism designed to enhance training and inference efficiency when processing long texts, priced lower than deepseek-v3.1.",
  "[\"deepseek-v3.2-think\"].description": "DeepSeek V3.2 Think, a full-performance deep reasoning model with enhanced long-chain reasoning capabilities.",
  "[\"deepseek/deepseek-chat-v3.1\"].description": "DeepSeek-V3.1 is a long-context hybrid reasoning model from DeepSeek, supporting cognitive/non-cognitive hybrid modes and tool integration.",
  "[\"deepseek/deepseek-v3.1-base\"].description": "DeepSeek V3.1 Base is an improved version of the DeepSeek V3 model.",
  "[\"doubao-1.5-lite-32k\"].description": "Doubao-1.5-lite is a new generation lightweight model, offering extreme response speed with performance and latency at a world-class level.",
  "[\"doubao-1.5-pro-256k\"].description": "Doubao-1.5-pro-256k is an upgraded version of Doubao-1.5-Pro, significantly enhancing overall performance by 10%. It supports reasoning with a 256k context window and an output length of up to 12k tokens. With higher performance, a larger window, and exceptional cost-effectiveness, it is suitable for a wider range of applications.",
  "[\"doubao-1.5-pro-32k\"].description": "Doubao-1.5-pro is a new generation flagship model with comprehensive performance upgrades, excelling in knowledge, coding, reasoning, and more.",
  "[\"doubao-1.5-thinking-pro\"].description": "Doubao-1.5 is a brand new deep thinking model that excels in specialized fields such as mathematics, programming, and scientific reasoning, as well as in general tasks like creative writing. It has achieved or is close to the top tier of industry standards in several authoritative benchmarks, including AIME 2024, Codeforces, and GPQA. It supports a 128k context window and 16k output.",
  "[\"doubao-1.5-thinking-pro-m\"].description": "Doubao-1.5 is a brand-new deep thinking model (the m version comes with native multimodal deep reasoning capabilities). It performs outstandingly in specialized fields such as mathematics, programming, scientific reasoning, as well as general tasks like creative writing. It achieves or approaches top-tier industry benchmarks on AIME 2024, Codeforces, GPQA, and more. Supports a 128k context window and 16k output.",
  "[\"doubao-1.5-thinking-vision-pro\"].description": "A new visual deep thinking model with enhanced general multimodal understanding and reasoning capabilities, achieving state-of-the-art (SOTA) results on 37 out of 59 public evaluation benchmarks.",
  "[\"doubao-1.5-ui-tars\"].description": "Doubao-1.5-UI-TARS is a native agent model designed for graphical user interface (GUI) interaction. It seamlessly interacts with GUIs through human-like abilities such as perception, reasoning, and action.",
  "[\"doubao-1.5-vision-lite\"].description": "Doubao-1.5-vision-lite is a newly upgraded multimodal large model that supports image recognition at any resolution and extreme aspect ratios, enhancing visual reasoning, document recognition, detail comprehension, and instruction following capabilities. It supports a context window of 128k and an output length of up to 16k tokens.",
  "[\"doubao-1.5-vision-pro\"].description": "Doubao-1.5-vision-pro is a newly upgraded multimodal large model supporting image recognition at any resolution and extreme aspect ratios. It enhances visual reasoning, document recognition, detailed information understanding, and instruction compliance.",
  "[\"doubao-1.5-vision-pro-32k\"].description": "Doubao-1.5-vision-pro is a newly upgraded multimodal large model supporting image recognition at any resolution and extreme aspect ratios. It enhances visual reasoning, document recognition, detailed information understanding, and instruction compliance.",
  "[\"doubao-seed-1.6\"].description": "Doubao-Seed-1.6 is a brand-new multimodal deep thinking model supporting auto, thinking, and non-thinking modes. In non-thinking mode, its performance significantly surpasses Doubao-1.5-pro/250115. It supports a 256k context window and output lengths up to 16k tokens.",
  "[\"doubao-seed-1.6-flash\"].description": "Doubao-Seed-1.6-flash is an ultra-fast multimodal deep thinking model with TPOT inference speed as low as 10ms; it supports both text and visual understanding. Its text comprehension exceeds the previous lite generation, and its visual understanding rivals competitor pro series models. It supports a 256k context window and output lengths up to 16k tokens.",
  "[\"doubao-seed-1.6-lite\"].description": "Doubao-Seed-1.6-lite is a new multimodal deep reasoning model with adjustable reasoning effort—Minimal, Low, Medium, and High. It offers exceptional cost-performance and is an ideal choice for common tasks, supporting context windows up to 256k.",
  "[\"doubao-seed-1.6-thinking\"].description": "Doubao-Seed-1.6-thinking features greatly enhanced thinking capabilities. Compared to Doubao-1.5-thinking-pro, it further improves foundational skills such as coding, math, and logical reasoning, and supports visual understanding. It supports a 256k context window and output lengths up to 16k tokens.",
  "[\"doubao-seed-1.6-vision\"].description": "Doubao-Seed-1.6-vision is a visual deep thinking model that demonstrates stronger general multimodal understanding and reasoning capabilities in scenarios such as education, image review, inspection and security, and AI search Q&A. It supports a 256k context window and an output length of up to 64k tokens.",
  "[\"ernie-4.5-0.3b\"].description": "ERNIE 4.5 0.3B, an open-source lightweight model suitable for local and customized deployments.",
  "[\"ernie-4.5-21b-a3b\"].description": "ERNIE 4.5 21B A3B, an open-source large-parameter model with stronger performance in understanding and generation tasks.",
  "[\"ernie-4.5-300b-a47b\"].description": "ERNIE 4.5 300B A47B is a large-scale Mixture of Experts model from Baidu's Wenxin series, delivering exceptional reasoning performance.",
  "[\"ernie-4.5-8k-preview\"].description": "ERNIE 4.5 8K Preview, an 8K context preview model for experiencing and testing ERNIE 4.5 capabilities.",
  "[\"ernie-4.5-turbo-128k\"].description": "ERNIE 4.5 Turbo 128K, a high-performance general-purpose model supporting search augmentation and tool invocation, suitable for Q&A, coding, agents, and more.",
  "[\"ernie-4.5-turbo-128k-preview\"].description": "ERNIE 4.5 Turbo 128K Preview, offering the same capabilities as the official version, ideal for integration testing and staging.",
  "[\"ernie-4.5-turbo-32k\"].description": "ERNIE 4.5 Turbo 32K, a medium-to-long context version suitable for Q&A, knowledge retrieval, and multi-turn conversations.",
  "[\"ernie-4.5-turbo-latest\"].description": "ERNIE 4.5 Turbo Latest, optimized for overall performance, ideal as a general-purpose production model.",
  "[\"ernie-4.5-turbo-vl\"].description": "ERNIE 4.5 Turbo VL, a mature multimodal model for image-text understanding and recognition in production environments.",
  "[\"ernie-4.5-turbo-vl-32k\"].description": "ERNIE 4.5 Turbo VL 32K, a medium-to-long text multimodal version for joint understanding of long documents and images.",
  "[\"ernie-4.5-turbo-vl-32k-preview\"].description": "ERNIE 4.5 Turbo VL 32K Preview, a 32K multimodal preview version for evaluating long-context visual capabilities.",
  "[\"ernie-4.5-turbo-vl-latest\"].description": "ERNIE 4.5 Turbo VL Latest, the latest multimodal version offering improved image-text understanding and reasoning.",
  "[\"ernie-4.5-turbo-vl-preview\"].description": "ERNIE 4.5 Turbo VL Preview, a multimodal preview model supporting image-text understanding and generation, ideal for visual Q&A and content comprehension.",
  "[\"ernie-4.5-vl-28b-a3b\"].description": "ERNIE 4.5 VL 28B A3B, an open-source multimodal model supporting image-text understanding and reasoning tasks.",
  "[\"ernie-5.0-thinking-latest\"].description": "ERNIE 5.0 Thinking is a flagship native multimodal model that supports unified modeling of text, images, audio, and video. With comprehensive capability upgrades, it is well-suited for complex Q&A, creative tasks, and intelligent agent applications.",
  "[\"ernie-5.0-thinking-preview\"].description": "ERNIE 5.0 Thinking Preview, a native all-modality flagship model supporting unified modeling of text, image, audio, and video, with comprehensive capability upgrades for complex Q&A, creative tasks, and agent scenarios.",
  "[\"ernie-x1.1-preview\"].description": "ERNIE X1.1 Preview, a preview version of the ERNIE X1.1 reasoning model, suitable for capability validation and testing.",
  "[\"flux-pro-1.1\"].description": "Upgraded professional-grade AI image generation model — delivers outstanding image quality and precise adherence to prompts.",
  "[\"flux-pro-1.1-ultra\"].description": "Ultra-high-resolution AI image generation — supports up to 4-megapixel output, producing ultra-high-definition images in under 10 seconds.",
  "[\"flux.1-schnell\"].description": "FLUX.1-schnell, a high-performance image generation model for fast creation of multi-style images.",
  "[\"gemini-1.0-pro-001\"].description": "Gemini 1.0 Pro 001 (Tuning) offers stable and tunable performance, making it an ideal choice for complex task solutions.",
  "[\"gemini-1.0-pro-002\"].description": "Gemini 1.0 Pro 002 (Tuning) provides excellent multimodal support, focusing on effective solutions for complex tasks.",
  "[\"gemini-1.0-pro-latest\"].description": "Gemini 1.0 Pro is Google's high-performance AI model, designed for extensive task scaling.",
  "[\"gemini-1.5-flash-001\"].description": "Gemini 1.5 Flash 001 is an efficient multimodal model that supports extensive application scaling.",
  "[\"gemini-1.5-flash-002\"].description": "Gemini 1.5 Flash 002 is an efficient multimodal model that supports a wide range of applications.",
  "[\"gemini-1.5-flash-8b\"].description": "Gemini 1.5 Flash 8B is an efficient multimodal model that supports a wide range of applications.",
  "[\"gemini-1.5-flash-8b-exp-0924\"].description": "Gemini 1.5 Flash 8B 0924 is the latest experimental model, showcasing significant performance improvements in both text and multimodal use cases.",
  "[\"gemini-1.5-flash-8b-latest\"].description": "Gemini 1.5 Flash 8B is a highly efficient multimodal model designed for scalable applications.",
  "[\"gemini-1.5-flash-exp-0827\"].description": "Gemini 1.5 Flash 0827 provides optimized multimodal processing capabilities, suitable for various complex task scenarios.",
  "[\"gemini-1.5-flash-latest\"].description": "Gemini 1.5 Flash is Google's latest multimodal AI model, featuring fast processing capabilities and supporting text, image, and video inputs, making it suitable for efficient scaling across various tasks.",
  "[\"gemini-1.5-pro-001\"].description": "Gemini 1.5 Pro 001 is a scalable multimodal AI solution that supports a wide range of complex tasks.",
  "[\"gemini-1.5-pro-002\"].description": "Gemini 1.5 Pro 002 is the latest production-ready model, delivering higher quality outputs, with notable enhancements in mathematics, long-context, and visual tasks.",
  "[\"gemini-1.5-pro-exp-0801\"].description": "Gemini 1.5 Pro 0801 offers excellent multimodal processing capabilities, providing greater flexibility for application development.",
  "[\"gemini-1.5-pro-exp-0827\"].description": "Gemini 1.5 Pro 0827 combines the latest optimization technologies for more efficient multimodal data processing.",
  "[\"gemini-1.5-pro-latest\"].description": "Gemini 1.5 Pro supports up to 2 million tokens, making it an ideal choice for medium-sized multimodal models, providing multifaceted support for complex tasks.",
  "[\"gemini-2.0-flash\"].description": "Gemini 2.0 Flash offers next-generation features and improvements, including exceptional speed, native tool usage, multimodal generation, and a 1M token context window.",
  "[\"gemini-2.0-flash-001\"].description": "Gemini 2.0 Flash offers next-generation features and improvements, including exceptional speed, native tool usage, multimodal generation, and a 1M token context window.",
  "[\"gemini-2.0-flash-exp\"].description": "Gemini 2.0 Flash model variant optimized for cost-effectiveness and low latency.",
  "[\"gemini-2.0-flash-exp-image-generation\"].description": "Gemini 2.0 Flash experimental model, supports image generation",
  "[\"gemini-2.0-flash-lite\"].description": "Gemini 2.0 Flash is a variant of the model optimized for cost-effectiveness and low latency.",
  "[\"gemini-2.0-flash-lite-001\"].description": "Gemini 2.0 Flash is a variant of the model optimized for cost-effectiveness and low latency.",
  "[\"gemini-2.5-flash\"].description": "Gemini 2.5 Flash is Google's most cost-effective model, offering comprehensive capabilities.",
  "[\"gemini-2.5-flash-image\"].description": "Nano Banana is Google's latest, fastest, and most efficient native multimodal model, allowing you to generate and edit images through conversation.",
  "[\"gemini-2.5-flash-image-preview\"].description": "Nano Banana is Google's latest, fastest, and most efficient native multimodal model, enabling you to generate and edit images through conversation.",
  "[\"gemini-2.5-flash-image-preview:image\"].description": "Nano Banana is Google's latest, fastest, and most efficient native multimodal model, enabling you to generate and edit images through conversation.",
  "[\"gemini-2.5-flash-image:image\"].description": "Nano Banana is Google's latest, fastest, and most efficient native multimodal model, allowing you to generate and edit images through conversation.",
  "[\"gemini-2.5-flash-lite\"].description": "Gemini 2.5 Flash-Lite is Google's smallest and most cost-effective model, designed for large-scale use.",
  "[\"gemini-2.5-flash-lite-preview-06-17\"].description": "Gemini 2.5 Flash-Lite Preview is Google's smallest and most cost-efficient model, designed for large-scale usage.",
  "[\"gemini-2.5-flash-lite-preview-09-2025\"].description": "Preview release (September 25th, 2025) of Gemini 2.5 Flash-Lite",
  "[\"gemini-2.5-flash-preview-04-17\"].description": "Gemini 2.5 Flash Preview is Google's most cost-effective model, offering a comprehensive set of features.",
  "[\"gemini-2.5-flash-preview-09-2025\"].description": "Preview release (September 25th, 2025) of Gemini 2.5 Flash",
  "[\"gemini-2.5-pro\"].description": "Gemini 2.5 Pro is Google's most advanced reasoning model, capable of tackling complex problems in coding, mathematics, and STEM fields, as well as analyzing large datasets, codebases, and documents using long-context processing.",
  "[\"gemini-2.5-pro-preview-03-25\"].description": "Gemini 2.5 Pro Preview is Google's most advanced thinking model, capable of reasoning about complex problems in code, mathematics, and STEM fields, as well as analyzing large datasets, codebases, and documents using long-context analysis.",
  "[\"gemini-2.5-pro-preview-05-06\"].description": "Gemini 2.5 Pro Preview is Google's most advanced reasoning model, capable of reasoning about complex problems in code, mathematics, and STEM fields, as well as analyzing large datasets, codebases, and documents using long context.",
  "[\"gemini-2.5-pro-preview-06-05\"].description": "Gemini 2.5 Pro Preview is Google's most advanced cognitive model, capable of reasoning through complex problems in code, mathematics, and STEM fields, as well as analyzing large datasets, codebases, and documents using long-context understanding.",
  "[\"generalv3.5\"].description": "Spark3.5 Max is the most comprehensive version, supporting online search and numerous built-in plugins. Its fully optimized core capabilities, along with system role settings and function calling features, enable it to perform exceptionally well in various complex application scenarios.",
  "[\"glm-4.1v-thinking-flash\"].description": "The GLM-4.1V-Thinking series represents the most powerful vision-language models known at the 10B parameter scale, integrating state-of-the-art capabilities across various vision-language tasks such as video understanding, image question answering, academic problem solving, OCR text recognition, document and chart interpretation, GUI agents, front-end web coding, and grounding. Its performance in many tasks even surpasses that of Qwen2.5-VL-72B, which has over eight times the parameters. Leveraging advanced reinforcement learning techniques, the model masters Chain-of-Thought reasoning to improve answer accuracy and richness, significantly outperforming traditional non-thinking models in final results and interpretability.",
  "[\"glm-4.1v-thinking-flashx\"].description": "The GLM-4.1V-Thinking series represents the most powerful vision-language models known at the 10B parameter scale, integrating state-of-the-art capabilities across various vision-language tasks such as video understanding, image question answering, academic problem solving, OCR text recognition, document and chart interpretation, GUI agents, front-end web coding, and grounding. Its performance in many tasks even surpasses that of Qwen2.5-VL-72B, which has over eight times the parameters. Leveraging advanced reinforcement learning techniques, the model masters Chain-of-Thought reasoning to improve answer accuracy and richness, significantly outperforming traditional non-thinking models in final results and interpretability.",
  "[\"glm-4.5\"].description": "Zhipu's flagship model supports thinking mode switching, with comprehensive capabilities reaching the state-of-the-art level among open-source models, and a context length of up to 128K.",
  "[\"glm-4.5-air\"].description": "A lightweight version of GLM-4.5 balancing performance and cost-effectiveness, capable of flexibly switching hybrid thinking models.",
  "[\"glm-4.5-airx\"].description": "The ultra-fast version of GLM-4.5-Air, offering faster response speeds, designed for large-scale high-speed demands.",
  "[\"glm-4.5-flash\"].description": "The free version of GLM-4.5, delivering excellent performance in inference, coding, and agent tasks.",
  "[\"glm-4.5-x\"].description": "The high-speed version of GLM-4.5, combining strong performance with generation speeds up to 100 tokens per second.",
  "[\"glm-4.5v\"].description": "Zhipu's next-generation visual reasoning model is built on a Mixture-of-Experts (MoE) architecture. With 106B total parameters and 12B activated parameters, it achieves state-of-the-art performance among open-source multimodal models of similar scale across various benchmarks, supporting common tasks such as image, video, document understanding, and GUI-related tasks.",
  "[\"glm-4.6\"].description": "Zhipu's latest flagship model GLM-4.6 (355B) surpasses its predecessor comprehensively in advanced encoding, long text processing, reasoning, and agent capabilities, especially aligning with Claude Sonnet 4 in programming skills, making it a top-tier coding model in China.",
  "[\"global.anthropic.claude-opus-4-5-20251101-v1:0\"].description": "Claude Opus 4.5 is Anthropic's flagship model, combining exceptional intelligence with scalable performance, ideal for complex tasks that demand top-tier response quality and reasoning capabilities.",
  "[\"google/gemini-2.0-flash\"].description": "Gemini 2.0 Flash is Google’s high-performance reasoning model, suitable for extended multimodal tasks.",
  "[\"google/gemini-2.0-flash-001\"].description": "Gemini 2.0 Flash offers next-generation features and improvements, including exceptional speed, native tool usage, multimodal generation, and a 1M token context window.",
  "[\"google/gemini-2.0-flash-exp:free\"].description": "Gemini 2.0 Flash Experimental is Google's latest experimental multimodal AI model, showing a quality improvement compared to historical versions, especially in world knowledge, code, and long context.",
  "[\"google/gemini-2.0-flash-lite\"].description": "Gemini 2.0 Flash Lite provides next-generation features and improvements, including exceptional speed, built-in tool usage, multimodal generation, and a 1 million token context window.",
  "[\"google/gemini-2.0-flash-lite-001\"].description": "Gemini 2.0 Flash Lite is a lightweight version of the Gemini family. By default, it disables reasoning to improve latency and cost efficiency, but it can be enabled via parameters.",
  "[\"google/gemini-2.5-flash\"].description": "The Gemini 2.5 Flash (Lite/Pro/Flash) series are Google’s reasoning models ranging from low-latency to high-performance.",
  "[\"google/gemini-2.5-flash-image\"].description": "Gemini 2.5 Flash Image (Nano Banana) is Google’s image generation model, also supporting multimodal dialogue.",
  "[\"google/gemini-2.5-flash-image-free\"].description": "Gemini 2.5 Flash Image Free Edition supports limited multimodal generation.",
  "[\"google/gemini-2.5-flash-image-preview\"].description": "Gemini 2.5 Flash experimental model, supporting image generation.",
  "[\"google/gemini-2.5-flash-lite\"].description": "Gemini 2.5 Flash Lite is a lightweight version of Gemini 2.5, optimized for latency and cost, ideal for high-throughput scenarios.",
  "[\"google/gemini-2.5-flash-preview\"].description": "Gemini 2.5 Flash is Google's most advanced flagship model, designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in 'thinking' capabilities that allow it to provide responses with higher accuracy and detailed context handling.\n\nNote: This model has two variants: thinking and non-thinking. Output pricing varies significantly based on whether the thinking capability is activated. If you choose the standard variant (without the ':thinking' suffix), the model will explicitly avoid generating thinking tokens.\n\nTo leverage the thinking capability and receive thinking tokens, you must select the ':thinking' variant, which will incur higher thinking output pricing.\n\nAdditionally, Gemini 2.5 Flash can be configured via the 'maximum tokens for reasoning' parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
  "[\"google/gemini-2.5-flash-preview:thinking\"].description": "Gemini 2.5 Flash is Google's most advanced flagship model, designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in 'thinking' capabilities that allow it to provide responses with higher accuracy and detailed context handling.\n\nNote: This model has two variants: thinking and non-thinking. Output pricing varies significantly based on whether the thinking capability is activated. If you choose the standard variant (without the ':thinking' suffix), the model will explicitly avoid generating thinking tokens.\n\nTo leverage the thinking capability and receive thinking tokens, you must select the ':thinking' variant, which will incur higher thinking output pricing.\n\nAdditionally, Gemini 2.5 Flash can be configured via the 'maximum tokens for reasoning' parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
  "[\"google/gemini-2.5-pro\"].description": "Gemini 2.5 Pro is Google’s flagship reasoning model, supporting long context and complex tasks.",
  "[\"google/gemini-2.5-pro-free\"].description": "Gemini 2.5 Pro Free Edition supports limited multimodal long-context usage, ideal for trials and lightweight workflows.",
  "[\"google/gemini-2.5-pro-preview\"].description": "Gemini 2.5 Pro Preview is Google's most advanced thinking model, capable of reasoning through complex problems in code, mathematics, and STEM fields, as well as analyzing large datasets, codebases, and documents using extended context.",
  "[\"google/gemini-flash-1.5\"].description": "Gemini 1.5 Flash offers optimized multimodal processing capabilities, suitable for various complex task scenarios.",
  "[\"google/gemini-pro-1.5\"].description": "Gemini 1.5 Pro combines the latest optimization technologies to deliver more efficient multimodal data processing capabilities.",
  "[\"gpt-3.5-turbo\"].description": "GPT 3.5 Turbo is suitable for various text generation and understanding tasks. Currently points to gpt-3.5-turbo-0125.",
  "[\"gpt-3.5-turbo-0125\"].description": "GPT 3.5 Turbo is suitable for various text generation and understanding tasks. Currently points to gpt-3.5-turbo-0125.",
  "[\"gpt-3.5-turbo-1106\"].description": "GPT 3.5 Turbo is suitable for various text generation and understanding tasks. Currently points to gpt-3.5-turbo-0125.",
  "[\"gpt-3.5-turbo-instruct\"].description": "GPT 3.5 Turbo is suitable for various text generation and understanding tasks. Currently points to gpt-3.5-turbo-0125.",
  "[\"gpt-4.1\"].description": "GPT-4.1 is our flagship model for complex tasks. It excels at solving problems across various domains.",
  "[\"gpt-4.1-mini\"].description": "GPT-4.1 mini offers a balance of intelligence, speed, and cost, making it an attractive model for many use cases.",
  "[\"gpt-4.1-nano\"].description": "GPT-4.1 nano provides a balance of intelligence, speed, and cost, making it an appealing model for numerous applications.",
  "[\"gpt-4.5-preview\"].description": "GPT-4.5-preview is the latest general-purpose model, offering extensive world knowledge and an improved understanding of user intent. It excels at creative tasks and agent-style planning. The model's knowledge cutoff is October 2023.",
  "[\"gpt-5.1\"].description": "GPT-5.1 — A flagship model optimized for coding and agent tasks, featuring configurable reasoning strength and extended context support.",
  "[\"gpt-5.1-chat-latest\"].description": "GPT-5.1 Chat: A variant of GPT-5.1 tailored for ChatGPT, ideal for conversational scenarios.",
  "[\"gpt-5.1-codex\"].description": "GPT-5.1 Codex: A GPT-5.1 variant optimized for agentic coding tasks, designed for more complex code/agent workflows via the Responses API.",
  "[\"gpt-5.1-codex-mini\"].description": "GPT-5.1 Codex mini: A smaller, more cost-effective Codex variant optimized for agentic coding tasks.",
  "[\"gpt-5.2\"].description": "GPT-5.2 — A flagship model designed for coding and agentic workflows, offering enhanced reasoning and long-context capabilities.",
  "[\"gpt-5.2-chat-latest\"].description": "GPT-5.2 Chat: The GPT-5.2 variant used by ChatGPT (chat-latest), featuring the latest improvements in conversational experience.",
  "[\"gpt-5.2-pro\"].description": "GPT-5.2 Pro: A smarter, more precise version of GPT-5.2 (Responses API Only), ideal for complex problems and extended multi-turn reasoning.",
  "[\"imagen-4.0-fast-generate-001\"].description": "Imagen 4th-generation text-to-image model, Fast version",
  "[\"imagen-4.0-generate-001\"].description": "Imagen 4th-generation text-to-image model series",
  "[\"imagen-4.0-generate-preview-06-06\"].description": "Imagen 4th-generation text-to-image model series.",
  "[\"imagen-4.0-ultra-generate-001\"].description": "Imagen 4th-generation text-to-image model, Ultra version",
  "[\"imagen-4.0-ultra-generate-preview-06-06\"].description": "Ultra version of the 4th-generation Imagen text-to-image model series.",
  "[\"inclusionAI/Ling-flash-2.0\"].description": "Ling-flash-2.0 is the third model in the Ling 2.0 architecture series released by Ant Group's Bailing team. It is a mixture-of-experts (MoE) model with a total of 100 billion parameters, but activates only 6.1 billion parameters per token (4.8 billion non-embedding). As a lightweight configuration model, Ling-flash-2.0 demonstrates performance comparable to or surpassing 40-billion-parameter dense models and larger MoE models across multiple authoritative benchmarks. The model aims to explore efficient pathways under the consensus that \"large models equal large parameters\" through extreme architectural design and training strategies.",
  "[\"inclusionAI/Ling-mini-2.0\"].description": "Ling-mini-2.0 is a small-sized, high-performance large language model based on the MoE architecture. It has 16 billion total parameters but activates only 1.4 billion per token (789 million non-embedding), achieving extremely high generation speed. Thanks to the efficient MoE design and large-scale high-quality training data, despite activating only 1.4 billion parameters, Ling-mini-2.0 still delivers top-tier performance comparable to dense LLMs under 10 billion parameters and larger MoE models on downstream tasks.",
  "[\"inclusionAI/Ring-flash-2.0\"].description": "Ring-flash-2.0 is a high-performance reasoning model deeply optimized based on Ling-flash-2.0-base. It employs a mixture-of-experts (MoE) architecture with a total of 100 billion parameters but activates only 6.1 billion parameters per inference. The model uses the proprietary icepop algorithm to solve the instability issues of MoE large models during reinforcement learning (RL) training, enabling continuous improvement of complex reasoning capabilities over long training cycles. Ring-flash-2.0 has achieved significant breakthroughs in challenging benchmarks such as math competitions, code generation, and logical reasoning. Its performance not only surpasses top dense models under 40 billion parameters but also rivals larger open-source MoE models and closed-source high-performance reasoning models. Although focused on complex reasoning, it also performs well in creative writing tasks. Additionally, thanks to its efficient architecture, Ring-flash-2.0 delivers strong performance with high-speed inference, significantly reducing deployment costs for reasoning models in high-concurrency scenarios.",
  "[\"inclusionai/ling-flash-2.0\"].description": "Ling-flash-2.0 is inclusionAI’s MoE model, optimized for efficiency and reasoning performance, suitable for medium to large-scale tasks.",
  "[\"inclusionai/ling-mini-2.0\"].description": "Ling-mini-2.0 is a lightweight MoE model from inclusionAI, significantly reducing cost while maintaining reasoning capabilities.",
  "[\"inclusionai/ring-flash-2.0\"].description": "Ring-flash-2.0 is a high-throughput variant of the Ring model from inclusionAI, emphasizing speed and cost efficiency.",
  "[\"inclusionai/ring-mini-2.0\"].description": "Ring-mini-2.0 is a high-throughput, lightweight MoE version from inclusionAI, primarily used in concurrent scenarios.",
  "[\"internlm2.5-latest\"].description": "A legacy model line we continue to maintain. After multiple iterations, it remains highly capable and stable, with 7B and 20B variants available. It supports up to a 1M context window and offers stronger instruction following and tool-calling. By default, it points to our latest InternLM2.5 series model (currently `internlm2.5-20b-chat`).",
  "[\"internvl2.5-38b-mpo\"].description": "InternVL2.5 38B MPO, a multimodal pre-trained model supporting complex image-text reasoning tasks.",
  "[\"internvl2.5-latest\"].description": "The InternVL2.5 version we continue to maintain, offering excellent and stable performance. It defaults to our latest released InternVL2.5 series model, currently pointing to internvl2.5-78b.",
  "[\"irag-1.0\"].description": "ERNIE iRAG, an image retrieval-augmented generation model supporting image search, image-text retrieval, and content generation.",
  "[\"learnlm-1.5-pro-experimental\"].description": "LearnLM is an experimental, task-specific language model trained to align with learning science principles, capable of following systematic instructions in teaching and learning scenarios, acting as an expert tutor, among other roles.",
  "[\"learnlm-2.0-flash-experimental\"].description": "LearnLM is an experimental, task-specific language model trained to align with the principles of learning science, capable of following systematic instructions in teaching and learning scenarios, acting as an expert tutor, among other roles.",
  "[\"llama-3.1-70b-versatile\"].description": "Llama 3.1 70B provides enhanced AI reasoning capabilities, suitable for complex applications, supporting extensive computational processing while ensuring efficiency and accuracy.",
  "[\"llama-3.1-8b-instant\"].description": "Llama 3.1 8B is a high-performance model that offers rapid text generation capabilities, making it ideal for applications requiring large-scale efficiency and cost-effectiveness.",
  "[\"llama-3.1-instruct\"].description": "The Llama 3.1 instruction-tuned model is optimized for conversational scenarios, outperforming many existing open-source chat models on common industry benchmarks.",
  "[\"llama-3.2-11b-vision-instruct\"].description": "Excellent image reasoning capabilities on high-resolution images, suitable for visual understanding applications.",
  "[\"llama-3.2-11b-vision-preview\"].description": "Llama 3.2 is designed to handle tasks that combine visual and textual data. It excels in tasks such as image description and visual question answering, bridging the gap between language generation and visual reasoning.",
  "[\"llama-3.2-90b-vision-instruct\"].description": "Advanced image reasoning capabilities suitable for visual understanding agent applications.",
  "[\"llama-3.2-90b-vision-preview\"].description": "Llama 3.2 is designed to handle tasks that combine visual and textual data. It excels in tasks such as image description and visual question answering, bridging the gap between language generation and visual reasoning.",
  "[\"llama-3.2-vision-instruct\"].description": "The Llama 3.2-Vision instruction-tuned model is optimized for visual recognition, image reasoning, image captioning, and answering general questions related to images.",
  "[\"llama-3.3-70b\"].description": "Llama 3.3 70B: A mid-to-large scale Llama model that balances reasoning power and throughput.",
  "[\"llama-3.3-70b-versatile\"].description": "Meta Llama 3.3 is a multilingual large language model (LLM) with 70 billion parameters (text input/text output), featuring pre-training and instruction-tuning. The instruction-tuned pure text model of Llama 3.3 is optimized for multilingual conversational use cases and outperforms many available open-source and closed chat models on common industry benchmarks.",
  "[\"llama-3.3-instruct\"].description": "The Llama 3.3 instruction-tuned model is optimized for conversational scenarios, outperforming many existing open-source chat models on common industry benchmarks.",
  "[\"llama3.1\"].description": "Llama 3.1 is a leading model launched by Meta, supporting up to 405B parameters, applicable in complex dialogues, multilingual translation, and data analysis.",
  "[\"llama3.1-8b\"].description": "Llama 3.1 8B: A lightweight, low-latency variant of Llama, well-suited for real-time inference and interactive applications.",
  "[\"llama3.1:405b\"].description": "Llama 3.1 is a leading model launched by Meta, supporting up to 405B parameters, applicable in complex dialogues, multilingual translation, and data analysis.",
  "[\"llama3.1:70b\"].description": "Llama 3.1 is a leading model launched by Meta, supporting up to 405B parameters, applicable in complex dialogues, multilingual translation, and data analysis.",
  "[\"llava-v1.5-7b-4096-preview\"].description": "LLaVA 1.5 7B offers integrated visual processing capabilities, generating complex outputs from visual information inputs.",
  "[\"meta-llama-3.1-405b-instruct\"].description": "The Llama 3.1 instruction-tuned text-only models are optimized for multilingual dialogue use cases and outperform many of the available open-source and closed chat models on common industry benchmarks.",
  "[\"meta-llama-3.1-70b-instruct\"].description": "The Llama 3.1 instruction-tuned text-only models are optimized for multilingual dialogue use cases and outperform many of the available open-source and closed chat models on common industry benchmarks.",
  "[\"meta-llama-3.1-8b-instruct\"].description": "The Llama 3.1 instruction-tuned text-only models are optimized for multilingual dialogue use cases and outperform many of the available open-source and closed chat models on common industry benchmarks.",
  "[\"meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\"].description": "LLaMA 3.2 is designed for tasks involving both visual and textual data. It excels in tasks like image description and visual question answering, bridging the gap between language generation and visual reasoning.",
  "[\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"].description": "LLaMA 3.2 is designed for tasks involving both visual and textual data. It excels in tasks like image description and visual question answering, bridging the gap between language generation and visual reasoning.",
  "[\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\"].description": "LLaMA 3.2 is designed for tasks involving both visual and textual data. It excels in tasks like image description and visual question answering, bridging the gap between language generation and visual reasoning.",
  "[\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"].description": "Meta Llama 3.3 is a multilingual large language model (LLM) that is a pre-trained and instruction-tuned generative model within the 70B (text input/text output) framework. The instruction-tuned pure text model is optimized for multilingual dialogue use cases and outperforms many available open-source and closed chat models on common industry benchmarks.",
  "[\"meta-llama/Meta-Llama-3.1-405B-Instruct\"].description": "LLaMA 3.1 405B is a powerful model for pre-training and instruction tuning.",
  "[\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\"].description": "The 405B Llama 3.1 Turbo model provides massive context support for big data processing, excelling in large-scale AI applications.",
  "[\"meta-llama/Meta-Llama-3.1-70B\"].description": "Llama 3.1 is a leading model launched by Meta, supporting up to 405B parameters, applicable in complex conversations, multilingual translation, and data analysis.",
  "[\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"].description": "Llama 3.1 70B model is finely tuned for high-load applications, quantized to FP8 for enhanced computational efficiency and accuracy, ensuring outstanding performance in complex scenarios.",
  "[\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"].description": "Llama 3.1 8B model utilizes FP8 quantization, supporting up to 131,072 context tokens, making it a standout in open-source models, excelling in complex tasks and outperforming many industry benchmarks.",
  "[\"meta-llama/llama-3.1-70b-instruct\"].description": "Llama 3.1 70B Instruct is designed for high-quality conversations, excelling in human evaluations, particularly in highly interactive scenarios.",
  "[\"meta-llama/llama-3.1-8b-instruct\"].description": "Llama 3.1 8B Instruct is the latest version released by Meta, optimized for high-quality conversational scenarios, outperforming many leading closed-source models.",
  "[\"meta-llama/llama-3.1-8b-instruct:free\"].description": "LLaMA 3.1 offers multilingual support and is one of the industry's leading generative models.",
  "[\"meta-llama/llama-3.2-11b-vision-instruct\"].description": "LLaMA 3.2 is designed to handle tasks that combine visual and textual data. It excels in tasks such as image description and visual question answering, bridging the gap between language generation and visual reasoning.",
  "[\"meta-llama/llama-3.2-3b-instruct\"].description": "meta-llama/llama-3.2-3b-instruct",
  "[\"meta-llama/llama-3.2-90b-vision-instruct\"].description": "LLaMA 3.2 is designed to handle tasks that combine visual and textual data. It excels in tasks such as image description and visual question answering, bridging the gap between language generation and visual reasoning.",
  "[\"meta-llama/llama-3.3-70b-instruct\"].description": "Llama 3.3 is the most advanced multilingual open-source large language model in the Llama series, offering performance comparable to a 405B model at an extremely low cost. Based on the Transformer architecture, it enhances usability and safety through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Its instruction-tuned version is optimized for multilingual dialogue and outperforms many open-source and closed chat models on various industry benchmarks. Knowledge cutoff date is December 2023.",
  "[\"meta-llama/llama-3.3-70b-instruct:free\"].description": "Llama 3.3 is the most advanced multilingual open-source large language model in the Llama series, offering performance comparable to a 405B model at an extremely low cost. Based on the Transformer architecture, it enhances usability and safety through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Its instruction-tuned version is optimized for multilingual dialogue and outperforms many open-source and closed chat models on various industry benchmarks. Knowledge cutoff date is December 2023.",
  "[\"meta.llama3-1-405b-instruct-v1:0\"].description": "Meta Llama 3.1 405B Instruct is the largest and most powerful model in the Llama 3.1 Instruct series. It is a highly advanced conversational reasoning and synthetic data generation model, which can also serve as a foundation for specialized continuous pre-training or fine-tuning in specific domains. The multilingual large language models (LLMs) provided by Llama 3.1 are a set of pre-trained, instruction-tuned generative models, including sizes of 8B, 70B, and 405B (text input/output). The instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual conversational use cases and have outperformed many available open-source chat models in common industry benchmarks. Llama 3.1 is designed for commercial and research purposes across multiple languages. The instruction-tuned text models are suitable for assistant-like chat, while the pre-trained models can adapt to various natural language generation tasks. The Llama 3.1 models also support improving other models using their outputs, including synthetic data generation and refinement. Llama 3.1 is an autoregressive language model built using an optimized transformer architecture. The tuned versions utilize supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
  "[\"meta.llama3-1-70b-instruct-v1:0\"].description": "The updated version of Meta Llama 3.1 70B Instruct includes an extended 128K context length, multilingual capabilities, and improved reasoning abilities. The multilingual large language models (LLMs) provided by Llama 3.1 are a set of pre-trained, instruction-tuned generative models, including sizes of 8B, 70B, and 405B (text input/output). The instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and have surpassed many available open-source chat models in common industry benchmarks. Llama 3.1 is designed for commercial and research purposes in multiple languages. The instruction-tuned text models are suitable for assistant-like chat, while the pre-trained models can adapt to various natural language generation tasks. The Llama 3.1 model also supports using its outputs to improve other models, including synthetic data generation and refinement. Llama 3.1 is an autoregressive language model using optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
  "[\"meta.llama3-1-8b-instruct-v1:0\"].description": "The updated version of Meta Llama 3.1 8B Instruct includes an extended 128K context length, multilingual capabilities, and improved reasoning abilities. The multilingual large language models (LLMs) provided by Llama 3.1 are a set of pre-trained, instruction-tuned generative models, including sizes of 8B, 70B, and 405B (text input/output). The instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and have surpassed many available open-source chat models in common industry benchmarks. Llama 3.1 is designed for commercial and research purposes in multiple languages. The instruction-tuned text models are suitable for assistant-like chat, while the pre-trained models can adapt to various natural language generation tasks. The Llama 3.1 model also supports using its outputs to improve other models, including synthetic data generation and refinement. Llama 3.1 is an autoregressive language model using optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
  "[\"meta.llama3-70b-instruct-v1:0\"].description": "Meta Llama 3 is an open large language model (LLM) aimed at developers, researchers, and enterprises, designed to help them build, experiment, and responsibly scale their generative AI ideas. As part of a foundational system for global community innovation, it is particularly suitable for content creation, conversational AI, language understanding, R&D, and enterprise applications.",
  "[\"meta.llama3-8b-instruct-v1:0\"].description": "Meta Llama 3 is an open large language model (LLM) aimed at developers, researchers, and enterprises, designed to help them build, experiment, and responsibly scale their generative AI ideas. As part of a foundational system for global community innovation, it is particularly suitable for those with limited computational power and resources, edge devices, and faster training times.",
  "[\"meta/Llama-3.2-11B-Vision-Instruct\"].description": "Exhibits excellent image reasoning capabilities on high-resolution images, suitable for visual understanding applications.",
  "[\"meta/Llama-3.2-90B-Vision-Instruct\"].description": "Advanced image reasoning capabilities designed for visual understanding agent applications.",
  "[\"meta/Llama-3.3-70B-Instruct\"].description": "Llama 3.3 is the most advanced multilingual open-source large language model in the Llama series, offering performance comparable to a 405B model at a very low cost. Based on the Transformer architecture, it is enhanced through supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to improve usefulness and safety. Its instruction-tuned version is optimized for multilingual dialogue and outperforms many open-source and closed chat models on multiple industry benchmarks. Knowledge cutoff date: December 2023.",
  "[\"meta/Meta-Llama-3.1-405B-Instruct\"].description": "Llama 3.1 instruction-tuned text model optimized for multilingual dialogue use cases, performing excellently on common industry benchmarks among many available open-source and closed chat models.",
  "[\"meta/Meta-Llama-3.1-70B-Instruct\"].description": "Llama 3.1 instruction-tuned text model optimized for multilingual dialogue use cases, performing excellently on common industry benchmarks among many available open-source and closed chat models.",
  "[\"meta/Meta-Llama-3.1-8B-Instruct\"].description": "Llama 3.1 instruction-tuned text model optimized for multilingual dialogue use cases, performing excellently on common industry benchmarks among many available open-source and closed chat models.",
  "[\"meta/llama-3.1-405b-instruct\"].description": "An advanced LLM supporting synthetic data generation, knowledge distillation, and reasoning, suitable for chatbots, programming, and domain-specific tasks.",
  "[\"meta/llama-3.1-70b\"].description": "An updated version of Meta Llama 3 70B Instruct, featuring extended 128K context length, multilingual support, and improved reasoning capabilities.",
  "[\"meta/llama-3.1-70b-instruct\"].description": "Empowering complex conversations with exceptional context understanding, reasoning capabilities, and text generation abilities.",
  "[\"meta/llama-3.1-8b\"].description": "Llama 3.1 8B supports a 128K context window, making it ideal for real-time conversational interfaces and data analysis, while offering significant cost savings compared to larger models. Served by Groq using its custom Language Processing Unit (LPU) hardware for fast, efficient inference.",
  "[\"meta/llama-3.1-8b-instruct\"].description": "An advanced cutting-edge model with language understanding, excellent reasoning capabilities, and text generation abilities.",
  "[\"meta/llama-3.2-11b\"].description": "Instruction-tuned image reasoning generation model (text + image input / text output), optimized for visual recognition, image reasoning, captioning, and answering general questions about images.",
  "[\"meta/llama-3.2-11b-vision-instruct\"].description": "A state-of-the-art vision-language model adept at high-quality reasoning from images.",
  "[\"meta/llama-3.2-1b\"].description": "Text-only model supporting on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.",
  "[\"meta/llama-3.2-1b-instruct\"].description": "A cutting-edge small language model with language understanding, excellent reasoning capabilities, and text generation abilities.",
  "[\"meta/llama-3.2-3b\"].description": "Text-only model carefully tuned to support on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.",
  "[\"meta/llama-3.2-3b-instruct\"].description": "A cutting-edge small language model with language understanding, excellent reasoning capabilities, and text generation abilities.",
  "[\"meta/llama-3.2-90b\"].description": "Instruction-tuned image reasoning generation model (text + image input / text output), optimized for visual recognition, image reasoning, captioning, and answering general questions about images.",
  "[\"meta/llama-3.2-90b-vision-instruct\"].description": "A state-of-the-art vision-language model adept at high-quality reasoning from images.",
  "[\"meta/llama-3.3-70b\"].description": "The perfect blend of performance and efficiency. This model supports high-performance conversational AI, designed for content creation, enterprise applications, and research, offering advanced language understanding capabilities including text summarization, classification, sentiment analysis, and code generation.",
  "[\"meta/llama-3.3-70b-instruct\"].description": "An advanced LLM skilled in reasoning, mathematics, common sense, and function calling.",
  "[\"microsoft/Phi-3.5-mini-instruct\"].description": "An updated version of the Phi-3-mini model.",
  "[\"microsoft/Phi-3.5-vision-instruct\"].description": "An updated version of the Phi-3-vision model.",
  "[\"mistralai/Mistral-7B-Instruct-v0.1\"].description": "Mistral (7B) Instruct is known for its high performance, suitable for various language tasks.",
  "[\"mistralai/Mistral-7B-Instruct-v0.2\"].description": "Mistral 7B is a model fine-tuned on demand, providing optimized answers for tasks.",
  "[\"mistralai/Mistral-7B-Instruct-v0.3\"].description": "Mistral (7B) Instruct v0.3 offers efficient computational power and natural language understanding, suitable for a wide range of applications.",
  "[\"mistralai/Mistral-7B-v0.1\"].description": "Mistral 7B is a compact yet high-performance model, adept at handling batch processing and simple tasks like classification and text generation, featuring good reasoning capabilities.",
  "[\"mistralai/Mixtral-8x22B-Instruct-v0.1\"].description": "Mixtral-8x22B Instruct (141B) is a super large language model that supports extremely high processing demands.",
  "[\"mistralai/Mixtral-8x7B-Instruct-v0.1\"].description": "Mixtral 8x7B is a pre-trained sparse mixture of experts model for general text tasks.",
  "[\"mistralai/Mixtral-8x7B-v0.1\"].description": "Mixtral 8x7B is a sparse expert model that utilizes multiple parameters to enhance reasoning speed, suitable for multilingual and code generation tasks.",
  "[\"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"].description": "Llama 3.1 Nemotron 70B is a large language model customized by NVIDIA, designed to enhance the helpfulness of LLM-generated responses to user queries. The model has excelled in benchmark tests such as Arena Hard, AlpacaEval 2 LC, and GPT-4-Turbo MT-Bench, ranking first in all three automatic alignment benchmarks as of October 1, 2024. The model is trained using RLHF (specifically REINFORCE), Llama-3.1-Nemotron-70B-Reward, and HelpSteer2-Preference prompts based on the Llama-3.1-70B-Instruct model.",
  "[\"nvidia/llama-3.1-nemotron-51b-instruct\"].description": "A unique language model offering unparalleled accuracy and efficiency.",
  "[\"nvidia/llama-3.1-nemotron-70b-instruct\"].description": "Llama-3.1-Nemotron-70B-Instruct is a custom large language model by NVIDIA designed to enhance the helpfulness of LLM-generated responses.",
  "[\"openai/gpt-3.5-turbo\"].description": "OpenAI's most capable and cost-effective model in the GPT-3.5 series, optimized for chat purposes but also performing well on traditional completion tasks.",
  "[\"openai/gpt-3.5-turbo-instruct\"].description": "Capabilities similar to GPT-3 era models. Compatible with traditional completion endpoints rather than chat completion endpoints.",
  "[\"openai/gpt-4.1\"].description": "The GPT-4.1 series offers extended context and enhanced engineering and reasoning capabilities.",
  "[\"openai/gpt-4.1-mini\"].description": "GPT-4.1 Mini provides lower latency and better cost-efficiency, suitable for medium-context scenarios.",
  "[\"openai/gpt-4.1-nano\"].description": "GPT-4.1 Nano is a low-cost, low-latency option ideal for high-frequency short conversations or classification tasks.",
  "[\"openai/gpt-5.1\"].description": "GPT-5.1 is the latest flagship model in the GPT-5 series, significantly improved in general reasoning, instruction following, and conversational naturalness, suitable for a wide range of tasks.",
  "[\"openai/gpt-5.1-chat\"].description": "GPT-5.1 Chat is a lightweight member of the GPT-5.1 family, optimized for low-latency conversations while retaining strong reasoning and instruction execution capabilities.",
  "[\"openai/gpt-5.1-codex\"].description": "GPT-5.1-Codex is a GPT-5.1 variant optimized for software engineering and coding workflows, ideal for large-scale refactoring, complex debugging, and long-term autonomous coding tasks.",
  "[\"openai/gpt-5.1-codex-mini\"].description": "GPT-5.1-Codex-Mini is a smaller, faster version of GPT-5.1-Codex, better suited for latency- and cost-sensitive coding scenarios.",
  "[\"qwen/qwen-2.5-72b-instruct\"].description": "Qwen2.5-72B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 72B model has significantly improved capabilities in coding and mathematics. The model also offers multilingual support, covering over 29 languages, including Chinese and English. It shows significant enhancements in instruction following, understanding structured data, and generating structured outputs (especially JSON).",
  "[\"qwen/qwen2.5-32b-instruct\"].description": "Qwen2.5-32B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 32B model has significantly improved capabilities in coding and mathematics. The model provides multilingual support, covering over 29 languages, including Chinese and English. It shows significant enhancements in instruction following, understanding structured data, and generating structured outputs (especially JSON).",
  "[\"qwen/qwen2.5-7b-instruct\"].description": "An LLM focused on both Chinese and English, targeting language, programming, mathematics, reasoning, and more.",
  "[\"qwen/qwen2.5-coder-32b-instruct\"].description": "An advanced LLM supporting code generation, reasoning, and debugging, covering mainstream programming languages.",
  "[\"qwen/qwen2.5-coder-7b-instruct\"].description": "A powerful medium-sized code model supporting 32K context length, proficient in multilingual programming.",
  "[\"qwen2.5\"].description": "Qwen2.5 is Alibaba's next-generation large-scale language model, supporting diverse application needs with outstanding performance.",
  "[\"qwen2.5-14b-instruct\"].description": "The 14B model of Tongyi Qianwen 2.5 is open-sourced.",
  "[\"qwen2.5-14b-instruct-1m\"].description": "The Tongyi Qianwen 2.5 model is open-sourced at a scale of 72B.",
  "[\"qwen2.5-32b-instruct\"].description": "The 32B model of Tongyi Qianwen 2.5 is open-sourced.",
  "[\"qwen2.5-72b-instruct\"].description": "The 72B model of Tongyi Qianwen 2.5 is open-sourced.",
  "[\"qwen2.5-7b-instruct\"].description": "Qwen2.5 7B Instruct, a mature open-source instruction model designed for versatile dialogue and content generation.",
  "[\"qwen2.5-coder-1.5b-instruct\"].description": "Open-source version of the Qwen coding model.",
  "[\"qwen2.5-coder-14b-instruct\"].description": "Open-source version of Tongyi Qianwen coding model.",
  "[\"qwen2.5-coder-32b-instruct\"].description": "Open-source version of the Tongyi Qianwen code model.",
  "[\"qwen2.5-coder-7b-instruct\"].description": "The open-source version of the Tongyi Qianwen Coder model.",
  "[\"qwen2.5-coder-instruct\"].description": "Qwen2.5-Coder is the latest code-specific large language model in the Qwen series (formerly known as CodeQwen).",
  "[\"qwen2.5-instruct\"].description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we have released multiple base language models and instruction-tuned language models with parameter sizes ranging from 0.5 billion to 7.2 billion.",
  "[\"qwen2.5-math-1.5b-instruct\"].description": "Qwen-Math model has powerful mathematical problem-solving capabilities.",
  "[\"qwen2.5-math-72b-instruct\"].description": "The Qwen-Math model possesses strong capabilities for solving mathematical problems.",
  "[\"qwen2.5-math-7b-instruct\"].description": "The Qwen-Math model possesses strong capabilities for solving mathematical problems.",
  "[\"qwen2.5-omni-7b\"].description": "The Qwen-Omni series models support input of various modalities, including video, audio, images, and text, and output audio and text.",
  "[\"qwen2.5-vl-32b-instruct\"].description": "Qwen2.5 VL 32B Instruct, a multimodal open-source model ideal for private deployment and diverse applications.",
  "[\"qwen2.5-vl-72b-instruct\"].description": "This version enhances instruction following, mathematics, problem-solving, and coding capabilities, improving the ability to recognize various formats and accurately locate visual elements. It supports understanding long video files (up to 10 minutes) and pinpointing events in seconds, comprehending the sequence and speed of time, and based on parsing and locating capabilities, it supports controlling OS or Mobile agents. It has strong key information extraction and JSON output capabilities, and this version is the most powerful in the series at 72B.",
  "[\"qwen2.5-vl-7b-instruct\"].description": "Qwen2.5 VL 7B Instruct, a lightweight multimodal model balancing deployment cost and recognition capabilities.",
  "[\"qwen2.5-vl-instruct\"].description": "Qwen2.5-VL is the latest version of the visual language model in the Qwen model family.",
  "[\"qwen2.5:0.5b\"].description": "Qwen2.5 is Alibaba's next-generation large-scale language model, supporting diverse application needs with outstanding performance.",
  "[\"qwen2.5:1.5b\"].description": "Qwen2.5 is Alibaba's next-generation large-scale language model, supporting diverse application needs with outstanding performance.",
  "[\"qwen2.5:72b\"].description": "Qwen2.5 is Alibaba's next-generation large-scale language model, supporting diverse application needs with outstanding performance.",
  "[\"qwen2:0.5b\"].description": "Qwen2 is Alibaba's next-generation large-scale language model, supporting diverse application needs with excellent performance.",
  "[\"qwen2:1.5b\"].description": "Qwen2 is Alibaba's next-generation large-scale language model, supporting diverse application needs with excellent performance.",
  "[\"qwen3-0.6b\"].description": "Qwen3 0.6B, an entry-level model suitable for basic reasoning and highly resource-constrained environments.",
  "[\"qwen3-1.7b\"].description": "Qwen3 1.7B, an ultra-lightweight model optimized for edge and on-device deployment.",
  "[\"stable-diffusion-3.5-large\"].description": "stable-diffusion-3.5-large is an 800-million-parameter multimodal diffusion transformer (MMDiT) text-to-image generation model, offering excellent image quality and prompt matching. It supports generating high-resolution images up to 1 million pixels and runs efficiently on consumer-grade hardware.",
  "[\"stable-diffusion-3.5-large-turbo\"].description": "stable-diffusion-3.5-large-turbo is a model based on stable-diffusion-3.5-large that employs adversarial diffusion distillation (ADD) technology, providing faster generation speed.",
  "[\"stable-diffusion-v1.5\"].description": "stable-diffusion-v1.5 is initialized with weights from the stable-diffusion-v1.2 checkpoint and fine-tuned for 595k steps at 512x512 resolution on \"laion-aesthetics v2 5+\", reducing text conditioning by 10% to improve classifier-free guidance sampling.",
  "[\"stable-diffusion-xl-base-1.0\"].description": "A text-to-image large model developed and open-sourced by Stability AI, leading the industry in creative image generation capabilities. It has excellent instruction understanding and supports inverse prompt definitions for precise content generation.",
  "[\"step-1.5v-mini\"].description": "This model has powerful video understanding capabilities.",
  "[\"upstage/SOLAR-10.7B-Instruct-v1.0\"].description": "Upstage SOLAR Instruct v1 (11B) is suitable for refined instruction tasks, offering excellent language processing capabilities.",
  "[\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"].description": "Claude 3.5 Sonnet raises the industry standard, outperforming competitor models and Claude 3 Opus, excelling in a wide range of evaluations while maintaining the speed and cost of our mid-tier models.",
  "[\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"].description": "Claude 3.7 Sonnet is Anthropic's fastest next-generation model. Compared to Claude 3 Haiku, Claude 3.7 Sonnet shows improvements across various skills and surpasses the previous generation's largest model, Claude 3 Opus, in many intelligence benchmark tests.",
  "[\"us.anthropic.claude-haiku-4-5-20251001-v1:0\"].description": "Claude Haiku 4.5 is Anthropic's fastest and most intelligent Haiku model, offering lightning-fast speed and advanced reasoning capabilities.",
  "[\"us.anthropic.claude-sonnet-4-5-20250929-v1:0\"].description": "Claude Sonnet 4.5 is the most intelligent model Anthropic has developed to date.",
  "[\"v0-1.0-md\"].description": "The v0-1.0-md model is a legacy model served through the v0 API.",
  "[\"v0-1.5-lg\"].description": "The v0-1.5-lg model is suitable for advanced thinking or reasoning tasks.",
  "[\"v0-1.5-md\"].description": "The v0-1.5-md model is suitable for everyday tasks and user interface (UI) generation.",
  "[\"vercel/v0-1.0-md\"].description": "Access the model behind v0 to generate, fix, and optimize modern web applications, with framework-specific reasoning and up-to-date knowledge.",
  "[\"vercel/v0-1.5-md\"].description": "Access the model behind v0 to generate, fix, and optimize modern web applications, with framework-specific reasoning and up-to-date knowledge.",
  "[\"wan2.2-t2i-flash\"].description": "Wanxiang 2.2 Flash version, the latest model currently available. Fully upgraded in creativity, stability, and realism, with fast generation speed and high cost-effectiveness.",
  "[\"wan2.2-t2i-plus\"].description": "Wanxiang 2.2 Professional version, the latest model currently available. Fully upgraded in creativity, stability, and realism, generating images with rich details.",
  "[\"wanx2.0-t2i-turbo\"].description": "Specializes in textured portraits, with moderate speed and low cost. Corresponds to Tongyi Wanxiang official website's 2.0 turbo model.",
  "[\"wanx2.1-t2i-plus\"].description": "Fully upgraded version. Generates images with richer details, slightly slower speed. Corresponds to Tongyi Wanxiang official website's 2.1 professional model.",
  "[\"wanx2.1-t2i-turbo\"].description": "Fully upgraded version. Fast generation speed, comprehensive effects, and high overall cost-effectiveness. Corresponds to Tongyi Wanxiang official website's 2.1 turbo model.",
  "[\"x-ai/grok-4.1-fast\"].description": "Grok 4 Fast is xAI’s high-throughput, low-cost model (supports 2M context window), ideal for high-concurrency and long-context scenarios.",
  "[\"x-ai/grok-4.1-fast-non-reasoning\"].description": "Grok 4 Fast (Non-Reasoning) is xAI’s high-throughput, low-cost multimodal model (supports 2M context window), designed for latency- and cost-sensitive scenarios that do not require internal reasoning. It runs alongside the reasoning version of Grok 4 Fast and can enable reasoning via the API’s reasoning enable parameter. Prompts and completions may be used by xAI or OpenRouter to improve future models.",
  "[\"z-ai/glm-4.5\"].description": "GLM 4.5 is Z.AI’s flagship model, supporting hybrid reasoning and optimized for engineering and long-context tasks.",
  "[\"z-ai/glm-4.5-air\"].description": "GLM 4.5 Air is a lightweight version of GLM 4.5, suitable for cost-sensitive scenarios while retaining strong reasoning capabilities.",
  "[\"z-ai/glm-4.6\"].description": "GLM 4.6 is Z.AI’s flagship model, with extended context length and enhanced coding capabilities.",
  "[\"zai-glm-4.6\"].description": "Performs well in programming and reasoning tasks, supports streaming and tool calling, and is suitable for agentic coding and complex reasoning scenarios.",
  "[\"zai-org/GLM-4.5\"].description": "GLM-4.5 is a foundational model designed specifically for agent applications, using a Mixture-of-Experts (MoE) architecture. It is deeply optimized for tool invocation, web browsing, software engineering, and front-end programming, supporting seamless integration with code agents like Claude Code and Roo Code. GLM-4.5 employs a hybrid inference mode, adaptable to complex reasoning and everyday use scenarios.",
  "[\"zai-org/GLM-4.5-Air\"].description": "GLM-4.5-Air is a foundational model designed specifically for agent applications, using a Mixture-of-Experts (MoE) architecture. It is deeply optimized for tool invocation, web browsing, software engineering, and front-end programming, supporting seamless integration with code agents like Claude Code and Roo Code. GLM-4.5 employs a hybrid inference mode, adaptable to complex reasoning and everyday use scenarios.",
  "[\"zai-org/GLM-4.5V\"].description": "GLM-4.5V is the latest-generation vision-language model (VLM) released by Zhipu AI. It is built on the flagship text model GLM-4.5-Air, which has 106B total parameters and 12B active parameters, and adopts a Mixture-of-Experts (MoE) architecture to deliver outstanding performance at reduced inference cost. Technically, GLM-4.5V continues the trajectory of GLM-4.1V-Thinking and introduces innovations such as three-dimensional rotary position encoding (3D-RoPE), significantly improving perception and reasoning of three-dimensional spatial relationships. Through optimizations across pretraining, supervised fine-tuning, and reinforcement learning stages, the model can handle a wide range of visual content including images, video, and long documents, and has achieved top-tier performance among comparable open-source models across 41 public multimodal benchmarks. The model also adds a \"Thinking Mode\" toggle that lets users flexibly choose between fast responses and deep reasoning to balance efficiency and effectiveness.",
  "[\"zai-org/GLM-4.6\"].description": "Compared to GLM-4.5, GLM-4.6 introduces several key improvements. Its context window expands from 128K to 200K tokens, enabling the model to handle more complex agent tasks. The model achieves higher scores on code benchmarks and demonstrates stronger real-world performance in applications such as Claude Code, Cline, Roo Code, and Kilo Code, including improvements in generating visually refined front-end pages. GLM-4.6 shows significant enhancements in inference performance and supports tool usage during inference, resulting in stronger overall capabilities. It excels in tool utilization and search-based agents and integrates more effectively into agent frameworks. In writing, the model better aligns with human preferences in style and readability and performs more naturally in role-playing scenarios.",
  "[\"zai/glm-4.5\"].description": "The GLM-4.5 series models are foundational models specifically designed for agents. The flagship GLM-4.5 integrates 355 billion total parameters (32 billion active), unifying reasoning, coding, and agent capabilities to address complex application needs. As a hybrid reasoning system, it offers dual operating modes.",
  "[\"zai/glm-4.5-air\"].description": "GLM-4.5 and GLM-4.5-Air are our latest flagship models, specifically designed as foundational models for agent applications. Both utilize a Mixture of Experts (MoE) architecture. GLM-4.5 has 355 billion total parameters with 32 billion active per forward pass, while GLM-4.5-Air features a streamlined design with 106 billion total parameters and 12 billion active.",
  "[\"zai/glm-4.5v\"].description": "GLM-4.5V is built on the GLM-4.5-Air foundational model, inheriting the proven techniques of GLM-4.1V-Thinking while achieving efficient scaling through a powerful 106 billion parameter MoE architecture.",
  "accounts/fireworks/models/deepseek-r1.description": "DeepSeek-R1 is a state-of-the-art large language model optimized through reinforcement learning and cold-start data, excelling in reasoning, mathematics, and programming performance.",
  "accounts/fireworks/models/deepseek-v3.description": "A powerful Mixture-of-Experts (MoE) language model provided by Deepseek, with a total parameter count of 671B, activating 37B parameters per token.",
  "accounts/fireworks/models/llama-v3-70b-instruct.description": "Llama 3 70B instruction model, optimized for multilingual dialogues and natural language understanding, outperforming most competitive models.",
  "accounts/fireworks/models/llama-v3-8b-instruct-hf.description": "Llama 3 8B instruction model (HF version), consistent with official implementation results, featuring high consistency and cross-platform compatibility.",
  "accounts/fireworks/models/llama-v3-8b-instruct.description": "Llama 3 8B instruction model, optimized for dialogues and multilingual tasks, delivering outstanding and efficient performance.",
  "accounts/fireworks/models/llama-v3p1-405b-instruct.description": "Llama 3.1 405B instruction model, equipped with massive parameters, suitable for complex tasks and instruction following in high-load scenarios.",
  "accounts/fireworks/models/llama-v3p1-70b-instruct.description": "Llama 3.1 70B instruction model provides exceptional natural language understanding and generation capabilities, making it an ideal choice for dialogue and analysis tasks.",
  "accounts/fireworks/models/llama-v3p1-8b-instruct.description": "Llama 3.1 8B instruction model, optimized for multilingual dialogues, capable of surpassing most open-source and closed-source models on common industry benchmarks.",
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct.description": "Meta's 11B parameter instruction-tuned image reasoning model. This model is optimized for visual recognition, image reasoning, image description, and answering general questions about images. It understands visual data like charts and graphs, generating text descriptions of image details to bridge the gap between vision and language.",
  "accounts/fireworks/models/llama-v3p2-3b-instruct.description": "The Llama 3.2 3B instruction model is a lightweight multilingual model introduced by Meta. This model aims to enhance efficiency, providing significant improvements in latency and cost compared to larger models. Sample use cases include querying, prompt rewriting, and writing assistance.",
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct.description": "Meta's 90B parameter instruction-tuned image reasoning model. This model is optimized for visual recognition, image reasoning, image description, and answering general questions about images. It understands visual data like charts and graphs, generating text descriptions of image details to bridge the gap between vision and language.",
  "accounts/fireworks/models/llama-v3p3-70b-instruct.description": "Llama 3.3 70B Instruct is the December update of Llama 3.1 70B. This model builds upon Llama 3.1 70B (released in July 2024) with enhancements in tool invocation, multilingual text support, mathematics, and programming capabilities. It achieves industry-leading performance in reasoning, mathematics, and instruction following, providing similar performance to 3.1 405B while offering significant advantages in speed and cost.",
  "accounts/fireworks/models/mistral-small-24b-instruct-2501.description": "A 24B parameter model that possesses state-of-the-art capabilities comparable to larger models.",
  "accounts/fireworks/models/mixtral-8x22b-instruct.description": "Mixtral MoE 8x22B instruction model, featuring large-scale parameters and a multi-expert architecture, fully supporting efficient processing of complex tasks.",
  "accounts/fireworks/models/mixtral-8x7b-instruct.description": "Mixtral MoE 8x7B instruction model, with a multi-expert architecture providing efficient instruction following and execution.",
  "accounts/fireworks/models/mythomax-l2-13b.description": "MythoMax L2 13B model, combining novel merging techniques, excels in narrative and role-playing.",
  "accounts/fireworks/models/phi-3-vision-128k-instruct.description": "Phi 3 Vision instruction model, a lightweight multimodal model capable of handling complex visual and textual information, with strong reasoning abilities.",
  "accounts/fireworks/models/qwen-qwq-32b-preview.description": "The QwQ model is an experimental research model developed by the Qwen team, focusing on enhancing AI reasoning capabilities.",
  "accounts/fireworks/models/qwen2-vl-72b-instruct.description": "The 72B version of the Qwen-VL model is the latest iteration from Alibaba, representing nearly a year of innovation.",
  "accounts/fireworks/models/qwen2p5-72b-instruct.description": "Qwen2.5 is a series of decoder-only language models developed by the Alibaba Cloud Qwen team. These models come in different sizes including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B, available in both base and instruct variants.",
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct.description": "Qwen2.5 Coder 32B Instruct is the latest version in Alibaba Cloud's series of code-specific large language models. This model significantly enhances code generation, reasoning, and repair capabilities based on Qwen2.5, trained on 55 trillion tokens. It not only improves coding abilities but also maintains advantages in mathematics and general capabilities, providing a more comprehensive foundation for practical applications such as code agents.",
  "accounts/yi-01-ai/models/yi-large.description": "Yi-Large model, featuring exceptional multilingual processing capabilities, suitable for various language generation and understanding tasks.",
  "alibaba/qwen-3-14b.description": "Qwen3 is the latest generation large language model in the Qwen series, offering a comprehensive set of dense and Mixture of Experts (MoE) models. Built on extensive training, Qwen3 delivers breakthrough advancements in reasoning, instruction following, agent capabilities, and multilingual support.",
  "alibaba/qwen-3-235b.description": "Qwen3 is the latest generation large language model in the Qwen series, offering a comprehensive set of dense and Mixture of Experts (MoE) models. Built on extensive training, Qwen3 delivers breakthrough advancements in reasoning, instruction following, agent capabilities, and multilingual support.",
  "alibaba/qwen-3-30b.description": "Qwen3 is the latest generation large language model in the Qwen series, offering a comprehensive set of dense and Mixture of Experts (MoE) models. Built on extensive training, Qwen3 delivers breakthrough advancements in reasoning, instruction following, agent capabilities, and multilingual support.",
  "alibaba/qwen-3-32b.description": "Qwen3 is the latest generation large language model in the Qwen series, offering a comprehensive set of dense and Mixture of Experts (MoE) models. Built on extensive training, Qwen3 delivers breakthrough advancements in reasoning, instruction following, agent capabilities, and multilingual support.",
  "alibaba/qwen3-coder.description": "Qwen3-Coder-480B-A35B-Instruct is Qwen's most agent-capable code model, demonstrating remarkable performance in agent coding, agent browser usage, and other fundamental coding tasks, achieving results comparable to Claude Sonnet.",
  "amazon/nova-lite.description": "A very low-cost multimodal model that processes image, video, and text inputs at extremely high speed.",
  "amazon/nova-micro.description": "A text-only model delivering the lowest latency responses at a very low cost.",
  "amazon/nova-pro.description": "A highly capable multimodal model offering the best combination of accuracy, speed, and cost, suitable for a wide range of tasks.",
  "amazon/titan-embed-text-v2.description": "Amazon Titan Text Embeddings V2 is a lightweight, efficient multilingual embedding model supporting 1024, 512, and 256 dimensions.",
  "anthropic/claude-3-haiku.description": "Claude 3 Haiku is Anthropic's fastest model to date, designed for enterprise workloads that typically involve longer prompts. Haiku can quickly analyze large volumes of documents such as quarterly filings, contracts, or legal cases, at half the cost of other models in its performance tier.",
  "anthropic/claude-3-opus.description": "Claude 3 Opus is Anthropic's smartest model, delivering market-leading performance on highly complex tasks. It navigates open-ended prompts and novel scenarios with exceptional fluency and human-like understanding.",
  "anthropic/claude-opus-4.description": "Opus 4 is Anthropic’s flagship model, designed for complex tasks and enterprise-grade applications.",
  "anthropic/claude-sonnet-4.description": "Claude Sonnet 4 is Anthropic’s hybrid reasoning model, offering a blend of cognitive and non-cognitive capabilities.",
  "ascend-tribe/pangu-pro-moe.description": "Pangu-Pro-MoE 72B-A16B is a sparse large language model with 72 billion parameters and 16 billion activated parameters. It is based on the Group Mixture of Experts (MoGE) architecture, which groups experts during the expert selection phase and constrains tokens to activate an equal number of experts within each group, achieving expert load balancing and significantly improving deployment efficiency on the Ascend platform.",
  "aya.description": "Aya 23 is a multilingual model launched by Cohere, supporting 23 languages, facilitating diverse language applications.",
  "aya:35b.description": "Aya 23 is a multilingual model launched by Cohere, supporting 23 languages, facilitating diverse language applications.",
  "azure-DeepSeek-R1-0528.description": "Deployed and provided by Microsoft; the DeepSeek R1 model has undergone a minor version upgrade, currently at DeepSeek-R1-0528. In the latest update, DeepSeek R1 significantly improves inference depth and reasoning ability by increasing computational resources and introducing algorithmic optimizations in the post-training phase. This model excels in benchmarks including mathematics, programming, and general logic, with overall performance approaching leading models such as O3 and Gemini 2.5 Pro.",
  "baichuan-m2-32b.description": "Baichuan M2 32B is a Mixture of Experts model developed by Baichuan Intelligence, featuring powerful reasoning capabilities.",
  "baichuan/baichuan2-13b-chat.description": "Baichuan-13B is an open-source, commercially usable large language model developed by Baichuan Intelligence, containing 13 billion parameters, achieving the best results in its size on authoritative Chinese and English benchmarks.",
  "black-forest-labs/flux-dev.description": "FLUX Dev - Development version of FLUX, for non-commercial use only.",
  "black-forest-labs/flux-pro.description": "FLUX Pro - The professional edition of the FLUX model, producing high-quality images.",
  "black-forest-labs/flux-schnell.description": "FLUX Schnell - A fast image generation model optimized for speed.",
  "c4ai-aya-expanse-32b.description": "Aya Expanse is a high-performance 32B multilingual model designed to challenge the performance of single-language models through innovations in instruction tuning, data arbitrage, preference training, and model merging. It supports 23 languages.",
  "c4ai-aya-expanse-8b.description": "Aya Expanse is a high-performance 8B multilingual model designed to challenge the performance of single-language models through innovations in instruction tuning, data arbitrage, preference training, and model merging. It supports 23 languages.",
  "c4ai-aya-vision-32b.description": "Aya Vision is a state-of-the-art multimodal model that excels in multiple key benchmarks for language, text, and image capabilities. This 32 billion parameter version focuses on cutting-edge multilingual performance and supports 23 languages.",
  "c4ai-aya-vision-8b.description": "Aya Vision is a state-of-the-art multimodal model that excels in multiple key benchmarks for language, text, and image capabilities. This 8 billion parameter version focuses on low latency and optimal performance.",
  "charglm-3.description": "CharGLM-3 is designed for role-playing and emotional companionship, supporting ultra-long multi-turn memory and personalized dialogue, with wide applications.",
  "charglm-4.description": "CharGLM-4 is designed for role-playing and emotional companionship, supporting ultra-long multi-turn memory and personalized dialogue, with wide-ranging applications.",
  "chatgpt-4o-latest.description": "ChatGPT-4o is a dynamic model that updates in real-time to stay current with the latest version. It combines powerful language understanding and generation capabilities, making it suitable for large-scale applications, including customer service, education, and technical support.",
  "claude-3-5-haiku-20241022.description": "Claude 3.5 Haiku is Anthropic's fastest next-generation model. Compared to Claude 3 Haiku, Claude 3.5 Haiku has improved in various skills and has surpassed the previous generation's largest model, Claude 3 Opus, in many intelligence benchmark tests.",
  "claude-3-5-haiku-latest.description": "Claude 3.5 Haiku offers fast responses, ideal for lightweight tasks.",
  "claude-3-7-sonnet-20250219.description": "Claude 3.7 Sonnet is Anthropic's latest model, offering a balance of speed and performance. It excels in a wide range of tasks, including programming, data science, visual processing, and agent tasks.",
  "claude-3-7-sonnet-latest.description": "Claude 3.7 Sonnet is Anthropic's latest and most powerful model for handling highly complex tasks. It excels in performance, intelligence, fluency, and comprehension.",
  "claude-3-haiku-20240307.description": "Claude 3 Haiku is Anthropic's fastest and most compact model, designed for near-instantaneous responses. It features rapid and accurate directional performance.",
  "claude-3-opus-20240229.description": "Claude 3 Opus is Anthropic's most powerful model for handling highly complex tasks. It excels in performance, intelligence, fluency, and comprehension.",
  "claude-3-sonnet-20240229.description": "Claude 3 Sonnet provides an ideal balance of intelligence and speed for enterprise workloads. It offers maximum utility at a lower price, reliable and suitable for large-scale deployment.",
  "claude-haiku-4-5-20251001.description": "Claude Haiku 4.5 is Anthropic's fastest and most intelligent Haiku model, offering lightning-fast speed and advanced reasoning capabilities.",
  "claude-opus-4-1-20250805-thinking.description": "Claude Opus 4.1 Thinking model, an advanced version capable of demonstrating its reasoning process.",
  "claude-opus-4-1-20250805.description": "Claude Opus 4.1 is Anthropic's latest and most powerful model designed for handling highly complex tasks. It demonstrates outstanding performance in intelligence, fluency, and comprehension.",
  "claude-opus-4-20250514.description": "Claude Opus 4 is Anthropic's most powerful model for handling highly complex tasks. It excels in performance, intelligence, fluency, and comprehension.",
  "claude-opus-4-5-20251101.description": "Claude Opus 4.5 is Anthropic's flagship model, combining exceptional intelligence with scalable performance. It is ideal for complex tasks that demand top-tier response quality and reasoning capabilities.",
  "claude-sonnet-4-20250514-thinking.description": "Claude Sonnet 4 Thinking model can produce near-instant responses or extended step-by-step reasoning, enabling users to clearly see these processes.",
  "claude-sonnet-4-20250514.description": "Claude Sonnet 4 can generate near-instant responses or extended step-by-step reasoning, allowing users to clearly observe these processes.",
  "claude-sonnet-4-5-20250929.description": "Claude Sonnet 4.5 is Anthropic's most intelligent model to date.",
  "codegeex-4.description": "CodeGeeX-4 is a powerful AI programming assistant that supports intelligent Q&A and code completion in various programming languages, enhancing development efficiency.",
  "codegeex4-all-9b.description": "CodeGeeX4-ALL-9B is a multilingual code generation model that supports comprehensive functions including code completion and generation, code interpretation, web search, function calls, and repository-level code Q&A, covering various scenarios in software development. It is a top-tier code generation model with fewer than 10B parameters.",
  "codegemma.description": "CodeGemma is a lightweight language model dedicated to various programming tasks, supporting rapid iteration and integration.",
  "codegemma:2b.description": "CodeGemma is a lightweight language model dedicated to various programming tasks, supporting rapid iteration and integration.",
  "codellama.description": "Code Llama is an LLM focused on code generation and discussion, combining extensive programming language support, suitable for developer environments.",
  "codellama/CodeLlama-34b-Instruct-hf.description": "Code Llama is an LLM focused on code generation and discussion, with extensive support for various programming languages, suitable for developer environments.",
  "codellama:13b.description": "Code Llama is an LLM focused on code generation and discussion, combining extensive programming language support, suitable for developer environments.",
  "codellama:34b.description": "Code Llama is an LLM focused on code generation and discussion, combining extensive programming language support, suitable for developer environments.",
  "codellama:70b.description": "Code Llama is an LLM focused on code generation and discussion, combining extensive programming language support, suitable for developer environments.",
  "codeqwen.description": "CodeQwen1.5 is a large language model trained on extensive code data, specifically designed to solve complex programming tasks.",
  "codestral-latest.description": "Codestral is a cutting-edge generative model focused on code generation, optimized for intermediate filling and code completion tasks.",
  "codestral.description": "Codestral is Mistral AI's first code model, providing excellent support for code generation tasks.",
  "codex-mini-latest.description": "codex-mini-latest is a fine-tuned version of o4-mini, specifically designed for Codex CLI. For direct API usage, we recommend starting with gpt-4.1.",
  "cogview-4.description": "CogView-4 is Zhipu's first open-source text-to-image model supporting Chinese character generation. It offers comprehensive improvements in semantic understanding, image generation quality, and bilingual Chinese-English text generation capabilities. It supports bilingual input of any length and can generate images at any resolution within a specified range.",
  "cohere-command-r-plus.description": "Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads.",
  "cohere-command-r.description": "Command R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprises.",
  "cohere/Cohere-command-r-plus.description": "Command R+ is a state-of-the-art RAG-optimized model designed to handle enterprise-level workloads.",
  "cohere/Cohere-command-r.description": "Command R is a scalable generative model designed for RAG and tool usage, enabling enterprises to achieve production-grade AI.",
  "cohere/command-a.description": "Command A is Cohere's most powerful model to date, excelling in tool use, agents, retrieval-augmented generation (RAG), and multilingual use cases. With a context length of 256K, it runs on just two GPUs and achieves 150% higher throughput compared to Command R+ 08-2024.",
  "cohere/command-r-plus.description": "Command R+ is Cohere's latest large language model optimized for conversational interactions and long-context tasks. It aims for exceptional performance, enabling companies to transition from proof of concept to production.",
  "cohere/command-r.description": "Command R is a large language model optimized for conversational interactions and long-context tasks. Positioned in the \"scalable\" category, it balances high performance and strong accuracy, enabling companies to move beyond proof of concept into production.",
  "comfyui/flux-dev.description": "FLUX.1 Dev - High-quality text-to-image model, generates in 10–50 steps, ideal for high-end creative and artistic image generation.",
  "comfyui/flux-kontext-dev.description": "FLUX.1 Kontext-dev - Image editing model that supports modifying existing images based on text instructions, including localized edits and style transfer.",
  "comfyui/flux-krea-dev.description": "FLUX.1 Krea-dev - Enhanced safety text-to-image model developed in collaboration with Krea, featuring built-in safety filters.",
  "comfyui/flux-schnell.description": "FLUX.1 Schnell - Ultra-fast text-to-image model capable of generating high-quality images in just 1–4 steps, ideal for real-time applications and rapid prototyping.",
  "comfyui/stable-diffusion-15.description": "Stable Diffusion 1.5 text-to-image model, classic 512x512 resolution generation, suitable for quick prototyping and creative experimentation.",
  "comfyui/stable-diffusion-35-inclclip.description": "Stable Diffusion 3.5 with built-in CLIP/T5 encoders, no need for external encoder files, compatible with models like sd3.5_medium_incl_clips, optimized for lower resource usage.",
  "comfyui/stable-diffusion-35.description": "Stable Diffusion 3.5 next-generation text-to-image model, available in Large and Medium versions, requires external CLIP encoder files, delivers exceptional image quality and prompt alignment.",
  "comfyui/stable-diffusion-custom-refiner.description": "Custom SDXL image-to-image model. Please name the model file as custom_sd_lobe.safetensors. If using a VAE, name it custom_sd_vae_lobe.safetensors. Model files must be placed in the designated folder as required by Comfy.",
  "comfyui/stable-diffusion-custom.description": "Custom SD text-to-image model. Please name the model file as custom_sd_lobe.safetensors. If using a VAE, name it custom_sd_vae_lobe.safetensors. Model files must be placed in the designated folder as required by Comfy.",
  "comfyui/stable-diffusion-refiner.description": "SDXL image-to-image model for high-quality transformations based on input images, supporting style transfer, image restoration, and creative modifications.",
  "comfyui/stable-diffusion-xl.description": "SDXL text-to-image model supporting high-resolution 1024x1024 generation, offering superior image quality and detail rendering.",
  "command-a-03-2025.description": "Command A is our most powerful model to date, excelling in tool usage, agent tasks, retrieval-augmented generation (RAG), and multilingual applications. Command A features a context length of 256K and can run on just two GPUs, achieving a 150% increase in throughput compared to Command R+ 08-2024.",
  "command-light-nightly.description": "To shorten the time interval between major version releases, we have launched nightly versions of the Command model. For the command-light series, this version is called command-light-nightly. Please note that command-light-nightly is the latest, most experimental, and (potentially) unstable version. Nightly versions are updated regularly without prior notice, so they are not recommended for production use.",
  "command-light.description": "A smaller, faster version of Command that is nearly as powerful but operates at a higher speed.",
  "command-nightly.description": "To shorten the time interval between major version releases, we have launched nightly versions of the Command model. For the Command series, this version is called command-cightly. Please note that command-nightly is the latest, most experimental, and (potentially) unstable version. Nightly versions are updated regularly without prior notice, so they are not recommended for production use.",
  "command-r-03-2024.description": "Command R is an instruction-following dialogue model that provides higher quality and reliability in language tasks, with a longer context length than previous models. It can be used for complex workflows such as code generation, retrieval-augmented generation (RAG), tool usage, and agent tasks.",
  "command-r-08-2024.description": "command-r-08-2024 is an updated version of the Command R model, released in August 2024.",
  "command-r-plus-04-2024.description": "Command R+ is an instruction-following dialogue model that delivers higher quality and reliability in language tasks, with a longer context length than previous models. It is best suited for complex RAG workflows and multi-step tool usage.",
  "command-r-plus-08-2024.description": "Command R+ is an instruction-following conversational model that delivers higher quality and reliability in language tasks, with a longer context length compared to previous models. It is best suited for complex RAG workflows and multi-step tool usage.",
  "command-r-plus.description": "Command R+ is a high-performance large language model designed for real enterprise scenarios and complex applications.",
  "command-r.description": "Command R is an LLM optimized for dialogue and long context tasks, particularly suitable for dynamic interactions and knowledge management.",
  "command-r7b-12-2024.description": "command-r7b-12-2024 is a compact and efficient updated version, released in December 2024. It excels in tasks requiring complex reasoning and multi-step processing, such as RAG, tool usage, and agent tasks.",
  "command.description": "An instruction-following dialogue model that delivers high quality and reliability in language tasks, with a longer context length compared to our base generation models.",
  "computer-use-preview.description": "The computer-use-preview model is a dedicated model designed for \"computer usage tools,\" trained to understand and execute computer-related tasks.",
  "dall-e-2.description": "The second generation DALL·E model, supporting more realistic and accurate image generation, with a resolution four times that of the first generation.",
  "dall-e-3.description": "The latest DALL·E model, released in November 2023. It supports more realistic and accurate image generation with enhanced detail representation.",
  "databricks/dbrx-instruct.description": "DBRX Instruct provides highly reliable instruction processing capabilities, supporting applications across multiple industries.",
  "deepseek-ai/DeepSeek-OCR.description": "DeepSeek-OCR is a vision-language model developed by DeepSeek AI, focused on Optical Character Recognition (OCR) and 'contextual optical compression.' The model explores the limits of compressing contextual information from images and efficiently processes documents into structured text formats such as Markdown. It accurately recognizes textual content within images, making it particularly suitable for document digitization, text extraction, and structured data processing applications.",
  "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B.description": "DeepSeek-R1-0528-Qwen3-8B is a model distilled from DeepSeek-R1-0528's chain of thought into Qwen3 8B Base. It achieves state-of-the-art (SOTA) performance among open-source models, surpassing Qwen3 8B by 10% in the AIME 2024 test and reaching the performance level of Qwen3-235B-thinking. The model excels in mathematics reasoning, programming, and general logic benchmarks. It shares the same architecture as Qwen3-8B but uses the tokenizer configuration from DeepSeek-R1-0528.",
  "deepseek-ai/DeepSeek-R1-0528.description": "DeepSeek R1 significantly enhances its reasoning and inference depth by leveraging increased computational resources and introducing algorithmic optimizations during post-training. The model performs excellently across various benchmarks, including mathematics, programming, and general logic. Its overall performance now approaches leading models such as O3 and Gemini 2.5 Pro.",
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B.description": "The DeepSeek-R1 distillation model optimizes inference performance through reinforcement learning and cold-start data, refreshing the benchmark for open-source models across multiple tasks.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.description": "The DeepSeek-R1 distillation model optimizes inference performance through reinforcement learning and cold-start data, refreshing the benchmark for open-source models across multiple tasks.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.description": "DeepSeek-R1-Distill-Qwen-32B is a model obtained through knowledge distillation based on Qwen2.5-32B. This model is fine-tuned using 800,000 selected samples generated by DeepSeek-R1, demonstrating exceptional performance in mathematics, programming, and reasoning across multiple domains. It has achieved excellent results in various benchmark tests, including a 94.3% accuracy rate on MATH-500, showcasing strong mathematical reasoning capabilities.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7B is a model obtained through knowledge distillation based on Qwen2.5-Math-7B. This model is fine-tuned using 800,000 selected samples generated by DeepSeek-R1, demonstrating excellent reasoning capabilities. It has performed outstandingly in multiple benchmark tests, achieving a 92.8% accuracy rate on MATH-500, a 55.5% pass rate on AIME 2024, and a score of 1189 on CodeForces, showcasing strong mathematical and programming abilities as a 7B scale model.",
  "deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1 is a reinforcement learning (RL) driven inference model that addresses issues of repetitiveness and readability within the model. Prior to RL, DeepSeek-R1 introduced cold start data to further optimize inference performance. It performs comparably to OpenAI-o1 in mathematical, coding, and reasoning tasks, and enhances overall effectiveness through meticulously designed training methods.",
  "deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3 is a mixture of experts (MoE) language model with 671 billion parameters, utilizing multi-head latent attention (MLA) and the DeepSeekMoE architecture, combined with a load balancing strategy that does not rely on auxiliary loss, optimizing inference and training efficiency. Pre-trained on 14.8 trillion high-quality tokens and fine-tuned with supervision and reinforcement learning, DeepSeek-V3 outperforms other open-source models and approaches leading closed-source models in performance.",
  "deepseek-ai/deepseek-llm-67b-chat.description": "DeepSeek 67B is an advanced model trained for highly complex conversations.",
  "deepseek-ai/deepseek-r1.description": "A state-of-the-art efficient LLM skilled in reasoning, mathematics, and programming.",
  "deepseek-ai/deepseek-vl2.description": "DeepSeek-VL2 is a mixture of experts (MoE) visual language model developed based on DeepSeekMoE-27B, employing a sparsely activated MoE architecture that achieves outstanding performance while activating only 4.5 billion parameters. This model excels in various tasks, including visual question answering, optical character recognition, document/table/chart understanding, and visual localization.",
  "deepseek-chat.description": "DeepSeek V3.2 is DeepSeek’s latest general-purpose model. It supports a hybrid reasoning architecture and offers stronger agent capabilities.",
  "deepseek-coder-33B-instruct.description": "DeepSeek Coder 33B is a code language model trained on 20 trillion data points, of which 87% are code and 13% are in Chinese and English. The model introduces a 16K window size and fill-in-the-blank tasks, providing project-level code completion and snippet filling capabilities.",
  "deepseek-coder-v2.description": "DeepSeek Coder V2 is an open-source hybrid expert code model that performs excellently in coding tasks, comparable to GPT4-Turbo.",
  "deepseek-coder-v2:236b.description": "DeepSeek Coder V2 is an open-source hybrid expert code model that performs excellently in coding tasks, comparable to GPT4-Turbo.",
  "deepseek-ocr.description": "DeepSeek-OCR is a vision-language model developed by DeepSeek AI, focused on Optical Character Recognition (OCR) and 'contextual optical compression.' It explores the limits of compressing contextual information from images, efficiently processing documents and converting them into structured formats like Markdown. It excels at accurately recognizing text within images, making it well-suited for document digitization, text extraction, and structured data processing.",
  "deepseek-r1-0528.description": "The full-capacity 685B model released on May 28, 2025. DeepSeek-R1 extensively employs reinforcement learning during post-training, significantly enhancing reasoning capabilities with minimal labeled data. It demonstrates strong performance in mathematics, coding, and natural language reasoning tasks.",
  "deepseek-r1-250528.description": "DeepSeek R1 250528, the full-performance DeepSeek-R1 inference model, ideal for complex mathematical and logical tasks.",
  "deepseek-r1-70b-fast-online.description": "DeepSeek R1 70B fast version, supporting real-time online search, providing faster response times while maintaining model performance.",
  "deepseek-r1-70b-online.description": "DeepSeek R1 70B standard version, supporting real-time online search, suitable for dialogue and text processing tasks that require the latest information.",
  "deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70B, a distilled model combining general R1 inference capabilities with the Llama ecosystem.",
  "deepseek-r1-distill-llama-8b.description": "DeepSeek-R1-Distill-Llama-8B is a distilled large language model based on Llama-3.1-8B, utilizing outputs from DeepSeek R1.",
  "deepseek-r1-distill-llama.description": "deepseek-r1-distill-llama is a model distilled from DeepSeek-R1 based on Llama.",
  "deepseek-r1-distill-qianfan-70b.description": "DeepSeek R1 Distill Qianfan 70B, a cost-effective R1 distilled model based on Qianfan-70B.",
  "deepseek-r1-distill-qianfan-8b.description": "DeepSeek R1 Distill Qianfan 8B, an R1 distilled model based on Qianfan-8B, suitable for small to medium-scale applications.",
  "deepseek-r1-distill-qianfan-llama-70b.description": "DeepSeek R1 Distill Qianfan Llama 70B, an R1 distilled model based on Llama-70B.",
  "deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14B, a mid-sized R1 distilled model suitable for multi-scenario deployment.",
  "deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32B, an R1 distilled model based on Qwen-32B, balancing performance and cost.",
  "deepseek-r1-distill-qwen-7b.description": "DeepSeek R1 Distill Qwen 7B, a lightweight R1 distilled model ideal for edge computing and enterprise private deployments.",
  "deepseek-r1-distill-qwen.description": "deepseek-r1-distill-qwen is a model distilled from DeepSeek-R1 based on Qwen.",
  "deepseek-r1-fast-online.description": "DeepSeek R1 full fast version, supporting real-time online search, combining the powerful capabilities of 671B parameters with faster response times.",
  "deepseek-r1-online.description": "DeepSeek R1 full version, with 671B parameters, supporting real-time online search, offering enhanced understanding and generation capabilities.",
  "deepseek-r1.description": "DeepSeek-R1 is a reinforcement learning (RL) driven inference model that addresses issues of repetitiveness and readability within the model. Prior to RL, DeepSeek-R1 introduced cold start data to further optimize inference performance. It performs comparably to OpenAI-o1 in mathematical, coding, and reasoning tasks, and enhances overall effectiveness through meticulously designed training methods.",
  "deepseek-reasoner.description": "DeepSeek V3.2 Thinking Mode. Before outputting the final answer, the model first generates a chain of thought to improve the accuracy of the final response.",
  "deepseek-v2.description": "DeepSeek V2 is an efficient Mixture-of-Experts language model, suitable for cost-effective processing needs.",
  "deepseek-v2:236b.description": "DeepSeek V2 236B is the design code model of DeepSeek, providing powerful code generation capabilities.",
  "deepseek-v3-0324.description": "DeepSeek-V3-0324 is a 671B parameter MoE model, excelling in programming and technical capabilities, contextual understanding, and long text processing.",
  "deepseek-v3.description": "DeepSeek-V3 is a MoE model developed by Hangzhou DeepSeek Artificial Intelligence Technology Research Co., Ltd., achieving outstanding results in multiple evaluations and ranking first among open-source models on mainstream leaderboards. Compared to the V2.5 model, V3 has achieved a threefold increase in generation speed, providing users with a faster and smoother experience.",
  "deepseek-vl2-small.description": "DeepSeek VL2 Small, a lightweight multimodal version designed for resource-constrained and high-concurrency scenarios.",
  "deepseek-vl2.description": "DeepSeek VL2, a multimodal model supporting image-text understanding and fine-grained visual question answering.",
  "deepseek/deepseek-chat-v3-0324.description": "DeepSeek V3 is a 685B parameter expert mixture model, the latest iteration in the DeepSeek team's flagship chat model series.\n\nIt inherits from the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs excellently across various tasks.",
  "deepseek/deepseek-chat-v3-0324:free.description": "DeepSeek V3 is a 685B parameter expert mixture model, the latest iteration in the DeepSeek team's flagship chat model series.\n\nIt inherits from the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs excellently across various tasks.",
  "deepseek/deepseek-chat.description": "DeepSeek-V3 is a high-performance hybrid reasoning model from the DeepSeek team, suitable for complex tasks and tool integration.",
  "deepseek/deepseek-r1-0528.description": "DeepSeek R1 0528 is an updated variant from DeepSeek, focused on open-source usability and deep reasoning.",
  "deepseek/deepseek-r1-0528:free.description": "DeepSeek-R1 greatly improves model reasoning capabilities with minimal labeled data. Before outputting the final answer, the model first generates a chain of thought to enhance answer accuracy.",
  "deepseek/deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70B is a large language model based on Llama3.3 70B. Fine-tuned using outputs from DeepSeek R1, it achieves competitive performance on par with leading-edge large models.",
  "deepseek/deepseek-r1-distill-llama-8b.description": "DeepSeek R1 Distill Llama 8B is a distilled large language model based on Llama-3.1-8B-Instruct, trained using outputs from DeepSeek R1.",
  "deepseek/deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, trained using outputs from DeepSeek R1. This model has surpassed OpenAI's o1-mini in several benchmark tests, achieving state-of-the-art results for dense models. Here are some benchmark results:\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nThis model demonstrates competitive performance comparable to larger cutting-edge models through fine-tuning from DeepSeek R1 outputs.",
  "deepseek/deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on Qwen 2.5 32B, trained using outputs from DeepSeek R1. This model has surpassed OpenAI's o1-mini in several benchmark tests, achieving state-of-the-art results for dense models. Here are some benchmark results:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nThis model demonstrates competitive performance comparable to larger cutting-edge models through fine-tuning from DeepSeek R1 outputs.",
  "deepseek/deepseek-r1.description": "The DeepSeek R1 model has undergone minor version upgrades, currently at DeepSeek-R1-0528. The latest update significantly enhances inference depth and capability by leveraging increased compute resources and post-training algorithmic optimizations. The model performs excellently on benchmarks in mathematics, programming, and general logic, with overall performance approaching leading models like O3 and Gemini 2.5 Pro.",
  "deepseek/deepseek-r1/community.description": "DeepSeek R1 is the latest open-source model released by the DeepSeek team, featuring impressive inference performance, particularly in mathematics, programming, and reasoning tasks, reaching levels comparable to OpenAI's o1 model.",
  "deepseek/deepseek-r1:free.description": "DeepSeek-R1 significantly enhances model reasoning capabilities with minimal labeled data. Before outputting the final answer, the model first provides a chain of thought to improve the accuracy of the final response.",
  "deepseek/deepseek-reasoner.description": "DeepSeek-V3 Thinking (reasoner) is an experimental reasoning model from DeepSeek, designed for high-complexity reasoning tasks.",
  "deepseek/deepseek-v3.description": "A fast, general-purpose large language model with enhanced reasoning capabilities.",
  "deepseek/deepseek-v3/community.description": "DeepSeek-V3 has achieved a significant breakthrough in inference speed compared to previous models. It ranks first among open-source models and can compete with the world's most advanced closed-source models. DeepSeek-V3 employs Multi-Head Latent Attention (MLA) and DeepSeekMoE architectures, which have been thoroughly validated in DeepSeek-V2. Additionally, DeepSeek-V3 introduces an auxiliary lossless strategy for load balancing and sets multi-label prediction training objectives for enhanced performance.",
  "deepseek_r1.description": "DeepSeek-R1 is a reinforcement learning (RL) driven reasoning model that addresses issues of repetition and readability within the model. Prior to RL, DeepSeek-R1 introduced cold start data to further optimize inference performance. It performs comparably to OpenAI-o1 in mathematics, coding, and reasoning tasks, and enhances overall effectiveness through carefully designed training methods.",
  "deepseek_r1_distill_llama_70b.description": "DeepSeek-R1-Distill-Llama-70B is a model obtained through distillation training based on Llama-3.3-70B-Instruct. This model is part of the DeepSeek-R1 series and showcases excellent performance in mathematics, programming, and reasoning through fine-tuning with samples generated by DeepSeek-R1.",
  "deepseek_r1_distill_qwen_14b.description": "DeepSeek-R1-Distill-Qwen-14B is a model derived from Qwen2.5-14B through knowledge distillation. This model is fine-tuned using 800,000 curated samples generated by DeepSeek-R1, showcasing excellent reasoning capabilities.",
  "deepseek_r1_distill_qwen_32b.description": "DeepSeek-R1-Distill-Qwen-32B is a model derived from Qwen2.5-32B through knowledge distillation. This model is fine-tuned using 800,000 curated samples generated by DeepSeek-R1, demonstrating outstanding performance across multiple domains such as mathematics, programming, and reasoning.",
  "devstral-2:123b.description": "Devstral 2 123B model, excels at using tools to explore codebases, edit multiple files, and support software engineering agents.",
  "doubao-lite-128k.description": "Offers ultra-fast response times and better cost-effectiveness, providing customers with more flexible options for different scenarios. Supports inference and fine-tuning with a 128k context window.",
  "doubao-lite-32k.description": "Offers ultra-fast response times and better cost-effectiveness, providing customers with more flexible options for different scenarios. Supports inference and fine-tuning with a 32k context window.",
  "doubao-lite-4k.description": "Offers ultra-fast response times and better cost-effectiveness, providing customers with more flexible options for different scenarios. Supports inference and fine-tuning with a 4k context window.",
  "doubao-pro-256k.description": "The best-performing flagship model, suitable for handling complex tasks. It excels in scenarios such as reference Q&A, summarization, creative writing, text classification, and role-playing. Supports inference and fine-tuning with a 256k context window.",
  "doubao-pro-32k.description": "The best-performing flagship model, suitable for handling complex tasks. It excels in scenarios such as reference Q&A, summarization, creative writing, text classification, and role-playing. Supports inference and fine-tuning with a 32k context window.",
  "doubao-seed-code.description": "Doubao-Seed-Code is deeply optimized for agentic programming tasks, supporting multimodal inputs (text/image/video) and a 256K long context window. It is compatible with the Anthropic API and ideal for programming, visual understanding, and agent-based scenarios.",
  "doubao-seededit-3-0-i2i-250628.description": "Doubao image generation model developed by ByteDance Seed team supports both text and image inputs, providing a highly controllable and high-quality image generation experience. Supports image editing via text instructions, generating images with dimensions between 512 and 1536 pixels.",
  "doubao-seedream-3-0-t2i-250415.description": "Seedream 3.0 image generation model developed by ByteDance Seed team supports text and image inputs, delivering a highly controllable and high-quality image generation experience. Generates images based on text prompts.",
  "doubao-seedream-4-0-250828.description": "Seedream 4.0 image generation model developed by ByteDance Seed team supports text and image inputs, offering a highly controllable and high-quality image generation experience. Generates images based on text prompts.",
  "doubao-vision-lite-32k.description": "The Doubao-vision model is a multimodal large model launched by Doubao, featuring powerful image understanding and reasoning capabilities along with precise instruction comprehension. It demonstrates strong performance in image-text information extraction and image-based reasoning tasks, applicable to more complex and diverse visual question answering scenarios.",
  "doubao-vision-pro-32k.description": "The Doubao-vision model is a multimodal large model launched by Doubao, featuring powerful image understanding and reasoning capabilities along with precise instruction comprehension. It demonstrates strong performance in image-text information extraction and image-based reasoning tasks, applicable to more complex and diverse visual question answering scenarios.",
  "emohaa.description": "Emohaa is a psychological model with professional counseling capabilities, helping users understand emotional issues.",
  "ernie-char-8k.description": "ERNIE Character 8K, a persona dialogue model ideal for IP character building and long-term companion conversations.",
  "ernie-char-fiction-8k-preview.description": "ERNIE Character Fiction 8K Preview, a preview model for character and story creation, designed for feature testing and experience.",
  "ernie-char-fiction-8k.description": "ERNIE Character Fiction 8K, a persona model for novel and story creation, suitable for generating long-form narratives.",
  "ernie-irag-edit.description": "ERNIE iRAG Edit, an image editing model supporting image erasure, redrawing, and variant generation.",
  "ernie-lite-8k.description": "ERNIE Lite 8K, a lightweight general-purpose model suitable for cost-sensitive daily Q&A and content generation.",
  "ernie-lite-pro-128k.description": "ERNIE Lite Pro 128K, a lightweight high-performance model ideal for latency- and cost-sensitive business scenarios.",
  "ernie-novel-8k.description": "ERNIE Novel 8K, a model for long-form novel and IP story creation, skilled in multi-character and multi-threaded storytelling.",
  "ernie-speed-128k.description": "ERNIE Speed 128K, a large model with no input/output cost, suitable for long-text understanding and large-scale trials.",
  "ernie-speed-8k.description": "ERNIE Speed 8K, a free and fast model ideal for daily conversations and lightweight text tasks.",
  "ernie-speed-pro-128k.description": "ERNIE Speed Pro 128K, a high-concurrency, cost-effective model suitable for large-scale online services and enterprise applications.",
  "ernie-tiny-8k.description": "ERNIE Tiny 8K, an ultra-lightweight model for simple Q&A, classification, and other low-cost inference scenarios.",
  "ernie-x1-turbo-32k.description": "ERNIE X1 Turbo 32K, a high-speed reasoning model with 32K long context, ideal for complex reasoning and multi-turn dialogue.",
  "fal-ai/bytedance/seedream/v4.description": "Seedream 4.0 image generation model developed by ByteDance Seed team supports text and image inputs, providing a highly controllable and high-quality image generation experience. Generates images based on text prompts.",
  "fal-ai/flux-kontext/dev.description": "FLUX.1 model focused on image editing tasks, supporting both text and image inputs.",
  "fal-ai/flux-pro/kontext.description": "FLUX.1 Kontext [pro] can process text and reference images as inputs, seamlessly enabling targeted local edits and complex overall scene transformations.",
  "fal-ai/flux/krea.description": "Flux Krea [dev] is an image generation model with an aesthetic preference, aiming to produce more realistic and natural images.",
  "fal-ai/flux/schnell.description": "FLUX.1 [schnell] is a 12-billion-parameter image generation model focused on fast generation of high-quality images.",
  "fal-ai/hunyuan-image/v3.description": "A powerful native multimodal image generation model",
  "fal-ai/imagen4/preview.description": "High-quality image generation model provided by Google.",
  "fal-ai/nano-banana.description": "Nano Banana is Google's latest, fastest, and most efficient native multimodal model, allowing you to generate and edit images through conversation.",
  "fal-ai/qwen-image-edit.description": "Professional image editing model released by the Qwen team, supporting semantic and appearance editing, precise editing of Chinese and English text, style transfer, object rotation, and other high-quality image edits.",
  "fal-ai/qwen-image.description": "Powerful raw image model from the Qwen team, featuring impressive Chinese text generation capabilities and diverse visual styles.",
  "flux-1-schnell.description": "Developed by Black Forest Labs, this 12-billion-parameter text-to-image model uses latent adversarial diffusion distillation technology to generate high-quality images within 1 to 4 steps. Its performance rivals closed-source alternatives and is released under the Apache-2.0 license, suitable for personal, research, and commercial use.",
  "flux-dev.description": "FLUX.1 [dev] is an open-source weight and fine-tuned model for non-commercial applications. It maintains image quality and instruction-following capabilities close to the FLUX professional version while offering higher operational efficiency. Compared to standard models of the same size, it is more resource-efficient.",
  "flux-kontext-max.description": "State-of-the-art contextual image generation and editing — combining text and images for precise, coherent results.",
  "flux-kontext-pro.description": "State-of-the-art contextual image generation and editing — combining text and images for precise, coherent results.",
  "flux-merged.description": "The FLUX.1-merged model combines the deep features explored during the development phase of “DEV” with the high-speed execution advantages represented by “Schnell.” This integration not only pushes the model's performance boundaries but also broadens its application scope.",
  "flux-pro.description": "A top-tier commercial AI image generation model — delivering unparalleled image quality and a wide variety of outputs.",
  "flux-schnell.description": "FLUX.1 [schnell], currently the most advanced open-source few-step model, surpasses competitors and even powerful non-distilled models like Midjourney v6.0 and DALL·E 3 (HD). Finely tuned to retain the full output diversity from pretraining, FLUX.1 [schnell] significantly enhances visual quality, instruction compliance, size/aspect ratio variation, font handling, and output diversity compared to state-of-the-art models on the market, offering users a richer and more diverse creative image generation experience.",
  "gemini-3-pro-image-preview.description": "Gemini 3 Pro Image (Nano Banana Pro) is Google’s image generation model, also supporting multimodal dialogue.",
  "gemini-3-pro-image-preview:image.description": "Gemini 3 Pro Image (Nano Banana Pro) is Google’s image generation model, also supporting multimodal dialogue.",
  "gemini-3-pro-preview.description": "Gemini 3 Pro is the world’s leading multimodal understanding model and Google’s most powerful agent and ambient programming model to date, offering rich visual output and deep interactivity, all built on cutting-edge reasoning capabilities.",
  "gemini-flash-latest.description": "Latest release of Gemini Flash",
  "gemini-flash-lite-latest.description": "Latest release of Gemini Flash-Lite",
  "gemini-pro-latest.description": "Latest release of Gemini Pro",
  "gemma-7b-it.description": "Gemma 7B 适合中小规模任务处理，兼具成本效益。",
  "gemma2-9b-it.description": "Gemma 2 9B 是一款优化用于特定任务和工具整合的模型。",
  "gemma2.description": "Gemma 2 是 Google 推出的高效模型，涵盖从小型应用到复杂数据处理的多种应用场景。",
  "gemma2:27b.description": "Gemma 2 是 Google 推出的高效模型，涵盖从小型应用到复杂数据处理的多种应用场景。",
  "gemma2:2b.description": "Gemma 2 是 Google 推出的高效模型，涵盖从小型应用到复杂数据处理的多种应用场景。",
  "generalv3.5.description": "Spark Max 为功能最为全面的版本，支持联网搜索及众多内置插件。其全面优化的核心能力以及系统角色设定和函数调用功能，使其在各种复杂应用场景中的表现极为优异和出色。",
  "generalv3.description": "Spark Pro 是一款为专业领域优化的高性能大语言模型，专注数学、编程、医疗、教育等多个领域，并支持联网搜索及内置天气、日期等插件。其优化后模型在复杂知识问答、语言理解及高层次文本创作中展现出色表现和高效性能，是适合专业应用场景的理想选择。",
  "glm-4-0520.description": "GLM-4-0520 是最新模型版本，专为高度复杂和多样化任务设计，表现卓越。",
  "glm-4-32b-0414.description": "GLM-4 32B 0414，GLM 系列通用大模型版本，支持多任务文本生成与理解。",
  "glm-4-9b-chat.description": "GLM-4-9B-Chat 在语义、数学、推理、代码和知识等多方面均表现出较高性能。还具备网页浏览、代码执行、自定义工具调用和长文本推理。 支持包括日语，韩语，德语在内的 26 种语言。",
  "glm-4-air-250414.description": "GLM-4-Air 是性价比高的版本，性能接近GLM-4，提供快速度和实惠的价格。",
  "glm-4-air.description": "GLM-4-Air 是性价比高的版本，性能接近GLM-4，提供快速度和实惠的价格。",
  "glm-4-airx.description": "GLM-4-AirX 提供 GLM-4-Air 的高效版本，推理速度可达其2.6倍。",
  "glm-4-alltools.description": "GLM-4-AllTools 是一个多功能智能体模型，优化以支持复杂指令规划与工具调用，如网络浏览、代码解释和文本生成，适用于多任务执行。",
  "glm-4-flash-250414.description": "GLM-4-Flash 是处理简单任务的理想选择，速度最快且免费。",
  "glm-4-flash.description": "GLM-4-Flash 是处理简单任务的理想选择，速度最快且免费。",
  "glm-4-flashx.description": "GLM-4-FlashX 是Flash的增强版本，超快推理速度。",
  "glm-4-long.description": "GLM-4-Long 支持超长文本输入，适合记忆型任务与大规模文档处理。",
  "glm-4-plus.description": "GLM-4-Plus 作为高智能旗舰，具备强大的处理长文本和复杂任务的能力，性能全面提升。",
  "glm-4.1v-thinking-flash.description": "GLM-4.1V-Thinking 系列模型是目前已知10B级别的VLM模型中性能最强的视觉模型，融合了同级别SOTA的各项视觉语言任务，包括视频理解、图片问答、学科解题、OCR文字识别、文档和图表解读、GUI Agent、前端网页Coding、Grounding等，多项任务能力甚至超过8倍参数量的Qwen2.5-VL-72B。通过领先的强化学习技术，模型掌握了通过思维链推理的方式提升回答的准确性和丰富度，从最终效果和可解释性等维度都显著超过传统的非thinking模型。",
  "glm-4.1v-thinking-flashx.description": "GLM-4.1V-Thinking 系列模型是目前已知10B级别的VLM模型中性能最强的视觉模型，融合了同级别SOTA的各项视觉语言任务，包括视频理解、图片问答、学科解题、OCR文字识别、文档和图表解读、GUI Agent、前端网页Coding、Grounding等，多项任务能力甚至超过8倍参数量的Qwen2.5-VL-72B。通过领先的强化学习技术，模型掌握了通过思维链推理的方式提升回答的准确性和丰富度，从最终效果和可解释性等维度都显著超过传统的非thinking模型。",
  "glm-4.5-air.description": "GLM-4.5 的轻量版，兼顾性能与性价比，可灵活切换混合思考模型。",
  "glm-4.5-airx.description": "GLM-4.5-Air 的极速版，响应速度更快，专为大规模高速度需求打造。",
  "glm-4.5-flash.description": "GLM-4.5 的免费版，推理、代码、智能体等任务表现出色。",
  "glm-4.5-x.description": "GLM-4.5 的极速版，在性能强劲的同时，生成速度可达 100 tokens/秒。",
  "glm-4.5.description": "智谱旗舰模型，支持思考模式切换，综合能力达到开源模型的 SOTA 水平，上下文长度可达128K。",
  "glm-4.5v.description": "智谱新一代基于 MOE 架构的视觉推理模型，以106B的总参数量和12B激活参数量，在各类基准测试中达到全球同级别开源多模态模型 SOTA，涵盖图像、视频、文档理解及 GUI 任务等常见任务。",
  "glm-4.6.description": "智谱最新旗舰模型 GLM-4.6 (355B) 在高级编码、长文本处理、推理与智能体能力上全面超越前代，尤其在编程能力上对齐 Claude Sonnet 4，成为国内顶尖的 Coding 模型。",
  "glm-4.description": "GLM-4 是发布于2024年1月的旧旗舰版本，目前已被更强的 GLM-4-0520 取代。",
  "glm-4v-flash.description": "GLM-4V-Flash 专注于高效的单一图像理解，适用于快速图像解析的场景，例如实时图像分析或批量图像处理。",
  "glm-4v-plus-0111.description": "GLM-4V-Plus 具备对视频内容及多图片的理解能力，适合多模态任务。",
  "glm-4v-plus.description": "GLM-4V-Plus 具备对视频内容及多图片的理解能力，适合多模态任务。",
  "glm-4v.description": "GLM-4V 提供强大的图像理解与推理能力，支持多种视觉任务。",
  "glm-z1-air.description": "推理模型: 具备强大推理能力，适用于需要深度推理的任务。",
  "glm-z1-airx.description": "极速推理：具有超快的推理速度和强大的推理效果。",
  "glm-z1-flash.description": "GLM-Z1 系列具备强大的复杂推理能力，在逻辑推理、数学、编程等领域表现优异。",
  "glm-z1-flashx.description": "高速低价：Flash增强版本，超快推理速度，更快并发保障。",
  "glm-zero-preview.description": "GLM-Zero-Preview具备强大的复杂推理能力，在逻辑推理、数学、编程等领域表现优异。",
  "global.anthropic.claude-opus-4-5-20251101-v1:0.description": "Claude Opus 4.5 是 Anthropic 的旗舰模型，结合了卓越的智能与可扩展性能，适合需要最高质量回应和推理能力的复杂任务。",
  "google/gemini-2.0-flash-001.description": "Gemini 2.0 Flash 提供下一代功能和改进，包括卓越的速度、原生工具使用、多模态生成和1M令牌上下文窗口。",
  "google/gemini-2.0-flash-exp:free.description": "Gemini 2.0 Flash Experimental 是 Google 最新的实验性多模态AI模型，与历史版本相比有一定的质量提升，特别是对于世界知识、代码和长上下文。",
  "google/gemini-2.0-flash-lite-001.description": "Gemini 2.0 Flash Lite 是 Gemini 家族的轻量版本，默认不启用思考以提升延迟与成本表现，但可通过参数开启。",
  "google/gemini-2.0-flash-lite.description": "Gemini 2.0 Flash Lite 提供下一代功能和改进的功能，包括卓越的速度、内置工具使用、多模态生成和 100 万 token 的上下文窗口。",
  "google/gemini-2.0-flash.description": "Gemini 2.0 Flash 是 Google 的高性能推理模型，适用于延展的多模态任务。",
  "google/gemini-2.5-flash-image-free.description": "Gemini 2.5 Flash Image 免费版，支持受限额度的多模态生成。",
  "google/gemini-2.5-flash-image-preview.description": "Gemini 2.5 Flash 实验模型，支持图像生成",
  "google/gemini-2.5-flash-image.description": "Gemini 2.5 Flash Image（Nano Banana）是 Google 的图像生成模型，同时支持多模态对话。",
  "google/gemini-2.5-flash-lite.description": "Gemini 2.5 Flash Lite 是 Gemini 2.5 的轻量版本，优化了延迟与成本，适合高吞吐场景。",
  "google/gemini-2.5-flash-preview.description": "Gemini 2.5 Flash 是 Google 最先进的主力模型，专为高级推理、编码、数学和科学任务而设计。它包含内置的“思考”能力，使其能够提供具有更高准确性和细致上下文处理的响应。\n\n注意：此模型有两个变体：思考和非思考。输出定价根据思考能力是否激活而有显著差异。如果您选择标准变体（不带“:thinking”后缀），模型将明确避免生成思考令牌。\n\n要利用思考能力并接收思考令牌，您必须选择“:thinking”变体，这将产生更高的思考输出定价。\n\n此外，Gemini 2.5 Flash 可通过“推理最大令牌数”参数进行配置，如文档中所述 (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)。",
  "google/gemini-2.5-flash-preview:thinking.description": "Gemini 2.5 Flash 是 Google 最先进的主力模型，专为高级推理、编码、数学和科学任务而设计。它包含内置的“思考”能力，使其能够提供具有更高准确性和细致上下文处理的响应。\n\n注意：此模型有两个变体：思考和非思考。输出定价根据思考能力是否激活而有显著差异。如果您选择标准变体（不带“:thinking”后缀），模型将明确避免生成思考令牌。\n\n要利用思考能力并接收思考令牌，您必须选择“:thinking”变体，这将产生更高的思考输出定价。\n\n此外，Gemini 2.5 Flash 可通过“推理最大令牌数”参数进行配置，如文档中所述 (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)。",
  "google/gemini-2.5-flash.description": "Gemini 2.5 Flash（Lite/Pro/Flash）系列是 Google 的中低延迟到高性能推理模型。",
  "google/gemini-2.5-pro-free.description": "Gemini 2.5 Pro 免费版，支持受限额度的多模态长上下文，适合试用与轻量工作流。",
  "google/gemini-2.5-pro-preview.description": "Gemini 2.5 Pro Preview 是 Google 最先进的思维模型，能够对代码、数学和STEM领域的复杂问题进行推理，以及使用长上下文分析大型数据集、代码库和文档。",
  "google/gemini-2.5-pro.description": "Gemini 2.5 Pro 是 Google 的旗舰级推理模型，支持长上下文与复杂任务。",
  "google/gemini-3-pro-image-preview-free.description": "Gemini 3 Pro Image 免费版，支持受限额度的多模态生成。",
  "google/gemini-3-pro-image-preview.description": "Gemini 3 Pro Image（Nano Banana Pro）是 Google 的图像生成模型，同时支持多模态对话。",
  "google/gemini-3-pro-preview-free.description": "Gemini 3 Pro 免费预览版，具备与标准版相同的多模态理解与推理能力，但受免费额度与速率限制影响，更适合作为体验与低频使用。",
  "google/gemini-3-pro-preview.description": "Gemini 3 Pro 是 Gemini 系列下一代多模态推理模型，可理解文本、音频、图像、视频等多种输入，并处理复杂任务与大型代码库。",
  "google/gemini-embedding-001.description": "最先进的嵌入模型，在英语、多语言和代码任务中具有出色的性能。",
  "google/gemini-flash-1.5.description": "Gemini 1.5 Flash 提供了优化后的多模态处理能力，适用多种复杂任务场景。",
  "google/gemini-pro-1.5.description": "Gemini 1.5 Pro 结合最新优化技术，带来更高效的多模态数据处理能力。",
  "google/gemma-2-27b-it.description": "Gemma 2 27B 是一款通用大语言模型，具有优异的性能和广泛的应用场景。",
  "google/gemma-2-27b.description": "Gemma 2 是 Google 推出的高效模型，涵盖从小型应用到复杂数据处理的多种应用场景。",
  "google/gemma-2-2b-it.description": "面向边缘应用的高级小型语言生成 AI 模型。",
  "google/gemma-2-9b-it.description": "Gemma 2 9B 由Google开发，提供高效的指令响应和综合能力。",
  "google/gemma-2-9b-it:free.description": "Gemma 2 是Google轻量化的开源文本模型系列。",
  "google/gemma-2-9b.description": "Gemma 2 是 Google 推出的高效模型，涵盖从小型应用到复杂数据处理的多种应用场景。",
  "google/gemma-2b-it.description": "Gemma Instruct (2B) 提供基本的指令处理能力，适合轻量级应用。",
  "google/gemma-3-12b-it.description": "Gemma 3 12B 是谷歌的一款开源语言模型，以其在效率和性能方面设立了新的标准。",
  "google/gemma-3-27b-it.description": "Gemma 3 27B 是谷歌的一款开源语言模型，以其在效率和性能方面设立了新的标准。",
  "google/text-embedding-005.description": "针对代码和英语语言任务优化的英语聚焦文本嵌入模型。",
  "google/text-multilingual-embedding-002.description": "针对跨语言任务优化的多语言文本嵌入模型，支持多种语言。",
  "gpt-3.5-turbo-0125.description": "GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125",
  "gpt-3.5-turbo-1106.description": "GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125",
  "gpt-3.5-turbo-instruct.description": "GPT 3.5 Turbo，适用于各种文本生成和理解任务，对指令遵循的优化",
  "gpt-3.5-turbo.description": "GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125",
  "gpt-35-turbo-16k.description": "GPT 3.5 Turbo 16k，高容量文本生成模型，适合复杂任务。",
  "gpt-35-turbo.description": "GPT 3.5 Turbo，OpenAI提供的高效模型，适用于聊天和文本生成任务，支持并行函数调用。",
  "gpt-4-0125-preview.description": "最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。",
  "gpt-4-0613.description": "GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。",
  "gpt-4-1106-preview.description": "最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。",
  "gpt-4-32k-0613.description": "GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。",
  "gpt-4-32k.description": "GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。",
  "gpt-4-turbo-2024-04-09.description": "最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。",
  "gpt-4-turbo-preview.description": "最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。",
  "gpt-4-turbo.description": "最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。",
  "gpt-4-vision-preview.description": "GPT-4 视觉预览版，专为图像分析和处理任务设计。",
  "gpt-4.1-mini.description": "GPT-4.1 mini 提供了智能、速度和成本之间的平衡，使其成为许多用例中有吸引力的模型。",
  "gpt-4.1-nano.description": "GPT-4.1 nano 是最快，最具成本效益的GPT-4.1模型。",
  "gpt-4.1.description": "GPT-4.1 是我们用于复杂任务的旗舰模型。它非常适合跨领域解决问题。",
  "gpt-4.5-preview.description": "GPT-4.5-preview 是最新的通用模型，具有深厚的世界知识和对用户意图的更好理解，擅长创意任务和代理规划。该模型的知识截止2023年10月。",
  "gpt-4.description": "GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。",
  "gpt-4o-2024-05-13.description": "ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。",
  "gpt-4o-2024-08-06.description": "ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。",
  "gpt-4o-2024-11-20.description": "ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。",
  "gpt-4o-audio-preview.description": "GPT-4o Audio Preview 模型，支持音频输入输出",
  "gpt-4o-mini-audio-preview.description": "GPT-4o mini Audio 模型，支持音频输入输出",
  "gpt-4o-mini-realtime-preview.description": "GPT-4o-mini 实时版本，支持音频和文本实时输入输出",
  "gpt-4o-mini-search-preview.description": "GPT-4o mini 搜索预览版是一个专门训练用于理解和执行网页搜索查询的模型，使用的是 Chat Completions API。除了令牌费用之外，网页搜索查询还会按每次工具调用收取费用。",
  "gpt-4o-mini-transcribe.description": "GPT-4o Mini Transcribe 是一种使用 GPT-4o 转录音频的语音转文本模型。与原始 Whisper 模型相比，它提高了单词错误率，并提高了语言识别和准确性。使用它来获得更准确的转录。",
  "gpt-4o-mini-tts.description": "GPT-4o mini TTS 是一个基于 GPT-4o mini 构建的文本转语音模型，这是一种快速且强大的语言模型。使用它可以将文本转换为自然听起来的语音文本。最大输入标记数为 2000。",
  "gpt-4o-mini.description": "GPT-4o mini是OpenAI在GPT-4 Omni之后推出的最新模型，支持图文输入并输出文本。作为他们最先进的小型模型，它比其他近期的前沿模型便宜很多，并且比GPT-3.5 Turbo便宜超过60%。它保持了最先进的智能，同时具有显著的性价比。GPT-4o mini在MMLU测试中获得了 82% 的得分，目前在聊天偏好上排名高于 GPT-4。",
  "gpt-4o-realtime-preview-2024-10-01.description": "GPT-4o 实时版本，支持音频和文本实时输入输出",
  "gpt-4o-realtime-preview-2025-06-03.description": "GPT-4o 实时版本，支持音频和文本实时输入输出",
  "gpt-4o-realtime-preview.description": "GPT-4o 实时版本，支持音频和文本实时输入输出",
  "gpt-4o-search-preview.description": "GPT-4o 搜索预览版是一个专门训练用于理解和执行网页搜索查询的模型，使用的是 Chat Completions API。除了令牌费用之外，网页搜索查询还会按每次工具调用收取费用。",
  "gpt-4o-transcribe.description": "GPT-4o Transcribe 是一种使用 GPT-4o 转录音频的语音转文本模型。与原始 Whisper 模型相比，它提高了单词错误率，并提高了语言识别和准确性。使用它来获得更准确的转录。",
  "gpt-4o.description": "ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。",
  "gpt-5-chat-latest.description": "ChatGPT 中使用的 GPT-5 模型。结合了强大的语言理解与生成能力，适合对话式交互应用。",
  "gpt-5-chat.description": "GPT-5 Chat 专为对话场景优化的预览版本。支持文本和图像输入，仅输出文本，适用于聊天机器人和对话式AI应用。",
  "gpt-5-codex.description": "GPT-5 Codex 是一个针对 Codex 或类似环境中的代理编码任务优化的 GPT-5 版本。",
  "gpt-5-mini.description": "更快、更经济高效的 GPT-5 版本，适用于明确定义的任务。在保持高质量输出的同时，提供更快的响应速度。",
  "gpt-5-nano.description": "最快、最经济高效的 GPT-5 版本。非常适合需要快速响应且成本敏感的应用场景。",
  "gpt-5-pro.description": "GPT-5 pro 使用更多计算来更深入地思考，并持续提供更好的答案。",
  "gpt-5.1-chat-latest.description": "GPT-5.1 Chat：用于 ChatGPT 的 GPT-5.1 变体，适合聊天场景。",
  "gpt-5.1-codex-mini.description": "GPT-5.1 Codex mini：体积更小、成本更低的 Codex 变体，针对 agentic 编码任务进行了优化。",
  "gpt-5.1-codex.description": "GPT-5.1 Codex：针对 agentic 编码任务优化的 GPT-5.1 版本，可在 Responses API 中用于更复杂的代码/代理工作流。",
  "gpt-5.1.description": "GPT-5.1 — 针对编码和 agent 任务优化的旗舰模型，支持可配置的推理强度与更长上下文。",
  "gpt-5.2-chat-latest.description": "GPT-5.2 Chat：ChatGPT 使用的 GPT-5.2 变体（chat-latest），用于体验最新对话改进。",
  "gpt-5.2-pro.description": "GPT-5.2 pro：更聪明、更精确的 GPT-5.2 版本（Responses API Only），适合高难度问题与更长的多轮推理。",
  "gpt-5.2.description": "GPT-5.2 — 面向编码与 agentic 工作流的旗舰模型，提供更强推理与长上下文能力。",
  "gpt-5.description": "跨领域编码和代理任务的最佳模型。GPT-5 在准确性、速度、推理、上下文识别、结构化思维和问题解决方面实现了飞跃。",
  "gpt-audio.description": "GPT Audio 是面向音频输入输出的通用聊天模型，支持在 Chat Completions API 中使用音频 I/O。",
  "gpt-image-1-mini.description": "成本更低的 GPT Image 1 版本，原生支持文本与图像输入并生成图像输出。",
  "gpt-image-1.description": "ChatGPT 原生多模态图片生成模型",
  "gpt-oss-120b.description": "该模型需要申请体验。GPT-OSS-120B 是 OpenAI 推出的开源大规模语言模型，具备强大的文本生成能力。",
  "gpt-oss-20b.description": "该模型需要申请体验。GPT-OSS-20B 是 OpenAI 推出的开源中型语言模型，具备高效的文本生成能力。",
  "gpt-oss:120b.description": "GPT-OSS 120B 是 OpenAI 发布的大型开源语言模型，采用 MXFP4 量化技术，为旗舰级模型。需要多GPU或高性能工作站环境运行，在复杂推理、代码生成和多语言处理方面具备卓越性能，支持高级函数调用和工具集成。",
  "gpt-oss:20b.description": "GPT-OSS 20B 是 OpenAI 发布的开源大语言模型，采用 MXFP4 量化技术，适合在高端消费级GPU或Apple Silicon Mac上运行。该模型在对话生成、代码编写和推理任务方面表现出色，支持函数调用和工具使用。",
  "gpt-realtime.description": "通用实时模型，支持文本与音频的实时输入输出，并支持图像输入。",
  "grok-2-image-1212.description": "我们最新的图像生成模型可以根据文本提示生成生动逼真的图像。它在营销、社交媒体和娱乐等领域的图像生成方面表现出色。",
  "grok-2-vision-1212.description": "该模型在准确性、指令遵循和多语言能力方面有所改进。",
  "grok-3-mini.description": "轻量级模型，回话前会先思考。运行快速、智能，适用于不需要深层领域知识的逻辑任务，并能获取原始的思维轨迹。",
  "grok-3.description": "旗舰级模型，擅长数据提取、编程和文本摘要等企业级应用，拥有金融、医疗、法律和科学等领域的深厚知识。",
  "grok-4-0709.description": "xAI 的 Grok 4，具备强大的推理能力。",
  "grok-4-1-fast-non-reasoning.description": "前沿多模态模型，专门针对高性能代理工具调用进行优化。",
  "grok-4-1-fast-reasoning.description": "前沿多模态模型，专门针对高性能代理工具调用进行优化。",
  "grok-4-fast-non-reasoning.description": "我们很高兴发布 Grok 4 Fast，这是我们在成本效益推理模型方面的最新进展。",
  "grok-4-fast-reasoning.description": "我们很高兴发布 Grok 4 Fast，这是我们在成本效益推理模型方面的最新进展。",
  "grok-4.description": "我们最新最强大的旗舰模型，在自然语言处理、数学计算和推理方面表现卓越 —— 是一款完美的全能型选手。",
  "grok-code-fast-1.description": "我们很高兴推出 grok-code-fast-1，这是一款快速且经济高效的推理模型，在代理编码方面表现出色。",
  "groq/compound-mini.description": "Compound-mini 是一个复合 AI 系统，由 GroqCloud 中已经支持的公开可用模型提供支持，可以智能地、有选择地使用工具来回答用户查询。",
  "groq/compound.description": "Compound 是一个复合 AI 系统，由 GroqCloud 中已经支持的多个开放可用的模型提供支持，可以智能地、有选择地使用工具来回答用户查询。",
  "gryphe/mythomax-l2-13b.description": "MythoMax l2 13B 是一款合并了多个顶尖模型的创意与智能相结合的语言模型。",
  "hunyuan-a13b.description": "混元第一个混合推理模型，hunyuan-standard-256K 的升级版本，总参数80B，激活13B，默认是慢思考模式，支持通过参数或者指令进行快慢思考模式切换，慢快思考切换方式为 query 前加/ no_think；整体能力相对上一代全面提升，特别数学、科学、长文理解和 Agent 能力提升显著。",
  "hunyuan-code.description": "混元最新代码生成模型，经过 200B 高质量代码数据增训基座模型，迭代半年高质量 SFT 数据训练，上下文长窗口长度增大到 8K，五大语言代码生成自动评测指标上位居前列；五大语言10项考量各方面综合代码任务人工高质量评测上，性能处于第一梯队",
  "hunyuan-functioncall.description": "混元最新 MOE 架构 FunctionCall 模型，经过高质量的 FunctionCall 数据训练，上下文窗口达 32K，在多个维度的评测指标上处于领先。",
  "hunyuan-large-longcontext.description": "擅长处理长文任务如文档摘要和文档问答等，同时也具备处理通用文本生成任务的能力。在长文本的分析和生成上表现优异，能有效应对复杂和详尽的长文内容处理需求。",
  "hunyuan-large-vision.description": "此模型适用于图文理解场景，是基于混元Large训练的视觉语言大模型，支持任意分辨率多张图片+文本输入，生成文本内容，聚焦图文理解相关任务，在多语言图文理解能力上有显著提升。",
  "hunyuan-large.description": "Hunyuan-large 模型总参数量约 389B，激活参数量约 52B，是当前业界参数规模最大、效果最好的 Transformer 架构的开源 MoE 模型。",
  "hunyuan-lite-vision.description": "混元最新7B多模态模型，上下文窗口32K，支持中英文场景的多模态对话、图像物体识别、文档表格理解、多模态数学等，在多个维度上评测指标优于7B竞品模型。",
  "hunyuan-lite.description": "升级为 MOE 结构，上下文窗口为 256k ，在 NLP，代码，数学，行业等多项评测集上领先众多开源模型。",
  "hunyuan-pro.description": "万亿级参数规模 MOE-32K 长文模型。在各种 benchmark 上达到绝对领先的水平，复杂指令和推理，具备复杂数学能力，支持 functioncall，在多语言翻译、金融法律医疗等领域应用重点优化。",
  "hunyuan-role.description": "混元最新版角色扮演模型，混元官方精调训练推出的角色扮演模型，基于混元模型结合角色扮演场景数据集进行增训，在角色扮演场景具有更好的基础效果。",
  "hunyuan-standard-256K.description": "采用更优的路由策略，同时缓解了负载均衡和专家趋同的问题。长文方面，大海捞针指标达到99.9%。MOE-256K 在长度和效果上进一步突破，极大的扩展了可输入长度。",
  "hunyuan-standard-vision.description": "混元最新多模态模型，支持多语种作答，中英文能力均衡。",
  "hunyuan-standard.description": "采用更优的路由策略，同时缓解了负载均衡和专家趋同的问题。长文方面，大海捞针指标达到99.9%。MOE-32K 性价比相对更高，在平衡效果、价格的同时，可对实现对长文本输入的处理。",
  "hunyuan-t1-20250321.description": "全面搭建模型文理科能力，长文本信息捕捉能力强。支持推理解答各种难度的数学/逻辑推理/科学/代码等科学问题。",
  "hunyuan-t1-20250403.description": "提升项目级别代码生成能力；提升文本生成写作质量；提升文本理解 topic 的多轮、tob 指令遵循和字词理解能力；优化繁简混杂和中英混杂输出问题。",
  "hunyuan-t1-20250529.description": "优化文本创作、作文写作，优化代码前端、数学、逻辑推理等理科能力，提升指令遵循能力。",
  "hunyuan-t1-20250711.description": "大幅提升高难度数学、逻辑和代码能力，优化模型输出稳定性，提升模型长文能力。",
  "hunyuan-t1-latest.description": "大幅提升主模型慢思考模型的高难数学、复杂推理、高难代码、指令遵循、文本创作质量等能力。",
  "hunyuan-t1-vision-20250619.description": "混元最新版t1-vision多模态理解深度思考模型，支持多模态原生长思维链，相比上一代默认版本模型全面提升。",
  "hunyuan-t1-vision-20250916.description": "混元最新版 t1-vision 视觉深度思考模型，相比上一版模型在通用图文问答、视觉定位、OCR、图表、拍题解题、看图创作等任务上全面提升，显著优化了英文和小语种能力。",
  "hunyuan-turbo-20241223.description": "本版本优化：数据指令scaling，大幅提升模型通用泛化能力；大幅提升数学、代码、逻辑推理能力；优化文本理解字词理解相关能力；优化文本创作内容生成质量",
  "hunyuan-turbo-latest.description": "通用体验优化，包括NLP理解、文本创作、闲聊、知识问答、翻译、领域等；提升拟人性，优化模型情商；提升意图模糊时模型主动澄清能力；提升字词解析类问题的处理能力；提升创作的质量和可互动性；提升多轮体验。",
  "hunyuan-turbo-vision.description": "混元新一代视觉语言旗舰大模型，采用全新的混合专家模型（MoE）结构，在图文理解相关的基础识别、内容创作、知识问答、分析推理等能力上相比前一代模型全面提升。",
  "hunyuan-turbo.description": "混元全新一代大语言模型的预览版，采用全新的混合专家模型（MoE）结构，相比hunyuan-pro推理效率更快，效果表现更强。",
  "hunyuan-turbos-20250313.description": "统一数学解题步骤的风格，加强数学多轮问答。文本创作优化回答风格，去除AI味，增加文采。",
  "hunyuan-turbos-20250416.description": "预训练底座升级，增强底座的指令理解及遵循能力；对齐阶段增强数学、代码、逻辑、科学等理科能力；提升文创写作质量、文本理解、翻译准确率、知识问答等文科能力；增强各领域 Agent 能力，重点加强多轮对话理解能力等。",
  "hunyuan-turbos-20250604.description": "预训练底座升级，写作、阅读理解能力提升，较大幅度提升代码和理科能力，复杂指令遵循等持续提升。",
  "hunyuan-turbos-20250926.description": "预训练底座数据质量升级。优化 posttrain 阶段训练策略，持续提升 Agent、英语小语种、指令遵循、代码和理科能力。",
  "hunyuan-turbos-latest.description": "hunyuan-TurboS 混元旗舰大模型最新版本，具备更强的思考能力，更优的体验效果。",
  "hunyuan-turbos-longtext-128k-20250325.description": "擅长处理长文任务如文档摘要和文档问答等，同时也具备处理通用文本生成任务的能力。在长文本的分析和生成上表现优异，能有效应对复杂和详尽的长文内容处理需求。",
  "hunyuan-turbos-role-plus.description": "混元最新版角色扮演模型，混元官方精调训练推出的角色扮演模型，基于混元模型结合角色扮演场景数据集进行增训，在角色扮演场景具有更好的基础效果。",
  "hunyuan-turbos-vision-20250619.description": "混元最新版turbos-vision视觉语言旗舰大模型，在图文理解相关的任务上，包括基于图片的实体识别、知识问答、文案创作、拍照解题等上面相比上一代默认版本模型全面提升。",
  "hunyuan-turbos-vision.description": "此模型适用于图文理解场景，是基于混元最新 turbos 的新一代视觉语言旗舰大模型，聚焦图文理解相关任务，包括基于图片的实体识别、知识问答、文案创作、拍照解题等方面，相比前一代模型全面提升。",
  "hunyuan-vision.description": "混元最新多模态模型，支持图片+文本输入生成文本内容。",
  "image-01-live.description": "图像生成模型，画面表现细腻，支持文生图并进行画风设置",
  "image-01.description": "全新图像生成模型，画面表现细腻，支持文生图、图生图",
  "imagen-4.0-fast-generate-001.description": "Imagen 4th generation text-to-image model series Fast version",
  "imagen-4.0-generate-001.description": "Imagen 4th generation text-to-image model series",
  "imagen-4.0-generate-preview-06-06.description": "Imagen 第四代文生图模型系列",
  "imagen-4.0-ultra-generate-001.description": "Imagen 4th generation text-to-image model series Ultra version",
  "imagen-4.0-ultra-generate-preview-06-06.description": "Imagen 第四代文生图模型系列的 Ultra 版本",
  "inception/mercury-coder-small.description": "Mercury Coder Small 是代码生成、调试和重构任务的理想选择，具有最小延迟。",
  "inclusionAI/Ling-flash-2.0.description": "Ling-flash-2.0 是由蚂蚁集团百灵团队发布的 Ling 2.0 架构系列的第三款模型。它是一款混合专家（MoE）模型，总参数规模达到 1000 亿，但每个 token 仅激活 61 亿参数（非词向量激活 48 亿）。 作为一个轻量级配置的模型，Ling-flash-2.0 在多个权威评测中展现出媲美甚至超越 400 亿级别稠密（Dense）模型及更大规模 MoE 模型的性能。该模型旨在通过极致的架构设计与训练策略，在“大模型等于大参数”的共识下探索高效能的路径。",
  "inclusionAI/Ling-mini-2.0.description": "Ling-mini-2.0 是一款基于 MoE 架构的小尺寸高性能大语言模型。它拥有 16B 总参数，但每个 token 仅激活 1.4B（non-embedding 789M），从而实现了极高的生成速度。得益于高效的 MoE 设计与大规模高质量训练数据，尽管激活参数仅为 1.4B，Ling-mini-2.0 依然在下游任务中展现出可媲美 10B 以下 dense LLM 及更大规模 MoE 模型的顶尖性能。",
  "inclusionAI/Ring-flash-2.0.description": "Ring-flash-2.0 是一个基于 Ling-flash-2.0-base 深度优化的高性能思考模型。它采用混合专家（MoE）架构，总参数量为 100B，但在每次推理中仅激活 6.1B 参数。该模型通过独创的 icepop 算法，解决了 MoE 大模型在强化学习（RL）训练中的不稳定性难题，使其复杂推理能力在长周期训练中得以持续提升。Ring-flash-2.0 在数学竞赛、代码生成和逻辑推理等多个高难度基准测试中取得了显著突破，其性能不仅超越了 40B 参数规模以下的顶尖稠密模型，还能媲美更大规模的开源 MoE 模型及闭源的高性能思考模型。尽管该模型专注于复杂推理，它在创意写作等任务上也表现出色。此外，得益于其高效的架构设计，Ring-flash-2.0 在提供强大性能的同时，也实现了高速推理，显著降低了思考模型在高并发场景下的部署成本。",
  "inclusionai/ling-1t.description": "Ling-1T 是 inclusionAI 的 1T MoE 大模型，针对高强度推理任务与大规模上下文进行了优化。",
  "inclusionai/ling-flash-2.0.description": "Ling-flash-2.0 是 inclusionAI 的 MoE 模型，优化了效率与推理表现，适合中大型任务。",
  "inclusionai/ling-mini-2.0.description": "Ling-mini-2.0 是 inclusionAI 的轻量化 MoE 模型，在保持推理能力的同时显著降低成本。",
  "inclusionai/ming-flash-omini-preview.description": "Ming-flash-omni Preview 是 inclusionAI 的多模态模型，支持语音、图像和视频输入，优化了图像渲染与语音识别能力。",
  "inclusionai/ring-1t.description": "Ring-1T 是 inclusionAI 的 trillion-parameter MoE 思考模型，适合大规模推理与研究类任务。",
  "inclusionai/ring-flash-2.0.description": "Ring-flash-2.0 是 inclusionAI 面向高吞吐场景的 Ring 模型变体，强调速度与成本效率。",
  "inclusionai/ring-mini-2.0.description": "Ring-mini-2.0 是 inclusionAI 的高吞吐轻量化 MoE 版本，主要用于并发场景。",
  "internlm/internlm2_5-7b-chat.description": "InternLM2.5-7B-Chat 是一个开源的对话模型，基于 InternLM2 架构开发。该 7B 参数规模的模型专注于对话生成任务，支持中英双语交互。模型采用了最新的训练技术，旨在提供流畅、智能的对话体验。InternLM2.5-7B-Chat 适用于各种对话应用场景，包括但不限于智能客服、个人助手等领域",
  "internlm2.5-latest.description": "我们仍在维护的老版本模型，经过多轮迭代有着极其优异且稳定的性能，包含 7B、20B 多种模型参数量可选，支持 1M 的上下文长度以及更强的指令跟随和工具调用能力。默认指向我们最新发布的 InternLM2.5 系列模型，当前指向 internlm2.5-20b-chat。",
  "internlm3-latest.description": "我们最新的模型系列，有着卓越的推理性能，领跑同量级开源模型。默认指向我们最新发布的 InternLM3 系列模型，当前指向 internlm3-8b-instruct。",
  "internvl2.5-38b-mpo.description": "InternVL2.5 38B MPO，多模态预训练模型，支持复杂图文推理任务。",
  "internvl2.5-latest.description": "我们仍在维护的 InternVL2.5 版本，具备优异且稳定的性能。默认指向我们最新发布的 InternVL2.5 系列模型，当前指向 internvl2.5-78b。",
  "internvl3-14b.description": "InternVL3 14B，中等规模多模态模型，在性能与成本间取得平衡。",
  "internvl3-1b.description": "InternVL3 1B，轻量多模态模型，适合资源受限环境部署。",
  "internvl3-38b.description": "InternVL3 38B，大规模多模态开源模型，适用于高精度图文理解任务。",
  "internvl3-latest.description": "我们最新发布多模态大模型，具备更强的图文理解能力、长时序图片理解能力，性能比肩顶尖闭源模型。默认指向我们最新发布的 InternVL 系列模型，当前指向 internvl3-78b。",
  "irag-1.0.description": "ERNIE iRAG，图像检索增强生成模型，支持以图搜图、图文检索与内容生成。",
  "jamba-large.description": "我们最强大、最先进的模型，专为处理企业级复杂任务而设计，具备卓越的性能。",
  "jamba-mini.description": "在同级别中最高效的模型，兼顾速度与质量，具备更小的体积。",
  "jina-deepsearch-v1.description": "深度搜索结合了网络搜索、阅读和推理，可进行全面调查。您可以将其视为一个代理，接受您的研究任务 - 它会进行广泛搜索并经过多次迭代，然后才能给出答案。这个过程涉及持续的研究、推理和从各个角度解决问题。这与直接从预训练数据生成答案的标准大模型以及依赖一次性表面搜索的传统 RAG 系统有着根本的不同。",
  "kimi-k2-0711-preview.description": "kimi-k2 是一款具备超强代码和 Agent 能力的 MoE 架构基础模型，总参数 1T，激活参数 32B。在通用知识推理、编程、数学、Agent 等主要类别的基准性能测试中，K2 模型的性能超过其他主流开源模型。",
  "kimi-k2-0905-preview.description": "kimi-k2-0905-preview 模型上下文长度为 256k，具备更强的 Agentic Coding 能力、更突出的前端代码的美观度和实用性、以及更好的上下文理解能力。",
  "kimi-k2-instruct.description": "Kimi K2 Instruct，Kimi 官方推理模型，支持长上下文与代码、问答等多场景。",
  "kimi-k2-thinking-turbo.description": "K2 长思考模型的高速版本，支持 256k 上下文，擅长深度推理，输出速度提升至每秒 60-100 tokens 。",
  "kimi-k2-thinking.description": "kimi-k2-thinking模型是月之暗面提供的具有通用 Agentic能力和推理能力的思考模型，它擅长深度推理，并可通过多步工具调用，帮助解决各类难题。",
  "kimi-k2-turbo-preview.description": "kimi-k2 是一款具备超强代码和 Agent 能力的 MoE 架构基础模型，总参数 1T，激活参数 32B。在通用知识推理、编程、数学、Agent 等主要类别的基准性能测试中，K2 模型的性能超过其他主流开源模型。",
  "kimi-k2.description": "Kimi-K2 是一款Moonshot AI推出的具备超强代码和 Agent 能力的 MoE 架构基础模型，总参数 1T，激活参数 32B。在通用知识推理、编程、数学、Agent 等主要类别的基准性能测试中，K2 模型的性能超过其他主流开源模型。",
  "kimi-k2:1t.description": "Kimi K2 是由月之暗面 AI 开发的大规模混合专家 (MoE) 语言模型，具有 1 万亿总参数和每次前向传递 320 亿激活参数。它针对代理能力进行了优化，包括高级工具使用、推理和代码合成。",
  "kimi-latest.description": "Kimi 智能助手产品使用最新的 Kimi 大模型，可能包含尚未稳定的特性。支持图片理解，同时会自动根据请求的上下文长度选择 8k/32k/128k 模型作为计费模型",
  "kuaishou/kat-coder-pro-v1.description": "KAT-Coder-Pro-V1（限时免费）专注于代码理解与自动化编程，用于高效的编程代理任务。",
  "learnlm-1.5-pro-experimental.description": "LearnLM 是一个实验性的、特定于任务的语言模型，经过训练以符合学习科学原则，可在教学和学习场景中遵循系统指令，充当专家导师等。",
  "learnlm-2.0-flash-experimental.description": "LearnLM 是一个实验性的、特定于任务的语言模型，经过训练以符合学习科学原则，可在教学和学习场景中遵循系统指令，充当专家导师等。",
  "lite.description": "Spark Lite 是一款轻量级大语言模型，具备极低的延迟与高效的处理能力，完全免费开放，支持实时在线搜索功能。其快速响应的特性使其在低算力设备上的推理应用和模型微调中表现出色，为用户带来出色的成本效益和智能体验，尤其在知识问答、内容生成及搜索场景下表现不俗。",
  "llama-3.1-70b-versatile.description": "Llama 3.1 70B 提供更强大的AI推理能力，适合复杂应用，支持超多的计算处理并保证高效和准确率。",
  "llama-3.1-8b-instant.description": "Llama 3.1 8B 是一款高效能模型，提供了快速的文本生成能力，非常适合需要大规模效率和成本效益的应用场景。",
  "llama-3.1-instruct.description": "Llama 3.1 指令微调模型针对对话场景进行了优化，在常见的行业基准测试中，超越了许多现有的开源聊天模型。",
  "llama-3.2-11b-vision-instruct.description": "在高分辨率图像上表现出色的图像推理能力，适用于视觉理解应用。",
  "llama-3.2-11b-vision-preview.description": "Llama 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "llama-3.2-90b-vision-instruct.description": "适用于视觉理解代理应用的高级图像推理能力。",
  "llama-3.2-90b-vision-preview.description": "Llama 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "llama-3.2-vision-instruct.description": "Llama 3.2-Vision 指令微调模型针对视觉识别、图像推理、图像描述和回答与图像相关的常规问题进行了优化。",
  "llama-3.3-70b-versatile.description": "Meta Llama 3.3 多语言大语言模型 ( LLM ) 是 70B（文本输入/文本输出）中的预训练和指令调整生成模型。 Llama 3.3 指令调整的纯文本模型针对多语言对话用例进行了优化，并且在常见行业基准上优于许多可用的开源和封闭式聊天模型。",
  "llama-3.3-70b.description": "Llama 3.3 70B：中大型 Llama 模型，兼顾推理能力与吞吐。",
  "llama-3.3-instruct.description": "Llama 3.3 指令微调模型针对对话场景进行了优化，在常见的行业基准测试中，超越了许多现有的开源聊天模型。",
  "llama3-70b-8192.description": "Meta Llama 3 70B 提供无与伦比的复杂性处理能力，为高要求项目量身定制。",
  "llama3-8b-8192.description": "Meta Llama 3 8B 带来优质的推理效能，适合多场景应用需求。",
  "llama3-groq-70b-8192-tool-use-preview.description": "Llama 3 Groq 70B Tool Use 提供强大的工具调用能力，支持复杂任务的高效处理。",
  "llama3-groq-8b-8192-tool-use-preview.description": "Llama 3 Groq 8B Tool Use 是针对高效工具使用优化的模型，支持快速并行计算。",
  "llama3.1-8b.description": "Llama 3.1 8B：小体量、低延迟的 Llama 变体，适合轻量在线推理与交互场景。",
  "llama3.1.description": "Llama 3.1 是 Meta 推出的领先模型，支持高达 405B 参数，可应用于复杂对话、多语言翻译和数据分析领域。",
  "llama3.1:405b.description": "Llama 3.1 是 Meta 推出的领先模型，支持高达 405B 参数，可应用于复杂对话、多语言翻译和数据分析领域。",
  "llama3.1:70b.description": "Llama 3.1 是 Meta 推出的领先模型，支持高达 405B 参数，可应用于复杂对话、多语言翻译和数据分析领域。",
  "llava-v1.5-7b-4096-preview.description": "LLaVA 1.5 7B 提供视觉处理能力融合，通过视觉信息输入生成复杂输出。",
  "llava.description": "LLaVA 是结合视觉编码器和 Vicuna 的多模态模型，用于强大的视觉和语言理解。",
  "llava:13b.description": "LLaVA 是结合视觉编码器和 Vicuna 的多模态模型，用于强大的视觉和语言理解。",
  "llava:34b.description": "LLaVA 是结合视觉编码器和 Vicuna 的多模态模型，用于强大的视觉和语言理解。",
  "magistral-medium-latest.description": "Magistral Medium 1.2 是Mistral AI于2025年9月发布的前沿级推理模型，具有视觉支持。",
  "magistral-small-2509.description": "Magistral Small 1.2 是Mistral AI于2025年9月发布的开源小型推理模型，具有视觉支持。",
  "mathstral.description": "MathΣtral 专为科学研究和数学推理设计，提供有效的计算能力和结果解释。",
  "max-32k.description": "Spark Max 32K 配置了大上下文处理能力，更强的上下文理解和逻辑推理能力，支持32K tokens的文本输入，适用于长文档阅读、私有知识问答等场景",
  "megrez-3b-instruct.description": "Megrez 3B Instruct 是无问芯穹推出的小参数量高效模型。",
  "meituan/longcat-flash-chat.description": "美团开源的专为对话交互和智能体任务优化的非思维型基础模型，在工具调用和复杂多轮交互场景中表现突出",
  "meta-llama-3-70b-instruct.description": "一个强大的700亿参数模型，在推理、编码和广泛的语言应用方面表现出色。",
  "meta-llama-3-8b-instruct.description": "一个多功能的80亿参数模型，针对对话和文本生成任务进行了优化。",
  "meta-llama-3.1-405b-instruct.description": "Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。",
  "meta-llama-3.1-70b-instruct.description": "Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。",
  "meta-llama-3.1-8b-instruct.description": "Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。",
  "meta-llama/Llama-2-13b-chat-hf.description": "LLaMA-2 Chat (13B) 提供优秀的语言处理能力和出色的交互体验。",
  "meta-llama/Llama-2-70b-hf.description": "LLaMA-2 提供优秀的语言处理能力和出色的交互体验。",
  "meta-llama/Llama-3-70b-chat-hf.description": "Llama 3 70B Instruct Reference 是功能强大的聊天模型，支持复杂的对话需求。",
  "meta-llama/Llama-3-8b-chat-hf.description": "Llama 3 8B Instruct Reference 提供多语言支持，涵盖丰富的领域知识。",
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo.description": "LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "meta-llama/Llama-3.2-3B-Instruct-Turbo.description": "LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo.description": "LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "meta-llama/Llama-3.3-70B-Instruct-Turbo.description": "Meta Llama 3.3 多语言大语言模型 ( LLM ) 是 70B（文本输入/文本输出）中的预训练和指令调整生成模型。 Llama 3.3 指令调整的纯文本模型针对多语言对话用例进行了优化，并且在常见行业基准上优于许多可用的开源和封闭式聊天模型。",
  "meta-llama/Llama-Vision-Free.description": "LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite.description": "Llama 3 70B Instruct Lite 适合需要高效能和低延迟的环境。",
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo.description": "Llama 3 70B Instruct Turbo 提供卓越的语言理解和生成能力，适合最苛刻的计算任务。",
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite.description": "Llama 3 8B Instruct Lite 适合资源受限的环境，提供出色的平衡性能。",
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo.description": "Llama 3 8B Instruct Turbo 是一款高效能的大语言模型，支持广泛的应用场景。",
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo.description": "405B 的 Llama 3.1 Turbo 模型，为大数据处理提供超大容量的上下文支持，在超大规模的人工智能应用中表现突出。",
  "meta-llama/Meta-Llama-3.1-405B-Instruct.description": "Llama 3.1 是 Meta 推出的领先模型，支持高达 405B 参数，可应用于复杂对话、多语言翻译和数据分析领域。",
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo.description": "Llama 3.1 70B 模型经过精细调整，适用于高负载应用，量化至FP8提供更高效的计算能力和准确性，确保在复杂场景中的卓越表现。",
  "meta-llama/Meta-Llama-3.1-70B.description": "Llama 3.1 是 Meta 推出的领先模型，支持高达 405B 参数，可应用于复杂对话、多语言翻译和数据分析领域。",
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo.description": "Llama 3.1 8B 模型采用FP8量化，支持高达131,072个上下文标记，是开源模型中的佼佼者，适合复杂任务，表现优异于许多行业基准。",
  "meta-llama/llama-3-70b-instruct.description": "Llama 3 70B Instruct 优化用于高质量对话场景，在各类人类评估中表现优异。",
  "meta-llama/llama-3-8b-instruct.description": "Llama 3 8B Instruct 优化了高质量对话场景，性能优于许多闭源模型。",
  "meta-llama/llama-3.1-70b-instruct.description": "Meta最新一代的Llama 3.1模型系列，70B（700亿参数）的指令微调版本针对高质量对话场景进行了优化。在业界评估中，与领先的闭源模型相比，它展现出了强劲的性能。(仅针对企业实名认证通过主体开放）",
  "meta-llama/llama-3.1-8b-instruct.description": "Meta最新一代的Llama 3.1模型系列，8B（80亿参数）的指令微调版本特别快速高效。在业界评估中，表现出强劲的性能，超越了很多领先的闭源模型。(仅针对企业实名认证通过主体开放）",
  "meta-llama/llama-3.1-8b-instruct:free.description": "LLaMA 3.1 提供多语言支持，是业界领先的生成模型之一。",
  "meta-llama/llama-3.2-11b-vision-instruct.description": "LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "meta-llama/llama-3.2-3b-instruct.description": "meta-llama/llama-3.2-3b-instruct",
  "meta-llama/llama-3.2-90b-vision-instruct.description": "LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。",
  "meta-llama/llama-3.3-70b-instruct.description": "Llama 3.3 是 Llama 系列最先进的多语言开源大型语言模型，以极低成本体验媲美 405B 模型的性能。基于 Transformer 结构，并通过监督微调（SFT）和人类反馈强化学习（RLHF）提升有用性和安全性。其指令调优版本专为多语言对话优化，在多项行业基准上表现优于众多开源和封闭聊天模型。知识截止日期为 2023 年 12 月",
  "meta-llama/llama-3.3-70b-instruct:free.description": "Llama 3.3 是 Llama 系列最先进的多语言开源大型语言模型，以极低成本体验媲美 405B 模型的性能。基于 Transformer 结构，并通过监督微调（SFT）和人类反馈强化学习（RLHF）提升有用性和安全性。其指令调优版本专为多语言对话优化，在多项行业基准上表现优于众多开源和封闭聊天模型。知识截止日期为 2023 年 12 月",
  "meta.llama3-1-405b-instruct-v1:0.description": "Meta Llama 3.1 405B Instruct 是 Llama 3.1 Instruct 模型中最大、最强大的模型，是一款高度先进的对话推理和合成数据生成模型，也可以用作在特定领域进行专业持续预训练或微调的基础。Llama 3.1 提供的多语言大型语言模型 (LLMs) 是一组预训练的、指令调整的生成模型，包括 8B、70B 和 405B 大小 (文本输入/输出)。Llama 3.1 指令调整的文本模型 (8B、70B、405B) 专为多语言对话用例进行了优化，并在常见的行业基准测试中超过了许多可用的开源聊天模型。Llama 3.1 旨在用于多种语言的商业和研究用途。指令调整的文本模型适用于类似助手的聊天，而预训练模型可以适应各种自然语言生成任务。Llama 3.1 模型还支持利用其模型的输出来改进其他模型，包括合成数据生成和精炼。Llama 3.1 是使用优化的变压器架构的自回归语言模型。调整版本使用监督微调 (SFT) 和带有人类反馈的强化学习 (RLHF) 来符合人类对帮助性和安全性的偏好。",
  "meta.llama3-1-70b-instruct-v1:0.description": "Meta Llama 3.1 70B Instruct 的更新版，包括扩展的 128K 上下文长度、多语言性和改进的推理能力。Llama 3.1 提供的多语言大型语言模型 (LLMs) 是一组预训练的、指令调整的生成模型，包括 8B、70B 和 405B 大小 (文本输入/输出)。Llama 3.1 指令调整的文本模型 (8B、70B、405B) 专为多语言对话用例进行了优化，并在常见的行业基准测试中超过了许多可用的开源聊天模型。Llama 3.1 旨在用于多种语言的商业和研究用途。指令调整的文本模型适用于类似助手的聊天，而预训练模型可以适应各种自然语言生成任务。Llama 3.1 模型还支持利用其模型的输出来改进其他模型，包括合成数据生成和精炼。Llama 3.1 是使用优化的变压器架构的自回归语言模型。调整版本使用监督微调 (SFT) 和带有人类反馈的强化学习 (RLHF) 来符合人类对帮助性和安全性的偏好。",
  "meta.llama3-1-8b-instruct-v1:0.description": "Meta Llama 3.1 8B Instruct 的更新版，包括扩展的 128K 上下文长度、多语言性和改进的推理能力。Llama 3.1 提供的多语言大型语言模型 (LLMs) 是一组预训练的、指令调整的生成模型，包括 8B、70B 和 405B 大小 (文本输入/输出)。Llama 3.1 指令调整的文本模型 (8B、70B、405B) 专为多语言对话用例进行了优化，并在常见的行业基准测试中超过了许多可用的开源聊天模型。Llama 3.1 旨在用于多种语言的商业和研究用途。指令调整的文本模型适用于类似助手的聊天，而预训练模型可以适应各种自然语言生成任务。Llama 3.1 模型还支持利用其模型的输出来改进其他模型，包括合成数据生成和精炼。Llama 3.1 是使用优化的变压器架构的自回归语言模型。调整版本使用监督微调 (SFT) 和带有人类反馈的强化学习 (RLHF) 来符合人类对帮助性和安全性的偏好。",
  "meta.llama3-70b-instruct-v1:0.description": "Meta Llama 3 是一款面向开发者、研究人员和企业的开放大型语言模型 (LLM)，旨在帮助他们构建、实验并负责任地扩展他们的生成 AI 想法。作为全球社区创新的基础系统的一部分，它非常适合内容创建、对话 AI、语言理解、研发和企业应用。",
  "meta.llama3-8b-instruct-v1:0.description": "Meta Llama 3 是一款面向开发者、研究人员和企业的开放大型语言模型 (LLM)，旨在帮助他们构建、实验并负责任地扩展他们的生成 AI 想法。作为全球社区创新的基础系统的一部分，它非常适合计算能力和资源有限、边缘设备和更快的训练时间。",
  "meta/Llama-3.2-11B-Vision-Instruct.description": "在高分辨率图像上表现出色的图像推理能力，适用于视觉理解应用。",
  "meta/Llama-3.2-90B-Vision-Instruct.description": "适用于视觉理解代理应用的高级图像推理能力。",
  "meta/Llama-3.3-70B-Instruct.description": "Llama 3.3 是 Llama 系列最先进的多语言开源大型语言模型，以极低成本体验媲美 405B 模型的性能。基于 Transformer 结构，并通过监督微调（SFT）和人类反馈强化学习（RLHF）提升有用性和安全性。其指令调优版本专为多语言对话优化，在多项行业基准上表现优于众多开源和封闭聊天模型。知识截止日期为 2023 年 12 月",
  "meta/Meta-Llama-3-70B-Instruct.description": "一个强大的700亿参数模型，在推理、编码和广泛的语言应用方面表现出色。",
  "meta/Meta-Llama-3-8B-Instruct.description": "一个多功能的80亿参数模型，针对对话和文本生成任务进行了优化。",
  "meta/Meta-Llama-3.1-405B-Instruct.description": "Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。",
  "meta/Meta-Llama-3.1-70B-Instruct.description": "Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。",
  "meta/Meta-Llama-3.1-8B-Instruct.description": "Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。",
  "meta/llama-3-70b.description": "由 Meta 精心调整用于指令遵循目的的 700 亿参数开源模型。由 Groq 使用其自定义语言处理单元 (LPU) 硬件提供服务，以提供快速高效的推理。",
  "meta/llama-3-8b.description": "由 Meta 精心调整用于指令遵循目的的 80 亿参数开源模型。由 Groq 使用其自定义语言处理单元 (LPU) 硬件提供服务，以提供快速高效的推理。",
  "meta/llama-3.1-405b-instruct.description": "高级 LLM，支持合成数据生成、知识蒸馏和推理，适用于聊天机器人、编程和特定领域任务。",
  "meta/llama-3.1-70b-instruct.description": "赋能复杂对话，具备卓越的上下文理解、推理能力和文本生成能力。",
  "meta/llama-3.1-70b.description": "Meta Llama 3 70B Instruct 的更新版本，包括扩展的 128K 上下文长度、多语言和改进的推理能力。",
  "meta/llama-3.1-8b-instruct.description": "先进的最尖端模型，具备语言理解、卓越的推理能力和文本生成能力。",
  "meta/llama-3.1-8b.description": "Llama 3.1 8B 支持 128K 上下文窗口，使其成为实时对话界面和数据分析的理想选择，同时与更大的模型相比提供显著的成本节约。由 Groq 使用其自定义语言处理单元 (LPU) 硬件提供服务，以提供快速高效的推理。",
  "meta/llama-3.2-11b-vision-instruct.description": "尖端的视觉-语言模型，擅长从图像中进行高质量推理。",
  "meta/llama-3.2-11b.description": "指令调整的图像推理生成模型（文本 + 图像输入 / 文本输出），针对视觉识别、图像推理、标题生成和回答关于图像的一般问题进行了优化。",
  "meta/llama-3.2-1b-instruct.description": "先进的最尖端小型语言模型，具备语言理解、卓越的推理能力和文本生成能力。",
  "meta/llama-3.2-1b.description": "仅文本模型，支持设备上用例，如多语言本地知识检索、摘要和重写。",
  "meta/llama-3.2-3b-instruct.description": "先进的最尖端小型语言模型，具备语言理解、卓越的推理能力和文本生成能力。",
  "meta/llama-3.2-3b.description": "仅文本模型，精心调整用于支持设备上用例，如多语言本地知识检索、摘要和重写。",
  "meta/llama-3.2-90b-vision-instruct.description": "尖端的视觉-语言模型，擅长从图像中进行高质量推理。",
  "meta/llama-3.2-90b.description": "指令调整的图像推理生成模型（文本 + 图像输入 / 文本输出），针对视觉识别、图像推理、标题生成和回答关于图像的一般问题进行了优化。",
  "meta/llama-3.3-70b-instruct.description": "先进的 LLM，擅长推理、数学、常识和函数调用。",
  "meta/llama-3.3-70b.description": "性能与效率的完美结合。该模型支持高性能对话 AI，专为内容创建、企业应用和研究而设计，提供先进的语言理解能力，包括文本摘要、分类、情感分析和代码生成。",
  "meta/llama-4-maverick.description": "Llama 4 模型集合是原生多模态 AI 模型，支持文本和多模态体验。这些模型利用混合专家架构在文本和图像理解方面提供行业领先的性能。Llama 4 Maverick，一个 170 亿参数模型，具有 128 个专家。由 DeepInfra 提供服务。",
  "meta/llama-4-scout.description": "Llama 4 模型集合是原生多模态 AI 模型，支持文本和多模态体验。这些模型利用混合专家架构在文本和图像理解方面提供行业领先的性能。Llama 4 Scout，一个 170 亿参数模型，具有 16 个专家。由 DeepInfra 提供服务。",
  "microsoft/Phi-3-medium-128k-instruct.description": "相同的Phi-3-medium模型，但具有更大的上下文大小，适用于RAG或少量提示。",
  "microsoft/Phi-3-medium-4k-instruct.description": "一个140亿参数模型，质量优于Phi-3-mini，重点关注高质量、推理密集型数据。",
  "microsoft/Phi-3-mini-128k-instruct.description": "相同的Phi-3-mini模型，但具有更大的上下文大小，适用于RAG或少量提示。",
  "microsoft/Phi-3-mini-4k-instruct.description": "Phi-3家族中最小的成员，针对质量和低延迟进行了优化。",
  "microsoft/Phi-3-small-128k-instruct.description": "相同的Phi-3-small模型，但具有更大的上下文大小，适用于RAG或少量提示。",
  "microsoft/Phi-3-small-8k-instruct.description": "一个70亿参数模型，质量优于Phi-3-mini，重点关注高质量、推理密集型数据。",
  "microsoft/Phi-3.5-mini-instruct.description": "Phi-3-mini模型的更新版。",
  "microsoft/Phi-3.5-vision-instruct.description": "Phi-3-vision模型的更新版。",
  "microsoft/WizardLM-2-8x22B.description": "WizardLM 2 是微软AI提供的语言模型，在复杂对话、多语言、推理和智能助手领域表现尤为出色。",
  "microsoft/wizardlm-2-8x22b.description": "WizardLM-2 8x22B 是微软AI最先进的Wizard模型，显示出极其竞争力的表现。",
  "minicpm-v.description": "MiniCPM-V 是 OpenBMB 推出的新一代多模态大模型，具备卓越的 OCR 识别和多模态理解能力，支持广泛的应用场景。",
  "minimax-m2.description": "MiniMax M2 是专为编码和代理工作流程构建的高效大型语言模型。",
  "minimax/minimax-m2.description": "MiniMax-M2 是一款在编码与代理任务上表现出色的高性价比模型，适合多种工程场景。",
  "minimaxai/minimax-m2.description": "MiniMax-M2 是一款紧凑、快速且经济高效的混合专家（MoE）模型，拥有 2300 亿总参数和 100 亿激活参数，专为编码和智能体任务的顶级性能而打造，同时保持强大的通用智能。该模型在多文件编辑、编码-运行-修复闭环、测试校验修复以及复杂的长链接工具链方面表现优异，是开发者工作流的理想选择。",
  "ministral-3b-latest.description": "Ministral 3B 是Mistral的世界顶级边缘模型。",
  "ministral-8b-latest.description": "Ministral 8B 是Mistral的性价比极高的边缘模型。",
  "mistral-ai/Mistral-Large-2411.description": "Mistral的旗舰模型，适合需要大规模推理能力或高度专业化的复杂任务（合成文本生成、代码生成、RAG或代理）。",
  "mistral-ai/Mistral-Nemo.description": "Mistral Nemo是一种尖端的语言模型（LLM），在其尺寸类别中拥有最先进的推理、世界知识和编码能力。",
  "mistral-ai/mistral-small-2503.description": "Mistral Small可用于任何需要高效率和低延迟的基于语言的任务。",
  "mistral-large-instruct.description": "Mistral-Large-Instruct-2407 是一款先进的稠密大型语言模型（LLM），拥有 1230 亿参数，具备最先进的推理、知识和编码能力。",
  "mistral-large-latest.description": "Mistral Large是旗舰大模型，擅长多语言任务、复杂推理和代码生成，是高端应用的理想选择。",
  "mistral-large.description": "Mixtral Large 是 Mistral 的旗舰模型，结合代码生成、数学和推理的能力，支持 128k 上下文窗口。",
  "mistral-medium-latest.description": "Mistral Medium 3 以 8 倍的成本提供最先进的性能，并从根本上简化了企业部署。",
  "mistral-nemo-instruct.description": "Mistral-Nemo-Instruct-2407 大型语言模型（LLM）是 Mistral-Nemo-Base-2407 的指令微调版本。",
  "mistral-nemo.description": "Mistral Nemo 由 Mistral AI 和 NVIDIA 合作推出，是高效性能的 12B 模型。",
  "mistral-small-latest.description": "Mistral Small是成本效益高、快速且可靠的选项，适用于翻译、摘要和情感分析等用例。",
  "mistral-small.description": "Mistral Small可用于任何需要高效率和低延迟的基于语言的任务。",
  "mistral.description": "Mistral 是 Mistral AI 发布的 7B 模型，适合多变的语言处理需求。",
  "mistral/codestral-embed.description": "可以嵌入代码数据库和存储库以支持编码助手的代码嵌入模型。",
  "mistral/codestral.description": "Mistral Codestral 25.01 是最先进的编码模型，针对低延迟、高频率用例进行了优化。精通 80 多种编程语言，它在中间填充 (FIM)、代码纠正和测试生成等任务上表现出色。",
  "mistral/devstral-small.description": "Devstral 是一个用于软件工程任务的代理大型语言模型，使其成为软件工程代理的绝佳选择。",
  "mistral/magistral-medium.description": "复杂思维，由深刻理解支持，具有您可以遵循和验证的透明推理。该模型即使在任务中途切换语言时，也能在众多语言中保持高保真推理。",
  "mistral/magistral-small.description": "复杂思维，由深刻理解支持，具有您可以遵循和验证的透明推理。该模型即使在任务中途切换语言时，也能在众多语言中保持高保真推理。",
  "mistral/ministral-3b.description": "一个紧凑、高效的模型，用于智能助手和本地分析等设备上任务，提供低延迟性能。",
  "mistral/ministral-8b.description": "一个更强大的模型，具有更快、内存高效的推理，是复杂工作流程和要求苛刻的边缘应用的理想选择。",
  "mistral/mistral-embed.description": "用于语义搜索、相似性、聚类和 RAG 工作流的通用文本嵌入模型。",
  "mistral/mistral-large.description": "Mistral Large 是复杂任务的理想选择，这些任务需要大型推理能力或高度专业化——如合成文本生成、代码生成、RAG 或代理。",
  "mistral/mistral-small.description": "Mistral Small 是简单任务的理想选择，这些任务可以批量完成——如分类、客户支持或文本生成。它以可承受的价格点提供出色的性能。",
  "mistral/mixtral-8x22b-instruct.description": "8x22b Instruct 模型。8x22b 是由 Mistral 提供服务的混合专家开源模型。",
  "mistral/pixtral-12b.description": "一个具有图像理解能力的 12B 模型，以及文本。",
  "mistral/pixtral-large.description": "Pixtral Large 是我们多模态家族中的第二个模型，展示了前沿水平的图像理解。特别是，该模型能够理解文档、图表和自然图像，同时保持了 Mistral Large 2 的领先文本理解能力。",
  "mistralai/Mistral-7B-Instruct-v0.1.description": "Mistral (7B) Instruct 以高性能著称，适用于多种语言任务。",
  "mistralai/Mistral-7B-Instruct-v0.2.description": "Mistral (7B) Instruct v0.2 提供改进的指令处理能力和更精确的结果。",
  "mistralai/Mistral-7B-Instruct-v0.3.description": "Mistral (7B) Instruct v0.3 提供高效的计算能力和自然语言理解，适合广泛的应用。",
  "mistralai/Mistral-7B-v0.1.description": "Mistral 7B是一款紧凑但高性能的模型，擅长批量处理和简单任务，如分类和文本生成，具有良好的推理能力。",
  "mistralai/Mixtral-8x22B-Instruct-v0.1.description": "Mixtral-8x22B Instruct (141B) 是一款超级大语言模型，支持极高的处理需求。",
  "mistralai/Mixtral-8x7B-Instruct-v0.1.description": "Mixtral-8x7B Instruct (46.7B) 提供高容量的计算框架，适合大规模数据处理。",
  "mistralai/Mixtral-8x7B-v0.1.description": "Mixtral 8x7B是一个稀疏专家模型，利用多个参数提高推理速度，适合处理多语言和代码生成任务。",
  "mistralai/mistral-nemo.description": "Mistral Nemo 是多语言支持和高性能编程的7.3B参数模型。",
  "mixtral-8x7b-32768.description": "Mixtral 8x7B 提供高容错的并行计算能力，适合复杂任务。",
  "mixtral.description": "Mixtral 是 Mistral AI 的专家模型，具有开源权重，并在代码生成和语言理解方面提供支持。",
  "mixtral:8x22b.description": "Mixtral 是 Mistral AI 的专家模型，具有开源权重，并在代码生成和语言理解方面提供支持。",
  "moonshot-v1-128k-vision-preview.description": "Kimi 视觉模型（包括 moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview 等）能够理解图片内容，包括图片文字、图片颜色和物体形状等内容。",
  "moonshot-v1-128k.description": "Moonshot V1 128K 是一款拥有超长上下文处理能力的模型，适用于生成超长文本，满足复杂的生成任务需求，能够处理多达128,000个tokens的内容，非常适合科研、学术和大型文档生成等应用场景。",
  "moonshot-v1-32k-vision-preview.description": "Kimi 视觉模型（包括 moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview 等）能够理解图片内容，包括图片文字、图片颜色和物体形状等内容。",
  "moonshot-v1-32k.description": "Moonshot V1 32K 提供中等长度的上下文处理能力，能够处理32,768个tokens，特别适合生成各种长文档和复杂对话，应用于内容创作、报告生成和对话系统等领域。",
  "moonshot-v1-8k-vision-preview.description": "Kimi 视觉模型（包括 moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview 等）能够理解图片内容，包括图片文字、图片颜色和物体形状等内容。",
  "moonshot-v1-8k.description": "Moonshot V1 8K 专为生成短文本任务设计，具有高效的处理性能，能够处理8,192个tokens，非常适合简短对话、速记和快速内容生成。",
  "moonshot-v1-auto.description": "Moonshot V1 Auto 可以根据当前上下文占用的 Tokens 数量来选择合适的模型",
  "moonshotai/Kimi-Dev-72B.description": "Kimi-Dev-72B 是一款开源代码大模型，经过大规模强化学习优化，能输出稳健、可直接投产的补丁。该模型在 SWE-bench Verified 上取得 60.4 % 的新高分，刷新了开源模型在缺陷修复、代码评审等自动化软件工程任务上的纪录。",
  "moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905 是 Kimi K2 最新、最强大的版本。它是一款顶尖的混合专家（MoE）语言模型，拥有 1 万亿的总参数和 320 亿的激活参数。该模型的主要特性包括：增强的智能体编码智能，在公开基准测试和真实世界的编码智能体任务中表现出显著的性能提升；改进的前端编码体验，在前端编程的美观性和实用性方面均有进步。",
  "moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinking 是最新、最强大的开源思考模型。它通过大幅扩展多步推理深度，并在 200–300 次连续工具调用中保持稳定的工具使用，在 Humanity's Last Exam (HLE)、BrowseComp 及其他基准测试中树立了新的标杆。同时，K2 Thinking 在编程、数学、逻辑推理和 Agent 场景中表现卓越。该模型基于混合专家（MoE）架构，总参数约 1T，支持 256K 上下文窗口并支持工具调用。",
  "moonshotai/kimi-k2-0711.description": "Kimi K2 0711 是 Kimi 系列的 Instruct 版本，适合高质量代码与工具调用场景。",
  "moonshotai/kimi-k2-0905.description": "Kimi K2 0905 是 Kimi 系列的 0905 更新，扩充了上下文与推理性能，优化了编码场景。",
  "moonshotai/kimi-k2-instruct-0905.description": "kimi-k2-0905-preview 模型上下文长度为 256k，具备更强的 Agentic Coding 能力、更突出的前端代码的美观度和实用性、以及更好的上下文理解能力。",
  "moonshotai/kimi-k2-thinking-turbo.description": "Kimi K2 Thinking Turbo 是 Kimi K2 Thinking 的高速版本，在保持深度推理能力的同时，显著降低响应延迟。",
  "moonshotai/kimi-k2-thinking.description": "Kimi K2 Thinking 是 Moonshot 针对深度推理任务优化的思考模型，具备通用 Agent 能力。",
  "moonshotai/kimi-k2.description": "Kimi K2 是由月之暗面 AI 开发的大规模混合专家 (MoE) 语言模型，具有 1 万亿总参数和每次前向传递 320 亿激活参数。它针对代理能力进行了优化，包括高级工具使用、推理和代码合成。",
  "morph/morph-v3-fast.description": "Morph 提供了一个专门的 AI 模型，将前沿模型（如 Claude 或 GPT-4o）建议的代码更改应用到您的现有代码文件中 FAST - 4500+ tokens/秒。它充当 AI 编码工作流程中的最后一步。支持 16k 输入 tokens 和 16k 输出 tokens。",
  "morph/morph-v3-large.description": "Morph 提供了一个专门的 AI 模型，将前沿模型（如 Claude 或 GPT-4o）建议的代码更改应用到您的现有代码文件中 FAST - 2500+ tokens/秒。它充当 AI 编码工作流程中的最后一步。支持 16k 输入 tokens 和 16k 输出 tokens。",
  "nousresearch/hermes-2-pro-llama-3-8b.description": "Hermes 2 Pro Llama 3 8B 是 Nous Hermes 2的升级版本，包含最新的内部开发的数据集。",
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF.description": "Llama 3.1 Nemotron 70B 是由 NVIDIA 定制的大型语言模型，旨在提高 LLM 生成的响应对用户查询的帮助程度。该模型在 Arena Hard、AlpacaEval 2 LC 和 GPT-4-Turbo MT-Bench 等基准测试中表现出色，截至 2024 年 10 月 1 日，在所有三个自动对齐基准测试中排名第一。该模型使用 RLHF（特别是 REINFORCE）、Llama-3.1-Nemotron-70B-Reward 和 HelpSteer2-Preference 提示在 Llama-3.1-70B-Instruct 模型基础上进行训练",
  "nvidia/llama-3.1-nemotron-51b-instruct.description": "独特的语言模型，提供无与伦比的准确性和效率表现。",
  "nvidia/llama-3.1-nemotron-70b-instruct.description": "Llama-3.1-Nemotron-70B-Instruct 是 NVIDIA 定制的大型语言模型，旨在提高 LLM 生成的响应的帮助性。",
  "o1-mini.description": "比 o1-preview 更小、更快，成本低80%，在代码生成和小上下文操作方面表现良好。",
  "o1-preview.description": "专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深度上下文理解和自主工作流程的应用。",
  "o1-pro.description": "o1 系列模型经过强化学习训练，能够在回答前进行思考，并执行复杂的推理任务。o1-pro 模型使用了更多计算资源，以进行更深入的思考，从而持续提供更优质的回答。",
  "o1.description": "o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。",
  "o3-2025-04-16.description": "o3 是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务",
  "o3-deep-research.description": "o3-deep-research 是我们最先进的深度研究模型，专为处理复杂的多步骤研究任务而设计。它可以从互联网上搜索和综合信息，也可以通过 MCP 连接器访问并利用你的自有数据。",
  "o3-mini.description": "o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。",
  "o3-pro-2025-06-10.description": "o3 Pro 是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务",
  "o3-pro.description": "o3-pro 模型使用更多的计算来更深入地思考并始终提供更好的答案，仅支持 Responses API 下使用。",
  "o3.description": "o3 是一款全能强大的模型，在多个领域表现出色。它为数学、科学、编程和视觉推理任务树立了新标杆。它也擅长技术写作和指令遵循。用户可利用它分析文本、代码和图像，解决多步骤的复杂问题。",
  "o4-mini-2025-04-16.description": "o4-mini 是 OpenAI 的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文",
  "o4-mini-deep-research.description": "o4-mini-deep-research 是我们更快速、更实惠的深度研究模型——非常适合处理复杂的多步骤研究任务。它可以从互联网上搜索和综合信息，也可以通过 MCP 连接器访问并利用你的自有数据。",
  "o4-mini.description": "o4-mini 是我们最新的小型 o 系列模型。 它专为快速有效的推理而优化，在编码和视觉任务中表现出极高的效率和性能。",
  "open-codestral-mamba.description": "Codestral Mamba是专注于代码生成的Mamba 2语言模型，为先进的代码和推理任务提供强力支持。",
  "open-mistral-7b.description": "Mistral 7B是一款紧凑但高性能的模型，擅长批量处理和简单任务，如分类和文本生成，具有良好的推理能力。",
  "open-mistral-nemo.description": "Mistral Nemo是一个与Nvidia合作开发的12B模型，提供出色的推理和编码性能，易于集成和替换。",
  "open-mixtral-8x22b.description": "Mixtral 8x22B是一个更大的专家模型，专注于复杂任务，提供出色的推理能力和更高的吞吐量。",
  "open-mixtral-8x7b.description": "Mixtral 8x7B是一个稀疏专家模型，利用多个参数提高推理速度，适合处理多语言和代码生成任务。",
  "openai/gpt-3.5-turbo-instruct.description": "与 GPT-3 时代模型类似的能力。与传统的完成端点兼容，而不是聊天完成端点。",
  "openai/gpt-3.5-turbo.description": "OpenAI 在 GPT-3.5 系列中最能干且最具成本效益的模型，针对聊天目的进行了优化，但在传统完成任务中也表现良好。",
  "openai/gpt-4-turbo.description": "来自 OpenAI 的 gpt-4-turbo 具有广泛的通用知识和领域专长，使其能够遵循自然语言的复杂指令并准确解决困难问题。它的知识截止日期为 2023 年 4 月，上下文窗口为 128,000 个 token。",
  "openai/gpt-4.1-mini.description": "GPT-4.1 Mini 提供更低延迟与更佳性价比，适合中等上下文上下线路。",
  "openai/gpt-4.1-nano.description": "GPT-4.1 Nano 是极低成本低延迟选项，适合高频次短对话或分类场景。",
  "openai/gpt-4.1.description": "GPT-4.1 系列提供了更大上下文与更强的工程与推理能力。",
  "openai/gpt-4o-mini.description": "GPT-4o-mini 是 GPT-4o 的快速小模型版本，适合低延迟图文混合场景。",
  "openai/gpt-4o.description": "GPT-4o 系列是 OpenAI 的 Omni 模型，支持文本 + 图片输入与文本输出。",
  "openai/gpt-5-chat.description": "GPT-5 Chat 是为对话场景优化的 GPT-5 子型号，降低延迟以提升交互体验。",
  "openai/gpt-5-codex.description": "GPT-5-Codex 是针对编码场景进一步优化的 GPT-5 变体，适合大规模代码工作流。",
  "openai/gpt-5-mini.description": "GPT-5 Mini 是 GPT-5 家族的精简版，适用于低延迟低成本场景。",
  "openai/gpt-5-nano.description": "GPT-5 Nano 是家族中的超小型版本，适合对成本和延迟要求非常高的场景。",
  "openai/gpt-5-pro.description": "GPT-5 Pro 是 OpenAI 的旗舰模型，提供更强的推理、代码生成与企业级功能，支持测试时路由与更严谨的安全策略。",
  "openai/gpt-5.1-chat.description": "GPT-5.1 Chat 是 GPT-5.1 家族的轻量成员，针对低延迟对话进行优化，同时保留较强的推理与指令执行能力。",
  "openai/gpt-5.1-codex-mini.description": "GPT-5.1-Codex-Mini 是 GPT-5.1-Codex 的小型加速版本，更适合对延迟和成本敏感的编码场景。",
  "openai/gpt-5.1-codex.description": "GPT-5.1-Codex 是针对软件工程和编码工作流优化的 GPT-5.1 变体，适合大型重构、复杂调试与长时间自主编码任务。",
  "openai/gpt-5.1.description": "GPT-5.1 是 GPT-5 系列最新旗舰模型，相比 GPT-5 在通用推理、指令遵循和对话自然度上均有显著提升，适合广泛任务场景。",
  "openai/gpt-5.description": "GPT-5 是 OpenAI 的高性能模型，适用广泛的生产与研究任务。",
  "openai/gpt-oss-120b.description": "极其能干的通用大型语言模型，具有强大、可控的推理能力",
  "openai/gpt-oss-20b.description": "一个紧凑、开源权重的语言模型，针对低延迟和资源受限环境进行了优化，包括本地和边缘部署",
  "openai/o1-mini.description": "o1-mini是一款针对编程、数学和科学应用场景而设计的快速、经济高效的推理模型。该模型具有128K上下文和2023年10月的知识截止日期。",
  "openai/o1-preview.description": "o1是OpenAI新的推理模型，适用于需要广泛通用知识的复杂任务。该模型具有128K上下文和2023年10月的知识截止日期。",
  "openai/o1.description": "OpenAI 的 o1 是旗舰推理模型，专为需要深度思考的复杂问题而设计。它为复杂多步任务提供了强大的推理能力和更高的准确性。",
  "openai/o3-mini-high.description": "o3-mini 高推理等级版，在与 o1-mini 相同的成本和延迟目标下提供高智能。",
  "openai/o3-mini.description": "o3-mini 是 OpenAI 最新的小型推理模型，在 o1-mini 的相同成本和延迟目标下提供高智能。",
  "openai/o3.description": "OpenAI 的 o3 是最强大的推理模型，在编码、数学、科学和视觉感知方面设立了新的最先进水平。它擅长需要多方面分析的复杂查询，在分析图像、图表和图形方面具有特殊优势。",
  "openai/o4-mini-high.description": "o4-mini 高推理等级版，专为快速有效的推理而优化，在编码和视觉任务中表现出极高的效率和性能。",
  "openai/o4-mini.description": "OpenAI o4-mini 是 OpenAI 的小型高效推理模型，适合低延迟场景。",
  "openai/text-embedding-3-large.description": "OpenAI 最能干的嵌入模型，适用于英语和非英语任务。",
  "openai/text-embedding-3-small.description": "OpenAI 改进的、性能更高的 ada 嵌入模型版本。",
  "openai/text-embedding-ada-002.description": "OpenAI 的传统文本嵌入模型。",
  "openrouter/auto.description": "根据上下文长度、主题和复杂性，你的请求将发送到 Llama 3 70B Instruct、Claude 3.5 Sonnet（自我调节）或 GPT-4o。",
  "perplexity/sonar-pro.description": "Perplexity 的旗舰产品，具有搜索接地能力，支持高级查询和后续操作。",
  "perplexity/sonar-reasoning-pro.description": "一个高级推理聚焦模型，在响应中输出思维链 (CoT)，提供具有增强搜索能力和每个请求多个搜索查询的综合解释。",
  "perplexity/sonar-reasoning.description": "一个专注于推理的模型，在响应中输出思维链 (CoT)，提供具有搜索接地的详细解释。",
  "perplexity/sonar.description": "Perplexity 的轻量级产品，具有搜索接地能力，比 Sonar Pro 更快、更便宜。",
  "phi3.description": "Phi-3 是微软推出的轻量级开放模型，适用于高效集成和大规模知识推理。",
  "phi3:14b.description": "Phi-3 是微软推出的轻量级开放模型，适用于高效集成和大规模知识推理。",
  "pixtral-12b-2409.description": "Pixtral 模型在图表和图理解、文档问答、多模态推理和指令遵循等任务上表现出强大的能力，能够以自然分辨率和宽高比摄入图像，还能够在长达 128K 令牌的长上下文窗口中处理任意数量的图像。",
  "pixtral-large-latest.description": "Pixtral Large 是一款拥有 1240 亿参数的开源多模态模型，基于 Mistral Large 2 构建。这是我们多模态家族中的第二款模型，展现了前沿水平的图像理解能力。",
  "pro-128k.description": "Spark Pro 128K 配置了特大上下文处理能力，能够处理多达128K的上下文信息，特别适合需通篇分析和长期逻辑关联处理的长文内容，可在复杂文本沟通中提供流畅一致的逻辑与多样的引用支持。",
  "pro-deepseek-r1.description": "企业专属服务专用模型，包并发服务。",
  "pro-deepseek-v3.description": "企业专属服务专用模型，包并发服务。",
  "qianfan-70b.description": "Qianfan 70B，大参数中文模型，适合高质量内容生成与复杂推理任务。",
  "qianfan-8b.description": "Qianfan 8B，中型通用模型，适合成本与效果平衡的文本生成和问答场景。",
  "qianfan-agent-intent-32k.description": "Qianfan Agent Intent 32K，面向意图识别与智能体编排的模型，支持长上下文场景。",
  "qianfan-agent-lite-8k.description": "Qianfan Agent Lite 8K，轻量智能体模型，适合低成本多轮对话与业务编排。",
  "qianfan-agent-speed-32k.description": "Qianfan Agent Speed 32K，高流控智能体模型，适合大规模、多任务 Agent 应用。",
  "qianfan-agent-speed-8k.description": "Qianfan Agent Speed 8K，面向中短对话与快速响应的高并发智能体模型。",
  "qianfan-check-vl.description": "Qianfan Check VL，多模态内容审核与检测模型，支持图文合规与识别任务。",
  "qianfan-composition.description": "Qianfan Composition，多模态创作模型，支持图文混合理解与生成。",
  "qianfan-engcard-vl.description": "Qianfan EngCard VL，专注英文场景的多模态识别模型。",
  "qianfan-lightning-128b-a19b.description": "Qianfan Lightning 128B A19B，高性能中文通用模型，适用于复杂问答与大规模推理任务。",
  "qianfan-llama-vl-8b.description": "Qianfan Llama VL 8B，基于 Llama 的多模态模型，面向通用图文理解任务。",
  "qianfan-multipicocr.description": "Qianfan MultiPicOCR，多图 OCR 模型，支持多张图片文字检测与识别。",
  "qianfan-qi-vl.description": "Qianfan QI VL，多模态问答模型，支持复杂图文场景下的精准检索与问答。",
  "qianfan-singlepicocr.description": "Qianfan SinglePicOCR，单图 OCR 模型，支持高精度字符识别。",
  "qianfan-vl-70b.description": "Qianfan VL 70B，大参数视觉语言模型，适用于复杂图文理解场景。",
  "qianfan-vl-8b.description": "Qianfan VL 8B，轻量视觉语言模型，适合日常图文问答和分析。",
  "qvq-72b-preview.description": "QVQ-72B-Preview 是由 Qwen 团队开发的实验性研究模型，专注于提升视觉推理能力。",
  "qvq-max.description": "通义千问QVQ视觉推理模型，支持视觉输入及思维链输出，在数学、编程、视觉分析、创作以及通用任务上都表现了更强的能力。",
  "qvq-plus.description": "视觉推理模型。支持视觉输入及思维链输出，继qvq-max模型后推出的plus版本，相较于qvq-max模型，qvq-plus系列模型推理速度更快，效果和成本更均衡。",
  "qwen-3-32b.description": "Qwen 3 32B：Qwen 系列在多语言与编码任务上表现优良，适合中等规模生产化使用。",
  "qwen-coder-plus.description": "通义千问代码模型。",
  "qwen-coder-turbo-latest.description": "通义千问代码模型。",
  "qwen-coder-turbo.description": "通义千问代码模型。",
  "qwen-flash.description": "通义千问系列速度最快、成本极低的模型，适合简单任务。",
  "qwen-image-edit.description": "Qwen Image Edit 是一款图生图模型，支持基于输入图像和文本提示进行图像编辑和修改，能够根据用户需求对原图进行精准调整和创意改造。",
  "qwen-image.description": "Qwen-Image 是一款通用图像生成模型，支持多种艺术风格，尤其擅长复杂文本渲染，特别是中英文文本渲染。模型支持多行布局、段落级文本生成以及细粒度细节刻画，可实现复杂的图文混合布局设计。",
  "qwen-long.description": "通义千问超大规模语言模型，支持长文本上下文，以及基于长文档、多文档等多个场景的对话功能。",
  "qwen-math-plus-latest.description": "通义千问数学模型是专门用于数学解题的语言模型。",
  "qwen-math-plus.description": "通义千问数学模型是专门用于数学解题的语言模型。",
  "qwen-math-turbo-latest.description": "通义千问数学模型是专门用于数学解题的语言模型。",
  "qwen-math-turbo.description": "通义千问数学模型是专门用于数学解题的语言模型。",
  "qwen-max.description": "通义千问千亿级别超大规模语言模型，支持中文、英文等不同语言输入，当前通义千问2.5产品版本背后的API模型。",
  "qwen-omni-turbo.description": "Qwen-Omni 系列模型支持输入多种模态的数据，包括视频、音频、图片、文本，并输出音频与文本。",
  "qwen-plus.description": "通义千问超大规模语言模型增强版，支持中文、英文等不同语言输入。",
  "qwen-turbo.description": "通义千问 Turbo 后续不再更新，建议替换为通义千问 Flash 。通义千问超大规模语言模型，支持中文、英文等不同语言输入。",
  "qwen-vl-chat-v1.description": "通义千问VL支持灵活的交互方式，包括多图、多轮问答、创作等能力的模型。",
  "qwen-vl-max-latest.description": "通义千问超大规模视觉语言模型。相比增强版，再次提升视觉推理能力和指令遵循能力，提供更高的视觉感知和认知水平。",
  "qwen-vl-max.description": "通义千问超大规模视觉语言模型。相比增强版，再次提升视觉推理能力和指令遵循能力，提供更高的视觉感知和认知水平。",
  "qwen-vl-ocr.description": "通义千问OCR是文字提取专有模型，专注于文档、表格、试题、手写体文字等类型图像的文字提取能力。它能够识别多种文字，目前支持的语言有：汉语、英语、法语、日语、韩语、德语、俄语、意大利语、越南语、阿拉伯语。",
  "qwen-vl-plus-latest.description": "通义千问大规模视觉语言模型增强版。大幅提升细节识别能力和文字识别能力，支持超百万像素分辨率和任意长宽比规格的图像。",
  "qwen-vl-plus.description": "通义千问大规模视觉语言模型增强版。大幅提升细节识别能力和文字识别能力，支持超百万像素分辨率和任意长宽比规格的图像。",
  "qwen-vl-v1.description": "以 Qwen-7B 语言模型初始化，添加图像模型，图像输入分辨率为448的预训练模型。",
  "qwen/qwen-2-7b-instruct.description": "Qwen2是全新的Qwen大型语言模型系列。Qwen2 7B是一个基于transformer的模型，在语言理解、多语言能力、编程、数学和推理方面表现出色。",
  "qwen/qwen-2-7b-instruct:free.description": "Qwen2 是全新的大型语言模型系列，具有更强的理解和生成能力。",
  "qwen/qwen-2-vl-72b-instruct.description": "Qwen2-VL 是 Qwen-VL 模型的最新迭代版本，在视觉理解基准测试中达到了最先进的性能，包括 MathVista、DocVQA、RealWorldQA 和 MTVQA 等。Qwen2-VL 能够理解超过 20 分钟的视频，用于高质量的基于视频的问答、对话和内容创作。它还具备复杂推理和决策能力，可以与移动设备、机器人等集成，基于视觉环境和文本指令进行自动操作。除了英语和中文，Qwen2-VL 现在还支持理解图像中不同语言的文本，包括大多数欧洲语言、日语、韩语、阿拉伯语和越南语等",
  "qwen/qwen-2.5-72b-instruct.description": "Qwen2.5-72B-Instruct 是阿里云发布的最新大语言模型系列之一。该 72B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升。",
  "qwen/qwen2.5-32b-instruct.description": "Qwen2.5-32B-Instruct 是阿里云发布的最新大语言模型系列之一。该 32B 模型在编码和数学等领域具有显著改进的能力。该模型提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升。",
  "qwen/qwen2.5-7b-instruct.description": "面向中文和英文的 LLM，针对语言、编程、数学、推理等领域。",
  "qwen/qwen2.5-coder-32b-instruct.description": "高级 LLM，支持代码生成、推理和修复，涵盖主流编程语言。",
  "qwen/qwen2.5-coder-7b-instruct.description": "强大的中型代码模型，支持 32K 上下文长度，擅长多语言编程。",
  "qwen/qwen3-14b.description": "Qwen3-14B 是 Qwen 系列的 14B 版本，适合常规推理与对话场景。",
  "qwen/qwen3-14b:free.description": "Qwen3-14B 是 Qwen3 系列中一个密集的 148 亿参数因果语言模型，专为复杂推理和高效对话而设计。它支持在用于数学、编程和逻辑推理等任务的“思考”模式与用于通用对话的“非思考”模式之间无缝切换。该模型经过微调，可用于指令遵循、代理工具使用、创意写作以及跨 100 多种语言和方言的多语言任务。它原生处理 32K 令牌上下文，并可使用基于 YaRN 的扩展扩展到 131K 令牌。",
  "qwen/qwen3-235b-a22b-2507.description": "Qwen3-235B-A22B-Instruct-2507 为 Qwen3 系列的 Instruct 版本，兼顾多语言指令与长上下文场景。",
  "qwen/qwen3-235b-a22b-thinking-2507.description": "Qwen3-235B-A22B-Thinking-2507 为 Qwen3 的 Thinking 变体，针对复杂数学与推理任务进行了强化。",
  "qwen/qwen3-235b-a22b.description": "Qwen3-235B-A22B 是由 Qwen 开发的 235B 参数专家混合 (MoE) 模型，每次前向传递激活 22B 参数。它支持在用于复杂推理、数学和代码任务的“思考”模式与用于一般对话效率的“非思考”模式之间无缝切换。该模型展示了强大的推理能力、多语言支持（100 多种语言和方言）、高级指令遵循和代理工具调用能力。它原生处理 32K 令牌上下文窗口，并使用基于 YaRN 的扩展扩展到 131K 令牌。",
  "qwen/qwen3-235b-a22b:free.description": "Qwen3-235B-A22B 是由 Qwen 开发的 235B 参数专家混合 (MoE) 模型，每次前向传递激活 22B 参数。它支持在用于复杂推理、数学和代码任务的“思考”模式与用于一般对话效率的“非思考”模式之间无缝切换。该模型展示了强大的推理能力、多语言支持（100 多种语言和方言）、高级指令遵循和代理工具调用能力。它原生处理 32K 令牌上下文窗口，并使用基于 YaRN 的扩展扩展到 131K 令牌。",
  "qwen/qwen3-30b-a3b.description": "Qwen3 是 Qwen 大型语言模型系列的最新一代，具有密集和专家混合 (MoE) 架构，在推理、多语言支持和高级代理任务方面表现出色。其在复杂推理的思考模式和高效对话的非思考模式之间无缝切换的独特能力确保了多功能、高质量的性能。\n\nQwen3 显著优于 QwQ 和 Qwen2.5 等先前模型，提供卓越的数学、编码、常识推理、创意写作和交互式对话能力。Qwen3-30B-A3B 变体包含 305 亿个参数（33 亿个激活参数）、48 层、128 个专家（每个任务激活 8 个），并支持高达 131K 令牌上下文（使用 YaRN），为开源模型树立了新标准。",
  "qwen/qwen3-30b-a3b:free.description": "Qwen3 是 Qwen 大型语言模型系列的最新一代，具有密集和专家混合 (MoE) 架构，在推理、多语言支持和高级代理任务方面表现出色。其在复杂推理的思考模式和高效对话的非思考模式之间无缝切换的独特能力确保了多功能、高质量的性能。\n\nQwen3 显著优于 QwQ 和 Qwen2.5 等先前模型，提供卓越的数学、编码、常识推理、创意写作和交互式对话能力。Qwen3-30B-A3B 变体包含 305 亿个参数（33 亿个激活参数）、48 层、128 个专家（每个任务激活 8 个），并支持高达 131K 令牌上下文（使用 YaRN），为开源模型树立了新标准。",
  "qwen/qwen3-32b.description": "Qwen3-32B 是 Qwen3 系列中一个密集的 328 亿参数因果语言模型，针对复杂推理和高效对话进行了优化。它支持在用于数学、编码和逻辑推理等任务的“思考”模式与用于更快、通用对话的“非思考”模式之间无缝切换。该模型在指令遵循、代理工具使用、创意写作以及跨 100 多种语言和方言的多语言任务中表现出强大的性能。它原生处理 32K 令牌上下文，并可使用基于 YaRN 的扩展扩展到 131K 令牌。",
  "qwen/qwen3-32b:free.description": "Qwen3-32B 是 Qwen3 系列中一个密集的 328 亿参数因果语言模型，针对复杂推理和高效对话进行了优化。它支持在用于数学、编码和逻辑推理等任务的“思考”模式与用于更快、通用对话的“非思考”模式之间无缝切换。该模型在指令遵循、代理工具使用、创意写作以及跨 100 多种语言和方言的多语言任务中表现出强大的性能。它原生处理 32K 令牌上下文，并可使用基于 YaRN 的扩展扩展到 131K 令牌。",
  "qwen/qwen3-8b:free.description": "Qwen3-8B 是 Qwen3 系列中一个密集的 82 亿参数因果语言模型，专为推理密集型任务和高效对话而设计。它支持在用于数学、编码和逻辑推理的“思考”模式与用于一般对话的“非思考”模式之间无缝切换。该模型经过微调，可用于指令遵循、代理集成、创意写作以及跨 100 多种语言和方言的多语言使用。它原生支持 32K 令牌上下文窗口，并可通过 YaRN 扩展到 131K 令牌。",
  "qwen/qwen3-coder-plus.description": "Qwen3-Coder-Plus 为 Qwen 系列特别优化的编码代理模型，支持更复杂的工具调用与长期会话。",
  "qwen/qwen3-coder.description": "Qwen3-Coder 是 Qwen3 的代码生成器家族，擅长长文档内的代码理解与生成。",
  "qwen/qwen3-max-preview.description": "Qwen3 Max (preview) 是 Qwen 系列面向高级推理与工具集成的 Max 版本（预览）。",
  "qwen/qwen3-max.description": "Qwen3 Max 是 Qwen3 系列的高端推理模型，适合多语言推理和工具集成。",
  "qwen/qwen3-vl-plus.description": "Qwen3 VL-Plus 为 Qwen3 的视觉增强版本，提升了多模态推理与视频处理的能力。",
  "qwen2.5-14b-instruct-1m.description": "通义千问2.5对外开源的72B规模的模型。",
  "qwen2.5-14b-instruct.description": "通义千问2.5对外开源的14B规模的模型。",
  "qwen2.5-32b-instruct.description": "通义千问2.5对外开源的32B规模的模型。",
  "qwen2.5-72b-instruct.description": "通义千问2.5对外开源的72B规模的模型。",
  "qwen2.5-7b-instruct.description": "Qwen2.5 7B Instruct，成熟的开源指令模型，适用于多场景对话与生成。",
  "qwen2.5-coder-1.5b-instruct.description": "通义千问代码模型开源版。",
  "qwen2.5-coder-14b-instruct.description": "通义千问代码模型开源版。",
  "qwen2.5-coder-32b-instruct.description": "通义千问代码模型开源版。",
  "qwen2.5-coder-7b-instruct.description": "通义千问代码模型开源版。",
  "qwen2.5-coder-instruct.description": "Qwen2.5-Coder 是 Qwen 系列中最新的代码专用大型语言模型（前身为 CodeQwen）。",
  "qwen2.5-instruct.description": "Qwen2.5 是 Qwen 大型语言模型的最新系列。对于 Qwen2.5，我们发布了多个基础语言模型和指令微调语言模型，参数范围从 5 亿到 72 亿不等。",
  "qwen2.5-math-1.5b-instruct.description": "Qwen-Math 模型具有强大的数学解题能力。",
  "qwen2.5-math-72b-instruct.description": "Qwen-Math 模型具有强大的数学解题能力。",
  "qwen2.5-math-7b-instruct.description": "Qwen-Math 模型具有强大的数学解题能力。",
  "qwen2.5-omni-7b.description": "Qwen-Omni 系列模型支持输入多种模态的数据，包括视频、音频、图片、文本，并输出音频与文本。",
  "qwen2.5-vl-32b-instruct.description": "Qwen2.5 VL 32B Instruct，多模态开源模型，适合私有化部署与多场景应用。",
  "qwen2.5-vl-72b-instruct.description": "指令跟随、数学、解题、代码整体提升，万物识别能力提升，支持多样格式直接精准定位视觉元素，支持对长视频文件（最长10分钟）进行理解和秒级别的事件时刻定位，能理解时间先后和快慢，基于解析和定位能力支持操控OS或Mobile的Agent，关键信息抽取能力和Json格式输出能力强，此版本为72B版本，本系列能力最强的版本。",
  "qwen2.5-vl-7b-instruct.description": "Qwen2.5 VL 7B Instruct，轻量多模态模型，兼顾部署成本与识别能力。",
  "qwen2.5-vl-instruct.description": "Qwen2.5-VL 是 Qwen 模型家族中视觉语言模型的最新版本。",
  "qwen2.5.description": "Qwen2.5 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen2.5:0.5b.description": "Qwen2.5 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen2.5:1.5b.description": "Qwen2.5 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen2.5:72b.description": "Qwen2.5 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen2.description": "Qwen2 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen2:0.5b.description": "Qwen2 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen2:1.5b.description": "Qwen2 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen2:72b.description": "Qwen2 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwen3-0.6b.description": "Qwen3 0.6B，入门级模型，适用于简单推理和资源极度受限环境。",
  "qwen3-1.7b.description": "Qwen3 1.7B，超轻量模型，便于边缘与终端部署。",
  "qwen3-14b.description": "Qwen3 14B，中型模型，适合多语言问答与文本生成。",
  "qwen3-235b-a22b-instruct-2507.description": "Qwen3 235B A22B Instruct 2507，通用旗舰 Instruct 模型，适合多种生成与推理任务。",
  "qwen3-235b-a22b-thinking-2507.description": "Qwen3 235B A22B Thinking 2507，超大规模思考模型，适用于高难度推理。",
  "qwen3-235b-a22b.description": "Qwen3 235B A22B，通用大模型，面向多种复杂任务。",
  "qwen3-30b-a3b-instruct-2507.description": "Qwen3 30B A3B Instruct 2507，中大型 Instruct 模型，适合高质量生成与问答。",
  "qwen3-30b-a3b-thinking-2507.description": "Qwen3 30B A3B Thinking 2507，中大型思考模型，兼顾精度与成本。",
  "qwen3-30b-a3b.description": "Qwen3 30B A3B，中大型通用模型，在成本与效果间平衡。",
  "qwen3-32b.description": "Qwen3 32B，适合需要更强理解能力的通用任务场景。",
  "qwen3-4b.description": "Qwen3 4B，适合中小型应用和本地推理场景。",
  "qwen3-8b.description": "Qwen3 8B，轻量模型，部署灵活，适用于高并发业务。",
  "qwen3-coder-30b-a3b-instruct.description": "通义千问代码模型开源版。最新的 qwen3-coder-30b-a3b-instruct 是基于 Qwen3 的代码生成模型，具有强大的Coding Agent能力，擅长工具调用和环境交互，能够实现自主编程、代码能力卓越的同时兼具通用能力。",
  "qwen3-coder-480b-a35b-instruct.description": "Qwen3 Coder 480B A35B Instruct，旗舰级代码模型，支持多语言编程与复杂代码理解。",
  "qwen3-coder-flash.description": "通义千问代码模型。最新的 Qwen3-Coder 系列模型是基于 Qwen3 的代码生成模型，具有强大的Coding Agent能力，擅长工具调用和环境交互，能够实现自主编程，代码能力卓越的同时兼具通用能力。",
  "qwen3-coder-plus.description": "通义千问代码模型。最新的 Qwen3-Coder 系列模型是基于 Qwen3 的代码生成模型，具有强大的Coding Agent能力，擅长工具调用和环境交互，能够实现自主编程，代码能力卓越的同时兼具通用能力。",
  "qwen3-coder:480b.description": "阿里巴巴针对代理和编码任务的高性能长上下文模型。",
  "qwen3-max-preview.description": "通义千问系列效果最好的模型，适合复杂、多步骤的任务。预览版已支持思考。",
  "qwen3-max.description": "通义千问3系列Max模型，相较2.5系列整体通用能力有大幅度提升，中英文通用文本理解能力、复杂指令遵循能力、主观开放任务能力、多语言能力、工具调用能力均显著增强；模型知识幻觉更少。最新的qwen3-max模型：相较qwen3-max-preview版本，在智能体编程与工具调用方向进行了专项升级。本次发布的正式版模型达到领域SOTA水平，适配场景更加复杂的智能体需求。",
  "qwen3-next-80b-a3b-instruct.description": "基于 Qwen3 的新一代非思考模式开源模型，相较上一版本（通义千问3-235B-A22B-Instruct-2507）中文文本理解能力更佳、逻辑推理能力有增强、文本生成类任务表现更好。",
  "qwen3-next-80b-a3b-thinking.description": "Qwen3 Next 80B A3B Thinking，面向复杂任务的旗舰推理模型版本。",
  "qwen3-omni-flash.description": "Qwen-Omni 模型能够接收文本、图片、音频、视频等多种模态的组合输入，并生成文本或语音形式的回复， 提供多种拟人音色，支持多语言和方言的语音输出，可应用于文本创作、视觉识别、语音助手等场景。",
  "qwen3-vl-235b-a22b-instruct.description": "Qwen3 VL 235B A22B Instruct，旗舰多模态模型，面向高要求理解与创作场景。",
  "qwen3-vl-235b-a22b-thinking.description": "Qwen3 VL 235B A22B Thinking，旗舰思考版，用于复杂多模态推理与规划任务。",
  "qwen3-vl-30b-a3b-instruct.description": "Qwen3 VL 30B A3B Instruct，多模态大模型，兼顾精度与推理性能。",
  "qwen3-vl-30b-a3b-thinking.description": "Qwen3 VL 30B A3B Thinking，面向复杂多模态任务的深度思考版本。",
  "qwen3-vl-32b-instruct.description": "Qwen3 VL 32B Instruct，多模态指令微调模型，适用于高质量图文问答与创作。",
  "qwen3-vl-32b-thinking.description": "Qwen3 VL 32B Thinking，多模态深度思考版本，强化复杂推理与长链路分析。",
  "qwen3-vl-8b-instruct.description": "Qwen3 VL 8B Instruct，轻量多模态模型，适合日常视觉问答与应用集成。",
  "qwen3-vl-8b-thinking.description": "Qwen3 VL 8B Thinking，多模态思维链模型，适合对视觉信息进行细致推理。",
  "qwen3-vl-flash.description": "Qwen3 VL Flash：轻量化高速推理版本，适合对延迟敏感或大批量请求场景。",
  "qwen3-vl-plus.description": "通义千问VL是具有视觉（图像）理解能力的文本生成模型，不仅能进行OCR（图片文字识别），还能进一步总结和推理，例如从商品照片中提取属性，根据习题图进行解题等。",
  "qwen3.description": "Qwen3 是阿里巴巴的新一代大规模语言模型，以优异的性能支持多元化的应用需求。",
  "qwq-32b-preview.description": "QwQ模型是由 Qwen 团队开发的实验性研究模型，专注于增强 AI 推理能力。",
  "qwq-32b.description": "QwQ 是 Qwen 系列的推理模型。与传统的指令微调模型相比，QwQ 具备思考和推理能力，在下游任务中，尤其是复杂问题上，能够实现显著增强的性能。QwQ-32B 是一款中型推理模型，其性能可与最先进的推理模型（如 DeepSeek-R1、o1-mini）相媲美。",
  "qwq-plus.description": "基于 Qwen2.5 模型训练的 QwQ 推理模型，通过强化学习大幅度提升了模型推理能力。模型数学代码等核心指标（AIME 24/25、LiveCodeBench）以及部分通用指标（IFEval、LiveBench等）达到DeepSeek-R1 满血版水平。",
  "qwq.description": "QwQ 是 Qwen 系列的推理模型。与传统的指令调优模型相比，QwQ 具备思考和推理的能力，能够在下游任务中，尤其是困难问题上，显著提升性能。QwQ-32B 是中型推理模型，能够在与最先进的推理模型（如 DeepSeek-R1、o1-mini）竞争时取得可观的表现。",
  "qwq_32b.description": "Qwen 系列中等规模的推理模型。与传统的指令调优模型相比，具备思考和推理能力的 QwQ 在下游任务中，尤其是在解决难题时，能够显著提升性能。",
  "r1-1776.description": "R1-1776 是 DeepSeek R1 模型的一个版本，经过后训练，可提供未经审查、无偏见的事实信息。",
  "solar-mini-ja.description": "Solar Mini (Ja) 扩展了 Solar Mini 的能力，专注于日语，同时在英语和韩语的使用中保持高效和卓越性能。",
  "solar-mini.description": "Solar Mini 是一种紧凑型 LLM，性能优于 GPT-3.5，具备强大的多语言能力，支持英语和韩语，提供高效小巧的解决方案。",
  "solar-pro.description": "Solar Pro 是 Upstage 推出的一款高智能LLM，专注于单GPU的指令跟随能力，IFEval得分80以上。目前支持英语，正式版本计划于2024年11月推出，将扩展语言支持和上下文长度。",
  "sonar-deep-research.description": "Deep Research 进行全面的专家级研究，并将其综合成可访问、可作的报告。",
  "sonar-pro.description": "支持搜索上下文的高级搜索产品，支持高级查询和跟进。",
  "sonar-reasoning-pro.description": "支持搜索上下文的高级搜索产品，支持高级查询和跟进。",
  "sonar-reasoning.description": "支持搜索上下文的高级搜索产品，支持高级查询和跟进。",
  "sonar.description": "基于搜索上下文的轻量级搜索产品，比 Sonar Pro 更快、更便宜。",
  "spark-x.description": "X1.5能力介绍：（1）新增动态调整思考模式，通过thinking 字段控制；（2）上下文长度增大：输入、输出各64K；（3）支持FunctionCall功能。",
  "stable-diffusion-3-medium.description": "由 Stability AI 推出的最新文生图大模型。这一版本在继承了前代的优点上，对图像质量、文本理解和风格多样性等方面进行了显著改进，能够更准确地解读复杂的自然语言提示，并生成更为精确和多样化的图像。",
  "stable-diffusion-3.5-large-turbo.description": "stable-diffusion-3.5-large-turbo 是在 stable-diffusion-3.5-large 的基础上采用对抗性扩散蒸馏（ADD）技术的模型，具备更快的速度。",
  "stable-diffusion-3.5-large.description": "stable-diffusion-3.5-large 是一个具有8亿参数的多模态扩散变压器（MMDiT）文本到图像生成模型，具备卓越的图像质量和提示词匹配度，支持生成 100 万像素的高分辨率图像，且能够在普通消费级硬件上高效运行。",
  "stable-diffusion-v1.5.description": "stable-diffusion-v1.5 是以 stable-diffusion-v1.2 检查点的权重进行初始化，并在 \"laion-aesthetics v2 5+\" 上以 512x512 的分辨率进行了595k步的微调，减少了 10% 的文本条件化，以提高无分类器的引导采样。",
  "stable-diffusion-xl-base-1.0.description": "由 Stability AI 开发并开源的文生图大模型，其创意图像生成能力位居行业前列。具备出色的指令理解能力，能够支持反向 Prompt 定义来精确生成内容。",
  "stable-diffusion-xl.description": "stable-diffusion-xl 相比于 v1.5 做了重大的改进，并且与当前开源的文生图 SOTA 模型 midjourney 效果相当。具体改进之处包括： 更大的 unet backbone，是之前的 3 倍； 增加了 refinement 模块用于改善生成图片的质量；更高效的训练技巧等。",
  "step-1-128k.description": "平衡性能与成本，适合一般场景。",
  "step-1-256k.description": "具备超长上下文处理能力，尤其适合长文档分析。",
  "step-1-32k.description": "支持中等长度的对话，适用于多种应用场景。",
  "step-1-8k.description": "小型模型，适合轻量级任务。",
  "step-1-flash.description": "高速模型，适合实时对话。",
  "step-1.5v-mini.description": "该模型拥有强大的视频理解能力。",
  "step-1o-turbo-vision.description": "该模型拥有强大的图像理解能力，在数理、代码领域强于1o。模型比1o更小，输出速度更快。",
  "step-1o-vision-32k.description": "该模型拥有强大的图像理解能力。相比于 step-1v 系列模型，拥有更强的视觉性能。",
  "step-1v-32k.description": "支持视觉输入，增强多模态交互体验。",
  "step-1v-8k.description": "小型视觉模型，适合基本的图文任务。",
  "step-1x-edit.description": "该模型专注于图像编辑任务，能够根据用户提供的图片和文本描述，对图片进行修改和增强。支持多种输入格式，包括文本描述和示例图像。模型能够理解用户的意图，并生成符合要求的图像编辑结果。",
  "step-1x-medium.description": "该模型拥有强大的图像生成能力，支持文本描述作为输入方式。具备原生的中文支持，能够更好的理解和处理中文文本描述，并且能够更准确地捕捉文本描述中的语义信息，并将其转化为图像特征，从而实现更精准的图像生成。模型能够根据输入生成高分辨率、高质量的图像，并具备一定的风格迁移能力。",
  "step-2-16k-exp.description": "step-2模型的实验版本，包含最新的特性，滚动更新中。不推荐在正式生产环境使用。",
  "step-2-16k.description": "支持大规模上下文交互，适合复杂对话场景。",
  "step-2-mini.description": "基于新一代自研Attention架构MFA的极速大模型，用极低成本达到和step1类似的效果，同时保持了更高的吞吐和更快响应时延。能够处理通用任务，在代码能力上具备特长。",
  "step-2x-large.description": "阶跃星辰新一代生图模型,该模型专注于图像生成任务,能够根据用户提供的文本描述,生成高质量的图像。新模型生成图片质感更真实，中英文文字生成能力更强。",
  "step-3.description": "该模型拥有强大的视觉感知和复杂推理能力。可准确完成跨领域的复杂知识理解、数学与视觉信息的交叉分析，以及日常生活中的各类视觉分析问题。",
  "step-r1-v-mini.description": "该模型是拥有强大的图像理解能力的推理大模型，能够处理图像和文字信息，经过深度思考后输出文本生成文本内容。该模型在视觉推理领域表现突出，同时拥有第一梯队的数学、代码、文本推理能力。上下文长度为100k。",
  "stepfun-ai/step3.description": "Step3 是由阶跃星辰（StepFun）发布的前沿多模态推理模型，它基于拥有 321B 总参数和 38B 激活参数的专家混合（MoE）架构构建。该模型采用端到端设计，旨在最小化解码成本，同时在视觉语言推理方面提供顶级性能。通过多矩阵分解注意力（MFA）和注意力-FFN 解耦（AFD）的协同设计，Step3 在旗舰级和低端加速器上都能保持卓越的效率。在预训练阶段，Step3 处理了超过 20T 的文本 token 和 4T 的图文混合 token，覆盖十多种语言。该模型在数学、代码及多模态等多个基准测试中均达到了开源模型的领先水平。",
  "taichu_llm.description": "基于海量高质数据训练，具有更强的文本理解、内容创作、对话问答等能力",
  "taichu_o1.description": "taichu_o1是新一代推理大模型，通过多模态交互和强化学习实现类人思维链，支持复杂决策推演，在保持高精度输出的同时展现可模型推理的思维路径，适用于策略分析与深度思考等场景。",
  "taichu_vl.description": "融合了图像理解、知识迁移、逻辑归因等能力，在图文问答领域表现突出",
  "tencent/Hunyuan-A13B-Instruct.description": "Hunyuan-A13B-Instruct 参数量800 亿，激活 130 亿参数即可对标更大模型，支持“快思考/慢思考”混合推理；长文理解稳定；经 BFCL-v3 与 τ-Bench 验证，Agent 能力领先；结合 GQA 与多量化格式，实现高效推理。",
  "tencent/Hunyuan-MT-7B.description": "混元翻译模型（Hunyuan Translation Model）由一个翻译模型 Hunyuan-MT-7B 和一个集成模型 Hunyuan-MT-Chimera 组成。Hunyuan-MT-7B 是一个拥有 70 亿参数的轻量级翻译模型，用于将源文本翻译成目标语言。该模型支持 33 种语言以及 5 种中国少数民族语言的互译。在 WMT25 国际机器翻译竞赛中，Hunyuan-MT-7B 在其参与的 31 个语言类别中获得了 30 个第一名，展现了其卓越的翻译能力。针对翻译场景，腾讯混元提出了一个从预训练到监督微调、再到翻译强化和集成强化的完整训练范式，使其在同等规模的模型中达到了业界领先的性能。该模型计算效率高、易于部署，适合多种应用场景。",
  "text-embedding-3-large.description": "最强大的向量化模型，适用于英文和非英文任务",
  "text-embedding-3-small.description": "高效且经济的新一代 Embedding 模型，适用于知识检索、RAG 应用等场景",
  "thudm/glm-4-32b.description": "GLM-4-32B-0414 是一个 32B 双语（中英）开放权重语言模型，针对代码生成、函数调用和代理式任务进行了优化。它在 15T 高质量和重推理数据上进行了预训练，并使用人类偏好对齐、拒绝采样和强化学习进一步完善。该模型在复杂推理、工件生成和结构化输出任务方面表现出色，在多个基准测试中达到了与 GPT-4o 和 DeepSeek-V3-0324 相当的性能。",
  "thudm/glm-4-32b:free.description": "GLM-4-32B-0414 是一个 32B 双语（中英）开放权重语言模型，针对代码生成、函数调用和代理式任务进行了优化。它在 15T 高质量和重推理数据上进行了预训练，并使用人类偏好对齐、拒绝采样和强化学习进一步完善。该模型在复杂推理、工件生成和结构化输出任务方面表现出色，在多个基准测试中达到了与 GPT-4o 和 DeepSeek-V3-0324 相当的性能。",
  "thudm/glm-4-9b-chat.description": "智谱AI发布的GLM-4系列最新一代预训练模型的开源版本。",
  "thudm/glm-z1-32b.description": "GLM-Z1-32B-0414 是 GLM-4-32B 的增强推理变体，专为深度数学、逻辑和面向代码的问题解决而构建。它应用扩展强化学习（任务特定和基于通用成对偏好）来提高复杂多步骤任务的性能。与基础 GLM-4-32B 模型相比，Z1 显著提升了结构化推理和形式化领域的能力。\n\n该模型支持通过提示工程强制执行“思考”步骤，并为长格式输出提供改进的连贯性。它针对代理工作流进行了优化，并支持长上下文（通过 YaRN）、JSON 工具调用和用于稳定推理的细粒度采样配置。非常适合需要深思熟虑、多步骤推理或形式化推导的用例。",
  "thudm/glm-z1-rumination-32b.description": "GLM Z1 Rumination 32B 是 GLM-4-Z1 系列中的 32B 参数深度推理模型，针对需要长时间思考的复杂、开放式任务进行了优化。它建立在 glm-4-32b-0414 的基础上，增加了额外的强化学习阶段和多阶段对齐策略，引入了旨在模拟扩展认知处理的“反思”能力。这包括迭代推理、多跳分析和工具增强的工作流程，例如搜索、检索和引文感知合成。\n\n该模型在研究式写作、比较分析和复杂问答方面表现出色。它支持用于搜索和导航原语（`search`、`click`、`open`、`finish`）的函数调用，从而可以在代理式管道中使用。反思行为由具有基于规则的奖励塑造和延迟决策机制的多轮循环控制，并以 OpenAI 内部对齐堆栈等深度研究框架为基准。此变体适用于需要深度而非速度的场景。",
  "tngtech/deepseek-r1t-chimera:free.description": "DeepSeek-R1T-Chimera 通过合并 DeepSeek-R1 和 DeepSeek-V3 (0324) 创建，结合了 R1 的推理能力和 V3 的令牌效率改进。它基于 DeepSeek-MoE Transformer 架构，并针对通用文本生成任务进行了优化。\n\n该模型合并了两个源模型的预训练权重，以平衡推理、效率和指令遵循任务的性能。它根据 MIT 许可证发布，旨在用于研究和商业用途。",
  "togethercomputer/StripedHyena-Nous-7B.description": "StripedHyena Nous (7B) 通过高效的策略和模型架构，提供增强的计算能力。",
  "tts-1-hd.description": "最新的文本转语音模型，针对质量进行优化",
  "tts-1.description": "最新的文本转语音模型，针对实时场景优化速度",
  "upstage/SOLAR-10.7B-Instruct-v1.0.description": "Upstage SOLAR Instruct v1 (11B) 适用于精细化指令任务，提供出色的语言处理能力。",
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0.description": "Claude 3.5 Sonnet 提升了行业标准，性能超过竞争对手模型和 Claude 3 Opus，在广泛的评估中表现出色，同时具有我们中等层级模型的速度和成本。",
  "us.anthropic.claude-3-7-sonnet-20250219-v1:0.description": "Claude 3.7 sonnet 是 Anthropic 最快的下一代模型。与 Claude 3 Haiku 相比，Claude 3.7 Sonnet 在各项技能上都有所提升，并在许多智力基准测试中超越了上一代最大的模型 Claude 3 Opus。",
  "us.anthropic.claude-haiku-4-5-20251001-v1:0.description": "Claude Haiku 4.5 是 Anthropic 最快且最智能的 Haiku 模型，具有闪电般的速度和扩展思考能力。",
  "us.anthropic.claude-sonnet-4-5-20250929-v1:0.description": "Claude Sonnet 4.5 是 Anthropic 迄今为止最智能的模型。",
  "v0-1.0-md.description": "v0-1.0-md 模型是通过 v0 API 提供服务的旧版模型",
  "v0-1.5-lg.description": "v0-1.5-lg 模型适用于高级思考或推理任务",
  "v0-1.5-md.description": "v0-1.5-md 模型适用于日常任务和用户界面（UI）生成",
  "vercel/v0-1.0-md.description": "访问 v0 背后的模型以生成、修复和优化现代 Web 应用，具有特定框架的推理和最新知识。",
  "vercel/v0-1.5-md.description": "访问 v0 背后的模型以生成、修复和优化现代 Web 应用，具有特定框架的推理和最新知识。",
  "volcengine/doubao-seed-code.description": "Doubao-Seed-Code 是字节火山引擎面向 Agentic Programming 优化的大模型，在多项编程与代理基准上表现优异，支持 256K 上下文。",
  "wan2.2-t2i-flash.description": "万相2.2极速版，当前最新模型。在创意性、稳定性、写实质感上全面升级，生成速度快，性价比高。",
  "wan2.2-t2i-plus.description": "万相2.2专业版，当前最新模型。在创意性、稳定性、写实质感上全面升级，生成细节丰富。",
  "wanx-v1.description": "基础文生图模型。对应通义万相官网1.0通用模型。",
  "wanx2.0-t2i-turbo.description": "擅长质感人像，速度中等、成本较低。对应通义万相官网2.0极速模型。",
  "wanx2.1-t2i-plus.description": "全面升级版本。生成图像细节更丰富，速度稍慢。对应通义万相官网2.1专业模型。",
  "wanx2.1-t2i-turbo.description": "全面升级版本。生成速度快、效果全面、综合性价比高。对应通义万相官网2.1极速模型。",
  "whisper-1.description": "通用语音识别模型，支持多语言语音识别、语音翻译和语言识别。",
  "wizardlm2.description": "WizardLM 2 是微软AI提供的语言模型，在复杂对话、多语言、推理和智能助手领域表现尤为出色。",
  "wizardlm2:8x22b.description": "WizardLM 2 是微软AI提供的语言模型，在复杂对话、多语言、推理和智能助手领域表现尤为出色。",
  "x-ai/grok-4-fast-non-reasoning.description": "Grok 4 Fast（Non-Reasoning）是 xAI 的高吞吐、低成本多模态模型（支持 2M 上下文窗口），面向对延迟和成本敏感但不需要启用模型内推理的场景。它与 Grok 4 Fast 的 reasoning 版本并列，可通过 API 的 reasoning enable 参数在需要时开启推理功能。Prompts 和 completions 可能会被 xAI 或 OpenRouter 用于改进未来模型。",
  "x-ai/grok-4-fast.description": "Grok 4 Fast 是 xAI 的高吞吐、低成本模型（支持 2M 上下文窗口），适合需要高并发与长上下文的使用场景。",
  "x-ai/grok-4.1-fast-non-reasoning.description": "Grok 4 Fast（Non-Reasoning）是 xAI 的高吞吐、低成本多模态模型（支持 2M 上下文窗口），面向对延迟和成本敏感但不需要启用模型内推理的场景。它与 Grok 4 Fast 的 reasoning 版本并列，可通过 API 的 reasoning enable 参数在需要时开启推理功能。Prompts 和 completions 可能会被 xAI 或 OpenRouter 用于改进未来模型。",
  "x-ai/grok-4.1-fast.description": "Grok 4 Fast 是 xAI 的高吞吐、低成本模型（支持 2M 上下文窗口），适合需要高并发与长上下文的使用场景。",
  "x-ai/grok-4.description": "Grok 4 是 xAI 的旗舰推理模型，提供强大的推理与多模态能力。",
  "x-ai/grok-code-fast-1.description": "Grok Code Fast 1 是 xAI 的快速代码模型，输出具可读性与工程化适配。",
  "xai/grok-2-vision.description": "Grok 2 视觉模型在基于视觉的任务方面表现出色，在视觉数学推理 (MathVista) 和基于文档的问答 (DocVQA) 方面提供最先进的性能。它能够处理各种视觉信息，包括文档、图表、图表、屏幕截图和照片。",
  "xai/grok-2.description": "Grok 2 是一个具有最先进推理能力的前沿语言模型。它在聊天、编码和推理方面具有先进能力，在 LMSYS 排行榜上优于 Claude 3.5 Sonnet 和 GPT-4-Turbo。",
  "xai/grok-3-fast.description": "xAI 的旗舰模型，在企业用例方面表现出色，如数据提取、编码和文本摘要。在金融、医疗保健、法律和科学领域拥有深厚的领域知识。快速模型变体在更快的基础设施上提供服务，提供比标准快得多的响应时间。增加的速度以每个输出 token 更高的成本为代价。",
  "xai/grok-3-mini-fast.description": "xAI 的轻量级模型，在响应之前进行思考。非常适合不需要深厚领域知识的简单或基于逻辑的任务。原始思维轨迹可访问。快速模型变体在更快的基础设施上提供服务，提供比标准快得多的响应时间。增加的速度以每个输出 token 更高的成本为代价。",
  "xai/grok-3-mini.description": "xAI 的轻量级模型，在响应之前进行思考。非常适合不需要深厚领域知识的简单或基于逻辑的任务。原始思维轨迹可访问。",
  "xai/grok-3.description": "xAI 的旗舰模型，在企业用例方面表现出色，如数据提取、编码和文本摘要。在金融、医疗保健、法律和科学领域拥有深厚的领域知识。",
  "xai/grok-4.description": "xAI 最新和最伟大的旗舰模型，在自然语言、数学和推理方面提供无与伦比的性能——完美的全能选手。",
  "yi-large-fc.description": "在 yi-large 模型的基础上支持并强化了工具调用的能力，适用于各种需要搭建 agent 或 workflow 的业务场景。",
  "yi-large-preview.description": "初期版本，推荐使用 yi-large（新版本）。",
  "yi-large-rag.description": "基于 yi-large 超强模型的高阶服务，结合检索与生成技术提供精准答案，实时全网检索信息服务。",
  "yi-large-turbo.description": "超高性价比、卓越性能。根据性能和推理速度、成本，进行平衡性高精度调优。",
  "yi-large.description": "全新千亿参数模型，提供超强问答及文本生成能力。",
  "yi-lightning-lite.description": "轻量化版本，推荐使用 yi-lightning。",
  "yi-lightning.description": "最新高性能模型，保证高质量输出同时，推理速度大幅提升。",
  "yi-medium-200k.description": "200K 超长上下文窗口，提供长文本深度理解和生成能力。",
  "yi-medium.description": "中型尺寸模型升级微调，能力均衡，性价比高。深度优化指令遵循能力。",
  "yi-spark.description": "小而精悍，轻量极速模型。提供强化数学运算和代码编写能力。",
  "yi-vision-v2.description": "复杂视觉任务模型，提供基于多张图片的高性能理解、分析能力。",
  "yi-vision.description": "复杂视觉任务模型，提供高性能图片理解、分析能力。",
  "z-ai/glm-4.5-air.description": "GLM 4.5 Air 是 GLM 4.5 的轻量化版本，适合成本敏感场景但保留强推理能力。",
  "z-ai/glm-4.5.description": "GLM 4.5 是 Z.AI 的旗舰模型，支持混合推理模式并优化于工程与长上下文任务。",
  "z-ai/glm-4.6.description": "GLM 4.6 是 Z.AI 的旗舰模型，扩展了上下文长度和编码能力。",
  "zai-glm-4.6.description": "在编程与推理任务上表现优良，支持流式与工具调用，适合 agentic 编码与复杂推理场景。",
  "zai-org/GLM-4.5-Air.description": "GLM-4.5-Air 是一款专为智能体应用打造的基础模型，使用了混合专家（Mixture-of-Experts）架构。在工具调用、网页浏览、软件工程、前端编程领域进行了深度优化，支持无缝接入 Claude Code、Roo Code 等代码智能体中使用。GLM-4.5 采用混合推理模式，可以适应复杂推理和日常使用等多种应用场景。",
  "zai-org/GLM-4.5.description": "GLM-4.5 是一款专为智能体应用打造的基础模型，使用了混合专家（Mixture-of-Experts）架构。在工具调用、网页浏览、软件工程、前端编程领域进行了深度优化，支持无缝接入 Claude Code、Roo Code 等代码智能体中使用。GLM-4.5 采用混合推理模式，可以适应复杂推理和日常使用等多种应用场景。",
  "zai-org/GLM-4.5V.description": "GLM-4.5V 是由智谱 AI（Zhipu AI）发布的最新一代视觉语言模型（VLM）该模型基于拥有 106B 总参数和 12B 激活参数的旗舰文本模型 GLM-4.5-Air 构建，采用了混合专家（MoE）架构，旨在以更低的推理成本实现卓越性能 GLM-4.5V 在技术上延续了 GLM-4.1V-Thinking 的路线，并引入了三维旋转位置编码（3D-RoPE）等创新，显著增强了对三维空间关系的感知与推理能力。通过在预训练、监督微调和强化学习阶段的优化，该模型具备了处理图像、视频、长文档等多种视觉内容的能力，在 41 个公开的多模态基准测试中达到了同级别开源模型的顶尖水平此外，模型还新增了“思考模式”开关，允许用户在快速响应和深度推理之间灵活选择，以平衡效率与效果。",
  "zai-org/GLM-4.6.description": "与 GLM-4.5 相比，GLM-4.6 带来了多项关键改进。其上下文窗口从 128K 扩展到 200K tokens，使模型能够处理更复杂的智能体任务。模型在代码基准测试中取得了更高的分数，并在 Claude Code、Cline、Roo Code 和 Kilo Code 等应用中展现了更强的真实世界性能，包括在生成视觉效果精致的前端页面方面有所改进。GLM-4.6 在推理性能上表现出明显提升，并支持在推理过程中使用工具，从而带来了更强的综合能力。它在工具使用和基于搜索的智能体方面表现更强，并且能更有效地集成到智能体框架中。在写作方面，该模型在风格和可读性上更符合人类偏好，并在角色扮演场景中表现得更自然。",
  "zai/glm-4.5-air.description": "GLM-4.5 和 GLM-4.5-Air 是我们最新的旗舰模型，专门设计为面向代理应用的基础模型。两者都利用混合专家 (MoE) 架构。GLM-4.5 的总参数数为 3550 亿，每次前向传递有 320 亿活跃参数，而 GLM-4.5-Air 采用更简化的设计，总参数数为 1060 亿，活跃参数为 120 亿。",
  "zai/glm-4.5.description": "GLM-4.5 系列模型是专门为智能体设计的基础模型。旗舰 GLM-4.5 集成了 3550 亿总参数（320 亿活跃），统一了推理、编码和代理能力以解决复杂的应用需求。作为混合推理系统，它提供双重操作模式。",
  "zai/glm-4.5v.description": "GLM-4.5V 基于 GLM-4.5-Air 基础模型构建，继承了 GLM-4.1V-Thinking 的经过验证的技术，同时通过强大的 1060 亿参数 MoE 架构实现了有效的扩展。",
  "zenmux/auto.description": "ZenMux 的自动路由功能会根据你的请求内容，在支持的模型中自动选择当前性价比最高、表现最好的模型。"
}
