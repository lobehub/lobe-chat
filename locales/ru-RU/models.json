{
  "01-ai/Yi-1.5-34B-Chat-16K": {
    "description": "Yi-1.5 34B, с богатым набором обучающих образцов, демонстрирует превосходные результаты в отраслевых приложениях."
  },
  "01-ai/Yi-1.5-6B-Chat": {
    "description": "Yi-1.5-6B-Chat — это вариант серии Yi-1.5, относящийся к открытым моделям для чата. Yi-1.5 является обновленной версией Yi, которая была непрерывно предобучена на 500B высококачественных корпусах и дообучена на более чем 3M разнообразных образцах. По сравнению с Yi, Yi-1.5 демонстрирует более сильные способности в кодировании, математике, выводах и соблюдении инструкций, сохраняя при этом отличные навыки понимания языка, логического вывода и понимания прочитанного. Эта модель имеет версии с длиной контекста 4K, 16K и 32K, с общим объемом предобучения 3.6T токенов."
  },
  "01-ai/Yi-1.5-9B-Chat-16K": {
    "description": "Yi-1.5 9B поддерживает 16K токенов, обеспечивая эффективные и плавные возможности генерации языка."
  },
  "360gpt-pro": {
    "description": "360GPT Pro, как важный член серии моделей AI от 360, удовлетворяет разнообразные приложения обработки текста с высокой эффективностью, поддерживает понимание длинных текстов и многораундные диалоги."
  },
  "360gpt-turbo": {
    "description": "360GPT Turbo предлагает мощные вычислительные и диалоговые возможности, обладает выдающимся пониманием семантики и эффективностью генерации, что делает его идеальным решением для интеллектуальных помощников для предприятий и разработчиков."
  },
  "360gpt-turbo-responsibility-8k": {
    "description": "360GPT Turbo Responsibility 8K акцентирует внимание на семантической безопасности и ответственности, специально разработан для приложений с высокими требованиями к безопасности контента, обеспечивая точность и надежность пользовательского опыта."
  },
  "360gpt2-pro": {
    "description": "360GPT2 Pro — это продвинутая модель обработки естественного языка, выпущенная компанией 360, обладающая выдающимися способностями к генерации и пониманию текста, особенно в области генерации и творчества, способная обрабатывать сложные языковые преобразования и ролевые задачи."
  },
  "4.0Ultra": {
    "description": "Spark4.0 Ultra — это самая мощная версия в серии больших моделей Xinghuo, которая, обновив сетевые поисковые связи, улучшает понимание и обобщение текстового контента. Это всестороннее решение для повышения производительности в офисе и точного реагирования на запросы, являющееся ведущим интеллектуальным продуктом в отрасли."
  },
  "@cf/meta/llama-3-8b-instruct-awq": {},
  "@cf/openchat/openchat-3.5-0106": {},
  "@cf/qwen/qwen1.5-14b-chat-awq": {},
  "@hf/google/gemma-7b-it": {},
  "@hf/meta-llama/meta-llama-3-8b-instruct": {
    "description": "Поколение за поколением, Meta Llama 3 демонстрирует передовые результаты по широкому спектру отраслевых стандартов и предлагает новые возможности, включая улучшенное логическое мышление."
  },
  "@hf/mistral/mistral-7b-instruct-v0.2": {},
  "@hf/nexusflow/starling-lm-7b-beta": {},
  "@hf/nousresearch/hermes-2-pro-mistral-7b": {},
  "@hf/thebloke/deepseek-coder-6.7b-instruct-awq": {},
  "@hf/thebloke/neural-chat-7b-v3-1-awq": {},
  "@hf/thebloke/openhermes-2.5-mistral-7b-awq": {},
  "@hf/thebloke/zephyr-7b-beta-awq": {},
  "Baichuan2-Turbo": {
    "description": "Использует технологии улучшенного поиска для полной связи между большой моделью и отраслевыми знаниями, а также знаниями из сети. Поддерживает загрузку различных документов, таких как PDF и Word, а также ввод URL, обеспечивая своевременное и полное получение информации с точными и профессиональными результатами."
  },
  "Baichuan3-Turbo": {
    "description": "Оптимизирован для высокочастотных корпоративных сценариев, значительно улучшает результаты и предлагает высокую стоимость. По сравнению с моделью Baichuan2, создание контента увеличилось на 20%, ответы на вопросы на 17%, а способности ролевого взаимодействия на 40%. Общая эффективность лучше, чем у GPT3.5."
  },
  "Baichuan3-Turbo-128k": {
    "description": "Обладает 128K сверхдлинным контекстным окном, оптимизированным для высокочастотных корпоративных сценариев, значительно улучшает результаты и предлагает высокую стоимость. По сравнению с моделью Baichuan2, создание контента увеличилось на 20%, ответы на вопросы на 17%, а способности ролевого взаимодействия на 40%. Общая эффективность лучше, чем у GPT3.5."
  },
  "Baichuan4": {
    "description": "Модель обладает лучшими возможностями в стране, превосходя зарубежные модели в задачах на знание, длинные тексты и генерацию контента. Также обладает передовыми мультимодальными возможностями и показывает отличные результаты в нескольких авторитетных тестах."
  },
  "Baichuan4-Air": {
    "description": "Модель обладает лучшими в стране возможностями, превосходя зарубежные модели в задачах на китайском языке, таких как энциклопедические знания, длинные тексты и генерация контента. Также обладает передовыми мультимодальными возможностями и демонстрирует отличные результаты в нескольких авторитетных оценочных тестах."
  },
  "Baichuan4-Turbo": {
    "description": "Модель обладает лучшими в стране возможностями, превосходя зарубежные модели в задачах на китайском языке, таких как энциклопедические знания, длинные тексты и генерация контента. Также обладает передовыми мультимодальными возможностями и демонстрирует отличные результаты в нескольких авторитетных оценочных тестах."
  },
  "Doubao-lite-128k": {
    "description": "Doubao-lite обеспечивает выдающуюся скорость отклика и лучшее соотношение цены и качества, предлагая клиентам больше гибкости в различных сценариях. Поддерживает вывод и настройку с 128k контекстным окном."
  },
  "Doubao-lite-32k": {
    "description": "Doubao-lite обеспечивает выдающуюся скорость отклика и лучшее соотношение цены и качества, предлагая клиентам больше гибкости в различных сценариях. Поддерживает вывод и настройку с 32k контекстным окном."
  },
  "Doubao-lite-4k": {
    "description": "Doubao-lite обеспечивает выдающуюся скорость отклика и лучшее соотношение цены и качества, предлагая клиентам больше гибкости в различных сценариях. Поддерживает вывод и настройку с 4k контекстным окном."
  },
  "Doubao-pro-128k": {
    "description": "Модель основных характеристик с лучшими показателями, подходит для обработки сложных задач. Хорошо справляется с задачами референсного ответа, резюмирования, творчества, классификации текста, ролевого взаимодействия и т.д. Поддерживает вывод и настройку с 128k контекстным окном."
  },
  "Doubao-pro-32k": {
    "description": "Модель основных характеристик с лучшими показателями, подходит для обработки сложных задач. Хорошо справляется с задачами референсного ответа, резюмирования, творчества, классификации текста, ролевого взаимодействия и т.д. Поддерживает вывод и настройку с 32k контекстным окном."
  },
  "Doubao-pro-4k": {
    "description": "Модель основных характеристик с лучшими показателями, подходит для обработки сложных задач. Хорошо справляется с задачами референсного ответа, резюмирования, творчества, классификации текста, ролевого взаимодействия и т.д. Поддерживает вывод и настройку с 4k контекстным окном."
  },
  "ERNIE-3.5-128K": {
    "description": "Флагманская крупномасштабная языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными возможностями, способная удовлетворить большинство требований к диалоговым ответам, генерации контента и сценариям использования плагинов; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-3.5-8K": {
    "description": "Флагманская крупномасштабная языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными возможностями, способная удовлетворить большинство требований к диалоговым ответам, генерации контента и сценариям использования плагинов; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-3.5-8K-Preview": {
    "description": "Флагманская крупномасштабная языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными возможностями, способная удовлетворить большинство требований к диалоговым ответам, генерации контента и сценариям использования плагинов; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-4.0-8K-Latest": {
    "description": "Флагманская сверхкрупномасштабная языковая модель, разработанная Baidu, которая по сравнению с ERNIE 3.5 обеспечивает полное обновление возможностей модели и широко применяется в сложных задачах в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-4.0-8K-Preview": {
    "description": "Флагманская сверхкрупномасштабная языковая модель, разработанная Baidu, которая по сравнению с ERNIE 3.5 обеспечивает полное обновление возможностей модели и широко применяется в сложных задачах в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-4.0-Turbo-128K": {
    "description": "Флагманская модель Baidu, разработанная самостоятельно, с огромным масштабом, демонстрирует отличные результаты и широко применяется в сложных задачах в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах. По сравнению с ERNIE 4.0, демонстрирует лучшие результаты."
  },
  "ERNIE-4.0-Turbo-8K-Latest": {
    "description": "Флагманская 超大型 языковая модель, разработанная Baidu, демонстрирует отличные результаты и хорошо подходит для сложных задач в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая своевременность ответов. По сравнению с ERNIE 4.0 имеет лучшие показатели производительности."
  },
  "ERNIE-4.0-Turbo-8K-Preview": {
    "description": "Флагманская сверхкрупномасштабная языковая модель, разработанная Baidu, демонстрирующая отличные результаты в комплексной эффективности, широко применяемая в сложных задачах в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах. По сравнению с ERNIE 4.0, она демонстрирует лучшие показатели производительности."
  },
  "ERNIE-Character-8K": {
    "description": "Специализированная языковая модель, разработанная Baidu для вертикальных сценариев, подходящая для применения в играх (NPC), диалогах службы поддержки, ролевых играх и других сценариях, обладающая ярко выраженным и согласованным стилем персонажей, высокой способностью следовать инструкциям и отличной производительностью вывода."
  },
  "ERNIE-Lite-Pro-128K": {
    "description": "Легковесная языковая модель, разработанная Baidu, которая сочетает в себе отличные результаты модели и производительность вывода, превосходя ERNIE Lite, подходит для использования в системах с низкой вычислительной мощностью."
  },
  "ERNIE-Speed-128K": {
    "description": "Новая высокопроизводительная языковая модель, разработанная Baidu в 2024 году, обладающая выдающимися универсальными возможностями, подходит для использования в качестве базовой модели для тонкой настройки, лучше справляясь с задачами в специфических сценариях, при этом обладая отличной производительностью вывода."
  },
  "ERNIE-Speed-Pro-128K": {
    "description": "Новая высокопроизводительная языковая модель, разработанная Baidu в 2024 году, обладающая выдающимися универсальными возможностями, превосходящая ERNIE Speed, подходит для использования в качестве базовой модели для тонкой настройки, лучше справляясь с задачами в специфических сценариях, при этом обладая отличной производительностью вывода."
  },
  "Gryphe/MythoMax-L2-13b": {
    "description": "MythoMax-L2 (13B) — это инновационная модель, подходящая для многообластных приложений и сложных задач."
  },
  "InternVL2-8B": {
    "description": "InternVL2-8B — это мощная визуально-языковая модель, поддерживающая многомодальную обработку изображений и текста, способная точно распознавать содержимое изображений и генерировать соответствующие описания или ответы."
  },
  "InternVL2.5-26B": {
    "description": "InternVL2.5-26B — это мощная визуально-языковая модель, поддерживающая многомодальную обработку изображений и текста, способная точно распознавать содержимое изображений и генерировать соответствующие описания или ответы."
  },
  "LoRA/Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 72B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "LoRA/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 7B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Hermes 2 Mixtral 8x7B DPO — это высокоадаптивная многомодельная комбинация, предназначенная для предоставления выдающегося творческого опыта."
  },
  "NousResearch/Hermes-3-Llama-3.1-8B": {},
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) — это высокоточная модель команд, подходящая для сложных вычислений."
  },
  "OpenGVLab/InternVL2-26B": {
    "description": "InternVL2 демонстрирует превосходные результаты в различных визуально-языковых задачах, включая понимание документов и графиков, понимание текстов сцены, OCR, решение научных и математических задач."
  },
  "OpenGVLab/InternVL2-Llama3-76B": {
    "description": "InternVL2 демонстрирует превосходные результаты в различных визуально-языковых задачах, включая понимание документов и графиков, понимание текстов сцены, OCR, решение научных и математических задач."
  },
  "Phi-3-medium-128k-instruct": {
    "description": "Та же модель Phi-3-medium, но с большим размером контекста для RAG или нескольких подсказок."
  },
  "Phi-3-medium-4k-instruct": {
    "description": "Модель с 14B параметрами, демонстрирующая лучшее качество, чем Phi-3-mini, с акцентом на высококачественные, насыщенные рассуждениями данные."
  },
  "Phi-3-mini-128k-instruct": {
    "description": "Та же модель Phi-3-mini, но с большим размером контекста для RAG или нескольких подсказок."
  },
  "Phi-3-mini-4k-instruct": {
    "description": "Самая маленькая модель в семействе Phi-3. Оптимизирована как для качества, так и для низкой задержки."
  },
  "Phi-3-small-128k-instruct": {
    "description": "Та же модель Phi-3-small, но с большим размером контекста для RAG или нескольких подсказок."
  },
  "Phi-3-small-8k-instruct": {
    "description": "Модель с 7B параметрами, демонстрирующая лучшее качество, чем Phi-3-mini, с акцентом на высококачественные, насыщенные рассуждениями данные."
  },
  "Phi-3.5-mini-instruct": {
    "description": "Обновленная версия модели Phi-3-mini."
  },
  "Phi-3.5-vision-instrust": {
    "description": "Обновленная версия модели Phi-3-vision."
  },
  "Pro/OpenGVLab/InternVL2-8B": {
    "description": "InternVL2 демонстрирует превосходные результаты в различных визуально-языковых задачах, включая понимание документов и графиков, понимание текстов сцены, OCR, решение научных и математических задач."
  },
  "Pro/Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 1.5B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей. По сравнению с Qwen1.5-1.8B-Chat, Qwen2-1.5B-Instruct демонстрирует значительное улучшение производительности в тестах MMLU, HumanEval, GSM8K, C-Eval и IFEval, несмотря на немного меньшее количество параметров."
  },
  "Pro/Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-7B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 7B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она может обрабатывать большие объемы входных данных. Эта модель показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей и демонстрируя конкурентоспособность с проприетарными моделями в некоторых задачах. Qwen2-7B-Instruct показывает значительное улучшение производительности в нескольких оценках по сравнению с Qwen1.5-7B-Chat."
  },
  "Pro/Qwen/Qwen2-VL-7B-Instruct": {
    "description": "Qwen2-VL - это последняя версия модели Qwen-VL, которая достигла передовых результатов в тестировании визуального понимания."
  },
  "Pro/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 7B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct — это последняя версия серии языковых моделей, специфичных для кода, выпущенная Alibaba Cloud. Эта модель значительно улучшила способности генерации кода, вывода и исправления на основе Qwen2.5, обучаясь на 5.5 триллионах токенов. Она не только усилила кодирование, но и сохранила преимущества в математике и общих способностях. Модель предоставляет более полную основу для практических приложений, таких как интеллектуальные агенты кода."
  },
  "Pro/THUDM/glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat — это открытая версия предобученной модели из серии GLM-4, выпущенная Zhizhu AI. Эта модель показывает отличные результаты в семантике, математике, выводах, коде и знаниях. Кроме поддержки многократных диалогов, GLM-4-9B-Chat также обладает продвинутыми функциями, такими как веб-браузинг, выполнение кода, вызов пользовательских инструментов (Function Call) и вывод длинных текстов. Модель поддерживает 26 языков, включая китайский, английский, японский, корейский и немецкий. В нескольких бенчмарках GLM-4-9B-Chat демонстрирует отличные результаты, такие как AlignBench-v2, MT-Bench, MMLU и C-Eval. Эта модель поддерживает максимальную длину контекста 128K и подходит для академических исследований и коммерческих приложений."
  },
  "Pro/google/gemma-2-9b-it": {
    "description": "Gemma — это одна из легковесных, передовых открытых моделей, разработанных Google. Это крупная языковая модель с только декодером, поддерживающая английский язык, предлагающая открытые веса, предобученные варианты и варианты с дообучением на инструкциях. Модель Gemma подходит для различных задач генерации текста, включая вопросы и ответы, резюме и выводы. Эта 9B модель была обучена на 8 триллионах токенов. Ее относительно небольшой размер позволяет развертывать ее в условиях ограниченных ресурсов, таких как ноутбуки, настольные компьютеры или ваша собственная облачная инфраструктура, что позволяет большему количеству людей получить доступ к передовым моделям ИИ и способствовать инновациям."
  },
  "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "description": "Meta Llama 3.1 — это семейство многоязычных крупных языковых моделей, разработанных Meta, включая предобученные и дообученные на инструкциях варианты с параметрами 8B, 70B и 405B. Эта 8B модель с дообучением на инструкциях оптимизирована для многоязычных диалоговых сценариев и показывает отличные результаты в нескольких отраслевых бенчмарках. Обучение модели использовало более 150 триллионов токенов открытых данных и применяло такие технологии, как контролируемое дообучение и обучение с подкреплением на основе человеческой обратной связи для повышения полезности и безопасности модели. Llama 3.1 поддерживает генерацию текста и кода, с датой окончания знаний в декабре 2023 года."
  },
  "Qwen/QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview — это последняя экспериментальная исследовательская модель Qwen, сосредоточенная на повышении возможностей вывода ИИ. Исследуя сложные механизмы, такие как смешение языков и рекурсивные выводы, основные преимущества включают мощные аналитические способности, математические и программные навыки. В то же время существуют проблемы с переключением языков, циклом вывода, соображениями безопасности и различиями в других способностях."
  },
  "Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 1.5B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей. По сравнению с Qwen1.5-1.8B-Chat, Qwen2-1.5B-Instruct демонстрирует значительное улучшение производительности в тестах MMLU, HumanEval, GSM8K, C-Eval и IFEval, несмотря на немного меньшее количество параметров."
  },
  "Qwen/Qwen2-72B-Instruct": {
    "description": "Qwen2 — это передовая универсальная языковая модель, поддерживающая множество типов команд."
  },
  "Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-72B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 72B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она может обрабатывать большие объемы входных данных. Эта модель показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей и демонстрируя конкурентоспособность с проприетарными моделями в некоторых задачах."
  },
  "Qwen/Qwen2-VL-72B-Instruct": {
    "description": "Qwen2-VL - это последняя версия модели Qwen-VL, которая достигла передовых результатов в тестировании визуального понимания."
  },
  "Qwen/Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5 — это новая серия крупных языковых моделей, предназначенная для оптимизации обработки инструктивных задач."
  },
  "Qwen/Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5 — это новая серия крупных языковых моделей, предназначенная для оптимизации обработки инструктивных задач."
  },
  "Qwen/Qwen2.5-72B-Instruct": {
    "description": "Большая языковая модель, разработанная командой Alibaba Cloud Tongyi Qianwen."
  },
  "Qwen/Qwen2.5-72B-Instruct-128K": {
    "description": "Qwen2.5 - это новая серия крупных языковых моделей с улучшенными способностями понимания и генерации."
  },
  "Qwen/Qwen2.5-72B-Instruct-Turbo": {
    "description": "Qwen2.5 - это новая серия крупных языковых моделей, нацеленная на оптимизацию обработки задач с инструкциями."
  },
  "Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5 — это новая серия крупных языковых моделей, предназначенная для оптимизации обработки инструктивных задач."
  },
  "Qwen/Qwen2.5-7B-Instruct-Turbo": {
    "description": "Qwen2.5 - это новая серия крупных языковых моделей, нацеленная на оптимизацию обработки задач с инструкциями."
  },
  "Qwen/Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder сосредоточен на написании кода."
  },
  "Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct — это последняя версия серии языковых моделей, специфичных для кода, выпущенная Alibaba Cloud. Эта модель значительно улучшила способности генерации кода, вывода и исправления на основе Qwen2.5, обучаясь на 5.5 триллионах токенов. Она не только усилила кодирование, но и сохранила преимущества в математике и общих способностях. Модель предоставляет более полную основу для практических приложений, таких как интеллектуальные агенты кода."
  },
  "Qwen/Qwen2.5-Math-72B-Instruct": {
    "description": "Qwen2.5-Math сосредоточен на решении математических задач, предоставляя профессиональные ответы на сложные вопросы."
  },
  "Qwen2-72B-Instruct": {
    "description": "Qwen2 — это последняя серия моделей Qwen, поддерживающая контекст до 128k. По сравнению с текущими лучшими открытыми моделями, Qwen2-72B значительно превосходит ведущие модели по многим аспектам, включая понимание естественного языка, знания, код, математику и многоязычность."
  },
  "Qwen2-7B-Instruct": {
    "description": "Qwen2 — это последняя серия моделей Qwen, способная превосходить лучшие открытые модели сопоставимого размера и даже более крупные модели. Qwen2 7B демонстрирует значительные преимущества в нескольких тестах, особенно в понимании кода и китайского языка."
  },
  "Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5-14B-Instruct — это языковая модель с 14 миллиардами параметров, с отличными показателями производительности, оптимизированная для китайского и многоязычного контекста, поддерживает интеллектуальные ответы, генерацию контента и другие приложения."
  },
  "Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5-32B-Instruct — это языковая модель с 32 миллиардами параметров, с сбалансированными показателями производительности, оптимизированная для китайского и многоязычного контекста, поддерживает интеллектуальные ответы, генерацию контента и другие приложения."
  },
  "Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct поддерживает контекст до 16k, генерируя длинные тексты более 8K. Поддерживает вызовы функций и бесшовное взаимодействие с внешними системами, что значительно повышает гибкость и масштабируемость. Знания модели значительно увеличены, а способности в кодировании и математике значительно улучшены, поддерживает более 29 языков."
  },
  "Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct — это языковая модель с 7 миллиардами параметров, поддерживающая вызовы функций и бесшовное взаимодействие с внешними системами, что значительно повышает гибкость и масштабируемость. Оптимизирована для китайского и многоязычного контекста, поддерживает интеллектуальные ответы, генерацию контента и другие приложения."
  },
  "Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder-32B-Instruct — это крупная языковая модель, специально разработанная для генерации кода, понимания кода и эффективных сценариев разработки, с передовым масштабом параметров 32B, способная удовлетворить разнообразные потребности программирования."
  },
  "SenseChat": {
    "description": "Базовая версия модели (V4), длина контекста 4K, обладает мощными универсальными возможностями."
  },
  "SenseChat-128K": {
    "description": "Базовая версия модели (V4), длина контекста 128K, демонстрирует отличные результаты в задачах понимания и генерации длинных текстов."
  },
  "SenseChat-32K": {
    "description": "Базовая версия модели (V4), длина контекста 32K, гибко применяется в различных сценариях."
  },
  "SenseChat-5": {
    "description": "Последняя версия модели (V5.5), длина контекста 128K, значительно улучшенные способности в математическом рассуждении, английских диалогах, следовании инструкциям и понимании длинных текстов, сопоставимые с GPT-4o."
  },
  "SenseChat-5-Cantonese": {
    "description": "Длина контекста 32K, превосходит GPT-4 в понимании диалогов на кантонском, сопоставим с GPT-4 Turbo в таких областях, как знания, рассуждение, математика и написание кода."
  },
  "SenseChat-Character": {
    "description": "Стандартная версия модели, длина контекста 8K, высокая скорость отклика."
  },
  "SenseChat-Character-Pro": {
    "description": "Расширенная версия модели, длина контекста 32K, всеобъемлющие улучшения возможностей, поддерживает диалоги на китайском и английском языках."
  },
  "SenseChat-Turbo": {
    "description": "Подходит для быстрого ответа на вопросы и сценариев тонкой настройки модели."
  },
  "Skylark2-lite-8k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-lite имеет высокую скорость отклика, подходит для сценариев с высокими требованиями к оперативности, чувствительных к стоимости и с не такими высокими требованиями к точности модели. Длина контекстного окна составляет 8k."
  },
  "Skylark2-pro-32k": {
    "description": "Модель второго поколения Skylark (云雀), версия Skylark2-pro имеет высокую точность модели, подходит для более сложных сценариев генерации текста, таких как написание специализированной документации, создание романов, высококачественный перевод и т.д. Длина контекстного окна составляет 32k."
  },
  "Skylark2-pro-4k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-pro имеет высокую точность, подходит для более сложных сценариев генерации текста, таких как специализированная документация, создание романов, высококачественный перевод и т.д. Длина контекстного окна составляет 4k."
  },
  "Skylark2-pro-character-4k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-pro-character демонстрирует выдающиеся способности к ролевым взаимодействиям и чатам, умеет играть различные роли в зависимости от требований пользователя, что делает общение естественным и плавным. Подходит для разработки чат-ботов, виртуальных помощников и онлайн-сервисов с высокой скоростью отклика."
  },
  "Skylark2-pro-turbo-8k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-pro-turbo-8k обеспечивает более быструю обработку и сниженные затраты, длина контекстного окна составляет 8k."
  },
  "THUDM/chatglm3-6b": {
    "description": "ChatGLM3-6B — это открытая модель из серии ChatGLM, разработанная Zhizhu AI. Эта модель сохраняет отличные характеристики предыдущих моделей, такие как плавность диалога и низкий порог развертывания, одновременно вводя новые функции. Она использует более разнообразные обучающие данные, большее количество шагов обучения и более разумную стратегию обучения, показывая отличные результаты среди предобученных моделей объемом менее 10B. ChatGLM3-6B поддерживает многократные диалоги, вызовы инструментов, выполнение кода и задачи агента в сложных сценариях. Кроме диалоговой модели, также открыты базовая модель ChatGLM-6B-Base и модель для длинных текстовых диалогов ChatGLM3-6B-32K. Эта модель полностью открыта для академических исследований и также допускает бесплатное коммерческое использование после регистрации."
  },
  "THUDM/glm-4-9b-chat": {
    "description": "GLM-4 9B — это открытая версия, обеспечивающая оптимизированный диалоговый опыт для приложений."
  },
  "TeleAI/TeleChat2": {
    "description": "Модель TeleChat2 была разработана China Telecom с нуля и представляет собой генеративную семантическую модель, поддерживающую функции вопросов и ответов, генерации кода, генерации длинных текстов и т.д., предоставляя пользователям услуги консультаций в диалоговом формате, способную взаимодействовать с пользователями, отвечать на вопросы, помогать в творчестве и эффективно помогать пользователям получать информацию, знания и вдохновение. Модель показывает отличные результаты в решении проблем с галлюцинациями, генерацией длинных текстов и логическим пониманием."
  },
  "TeleAI/TeleMM": {
    "description": "Модель TeleMM — это многомодальная модель, разработанная China Telecom, способная обрабатывать текстовые, графические и другие виды входных данных, поддерживающая функции понимания изображений, анализа графиков и т.д., предоставляя пользователям услуги понимания на разных модальностях. Модель может взаимодействовать с пользователями в многомодальном формате, точно понимая входной контент, отвечая на вопросы, помогая в творчестве и эффективно предоставляя многомодальную информацию и поддержку вдохновения. Она показывает отличные результаты в задачах многомодального восприятия и логического вывода."
  },
  "Tencent/Hunyuan-A52B-Instruct": {
    "description": "Hunyuan-Large — это крупнейшая в отрасли открытая модель Transformer архитектуры MoE с общим количеством параметров 389 миллиардов и 52 миллиарда активных параметров."
  },
  "Vendor-A/Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-72B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 72B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она может обрабатывать большие объемы входных данных. Эта модель показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей и демонстрируя конкурентоспособность с проприетарными моделями в некоторых задачах."
  },
  "Vendor-A/Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 72B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "Yi-34B-Chat": {
    "description": "Yi-1.5-34B, сохраняя выдающиеся универсальные языковые способности оригинальной серии моделей, значительно улучшила математическую логику и способности к кодированию благодаря инкрементальному обучению на 500 миллиардов высококачественных токенов."
  },
  "abab5.5-chat": {
    "description": "Ориентирован на производственные сценарии, поддерживает обработку сложных задач и эффективную генерацию текста, подходит для профессиональных приложений."
  },
  "abab5.5s-chat": {
    "description": "Специально разработан для диалогов на китайском языке, обеспечивая высококачественную генерацию диалогов на китайском, подходит для различных приложений."
  },
  "abab6.5g-chat": {
    "description": "Специально разработан для многоязычных диалогов, поддерживает высококачественную генерацию диалогов на английском и других языках."
  },
  "abab6.5s-chat": {
    "description": "Подходит для широкого спектра задач обработки естественного языка, включая генерацию текста, диалоговые системы и т.д."
  },
  "abab6.5t-chat": {
    "description": "Оптимизирован для диалогов на китайском языке, обеспечивая плавную генерацию диалогов, соответствующую китайским языковым привычкам."
  },
  "accounts/fireworks/models/firefunction-v1": {
    "description": "Открытая модель вызова функций от Fireworks, обеспечивающая выдающиеся возможности выполнения команд и открытые настраиваемые функции."
  },
  "accounts/fireworks/models/firefunction-v2": {
    "description": "Firefunction-v2 от компании Fireworks — это высокопроизводительная модель вызова функций, разработанная на основе Llama-3 и оптимизированная для вызова функций, диалогов и выполнения команд."
  },
  "accounts/fireworks/models/firellava-13b": {
    "description": "fireworks-ai/FireLLaVA-13b — это визуальная языковая модель, способная одновременно обрабатывать изображения и текстовые вводы, обученная на высококачественных данных, подходящая для мультимодальных задач."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct": {
    "description": "Модель Llama 3 70B для команд, специально оптимизированная для многоязычных диалогов и понимания естественного языка, превосходит большинство конкурентных моделей."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct-hf": {
    "description": "Модель Llama 3 70B для команд (HF версия), результаты которой совпадают с официальной реализацией, подходит для высококачественных задач выполнения команд."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct": {
    "description": "Модель Llama 3 8B для команд, оптимизированная для диалогов и многоязычных задач, демонстрирует выдающиеся и эффективные результаты."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct-hf": {
    "description": "Модель Llama 3 8B для команд (HF версия), результаты которой совпадают с официальной реализацией, обладает высокой согласованностью и совместимостью между платформами."
  },
  "accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "description": "Модель Llama 3.1 405B для команд, обладающая огромным количеством параметров, подходит для сложных задач и сценариев с высокой нагрузкой."
  },
  "accounts/fireworks/models/llama-v3p1-70b-instruct": {
    "description": "Модель Llama 3.1 70B для команд, обеспечивающая выдающиеся возможности понимания и генерации естественного языка, является идеальным выбором для диалоговых и аналитических задач."
  },
  "accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "description": "Модель Llama 3.1 8B для команд, оптимизированная для многоязычных диалогов, способная превосходить большинство открытых и закрытых моделей по общим отраслевым стандартам."
  },
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct": {
    "description": "Модель Meta с 11B параметрами, оптимизированная для вывода изображений. Эта модель предназначена для визуального распознавания, вывода изображений, описания изображений и ответа на общие вопросы о изображениях. Эта модель способна понимать визуальные данные, такие как графики и диаграммы, и преодолевать разрыв между визуальным и языковым пониманием, генерируя текстовые описания деталей изображений."
  },
  "accounts/fireworks/models/llama-v3p2-1b-instruct": {
    "description": "Модель Llama 3.2 1B для инструкций - это компактная многоязычная модель, запущенная Meta. Эта модель предназначена для повышения эффективности и обеспечивает значительное улучшение в задержке и стоимости по сравнению с более крупными моделями. Примеры использования модели включают извлечение информации и резюме."
  },
  "accounts/fireworks/models/llama-v3p2-3b-instruct": {
    "description": "Модель Llama 3.2 3B для инструкций - это компактная многоязычная модель, запущенная Meta. Эта модель предназначена для повышения эффективности и обеспечивает значительное улучшение в задержке и стоимости по сравнению с более крупными моделями. Примеры использования модели включают запросы, переоформление подсказок и помощь в написании."
  },
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct": {
    "description": "Модель Meta с 90B параметрами, оптимизированная для вывода изображений. Эта модель предназначена для визуального распознавания, вывода изображений, описания изображений и ответа на общие вопросы о изображениях. Эта модель способна понимать визуальные данные, такие как графики и диаграммы, и преодолевать разрыв между визуальным и языковым пониманием, генерируя текстовые описания деталей изображений."
  },
  "accounts/fireworks/models/mixtral-8x22b-instruct": {
    "description": "Mixtral MoE 8x22B для команд, с большим количеством параметров и архитектурой с несколькими экспертами, всесторонне поддерживает эффективную обработку сложных задач."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct": {
    "description": "Mixtral MoE 8x7B для команд, архитектура с несколькими экспертами обеспечивает эффективное выполнение и следование командам."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct-hf": {
    "description": "Mixtral MoE 8x7B для команд (HF версия), производительность которой совпадает с официальной реализацией, подходит для множества эффективных задач."
  },
  "accounts/fireworks/models/mythomax-l2-13b": {
    "description": "Модель MythoMax L2 13B, использующая новые технологии объединения, хорошо подходит для повествования и ролевых игр."
  },
  "accounts/fireworks/models/phi-3-vision-128k-instruct": {
    "description": "Phi 3 Vision для команд, легковесная мультимодальная модель, способная обрабатывать сложную визуальную и текстовую информацию, обладая высокой способностью к выводу."
  },
  "accounts/fireworks/models/qwen-qwq-32b-preview": {
    "description": "Модель QwQ — это экспериментальная исследовательская модель, разработанная командой Qwen, сосредоточенная на улучшении возможностей вывода ИИ."
  },
  "accounts/fireworks/models/qwen2p5-72b-instruct": {
    "description": "Qwen2.5 - это серия языковых моделей, содержащая только декодеры, разработанная командой Qwen от Alibaba Cloud. Эти модели предлагаются в различных размерах: 0.5B, 1.5B, 3B, 7B, 14B, 32B и 72B, с вариантами базовой и инструкционной версии."
  },
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "description": "Qwen2.5 Coder 32B Instruct — это последняя версия серии языковых моделей, специфичных для кода, выпущенная Alibaba Cloud. Эта модель значительно улучшила способности генерации кода, вывода и исправления на основе Qwen2.5, обучаясь на 5.5 триллионах токенов. Она не только усилила кодирование, но и сохранила преимущества в математике и общих способностях. Модель предоставляет более полную основу для практических приложений, таких как интеллектуальные агенты кода."
  },
  "accounts/fireworks/models/starcoder-16b": {
    "description": "Модель StarCoder 15.5B, поддерживающая сложные задачи программирования, с улучшенными многоязычными возможностями, подходит для генерации и понимания сложного кода."
  },
  "accounts/fireworks/models/starcoder-7b": {
    "description": "Модель StarCoder 7B, обученная на более чем 80 языках программирования, обладает выдающимися способностями к заполнению кода и пониманию контекста."
  },
  "accounts/yi-01-ai/models/yi-large": {
    "description": "Модель Yi-Large, обладающая выдающимися возможностями обработки нескольких языков, подходит для различных задач генерации и понимания языка."
  },
  "ai21-jamba-1.5-large": {
    "description": "Многоязычная модель с 398B параметрами (94B активных), предлагающая контекстное окно длиной 256K, вызовы функций, структурированный вывод и основанное на фактах генерирование."
  },
  "ai21-jamba-1.5-mini": {
    "description": "Многоязычная модель с 52B параметрами (12B активных), предлагающая контекстное окно длиной 256K, вызовы функций, структурированный вывод и основанное на фактах генерирование."
  },
  "anthropic.claude-3-5-sonnet-20240620-v1:0": {
    "description": "Claude 3.5 Sonnet устанавливает новые отраслевые стандарты, превосходя модели конкурентов и Claude 3 Opus, демонстрируя отличные результаты в широком спектре оценок, при этом обладая скоростью и стоимостью наших моделей среднего уровня."
  },
  "anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet установил новые стандарты в отрасли, превзойдя модели конкурентов и Claude 3 Opus, продемонстрировав отличные результаты в широкомасштабных оценках, при этом обладая скоростью и стоимостью наших моделей среднего уровня."
  },
  "anthropic.claude-3-haiku-20240307-v1:0": {
    "description": "Claude 3 Haiku — это самая быстрая и компактная модель от Anthropic, обеспечивающая почти мгновенную скорость ответа. Она может быстро отвечать на простые запросы и запросы. Клиенты смогут создать бесшовный AI-опыт, имитирующий человеческое взаимодействие. Claude 3 Haiku может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "anthropic.claude-3-opus-20240229-v1:0": {
    "description": "Claude 3 Opus — это самый мощный AI-модель от Anthropic, обладающая передовыми характеристиками в области высоко сложных задач. Она может обрабатывать открытые подсказки и невидимые сценарии, демонстрируя отличную плавность и человеческое понимание. Claude 3 Opus демонстрирует передовые возможности генеративного AI. Claude 3 Opus может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "anthropic.claude-3-sonnet-20240229-v1:0": {
    "description": "Claude 3 Sonnet от Anthropic достигает идеального баланса между интеллектом и скоростью — особенно подходит для корпоративных рабочих нагрузок. Он предлагает максимальную полезность по цене ниже конкурентов и разработан как надежный, высокопрочный основной механизм для масштабируемых AI-развертываний. Claude 3 Sonnet может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "anthropic.claude-instant-v1": {
    "description": "Быстрая, экономичная и все еще очень мощная модель, способная обрабатывать широкий спектр задач, включая повседневные диалоги, текстовый анализ, резюме и вопросы к документам."
  },
  "anthropic.claude-v2": {
    "description": "Модель Anthropic демонстрирует высокие способности в широком спектре задач, от сложных диалогов и генерации креативного контента до детального следования инструкциям."
  },
  "anthropic.claude-v2:1": {
    "description": "Обновленная версия Claude 2, обладающая двойным контекстным окном и улучшениями в надежности, уровне галлюцинаций и точности на основе доказательств в длинных документах и контексте RAG."
  },
  "anthropic/claude-3-haiku": {
    "description": "Claude 3 Haiku — это самая быстрая и компактная модель от Anthropic, предназначенная для почти мгновенных ответов. Она обладает быстрой и точной направленной производительностью."
  },
  "anthropic/claude-3-opus": {
    "description": "Claude 3 Opus — это самая мощная модель от Anthropic для обработки высококомплексных задач. Она демонстрирует выдающиеся результаты по производительности, интеллекту, плавности и пониманию."
  },
  "anthropic/claude-3.5-sonnet": {
    "description": "Claude 3.5 Sonnet предлагает возможности, превосходящие Opus, и скорость, превышающую Sonnet, при этом сохраняя ту же цену. Sonnet особенно хорошо справляется с программированием, наукой о данных, визуальной обработкой и агентскими задачами."
  },
  "aya": {
    "description": "Aya 23 — это многоязычная модель, выпущенная Cohere, поддерживающая 23 языка, обеспечивая удобство для многоязычных приложений."
  },
  "aya:35b": {
    "description": "Aya 23 — это многоязычная модель, выпущенная Cohere, поддерживающая 23 языка, обеспечивая удобство для многоязычных приложений."
  },
  "charglm-3": {
    "description": "CharGLM-3 разработан для ролевых игр и эмоционального сопровождения, поддерживает сверхдлинную многократную память и персонализированные диалоги, имеет широкое применение."
  },
  "chatgpt-4o-latest": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "claude-2.0": {
    "description": "Claude 2 предлагает ключевые улучшения для бизнеса, включая ведущие в отрасли 200K токенов контекста, значительное снижение частоты галлюцинаций модели, системные подсказки и новую тестовую функцию: вызов инструментов."
  },
  "claude-2.1": {
    "description": "Claude 2 предлагает ключевые улучшения для бизнеса, включая ведущие в отрасли 200K токенов контекста, значительное снижение частоты галлюцинаций модели, системные подсказки и новую тестовую функцию: вызов инструментов."
  },
  "claude-3-5-haiku-20241022": {
    "description": "Claude 3.5 Haiku — это самая быстрая следующая модель от Anthropic. По сравнению с Claude 3 Haiku, Claude 3.5 Haiku продемонстрировала улучшения во всех навыках и превзошла предыдущую крупнейшую модель Claude 3 Opus во многих интеллектуальных тестах."
  },
  "claude-3-5-sonnet-20240620": {
    "description": "Claude 3.5 Sonnet предлагает возможности, превосходящие Opus, и скорость, быстрее Sonnet, при этом сохраняя ту же цену. Sonnet особенно хорош в программировании, науке о данных, визуальной обработке и задачах агентов."
  },
  "claude-3-5-sonnet-20241022": {
    "description": "Claude 3.5 Sonnet предлагает возможности, превышающие Opus, и скорость, превышающую Sonnet, при этом сохраняя ту же цену. Sonnet особенно хорош в программировании, данных, визуальной обработке и代理задачах."
  },
  "claude-3-haiku-20240307": {
    "description": "Claude 3 Haiku — это самая быстрая и компактная модель от Anthropic, предназначенная для достижения почти мгновенных ответов. Она обладает быстрой и точной направленной производительностью."
  },
  "claude-3-opus-20240229": {
    "description": "Claude 3 Opus — это самая мощная модель от Anthropic для обработки высококомплексных задач. Она демонстрирует выдающиеся результаты по производительности, интеллекту, плавности и пониманию."
  },
  "claude-3-sonnet-20240229": {
    "description": "Claude 3 Sonnet обеспечивает идеальный баланс между интеллектом и скоростью для корпоративных рабочих нагрузок. Он предлагает максимальную полезность по более низкой цене, надежен и подходит для масштабного развертывания."
  },
  "code-raccoon-v1": {
    "description": "Кодовый енот — это программный интеллектуальный помощник на основе языковой модели SenseTime, охватывающий такие этапы, как анализ требований к программному обеспечению, проектирование архитектуры, написание кода, тестирование программного обеспечения и т. д., удовлетворяющий различные потребности пользователей в написании кода и обучении программированию. Кодовый енот поддерживает более 90 популярных языков программирования, таких как Python, Java, JavaScript, C++, Go, SQL, а также популярные IDE, такие как VS Code и IntelliJ IDEA. В реальных приложениях кодовый енот может помочь разработчикам повысить эффективность программирования более чем на 50%."
  },
  "codegeex-4": {
    "description": "CodeGeeX-4 — это мощный AI помощник по программированию, поддерживающий интеллектуальные ответы и автозаполнение кода на различных языках программирования, повышая эффективность разработки."
  },
  "codegeex4-all-9b": {
    "description": "CodeGeeX4-ALL-9B — это многоязычная модель генерации кода, поддерживающая полный спектр функций, включая автозаполнение и генерацию кода, интерпретатор кода, веб-поиск, вызовы функций и вопросы по коду на уровне репозитория, охватывающая различные сценарии разработки программного обеспечения. Это одна из лучших моделей генерации кода с количеством параметров менее 10B."
  },
  "codegemma": {
    "description": "CodeGemma — это легковесная языковая модель, специально разработанная для различных задач программирования, поддерживающая быструю итерацию и интеграцию."
  },
  "codegemma:2b": {
    "description": "CodeGemma — это легковесная языковая модель, специально разработанная для различных задач программирования, поддерживающая быструю итерацию и интеграцию."
  },
  "codellama": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codellama/CodeLlama-34b-Instruct-hf": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, с поддержкой широкого спектра языков программирования, подходящая для среды разработчиков."
  },
  "codellama:13b": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codellama:34b": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codellama:70b": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codeqwen": {
    "description": "CodeQwen1.5 — это крупномасштабная языковая модель, обученная на большом объёме кодовых данных, специально разработанная для решения сложных задач программирования."
  },
  "codestral": {
    "description": "Codestral — это первая модель кода от Mistral AI, обеспечивающая отличную поддержку для задач генерации кода."
  },
  "codestral-latest": {
    "description": "Codestral — это передовая генеративная модель, сосредоточенная на генерации кода, оптимизированная для промежуточного заполнения и задач дополнения кода."
  },
  "cognitivecomputations/dolphin-mixtral-8x22b": {
    "description": "Dolphin Mixtral 8x22B — это модель, разработанная для соблюдения инструкций, диалогов и программирования."
  },
  "cohere-command-r": {
    "description": "Command R — это масштабируемая генеративная модель, нацеленная на RAG и использование инструментов для обеспечения AI на уровне производства для предприятий."
  },
  "cohere-command-r-plus": {
    "description": "Command R+ — это модель, оптимизированная для RAG, предназначенная для решения задач корпоративного уровня."
  },
  "command-light": {
    "description": ""
  },
  "command-r": {
    "description": "Command R — это LLM, оптимизированная для диалогов и задач с длинным контекстом, особенно подходит для динамического взаимодействия и управления знаниями."
  },
  "command-r-plus": {
    "description": "Command R+ — это высокопроизводительная большая языковая модель, специально разработанная для реальных бизнес-сценариев и сложных приложений."
  },
  "databricks/dbrx-instruct": {
    "description": "DBRX Instruct предлагает высокую надежность в обработке команд, поддерживая приложения в различных отраслях."
  },
  "deepseek-ai/DeepSeek-V2-Chat": {
    "description": "DeepSeek-V2 — это мощная и экономически эффективная языковая модель с гибридными экспертами (MoE). Она была предварительно обучена на высококачественном корпусе из 8.1 триллиона токенов и дополнительно улучшена с помощью контролируемой дообучения (SFT) и обучения с подкреплением (RL). По сравнению с DeepSeek 67B, DeepSeek-V2 обеспечивает более высокую производительность, экономя 42.5% затрат на обучение, снижая использование KV-кэша на 93.3% и увеличивая максимальную пропускную способность генерации в 5.76 раз. Эта модель поддерживает длину контекста до 128k и показывает отличные результаты в стандартных бенчмарках и оценках открытой генерации."
  },
  "deepseek-ai/DeepSeek-V2.5": {
    "description": "DeepSeek V2.5 объединяет отличительные черты предыдущих версий, улучшая общие и кодировочные способности."
  },
  "deepseek-ai/deepseek-llm-67b-chat": {
    "description": "DeepSeek 67B — это передовая модель, обученная для высококомплексных диалогов."
  },
  "deepseek-chat": {
    "description": "Новая открытая модель, объединяющая общие и кодовые возможности, не только сохраняет общие диалоговые способности оригинальной модели Chat и мощные возможности обработки кода модели Coder, но и лучше согласуется с человеческими предпочтениями. Кроме того, DeepSeek-V2.5 значительно улучшила производительность в таких задачах, как написание текстов и следование инструкциям."
  },
  "deepseek-coder-33B-instruct": {
    "description": "DeepSeek Coder 33B — это модель языкового кодирования, обученная на 20 триллионах данных, из которых 87% составляют код, а 13% — китайский и английский языки. Модель использует размер окна 16K и задачи заполнения пропусков, предоставляя функции автозаполнения кода и заполнения фрагментов на уровне проектов."
  },
  "deepseek-coder-v2": {
    "description": "DeepSeek Coder V2 — это открытая смешанная экспертная модель кода, показывающая отличные результаты в задачах кода, сопоставимая с GPT4-Turbo."
  },
  "deepseek-coder-v2:236b": {
    "description": "DeepSeek Coder V2 — это открытая смешанная экспертная модель кода, показывающая отличные результаты в задачах кода, сопоставимая с GPT4-Turbo."
  },
  "deepseek-v2": {
    "description": "DeepSeek V2 — это эффективная языковая модель Mixture-of-Experts, подходящая для экономически эффективных потребностей обработки."
  },
  "deepseek-v2:236b": {
    "description": "DeepSeek V2 236B — это модель кода DeepSeek, обеспечивающая мощные возможности генерации кода."
  },
  "deepseek/deepseek-chat": {
    "description": "Новая открытая модель, объединяющая общие и кодовые возможности, не только сохраняет общие диалоговые способности оригинальной модели Chat и мощные возможности обработки кода модели Coder, но и лучше соответствует человеческим предпочтениям. Кроме того, DeepSeek-V2.5 значительно улучшила свои результаты в задачах написания, следования инструкциям и других областях."
  },
  "emohaa": {
    "description": "Emohaa — это психологическая модель, обладающая профессиональными консультационными способностями, помогающая пользователям понимать эмоциональные проблемы."
  },
  "gemini-1.0-pro-001": {
    "description": "Gemini 1.0 Pro 001 (Тюнинг) предлагает стабильную и настраиваемую производительность, что делает её идеальным выбором для решения сложных задач."
  },
  "gemini-1.0-pro-002": {
    "description": "Gemini 1.0 Pro 002 (Тюнинг) предлагает выдающуюся поддержку многомодальности, сосредотачиваясь на эффективном решении сложных задач."
  },
  "gemini-1.0-pro-latest": {
    "description": "Gemini 1.0 Pro — это высокопроизводительная модель ИИ от Google, разработанная для масштабирования широкого спектра задач."
  },
  "gemini-1.5-flash-001": {
    "description": "Gemini 1.5 Flash 001 — это эффективная многомодальная модель, поддерживающая масштабирование для широкого спектра приложений."
  },
  "gemini-1.5-flash-002": {
    "description": "Gemini 1.5 Flash 002 — это эффективная мультимодальная модель, поддерживающая расширенные применения."
  },
  "gemini-1.5-flash-8b": {
    "description": "Gemini 1.5 Flash 8B — это высокоэффективная многомодальная модель, поддерживающая широкий спектр приложений."
  },
  "gemini-1.5-flash-8b-exp-0924": {
    "description": "Gemini 1.5 Flash 8B 0924 — это последняя экспериментальная модель, которая демонстрирует значительное улучшение производительности как в текстовых, так и в мультимодальных задачах."
  },
  "gemini-1.5-flash-exp-0827": {
    "description": "Gemini 1.5 Flash 0827 предлагает оптимизированные многомодальные возможности обработки, подходящие для различных сложных задач."
  },
  "gemini-1.5-flash-latest": {
    "description": "Gemini 1.5 Flash — это последняя многомодальная модель ИИ от Google, обладающая высокой скоростью обработки и поддерживающая текстовые, графические и видео входы, что делает её эффективной для масштабирования различных задач."
  },
  "gemini-1.5-pro-001": {
    "description": "Gemini 1.5 Pro 001 — это масштабируемое решение для многомодального ИИ, поддерживающее широкий спектр сложных задач."
  },
  "gemini-1.5-pro-002": {
    "description": "Gemini 1.5 Pro 002 — это последняя модель, готовая к производству, которая обеспечивает более высокое качество вывода, особенно в математических задачах, длинных контекстах и визуальных задачах."
  },
  "gemini-1.5-pro-exp-0801": {
    "description": "Gemini 1.5 Pro 0801 предлагает выдающиеся многомодальные возможности обработки, обеспечивая большую гибкость в разработке приложений."
  },
  "gemini-1.5-pro-exp-0827": {
    "description": "Gemini 1.5 Pro 0827 сочетает последние технологии оптимизации, обеспечивая более эффективную обработку многомодальных данных."
  },
  "gemini-1.5-pro-latest": {
    "description": "Gemini 1.5 Pro поддерживает до 2 миллионов токенов и является идеальным выбором для средних многомодальных моделей, обеспечивая многостороннюю поддержку для сложных задач."
  },
  "gemini-2.0-flash-exp": {
    "description": "Gemini 2.0 Flash Exp — это новейшая экспериментальная мультимодальная AI-модель от Google, обладающая функциями следующего поколения, выдающейся скоростью, нативными инструментами и мультимодальной генерацией."
  },
  "gemini-exp-1114": {
    "description": "Gemini Exp 1114 — это новейшая экспериментальная многомодальная ИИ модель от Google, обладающая высокой скоростью обработки и поддерживающая текстовые, изображенческие и видеовходы, что позволяет эффективно расширять применение для различных задач."
  },
  "gemini-exp-1121": {
    "description": "Gemini Exp 1121 — это последняя экспериментальная многомодальная AI модель от Google, обладающая высокой скоростью обработки, поддерживающая текстовые, графические и видеовходы, что делает её эффективной для масштабирования различных задач."
  },
  "gemini-exp-1206": {
    "description": "Gemini Exp 1206 — это новейшая экспериментальная многомодальная AI модель от Google, которая продемонстрировала определенное улучшение качества по сравнению с предыдущими версиями."
  },
  "gemma-7b-it": {
    "description": "Gemma 7B подходит для обработки задач среднего и малого масштаба, обеспечивая экономическую эффективность."
  },
  "gemma2": {
    "description": "Gemma 2 — это высокоэффективная модель, выпущенная Google, охватывающая широкий спектр приложений от малых до сложных задач обработки данных."
  },
  "gemma2-9b-it": {
    "description": "Gemma 2 9B — это модель, оптимизированная для конкретных задач и интеграции инструментов."
  },
  "gemma2:27b": {
    "description": "Gemma 2 — это высокоэффективная модель, выпущенная Google, охватывающая широкий спектр приложений от малых до сложных задач обработки данных."
  },
  "gemma2:2b": {
    "description": "Gemma 2 — это высокоэффективная модель, выпущенная Google, охватывающая широкий спектр приложений от малых до сложных задач обработки данных."
  },
  "generalv3": {
    "description": "Spark Pro — это высокопроизводительная большая языковая модель, оптимизированная для профессиональных областей, таких как математика, программирование, медицина и образование, поддерживающая сетевой поиск и встроенные плагины для погоды, даты и т.д. Оптимизированная модель демонстрирует выдающиеся результаты и высокую эффективность в сложных задачах на знание, понимании языка и высокоуровневом создании текстов, что делает ее идеальным выбором для профессиональных приложений."
  },
  "generalv3.5": {
    "description": "Spark3.5 Max — это самая полная версия, поддерживающая сетевой поиск и множество встроенных плагинов. Его полностью оптимизированные основные возможности, а также функции настройки системных ролей и вызовов функций делают его выдающимся и эффективным в различных сложных приложениях."
  },
  "glm-4": {
    "description": "GLM-4 — это старая флагманская версия, выпущенная в январе 2024 года, которая была заменена более мощной GLM-4-0520."
  },
  "glm-4-0520": {
    "description": "GLM-4-0520 — это последняя версия модели, специально разработанная для высоко сложных и разнообразных задач, демонстрирующая выдающиеся результаты."
  },
  "glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat демонстрирует высокую производительность в семантике, математике, логическом мышлении, кодировании и знаниях. Также поддерживает веб-браузинг, выполнение кода, вызовы пользовательских инструментов и длинное текстовое рассуждение. Поддерживает 26 языков, включая японский, корейский и немецкий."
  },
  "glm-4-air": {
    "description": "GLM-4-Air — это экономически эффективная версия, производительность которой близка к GLM-4, обеспечивая высокую скорость и доступную цену."
  },
  "glm-4-airx": {
    "description": "GLM-4-AirX предлагает эффективную версию GLM-4-Air, скорость вывода может достигать 2.6 раз быстрее."
  },
  "glm-4-alltools": {
    "description": "GLM-4-AllTools — это многофункциональная модель агента, оптимизированная для поддержки сложного планирования инструкций и вызовов инструментов, таких как веб-серфинг, интерпретация кода и генерация текста, подходящая для выполнения множества задач."
  },
  "glm-4-flash": {
    "description": "GLM-4-Flash — это идеальный выбор для обработки простых задач, с самой высокой скоростью и самой низкой ценой."
  },
  "glm-4-flashx": {
    "description": "GLM-4-FlashX — это улучшенная версия Flash с ультрабыстрой скоростью вывода."
  },
  "glm-4-long": {
    "description": "GLM-4-Long поддерживает сверхдлинные текстовые вводы, подходит для задач, требующих памяти, и обработки больших документов."
  },
  "glm-4-plus": {
    "description": "GLM-4-Plus, как флагман с высоким интеллектом, обладает мощными способностями обработки длинных текстов и сложных задач, с полным улучшением производительности."
  },
  "glm-4v": {
    "description": "GLM-4V предлагает мощные способности понимания и вывода изображений, поддерживает множество визуальных задач."
  },
  "glm-4v-flash": {
    "description": "GLM-4V-Flash сосредоточен на эффективном понимании одного изображения, подходит для сценариев быстрого анализа изображений, таких как анализ изображений в реальном времени или пакетная обработка изображений."
  },
  "glm-4v-plus": {
    "description": "GLM-4V-Plus обладает способностью понимать видео-контент и множество изображений, подходит для мультимодальных задач."
  },
  "google/gemini-flash-1.5": {
    "description": "Gemini 1.5 Flash предлагает оптимизированные возможности многомодальной обработки, подходящие для различных сложных задач."
  },
  "google/gemini-pro-1.5": {
    "description": "Gemini 1.5 Pro сочетает в себе новейшие технологии оптимизации, обеспечивая более эффективную обработку многомодальных данных."
  },
  "google/gemma-2-27b-it": {
    "description": "Gemma 2 продолжает концепцию легковесного и эффективного дизайна."
  },
  "google/gemma-2-2b-it": {
    "description": "Легковесная модель настройки инструкций от Google."
  },
  "google/gemma-2-9b-it": {
    "description": "Gemma 2 — это легковесная серия текстовых моделей с открытым исходным кодом от Google."
  },
  "google/gemma-2-9b-it:free": {
    "description": "Gemma 2 — это облегченная открытая текстовая модель от Google."
  },
  "google/gemma-2b-it": {
    "description": "Gemma Instruct (2B) предлагает базовые возможности обработки команд, подходящие для легковесных приложений."
  },
  "gpt-3.5-turbo": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-0125": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-1106": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-instruct": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-35-turbo": {
    "description": "GPT 3.5 Turbo — это эффективная модель от OpenAI, предназначенная для задач чата и генерации текста, поддерживающая параллельные вызовы функций."
  },
  "gpt-35-turbo-16k": {
    "description": "GPT 3.5 Turbo 16k — модель для генерации текста с высокой ёмкостью, подходящая для сложных задач."
  },
  "gpt-4": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-0125-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-0613": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-1106-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-32k": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-32k-0613": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-turbo": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-turbo-2024-04-09": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-turbo-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-vision-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4o": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-2024-05-13": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-2024-08-06": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-2024-11-20": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени для поддержания актуальной версии. Она сочетает в себе мощное понимание языка и генерацию текста, подходя для широкого спектра приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-mini": {
    "description": "GPT-4o mini — это последняя модель, выпущенная OpenAI после GPT-4 Omni, поддерживающая ввод изображений и текстов с выводом текста. Как их самый продвинутый компактный модель, она значительно дешевле других недавних передовых моделей и более чем на 60% дешевле GPT-3.5 Turbo. Она сохраняет передовой уровень интеллекта при значительном соотношении цена-качество. GPT-4o mini набрала 82% на тесте MMLU и в настоящее время занимает более высокое место в предпочтениях чата по сравнению с GPT-4."
  },
  "grok-2-1212": {
    "description": "Модель улучшена в точности, соблюдении инструкций и многоязычных возможностях."
  },
  "grok-2-vision-1212": {
    "description": "Модель улучшена в точности, соблюдении инструкций и многоязычных возможностях."
  },
  "grok-beta": {
    "description": "Обладает производительностью, сопоставимой с Grok 2, но с большей эффективностью, скоростью и функциональностью."
  },
  "grok-vision-beta": {
    "description": "Новейшая модель понимания изображений, способная обрабатывать разнообразную визуальную информацию, включая документы, графики, скриншоты и фотографии."
  },
  "gryphe/mythomax-l2-13b": {
    "description": "MythoMax l2 13B — это языковая модель, объединяющая креативность и интеллект, основанная на нескольких ведущих моделях."
  },
  "hunyuan-code": {
    "description": "Последняя модель генерации кода Hunyuan, обученная на базе 200B высококачественных данных кода, прошедшая полгода обучения на высококачественных данных SFT, с увеличенной длиной контекстного окна до 8K, занимает ведущие позиции по автоматическим оценочным показателям генерации кода на пяти языках; по десяти критериям оценки кода на пяти языках, производительность находится в первой группе."
  },
  "hunyuan-functioncall": {
    "description": "Последняя модель Hunyuan с архитектурой MOE FunctionCall, обученная на высококачественных данных FunctionCall, с контекстным окном до 32K, занимает лидирующие позиции по множеству оценочных показателей."
  },
  "hunyuan-large": {
    "description": ""
  },
  "hunyuan-lite": {
    "description": "Обновленная версия с MOE-структурой, контекстное окно составляет 256k, она опережает множество открытых моделей в оценках по NLP, коду, математике и другим областям."
  },
  "hunyuan-pro": {
    "description": "Модель длинного текста с параметрами уровня триллиона MOE-32K. Она достигает абсолютного лидерства на различных бенчмарках, обладает сложными инструкциями и выводом, имеет сложные математические способности и поддерживает вызовы функций, с акцентом на оптимизацию в области многоязычного перевода, финансов, права и медицины."
  },
  "hunyuan-role": {
    "description": "Последняя версия модели ролевого взаимодействия Hunyuan, выпущенная с официальной тонкой настройкой, основанная на модели Hunyuan и дополненная данными сценариев ролевого взаимодействия, демонстрирует лучшие базовые результаты в ролевых сценариях."
  },
  "hunyuan-standard": {
    "description": "Использует более оптимальную стратегию маршрутизации, одновременно смягчая проблемы с балансировкой нагрузки и сходимостью экспертов. В области длинных текстов показатель «найти иголку в стоге сена» достигает 99,9%. MOE-32K предлагает более высокую стоимость-эффективность, обеспечивая баланс между качеством и ценой, а также возможность обработки длинных текстовых вводов."
  },
  "hunyuan-standard-256K": {
    "description": "Использует более оптимальную стратегию маршрутизации, одновременно смягчая проблемы с балансировкой нагрузки и сходимостью экспертов. В области длинных текстов показатель «найти иголку в стоге сена» достигает 99,9%. MOE-256K делает дальнейший прорыв в длине и качестве, значительно расширяя допустимую длину ввода."
  },
  "hunyuan-turbo": {
    "description": "Предварительная версия нового поколения языковой модели Hunyuan, использующая совершенно новую структуру смешанной экспертной модели (MoE), которая обеспечивает более быструю эффективность вывода и более сильные результаты по сравнению с hunyuan-pro."
  },
  "hunyuan-vision": {
    "description": "Последняя многомодальная модель Hunyuan, поддерживающая ввод изображений и текста для генерации текстового контента."
  },
  "internlm/internlm2_5-20b-chat": {
    "description": "Инновационная открытая модель InternLM2.5, благодаря большому количеству параметров, повышает интеллектуальность диалогов."
  },
  "internlm/internlm2_5-7b-chat": {
    "description": "InternLM2.5 предлагает интеллектуальные решения для диалогов в различных сценариях."
  },
  "internlm2-pro-chat": {
    "description": "Старая версия модели, которую мы все еще поддерживаем, доступная с параметрами 7B и 20B."
  },
  "internlm2.5-latest": {
    "description": "Наша последняя серия моделей с выдающимися показателями вывода, поддерживающая длину контекста до 1M и обладающая улучшенными возможностями следования инструкциям и вызова инструментов."
  },
  "jamba-1.5-large": {},
  "jamba-1.5-mini": {},
  "learnlm-1.5-pro-experimental": {
    "description": "LearnLM — это экспериментальная языковая модель, ориентированная на конкретные задачи, обученная в соответствии с принципами науки о обучении, которая может следовать системным инструкциям в учебных и образовательных сценариях, выступая в роли эксперта-наставника и т.д."
  },
  "lite": {
    "description": "Spark Lite — это легковесная большая языковая модель с крайне низкой задержкой и высокой эффективностью обработки, полностью бесплатная и открытая, поддерживающая функции онлайн-поиска в реальном времени. Ее быстрая реакция делает ее отличным выбором для применения в устройствах с низкой вычислительной мощностью и для тонкой настройки моделей, обеспечивая пользователям отличное соотношение цены и качества, особенно в сценариях вопросов и ответов, генерации контента и поиска."
  },
  "llama-3.1-70b-instruct": {
    "description": "Модель Llama 3.1 70B для команд, обладающая 70B параметрами, обеспечивает выдающуюся производительность в задачах генерации текста и выполнения команд."
  },
  "llama-3.1-70b-versatile": {
    "description": "Llama 3.1 70B предлагает более мощные возможности ИИ вывода, подходит для сложных приложений, поддерживает огромное количество вычислительных процессов и гарантирует эффективность и точность."
  },
  "llama-3.1-8b-instant": {
    "description": "Llama 3.1 8B — это высокоэффективная модель, обеспечивающая быструю генерацию текста, идеально подходящая для приложений, требующих масштабной эффективности и экономичности."
  },
  "llama-3.1-8b-instruct": {
    "description": "Модель Llama 3.1 8B для команд, обладающая 8B параметрами, обеспечивает эффективное выполнение задач с указаниями и предлагает высококачественные возможности генерации текста."
  },
  "llama-3.1-sonar-huge-128k-online": {
    "description": "Модель Llama 3.1 Sonar Huge Online, обладающая 405B параметрами, поддерживает контекст длиной около 127,000 токенов, предназначена для сложных онлайн-чат-приложений."
  },
  "llama-3.1-sonar-large-128k-chat": {
    "description": "Модель Llama 3.1 Sonar Large Chat, обладающая 70B параметрами, поддерживает контекст длиной около 127,000 токенов, подходит для сложных оффлайн-чатов."
  },
  "llama-3.1-sonar-large-128k-online": {
    "description": "Модель Llama 3.1 Sonar Large Online, обладающая 70B параметрами, поддерживает контекст длиной около 127,000 токенов, подходит для задач с высокой нагрузкой и разнообразными чатами."
  },
  "llama-3.1-sonar-small-128k-chat": {
    "description": "Модель Llama 3.1 Sonar Small Chat, обладающая 8B параметрами, специально разработана для оффлайн-чатов и поддерживает контекст длиной около 127,000 токенов."
  },
  "llama-3.1-sonar-small-128k-online": {
    "description": "Модель Llama 3.1 Sonar Small Online, обладающая 8B параметрами, поддерживает контекст длиной около 127,000 токенов, специально разработана для онлайн-чатов и эффективно обрабатывает различные текстовые взаимодействия."
  },
  "llama-3.2-11b-vision-instruct": {
    "description": "Отличные способности к визуальному пониманию изображений на высоком разрешении, предназначенные для приложений визуального понимания."
  },
  "llama-3.2-11b-vision-preview": {
    "description": "Llama 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "llama-3.2-90b-vision-instruct": {
    "description": "Совершенные возможности визуального понимания для приложения-агента."
  },
  "llama-3.2-90b-vision-preview": {
    "description": "Llama 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "llama-3.3-70b-versatile": {
    "description": "Многоязычная большая языковая модель Meta Llama 3.3 (LLM) — это предобученная и откорректированная модель генерации на 70B (текстовый ввод/текстовый вывод). Откорректированная на чистом тексте модель Llama 3.3 оптимизирована для многоязычных диалоговых задач и превосходит многие доступные открытые и закрытые модели чата по общим промышленным стандартам."
  },
  "llama3-70b-8192": {
    "description": "Meta Llama 3 70B предлагает непревзойдённые возможности обработки сложности, специально разработанные для высоких требований проектов."
  },
  "llama3-8b-8192": {
    "description": "Meta Llama 3 8B обеспечивает высококачественную производительность вывода, подходящую для многообразных приложений."
  },
  "llama3-groq-70b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 70B Tool Use предлагает мощные возможности вызова инструментов, поддерживая эффективную обработку сложных задач."
  },
  "llama3-groq-8b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 8B Tool Use — это модель, оптимизированная для эффективного использования инструментов, поддерживающая быструю параллельную обработку."
  },
  "llama3.1": {
    "description": "Llama 3.1 — это передовая модель, выпущенная Meta, поддерживающая до 405B параметров, применимая в сложных диалогах, многоязычном переводе и анализе данных."
  },
  "llama3.1:405b": {
    "description": "Llama 3.1 — это передовая модель, выпущенная Meta, поддерживающая до 405B параметров, применимая в сложных диалогах, многоязычном переводе и анализе данных."
  },
  "llama3.1:70b": {
    "description": "Llama 3.1 — это передовая модель, выпущенная Meta, поддерживающая до 405B параметров, применимая в сложных диалогах, многоязычном переводе и анализе данных."
  },
  "llava": {
    "description": "LLaVA — это многомодальная модель, объединяющая визуальный кодировщик и Vicuna, предназначенная для мощного понимания визуальной и языковой информации."
  },
  "llava-v1.5-7b-4096-preview": {
    "description": "LLaVA 1.5 7B предлагает возможности визуальной обработки, генерируя сложные выходные данные на основе визуальной информации."
  },
  "llava:13b": {
    "description": "LLaVA — это многомодальная модель, объединяющая визуальный кодировщик и Vicuna, предназначенная для мощного понимания визуальной и языковой информации."
  },
  "llava:34b": {
    "description": "LLaVA — это многомодальная модель, объединяющая визуальный кодировщик и Vicuna, предназначенная для мощного понимания визуальной и языковой информации."
  },
  "mathstral": {
    "description": "MathΣtral специально разработан для научных исследований и математического вывода, обеспечивая эффективные вычислительные возможности и интерпретацию результатов."
  },
  "max-32k": {
    "description": "Spark Max 32K обладает большой способностью обработки контекста, улучшенным пониманием контекста и логическим выводом, поддерживает текстовый ввод до 32K токенов, подходит для чтения длинных документов, частных вопросов и ответов и других сценариев."
  },
  "meta-llama-3-70b-instruct": {
    "description": "Мощная модель с 70 миллиардами параметров, превосходящая в области рассуждений, кодирования и широких языковых приложений."
  },
  "meta-llama-3-8b-instruct": {
    "description": "Универсальная модель с 8 миллиардами параметров, оптимизированная для диалоговых и текстовых задач."
  },
  "meta-llama-3.1-405b-instruct": {
    "description": "Модели Llama 3.1, настроенные на инструкции, оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные модели открытого и закрытого чата по общим отраслевым стандартам."
  },
  "meta-llama-3.1-70b-instruct": {
    "description": "Модели Llama 3.1, настроенные на инструкции, оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные модели открытого и закрытого чата по общим отраслевым стандартам."
  },
  "meta-llama-3.1-8b-instruct": {
    "description": "Модели Llama 3.1, настроенные на инструкции, оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные модели открытого и закрытого чата по общим отраслевым стандартам."
  },
  "meta-llama/Llama-2-13b-chat-hf": {
    "description": "LLaMA-2 Chat (13B) предлагает отличные возможности обработки языка и выдающийся опыт взаимодействия."
  },
  "meta-llama/Llama-2-70b-hf": {
    "description": "LLaMA-2 предлагает превосходные способности обработки языка и выдающийся пользовательский опыт."
  },
  "meta-llama/Llama-3-70b-chat-hf": {
    "description": "LLaMA-3 Chat (70B) — мощная модель для чата, поддерживающая сложные диалоговые запросы."
  },
  "meta-llama/Llama-3-8b-chat-hf": {
    "description": "LLaMA-3 Chat (8B) предлагает многоязычную поддержку и охватывает широкий спектр областей знаний."
  },
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Llama-3.2-3B-Instruct-Turbo": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Llama-Vision-Free": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
    "description": "Llama 3 70B Instruct Lite подходит для сред, требующих высокой производительности и низкой задержки."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": {
    "description": "Llama 3 70B Instruct Turbo обеспечивает выдающиеся возможности понимания и генерации языка, подходящие для самых требовательных вычислительных задач."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite": {
    "description": "Llama 3 8B Instruct Lite подходит для ресурсов ограниченных сред, обеспечивая отличное соотношение производительности."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": {
    "description": "Llama 3 8B Instruct Turbo — это высокоэффективная большая языковая модель, поддерживающая широкий спектр приложений."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "description": "LLaMA 3.1 405B — это мощная модель, основанная на предобучении и настройке инструкций."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
    "description": "Модель Llama 3.1 Turbo 405B предлагает огромную поддержку контекста для обработки больших данных и демонстрирует выдающиеся результаты в масштабных приложениях искусственного интеллекта."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct": {
    "description": "LLaMA 3.1 70B предлагает эффективную поддержку диалогов на нескольких языках."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
    "description": "Модель Llama 3.1 70B была тщательно настроена для высоконагруженных приложений, квантованная до FP8 для повышения вычислительной мощности и точности, обеспечивая выдающиеся результаты в сложных сценариях."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "description": "LLaMA 3.1 предлагает поддержку нескольких языков и является одной из ведущих генеративных моделей в отрасли."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
    "description": "Модель Llama 3.1 8B использует FP8-квантование и поддерживает до 131,072 контекстных токенов, являясь выдающейся среди открытых моделей, подходящей для сложных задач и превосходящей многие отраслевые стандарты."
  },
  "meta-llama/llama-3-70b-instruct": {
    "description": "Llama 3 70B Instruct оптимизирован для высококачественных диалоговых сцен и показывает отличные результаты в различных оценках."
  },
  "meta-llama/llama-3-8b-instruct": {
    "description": "Llama 3 8B Instruct оптимизирован для высококачественных диалоговых сцен, его производительность превосходит многие закрытые модели."
  },
  "meta-llama/llama-3.1-405b-instruct": {
    "description": "Llama 3.1 405B Instruct — это последняя версия от Meta, оптимизированная для генерации высококачественных диалогов, превосходящая многие ведущие закрытые модели."
  },
  "meta-llama/llama-3.1-70b-instruct": {
    "description": "Llama 3.1 70B Instruct разработан для высококачественных диалогов и показывает выдающиеся результаты в оценках, особенно в высокоинтерактивных сценах."
  },
  "meta-llama/llama-3.1-8b-instruct": {
    "description": "Llama 3.1 8B Instruct — это последняя версия от Meta, оптимизированная для высококачественных диалоговых сцен, превосходящая многие ведущие закрытые модели."
  },
  "meta-llama/llama-3.1-8b-instruct:free": {
    "description": "LLaMA 3.1 предлагает поддержку нескольких языков и является одной из ведущих генеративных моделей в отрасли."
  },
  "meta-llama/llama-3.2-11b-vision-instruct": {
    "description": "LLaMA 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "meta-llama/llama-3.2-90b-vision-instruct": {
    "description": "LLaMA 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "meta.llama3-1-405b-instruct-v1:0": {
    "description": "Meta Llama 3.1 405B Instruct — это самая большая и мощная модель в линейке Llama 3.1 Instruct, представляющая собой высокоразвёрнутую модель для диалогового вывода и генерации синтетических данных, также может использоваться в качестве основы для специализированного предобучения или дообучения в определённых областях. Многоязычные большие языковые модели (LLMs), предлагаемые Llama 3.1, представляют собой набор предобученных генеративных моделей с настройкой на инструкции, включая размеры 8B, 70B и 405B (вход/выход текста). Модели текста с настройкой на инструкции Llama 3.1 (8B, 70B, 405B) оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные открытые модели чата в общепринятых отраслевых бенчмарках. Llama 3.1 предназначена для коммерческого и исследовательского использования на нескольких языках. Модели текста с настройкой на инструкции подходят для диалогов, похожих на помощников, в то время как предобученные модели могут адаптироваться к различным задачам генерации естественного языка. Модели Llama 3.1 также поддерживают использование их вывода для улучшения других моделей, включая генерацию синтетических данных и уточнение. Llama 3.1 является саморегрессионной языковой моделью, использующей оптимизированную архитектуру трансформеров. Настроенные версии используют контролируемое дообучение (SFT) и обучение с подкреплением с человеческой обратной связью (RLHF), чтобы соответствовать предпочтениям людей в отношении полезности и безопасности."
  },
  "meta.llama3-1-70b-instruct-v1:0": {
    "description": "Обновленная версия Meta Llama 3.1 70B Instruct, включающая расширенную длину контекста до 128K, многоязычность и улучшенные способности вывода. Многоязычные большие языковые модели (LLMs), предлагаемые Llama 3.1, представляют собой набор предобученных, настроенных на инструкции генеративных моделей, включая размеры 8B, 70B и 405B (ввод/вывод текста). Настроенные на инструкции текстовые модели (8B, 70B, 405B) оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные открытые модели чата в общих отраслевых бенчмарках. Llama 3.1 предназначена для коммерческого и исследовательского использования на нескольких языках. Настроенные на инструкции текстовые модели подходят для диалогов, похожих на помощника, в то время как предобученные модели могут адаптироваться к различным задачам генерации естественного языка. Модели Llama 3.1 также поддерживают использование вывода своих моделей для улучшения других моделей, включая генерацию синтетических данных и уточнение. Llama 3.1 — это саморегрессионная языковая модель, использующая оптимизированную архитектуру трансформеров. Настроенные версии используют контролируемую донастройку (SFT) и обучение с подкреплением с человеческой обратной связью (RLHF), чтобы соответствовать человеческим предпочтениям по полезности и безопасности."
  },
  "meta.llama3-1-8b-instruct-v1:0": {
    "description": "Обновленная версия Meta Llama 3.1 8B Instruct, включающая расширенную длину контекста до 128K, многоязычность и улучшенные способности вывода. Многоязычные большие языковые модели (LLMs), предлагаемые Llama 3.1, представляют собой набор предобученных, настроенных на инструкции генеративных моделей, включая размеры 8B, 70B и 405B (ввод/вывод текста). Настроенные на инструкции текстовые модели (8B, 70B, 405B) оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные открытые модели чата в общих отраслевых бенчмарках. Llama 3.1 предназначена для коммерческого и исследовательского использования на нескольких языках. Настроенные на инструкции текстовые модели подходят для диалогов, похожих на помощника, в то время как предобученные модели могут адаптироваться к различным задачам генерации естественного языка. Модели Llama 3.1 также поддерживают использование вывода своих моделей для улучшения других моделей, включая генерацию синтетических данных и уточнение. Llama 3.1 — это саморегрессионная языковая модель, использующая оптимизированную архитектуру трансформеров. Настроенные версии используют контролируемую донастройку (SFT) и обучение с подкреплением с человеческой обратной связью (RLHF), чтобы соответствовать человеческим предпочтениям по полезности и безопасности."
  },
  "meta.llama3-70b-instruct-v1:0": {
    "description": "Meta Llama 3 — это открытая большая языковая модель (LLM), ориентированная на разработчиков, исследователей и предприятия, предназначенная для помощи в создании, экспериментировании и ответственном масштабировании их идей по генеративному ИИ. В качестве части базовой системы для инноваций глобального сообщества она идеально подходит для создания контента, диалогового ИИ, понимания языка, НИОКР и корпоративных приложений."
  },
  "meta.llama3-8b-instruct-v1:0": {
    "description": "Meta Llama 3 — это открытая большая языковая модель (LLM), ориентированная на разработчиков, исследователей и предприятия, предназначенная для помощи в создании, экспериментировании и ответственном масштабировании их идей по генеративному ИИ. В качестве части базовой системы для инноваций глобального сообщества она идеально подходит для устройств с ограниченными вычислительными мощностями и ресурсами, а также для более быстрого времени обучения."
  },
  "microsoft/Phi-3.5-mini-instruct": {},
  "microsoft/WizardLM-2-8x22B": {
    "description": "WizardLM 2 — это языковая модель от Microsoft AI, которая особенно хорошо справляется с сложными диалогами, многоязычностью, выводами и интеллектуальными помощниками."
  },
  "microsoft/wizardlm 2-7b": {
    "description": "WizardLM 2 7B — это новая быстрая и легкая модель от Microsoft AI, производительность которой близка к 10-кратной производительности существующих открытых моделей."
  },
  "microsoft/wizardlm-2-8x22b": {
    "description": "WizardLM-2 8x22B — это передовая модель Wizard от Microsoft, демонстрирующая исключительно конкурентоспособные результаты."
  },
  "minicpm-v": {
    "description": "MiniCPM-V — это новое поколение мультимодальной большой модели от OpenBMB, обладающее выдающимися возможностями OCR и мультимодального понимания, поддерживающее широкий спектр приложений."
  },
  "ministral-3b-latest": {
    "description": "Ministral 3B - это выдающаяся модель от Mistral."
  },
  "ministral-8b-latest": {
    "description": "Ministral 8B - это экономически эффективная модель от Mistral."
  },
  "mistral": {
    "description": "Mistral — это 7B модель, выпущенная Mistral AI, подходящая для разнообразных языковых задач."
  },
  "mistral-large": {
    "description": "Mixtral Large — это флагманская модель от Mistral, объединяющая возможности генерации кода, математики и вывода, поддерживающая контекстное окно 128k."
  },
  "mistral-large-latest": {
    "description": "Mistral Large — это флагманская большая модель, хорошо подходящая для многоязычных задач, сложного вывода и генерации кода, идеальный выбор для высококлассных приложений."
  },
  "mistral-nemo": {
    "description": "Mistral Nemo, разработанный в сотрудничестве между Mistral AI и NVIDIA, является высокоэффективной 12B моделью."
  },
  "mistral-small": {
    "description": "Mistral Small может использоваться для любых языковых задач, требующих высокой эффективности и низкой задержки."
  },
  "mistral-small-latest": {
    "description": "Mistral Small — это экономически эффективный, быстрый и надежный вариант для таких случаев, как перевод, резюме и анализ настроений."
  },
  "mistralai/Mistral-7B-Instruct-v0.1": {
    "description": "Mistral (7B) Instruct известен своей высокой производительностью и подходит для множества языковых задач."
  },
  "mistralai/Mistral-7B-Instruct-v0.2": {
    "description": "Mistral 7B — это модель с настройкой по запросу, предлагающая оптимизированные ответы на задачи."
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "description": "Mistral (7B) Instruct v0.3 обеспечивает эффективные вычислительные возможности и понимание естественного языка, подходящие для широкого спектра приложений."
  },
  "mistralai/Mistral-7B-v0.1": {
    "description": "Mistral 7B - это компактная, но высокопроизводительная модель, хорошо подходящая для пакетной обработки и простых задач, таких как классификация и генерация текста, с хорошими способностями к рассуждению."
  },
  "mistralai/Mixtral-8x22B-Instruct-v0.1": {
    "description": "Mixtral-8x22B Instruct (141B) — это супер большая языковая модель, поддерживающая крайне высокие требования к обработке."
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "description": "Mixtral 8x7B — это предобученная модель разреженных смешанных экспертов, предназначенная для универсальных текстовых задач."
  },
  "mistralai/Mixtral-8x7B-v0.1": {
    "description": "Mixtral 8x7B - это разреженная модель эксперта, использующая множество параметров для повышения скорости вывода, подходит для обработки многоязычных и генеративных задач."
  },
  "mistralai/mistral-7b-instruct": {
    "description": "Mistral 7B Instruct — это высокопроизводительная модель стандартов отрасли, оптимизированная для скорости и поддержки длинного контекста."
  },
  "mistralai/mistral-nemo": {
    "description": "Mistral Nemo — это модель с 7.3B параметрами, поддерживающая несколько языков и высокопроизводительное программирование."
  },
  "mixtral": {
    "description": "Mixtral — это экспертная модель от Mistral AI, обладающая открытыми весами и поддерживающая генерацию кода и понимание языка."
  },
  "mixtral-8x7b-32768": {
    "description": "Mixtral 8x7B предлагает высокую отказоустойчивость параллельной обработки, подходящей для сложных задач."
  },
  "mixtral:8x22b": {
    "description": "Mixtral — это экспертная модель от Mistral AI, обладающая открытыми весами и поддерживающая генерацию кода и понимание языка."
  },
  "moonshot-v1-128k": {
    "description": "Moonshot V1 128K — это модель с возможностями обработки сверхдлинного контекста, подходящая для генерации очень длинных текстов, удовлетворяющая требованиям сложных задач генерации, способная обрабатывать до 128 000 токенов, идеально подходящая для научных исследований, академических и крупных документальных приложений."
  },
  "moonshot-v1-32k": {
    "description": "Moonshot V1 32K предлагает возможности обработки контекста средней длины, способная обрабатывать 32 768 токенов, особенно подходит для генерации различных длинных документов и сложных диалогов, применяется в создании контента, генерации отчетов и диалоговых систем."
  },
  "moonshot-v1-8k": {
    "description": "Moonshot V1 8K специально разработан для генерации коротких текстов, обладая высокой производительностью обработки, способный обрабатывать 8 192 токена, идеально подходит для кратких диалогов, стенографирования и быстрой генерации контента."
  },
  "nousresearch/hermes-2-pro-llama-3-8b": {
    "description": "Hermes 2 Pro Llama 3 8B — это обновленная версия Nous Hermes 2, содержащая последние внутренние разработанные наборы данных."
  },
  "nvidia/Llama-3.1-Nemotron-70B-Instruct": {
    "description": "Llama 3.1 Nemotron 70B - это специализированная языковая модель от NVIDIA, предназначенная для повышения степени полезности ответов, генерируемых LLM, к пользовательским запросам."
  },
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
    "description": "Llama 3.1 Nemotron 70B — это крупная языковая модель, созданная NVIDIA, предназначенная для повышения полезности ответов, генерируемых LLM, на запросы пользователей. Эта модель показала отличные результаты в таких бенчмарках, как Arena Hard, AlpacaEval 2 LC и GPT-4-Turbo MT-Bench, и на 1 октября 2024 года занимает первое место во всех трех автоматических тестах на согласование. Модель обучалась с использованием RLHF (в частности, REINFORCE), Llama-3.1-Nemotron-70B-Reward и HelpSteer2-Preference на основе модели Llama-3.1-70B-Instruct."
  },
  "o1-mini": {
    "description": "o1-mini — это быстрое и экономичное модель вывода, разработанная для программирования, математики и научных приложений. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "o1-preview": {
    "description": "o1 — это новая модель вывода от OpenAI, подходящая для сложных задач, требующих обширных общих знаний. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "open-codestral-mamba": {
    "description": "Codestral Mamba — это языковая модель Mamba 2, сосредоточенная на генерации кода, обеспечивающая мощную поддержку для сложных задач по коду и выводу."
  },
  "open-mistral-7b": {
    "description": "Mistral 7B — это компактная, но высокопроизводительная модель, хорошо подходящая для пакетной обработки и простых задач, таких как классификация и генерация текста, обладающая хорошими возможностями вывода."
  },
  "open-mistral-nemo": {
    "description": "Mistral Nemo — это 12B модель, разработанная в сотрудничестве с Nvidia, обеспечивающая выдающиеся возможности вывода и кодирования, легко интегрируемая и заменяемая."
  },
  "open-mixtral-8x22b": {
    "description": "Mixtral 8x22B — это более крупная экспертная модель, сосредоточенная на сложных задачах, предлагающая выдающиеся возможности вывода и более высокую пропускную способность."
  },
  "open-mixtral-8x7b": {
    "description": "Mixtral 8x7B — это разреженная экспертная модель, использующая несколько параметров для повышения скорости вывода, подходит для обработки многоязычных и кодовых задач."
  },
  "openai/gpt-4o": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощные способности понимания и генерации языка, подходит для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "openai/gpt-4o-mini": {
    "description": "GPT-4o mini — это последняя модель от OpenAI, выпущенная после GPT-4 Omni, поддерживающая ввод изображений и текста с выводом текста. Как их самый продвинутый компактный модель, она значительно дешевле других недавних передовых моделей и более чем на 60% дешевле GPT-3.5 Turbo. Она сохраняет передовой уровень интеллекта при значительном соотношении цена-качество. GPT-4o mini набрала 82% в тесте MMLU и в настоящее время занимает более высокое место по предпочтениям в чате, чем GPT-4."
  },
  "openai/o1-mini": {
    "description": "o1-mini — это быстрое и экономичное модель вывода, разработанная для программирования, математики и научных приложений. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "openai/o1-preview": {
    "description": "o1 — это новая модель вывода от OpenAI, подходящая для сложных задач, требующих обширных общих знаний. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "openchat/openchat-7b": {
    "description": "OpenChat 7B — это открытая языковая модель, оптимизированная с помощью стратегии \"C-RLFT (условное обучение с подкреплением)\"."
  },
  "openrouter/auto": {
    "description": "В зависимости от длины контекста, темы и сложности ваш запрос будет отправлен в Llama 3 70B Instruct, Claude 3.5 Sonnet (саморегулирующийся) или GPT-4o."
  },
  "phi3": {
    "description": "Phi-3 — это легковесная открытая модель, выпущенная Microsoft, подходящая для эффективной интеграции и масштабного вывода знаний."
  },
  "phi3:14b": {
    "description": "Phi-3 — это легковесная открытая модель, выпущенная Microsoft, подходящая для эффективной интеграции и масштабного вывода знаний."
  },
  "pixtral-12b-2409": {
    "description": "Модель Pixtral демонстрирует мощные способности в задачах графиков и понимания изображений, вопросов и ответов по документам, многомодального вывода и соблюдения инструкций, способная обрабатывать изображения в естественном разрешении и соотношении сторон, а также обрабатывать произвольное количество изображений в контекстном окне длиной до 128K токенов."
  },
  "pixtral-large-latest": {
    "description": "Pixtral Large — это открытая многомодальная модель с 1240 миллиардами параметров, основанная на Mistral Large 2. Это вторая модель в нашей многомодальной семье, демонстрирующая передовые уровни понимания изображений."
  },
  "pro-128k": {
    "description": "Spark Pro 128K оснащен огромной способностью обработки контекста, способной обрабатывать до 128K контекстной информации, что делает его особенно подходящим для анализа длинных текстов и обработки долгосрочных логических связей, обеспечивая плавную и последовательную логику и разнообразную поддержку ссылок в сложных текстовых коммуникациях."
  },
  "qwen-coder-plus-latest": {
    "description": "Модель кода Tongyi Qianwen."
  },
  "qwen-coder-turbo-latest": {
    "description": "Модель кода Tongyi Qwen."
  },
  "qwen-long": {
    "description": "Qwen — это сверхмасштабная языковая модель, поддерживающая длинный контекст текста и диалоговые функции на основе длинных документов и нескольких документов."
  },
  "qwen-math-plus-latest": {
    "description": "Математическая модель Tongyi Qwen, специально разработанная для решения математических задач."
  },
  "qwen-math-turbo-latest": {
    "description": "Математическая модель Tongyi Qwen, специально разработанная для решения математических задач."
  },
  "qwen-max": {
    "description": "Qwen-Max — это языковая модель масштаба триллиона, поддерживающая входные данные на различных языках, включая китайский и английский. В настоящее время это API, которое стоит за продуктовой версией Qwen 2.5."
  },
  "qwen-max-latest": {
    "description": "Модель языка Tongyi Qwen с уровнем масштабирования в триллионы, поддерживающая ввод на различных языках, включая китайский и английский, является API моделью, лежащей в основе продукта Tongyi Qwen 2.5."
  },
  "qwen-plus": {
    "description": "Улучшенная версия Qwen-Turbo, поддерживающая входные данные на разных языках, включая китайский и английский."
  },
  "qwen-plus-latest": {
    "description": "Улучшенная версия модели языка Tongyi Qwen, поддерживающая ввод на различных языках, включая китайский и английский."
  },
  "qwen-turbo": {
    "description": "Qwen-Turbo — это крупная языковая модель, поддерживающая входные данные на разных языках, включая китайский и английский."
  },
  "qwen-turbo-latest": {
    "description": "Модель языка Tongyi Qwen, поддерживающая ввод на различных языках, включая китайский и английский."
  },
  "qwen-vl-chat-v1": {
    "description": "Qwen VL поддерживает гибкие способы взаимодействия, включая многократные изображения, многократные вопросы и ответы, а также творческие способности."
  },
  "qwen-vl-max-latest": {
    "description": "Супер масштабная визуально-языковая модель Tongyi Qianwen. По сравнению с улучшенной версией, еще больше повышает способности визуального вывода и соблюдения инструкций, обеспечивая более высокий уровень визуального восприятия и когнитивных способностей."
  },
  "qwen-vl-plus-latest": {
    "description": "Улучшенная версия масштабной визуально-языковой модели Tongyi Qianwen. Значительно повышает способность распознавания деталей и текста, поддерживает разрешение более миллиона пикселей и изображения с произвольным соотношением сторон."
  },
  "qwen-vl-v1": {
    "description": "Инициализированная языковой моделью Qwen-7B, добавлена модель изображения, предобученная модель с разрешением входного изображения 448."
  },
  "qwen/qwen-2-7b-instruct:free": {
    "description": "Qwen2 — это новая серия крупных языковых моделей с более сильными возможностями понимания и генерации."
  },
  "qwen2": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwen2.5": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2.5-14b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 14B."
  },
  "qwen2.5-32b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 32B."
  },
  "qwen2.5-72b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 72B."
  },
  "qwen2.5-7b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 7B."
  },
  "qwen2.5-coder-1.5b-instruct": {
    "description": "Открытая версия модели кода Qwen."
  },
  "qwen2.5-coder-32b-instruct": {
    "description": "Открытая версия модели кода Tongyi Qianwen."
  },
  "qwen2.5-coder-7b-instruct": {
    "description": "Открытая версия модели кода Tongyi Qwen."
  },
  "qwen2.5-math-1.5b-instruct": {
    "description": "Модель Qwen-Math обладает выдающимися способностями к решению математических задач."
  },
  "qwen2.5-math-72b-instruct": {
    "description": "Модель Qwen-Math с мощными способностями решения математических задач."
  },
  "qwen2.5-math-7b-instruct": {
    "description": "Модель Qwen-Math с мощными способностями решения математических задач."
  },
  "qwen2.5:0.5b": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2.5:1.5b": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2.5:72b": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2:0.5b": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwen2:1.5b": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwen2:72b": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwq": {
    "description": "QwQ — это экспериментальная исследовательская модель, сосредоточенная на повышении возможностей вывода ИИ."
  },
  "qwq-32b-preview": {
    "description": "Модель QwQ — это экспериментальная исследовательская модель, разработанная командой Qwen, сосредоточенная на улучшении возможностей вывода ИИ."
  },
  "solar-1-mini-chat": {
    "description": "Solar Mini — это компактная LLM, производительность которой превосходит GPT-3.5, обладая мощными многоязычными возможностями, поддерживает английский и корейский языки, предлагая эффективное и компактное решение."
  },
  "solar-1-mini-chat-ja": {
    "description": "Solar Mini (Ja) расширяет возможности Solar Mini, сосредоточиваясь на японском языке, при этом сохраняя высокую эффективность и выдающуюся производительность в использовании английского и корейского языков."
  },
  "solar-pro": {
    "description": "Solar Pro — это высокоинтеллектуальная LLM, выпущенная Upstage, сосредоточенная на способности следовать инструкциям на одном GPU, с оценкой IFEval выше 80. В настоящее время поддерживает английский язык, официальная версия запланирована на ноябрь 2024 года, с расширением языковой поддержки и длины контекста."
  },
  "step-1-128k": {
    "description": "Балансирует производительность и стоимость, подходит для общих сценариев."
  },
  "step-1-256k": {
    "description": "Обладает сверхдлинной способностью обработки контекста, особенно подходит для анализа длинных документов."
  },
  "step-1-32k": {
    "description": "Поддерживает диалоги средней длины, подходит для различных приложений."
  },
  "step-1-8k": {
    "description": "Маленькая модель, подходящая для легковесных задач."
  },
  "step-1-flash": {
    "description": "Высокоскоростная модель, подходящая для реального времени диалогов."
  },
  "step-1.5v-mini": {
    "description": "Эта модель обладает мощными возможностями понимания видео."
  },
  "step-1v-32k": {
    "description": "Поддерживает визуальный ввод, улучшая мультимодальный опыт взаимодействия."
  },
  "step-1v-8k": {
    "description": "Небольшая визуальная модель, подходящая для базовых задач с текстом и изображениями."
  },
  "step-2-16k": {
    "description": "Поддерживает масштабные взаимодействия контекста, подходит для сложных диалоговых сценариев."
  },
  "taichu_llm": {
    "description": "Модель языка TaiChu обладает выдающимися способностями к пониманию языка, а также к созданию текстов, ответам на вопросы, программированию, математическим вычислениям, логическому выводу, анализу эмоций и резюмированию текстов. Инновационно сочетает предобучение на больших данных с богатством многопоточных знаний, постоянно совершенствуя алгоритмические технологии и поглощая новые знания о словах, структуре, грамматике и семантике из огромных объемов текстовых данных, обеспечивая пользователям более удобную информацию и услуги, а также более интеллектуальный опыт."
  },
  "togethercomputer/StripedHyena-Nous-7B": {
    "description": "StripedHyena Nous (7B) обеспечивает повышенные вычислительные возможности благодаря эффективным стратегиям и архитектуре модели."
  },
  "upstage/SOLAR-10.7B-Instruct-v1.0": {
    "description": "Upstage SOLAR Instruct v1 (11B) подходит для детализированных командных задач, обеспечивая отличные возможности обработки языка."
  },
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet устанавливает новые отраслевые стандарты, превосходя модели конкурентов и Claude 3 Opus, демонстрируя отличные результаты в широком спектре оценок, при этом обладая скоростью и стоимостью наших моделей среднего уровня."
  },
  "wizardlm2": {
    "description": "WizardLM 2 — это языковая модель, предоставляемая Microsoft AI, которая особенно хорошо проявляет себя в сложных диалогах, многоязычных задачах, выводе и интеллектуальных помощниках."
  },
  "wizardlm2:8x22b": {
    "description": "WizardLM 2 — это языковая модель, предоставляемая Microsoft AI, которая особенно хорошо проявляет себя в сложных диалогах, многоязычных задачах, выводе и интеллектуальных помощниках."
  },
  "yi-large": {
    "description": "Совершенно новая модель с триллионом параметров, обеспечивающая выдающиеся возможности для вопросов и ответов, а также генерации текста."
  },
  "yi-large-fc": {
    "description": "На основе модели yi-large поддерживает и усиливает возможности вызова инструментов, подходит для различных бизнес-сценариев, требующих создания агентов или рабочих процессов."
  },
  "yi-large-preview": {
    "description": "Начальная версия, рекомендуется использовать yi-large (новую версию)."
  },
  "yi-large-rag": {
    "description": "Высококлассный сервис на основе модели yi-large, объединяющий технологии поиска и генерации для предоставления точных ответов и услуг по поиску информации в реальном времени."
  },
  "yi-large-turbo": {
    "description": "Высокая стоимость и выдающаяся производительность. Балансировка высокой точности на основе производительности, скорости вывода и затрат."
  },
  "yi-lightning": {
    "description": "Новая высокопроизводительная модель, обеспечивающая высокое качество вывода при значительно повышенной скорости вывода."
  },
  "yi-lightning-lite": {
    "description": "Упрощенная версия, рекомендуется использовать yi-lightning."
  },
  "yi-medium": {
    "description": "Модель среднего размера с улучшенной настройкой, сбалансированная по возможностям и стоимости. Глубокая оптимизация способности следовать указаниям."
  },
  "yi-medium-200k": {
    "description": "200K сверхдлинное окно контекста, обеспечивающее глубокое понимание и генерацию длинных текстов."
  },
  "yi-spark": {
    "description": "Маленькая и мощная, легковесная и быстрая модель. Обеспечивает улучшенные математические вычисления и возможности написания кода."
  },
  "yi-vision": {
    "description": "Модель для сложных визуальных задач, обеспечивающая высокую производительность в понимании и анализе изображений."
  }
}
