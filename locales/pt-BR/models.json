{
  "01-ai/yi-1.5-34b-chat": {
    "description": "Zero Um, o mais recente modelo de ajuste fino de código aberto, com 34 bilhões de parâmetros, suporta múltiplos cenários de diálogo, com dados de treinamento de alta qualidade, alinhados às preferências humanas."
  },
  "01-ai/yi-1.5-9b-chat": {
    "description": "Zero Um, o mais recente modelo de ajuste fino de código aberto, com 9 bilhões de parâmetros, suporta múltiplos cenários de diálogo, com dados de treinamento de alta qualidade, alinhados às preferências humanas."
  },
  "360gpt-pro": {
    "description": "360GPT Pro, como um membro importante da série de modelos de IA da 360, atende a diversas aplicações de linguagem natural com sua capacidade eficiente de processamento de texto, suportando compreensão de longos textos e diálogos em múltiplas rodadas."
  },
  "360gpt-turbo": {
    "description": "360GPT Turbo oferece poderosas capacidades de computação e diálogo, com excelente compreensão semântica e eficiência de geração, sendo a solução ideal de assistente inteligente para empresas e desenvolvedores."
  },
  "360gpt-turbo-responsibility-8k": {
    "description": "360GPT Turbo Responsibility 8K enfatiza segurança semântica e responsabilidade, projetado especificamente para cenários de aplicação com altas exigências de segurança de conteúdo, garantindo precisão e robustez na experiência do usuário."
  },
  "360gpt2-o1": {
    "description": "O 360gpt2-o1 utiliza busca em árvore para construir cadeias de pensamento e introduz um mecanismo de reflexão, sendo treinado com aprendizado por reforço, o modelo possui a capacidade de auto-reflexão e correção de erros."
  },
  "360gpt2-pro": {
    "description": "360GPT2 Pro é um modelo avançado de processamento de linguagem natural lançado pela 360, com excelente capacidade de geração e compreensão de texto, destacando-se especialmente na geração e criação de conteúdo, capaz de lidar com tarefas complexas de conversão de linguagem e interpretação de papéis."
  },
  "360zhinao2-o1": {
    "description": "O 360zhinao2-o1 utiliza busca em árvore para construir cadeias de pensamento e introduz um mecanismo de reflexão, utilizando aprendizado por reforço para treinar, permitindo que o modelo tenha a capacidade de auto-reflexão e correção de erros."
  },
  "4.0Ultra": {
    "description": "Spark4.0 Ultra é a versão mais poderosa da série de grandes modelos Xinghuo, que, ao atualizar a conexão de busca online, melhora a capacidade de compreensão e resumo de conteúdo textual. É uma solução abrangente para aumentar a produtividade no trabalho e responder com precisão às demandas, sendo um produto inteligente líder na indústria."
  },
  "Baichuan2-Turbo": {
    "description": "Utiliza tecnologia de busca aprimorada para conectar completamente o grande modelo com conhecimento de domínio e conhecimento da web. Suporta upload de vários documentos, como PDF e Word, e entrada de URLs, garantindo acesso a informações de forma rápida e abrangente, com resultados precisos e profissionais."
  },
  "Baichuan3-Turbo": {
    "description": "Otimizado para cenários de alta frequência empresarial, com melhorias significativas de desempenho e excelente custo-benefício. Em comparação com o modelo Baichuan2, a criação de conteúdo aumentou em 20%, a resposta a perguntas de conhecimento em 17% e a capacidade de interpretação de papéis em 40%. O desempenho geral é superior ao do GPT-3.5."
  },
  "Baichuan3-Turbo-128k": {
    "description": "Possui uma janela de contexto ultra longa de 128K, otimizada para cenários de alta frequência empresarial, com melhorias significativas de desempenho e excelente custo-benefício. Em comparação com o modelo Baichuan2, a criação de conteúdo aumentou em 20%, a resposta a perguntas de conhecimento em 17% e a capacidade de interpretação de papéis em 40%. O desempenho geral é superior ao do GPT-3.5."
  },
  "Baichuan4": {
    "description": "O modelo é o melhor do país, superando modelos estrangeiros em tarefas em chinês, como enciclopédias, textos longos e criação de conteúdo. Também possui capacidades multimodais líderes na indústria, com desempenho excepcional em várias avaliações de referência."
  },
  "Baichuan4-Air": {
    "description": "Modelo com a melhor capacidade do país, superando modelos estrangeiros em tarefas em chinês como enciclopédia, textos longos e criação de conteúdo. Também possui capacidades multimodais líderes da indústria, com excelente desempenho em várias avaliações de referência."
  },
  "Baichuan4-Turbo": {
    "description": "Modelo com a melhor capacidade do país, superando modelos estrangeiros em tarefas em chinês como enciclopédia, textos longos e criação de conteúdo. Também possui capacidades multimodais líderes da indústria, com excelente desempenho em várias avaliações de referência."
  },
  "DeepSeek-R1": {
    "description": "LLM eficiente de ponta, especializado em raciocínio, matemática e programação."
  },
  "DeepSeek-R1-Distill-Llama-70B": {
    "description": "DeepSeek R1 — o modelo maior e mais inteligente do conjunto DeepSeek — foi destilado para a arquitetura Llama 70B. Com base em testes de benchmark e avaliações humanas, este modelo é mais inteligente do que o Llama 70B original, destacando-se especialmente em tarefas que exigem precisão matemática e factual."
  },
  "DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Modelo de destilação DeepSeek-R1 baseado no Qwen2.5-Math-1.5B, otimizado para desempenho de inferência através de aprendizado por reforço e dados de inicialização fria, modelo de código aberto que redefine os padrões de múltiplas tarefas."
  },
  "DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Modelo de destilação DeepSeek-R1 baseado no Qwen2.5-14B, otimizado para desempenho de inferência através de aprendizado por reforço e dados de inicialização fria, modelo de código aberto que redefine os padrões de múltiplas tarefas."
  },
  "DeepSeek-R1-Distill-Qwen-32B": {
    "description": "A série DeepSeek-R1 otimiza o desempenho de inferência através de aprendizado por reforço e dados de inicialização fria, modelo de código aberto que redefine os padrões de múltiplas tarefas, superando o nível do OpenAI-o1-mini."
  },
  "DeepSeek-R1-Distill-Qwen-7B": {
    "description": "Modelo de destilação DeepSeek-R1 baseado no Qwen2.5-Math-7B, otimizado para desempenho de inferência através de aprendizado por reforço e dados de inicialização fria, modelo de código aberto que redefine os padrões de múltiplas tarefas."
  },
  "Doubao-1.5-vision-pro-32k": {
    "description": "Doubao-1.5-vision-pro é um modelo multimodal de grande porte totalmente atualizado, que suporta reconhecimento de imagens em qualquer resolução e proporções extremas, melhorando a capacidade de raciocínio visual, reconhecimento de documentos, compreensão de informações detalhadas e seguimento de instruções."
  },
  "Doubao-lite-128k": {
    "description": "Doubao-lite possui uma velocidade de resposta excepcional e uma melhor relação custo-benefício, oferecendo opções mais flexíveis para diferentes cenários dos clientes. Suporta raciocínio e ajuste fino em janelas de contexto de 128k."
  },
  "Doubao-lite-32k": {
    "description": "Doubao-lite possui uma velocidade de resposta excepcional e uma melhor relação custo-benefício, oferecendo opções mais flexíveis para diferentes cenários dos clientes. Suporta raciocínio e ajuste fino em janelas de contexto de 32k."
  },
  "Doubao-lite-4k": {
    "description": "Doubao-lite possui uma velocidade de resposta excepcional e uma melhor relação custo-benefício, oferecendo opções mais flexíveis para diferentes cenários dos clientes. Suporta raciocínio e ajuste fino em janelas de contexto de 4k."
  },
  "Doubao-pro-128k": {
    "description": "O modelo principal com o melhor desempenho, adequado para tarefas complexas, apresentando excelentes resultados em cenários como perguntas e respostas, resumos, criação, classificação de texto e interpretação de papéis. Suporta raciocínio e ajuste fino em janelas de contexto de 128k."
  },
  "Doubao-pro-256k": {
    "description": "O modelo principal com o melhor desempenho, adequado para lidar com tarefas complexas, apresentando bons resultados em cenários como perguntas e respostas de referência, resumos, criação, classificação de texto e interpretação de papéis. Suporta raciocínio e ajuste fino com janelas de contexto de 256k."
  },
  "Doubao-pro-32k": {
    "description": "O modelo principal com o melhor desempenho, adequado para tarefas complexas, apresentando excelentes resultados em cenários como perguntas e respostas, resumos, criação, classificação de texto e interpretação de papéis. Suporta raciocínio e ajuste fino em janelas de contexto de 32k."
  },
  "Doubao-pro-4k": {
    "description": "O modelo principal com o melhor desempenho, adequado para tarefas complexas, apresentando excelentes resultados em cenários como perguntas e respostas, resumos, criação, classificação de texto e interpretação de papéis. Suporta raciocínio e ajuste fino em janelas de contexto de 4k."
  },
  "Doubao-vision-lite-32k": {
    "description": "O modelo Doubao-vision é um modelo multimodal de grande porte lançado pela Doubao, com poderosas capacidades de compreensão e raciocínio de imagens, além de uma compreensão precisa de instruções. O modelo demonstrou um desempenho robusto em extração de informações textuais de imagens e tarefas de raciocínio baseadas em imagens, podendo ser aplicado em tarefas de perguntas e respostas visuais mais complexas e abrangentes."
  },
  "Doubao-vision-pro-32k": {
    "description": "O modelo Doubao-vision é um modelo multimodal de grande porte lançado pela Doubao, com poderosas capacidades de compreensão e raciocínio de imagens, além de uma compreensão precisa de instruções. O modelo demonstrou um desempenho robusto em extração de informações textuais de imagens e tarefas de raciocínio baseadas em imagens, podendo ser aplicado em tarefas de perguntas e respostas visuais mais complexas e abrangentes."
  },
  "ERNIE-3.5-128K": {
    "description": "Modelo de linguagem de grande escala desenvolvido pela Baidu, cobrindo uma vasta quantidade de dados em chinês e inglês, com poderosas capacidades gerais, capaz de atender à maioria das demandas de perguntas e respostas em diálogos, geração de conteúdo e aplicações de plugins; suporta integração automática com o plugin de busca da Baidu, garantindo a atualidade das informações nas respostas."
  },
  "ERNIE-3.5-8K": {
    "description": "Modelo de linguagem de grande escala desenvolvido pela Baidu, cobrindo uma vasta quantidade de dados em chinês e inglês, com poderosas capacidades gerais, capaz de atender à maioria das demandas de perguntas e respostas em diálogos, geração de conteúdo e aplicações de plugins; suporta integração automática com o plugin de busca da Baidu, garantindo a atualidade das informações nas respostas."
  },
  "ERNIE-3.5-8K-Preview": {
    "description": "Modelo de linguagem de grande escala desenvolvido pela Baidu, cobrindo uma vasta quantidade de dados em chinês e inglês, com poderosas capacidades gerais, capaz de atender à maioria das demandas de perguntas e respostas em diálogos, geração de conteúdo e aplicações de plugins; suporta integração automática com o plugin de busca da Baidu, garantindo a atualidade das informações nas respostas."
  },
  "ERNIE-4.0-8K-Latest": {
    "description": "Modelo de linguagem ultra grande escala desenvolvido pela Baidu, que em comparação com o ERNIE 3.5, apresenta uma atualização completa nas capacidades do modelo, amplamente aplicável em cenários de tarefas complexas em diversas áreas; suporta integração automática com o plugin de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas."
  },
  "ERNIE-4.0-8K-Preview": {
    "description": "Modelo de linguagem ultra grande escala desenvolvido pela Baidu, que em comparação com o ERNIE 3.5, apresenta uma atualização completa nas capacidades do modelo, amplamente aplicável em cenários de tarefas complexas em diversas áreas; suporta integração automática com o plugin de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas."
  },
  "ERNIE-4.0-Turbo-8K-Latest": {
    "description": "Modelo de linguagem de última geração desenvolvido pela Baidu, com desempenho excepcional em uma ampla gama de cenários de tarefas complexas; suporta integração automática com plugins de busca da Baidu, garantindo a relevância da informação nas respostas. Supera o desempenho do ERNIE 4.0."
  },
  "ERNIE-4.0-Turbo-8K-Preview": {
    "description": "Modelo de linguagem ultra grande escala desenvolvido pela Baidu, com desempenho excepcional em resultados gerais, amplamente aplicável em cenários de tarefas complexas em diversas áreas; suporta integração automática com o plugin de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas. Em comparação com o ERNIE 4.0, apresenta desempenho superior."
  },
  "ERNIE-Character-8K": {
    "description": "Modelo de linguagem vertical desenvolvido pela Baidu, adequado para aplicações como NPCs em jogos, diálogos de atendimento ao cliente e interpretação de personagens em diálogos, com estilos de personagem mais distintos e consistentes, maior capacidade de seguir instruções e desempenho de inferência superior."
  },
  "ERNIE-Lite-Pro-128K": {
    "description": "Modelo de linguagem leve desenvolvido pela Baidu, que combina excelente desempenho do modelo com eficiência de inferência, apresentando resultados superiores ao ERNIE Lite, adequado para uso em inferência com placas de aceleração de IA de baixo poder computacional."
  },
  "ERNIE-Speed-128K": {
    "description": "Modelo de linguagem de alto desempenho desenvolvido pela Baidu, lançado em 2024, com capacidades gerais excepcionais, adequado como modelo base para ajuste fino, melhorando o tratamento de problemas em cenários específicos, enquanto mantém excelente desempenho de inferência."
  },
  "ERNIE-Speed-Pro-128K": {
    "description": "Modelo de linguagem de alto desempenho desenvolvido pela Baidu, lançado em 2024, com capacidades gerais excepcionais, apresentando resultados superiores ao ERNIE Speed, adequado como modelo base para ajuste fino, melhorando o tratamento de problemas em cenários específicos, enquanto mantém excelente desempenho de inferência."
  },
  "Gryphe/MythoMax-L2-13b": {
    "description": "MythoMax-L2 (13B) é um modelo inovador, adequado para aplicações em múltiplas áreas e tarefas complexas."
  },
  "InternVL2-8B": {
    "description": "InternVL2-8B é um poderoso modelo de linguagem visual, que suporta processamento multimodal de imagens e textos, capaz de identificar com precisão o conteúdo da imagem e gerar descrições ou respostas relevantes."
  },
  "InternVL2.5-26B": {
    "description": "InternVL2.5-26B é um poderoso modelo de linguagem visual, que suporta processamento multimodal de imagens e textos, capaz de identificar com precisão o conteúdo da imagem e gerar descrições ou respostas relevantes."
  },
  "Llama-3.2-11B-Vision-Instruct": {
    "description": "Capacidade de raciocínio de imagem excepcional em imagens de alta resolução, adequada para aplicações de compreensão visual."
  },
  "Llama-3.2-90B-Vision-Instruct\t": {
    "description": "Capacidade avançada de raciocínio de imagem para aplicações de agentes de compreensão visual."
  },
  "Meta-Llama-3.1-405B-Instruct": {
    "description": "Modelo de texto ajustado para instruções Llama 3.1, otimizado para casos de uso de diálogos multilíngues, apresentando desempenho superior em muitos modelos de chat de código aberto e fechados em benchmarks da indústria."
  },
  "Meta-Llama-3.1-70B-Instruct": {
    "description": "Modelo de texto ajustado para instruções Llama 3.1, otimizado para casos de uso de diálogos multilíngues, apresentando desempenho superior em muitos modelos de chat de código aberto e fechados em benchmarks da indústria."
  },
  "Meta-Llama-3.1-8B-Instruct": {
    "description": "Modelo de texto ajustado para instruções Llama 3.1, otimizado para casos de uso de diálogos multilíngues, apresentando desempenho superior em muitos modelos de chat de código aberto e fechados em benchmarks da indústria."
  },
  "Meta-Llama-3.2-1B-Instruct": {
    "description": "Modelo de linguagem pequeno de ponta, com compreensão de linguagem, excelente capacidade de raciocínio e geração de texto."
  },
  "Meta-Llama-3.2-3B-Instruct": {
    "description": "Modelo de linguagem pequeno de ponta, com compreensão de linguagem, excelente capacidade de raciocínio e geração de texto."
  },
  "Meta-Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 é o modelo de linguagem de código aberto multilíngue mais avançado da série Llama, oferecendo desempenho comparável ao modelo de 405B a um custo extremamente baixo. Baseado na estrutura Transformer, e aprimorado por meio de ajuste fino supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para aumentar a utilidade e a segurança. Sua versão ajustada para instruções é otimizada para diálogos multilíngues, superando muitos modelos de chat de código aberto e fechados em vários benchmarks da indústria. A data limite de conhecimento é dezembro de 2023."
  },
  "MiniMax-Text-01": {
    "description": "Na série de modelos MiniMax-01, fizemos inovações ousadas: pela primeira vez, implementamos em larga escala um mecanismo de atenção linear, tornando a arquitetura Transformer tradicional não mais a única opção. Este modelo possui um total de 456 bilhões de parâmetros, com 45,9 bilhões ativados em uma única vez. O desempenho geral do modelo é comparável aos melhores modelos internacionais, enquanto lida eficientemente com contextos de até 4 milhões de tokens, 32 vezes mais que o GPT-4o e 20 vezes mais que o Claude-3.5-Sonnet."
  },
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) é um modelo de instrução de alta precisão, adequado para cálculos complexos."
  },
  "Phi-3-medium-128k-instruct": {
    "description": "Mesmo modelo Phi-3-medium, mas com um tamanho de contexto maior para RAG ou prompting de poucos exemplos."
  },
  "Phi-3-medium-4k-instruct": {
    "description": "Um modelo de 14B parâmetros, que apresenta melhor qualidade do que o Phi-3-mini, com foco em dados densos de raciocínio de alta qualidade."
  },
  "Phi-3-mini-128k-instruct": {
    "description": "Mesmo modelo Phi-3-mini, mas com um tamanho de contexto maior para RAG ou prompting de poucos exemplos."
  },
  "Phi-3-mini-4k-instruct": {
    "description": "O menor membro da família Phi-3. Otimizado tanto para qualidade quanto para baixa latência."
  },
  "Phi-3-small-128k-instruct": {
    "description": "Mesmo modelo Phi-3-small, mas com um tamanho de contexto maior para RAG ou prompting de poucos exemplos."
  },
  "Phi-3-small-8k-instruct": {
    "description": "Um modelo de 7B parâmetros, que apresenta melhor qualidade do que o Phi-3-mini, com foco em dados densos de raciocínio de alta qualidade."
  },
  "Phi-3.5-mini-instruct": {
    "description": "Versão atualizada do modelo Phi-3-mini."
  },
  "Phi-3.5-vision-instrust": {
    "description": "Versão atualizada do modelo Phi-3-vision."
  },
  "Pro/Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct é um modelo de linguagem de grande escala com ajuste fino para instruções na série Qwen2, com um tamanho de parâmetro de 1.5B. Este modelo é baseado na arquitetura Transformer, utilizando funções de ativação SwiGLU, viés de atenção QKV e atenção de consulta em grupo. Ele se destaca em compreensão de linguagem, geração, capacidade multilíngue, codificação, matemática e raciocínio em vários benchmarks, superando a maioria dos modelos de código aberto. Em comparação com o Qwen1.5-1.8B-Chat, o Qwen2-1.5B-Instruct mostrou melhorias significativas de desempenho em testes como MMLU, HumanEval, GSM8K, C-Eval e IFEval, apesar de ter um número de parâmetros ligeiramente menor."
  },
  "Pro/Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-7B-Instruct é um modelo de linguagem de grande escala com ajuste fino para instruções na série Qwen2, com um tamanho de parâmetro de 7B. Este modelo é baseado na arquitetura Transformer, utilizando funções de ativação SwiGLU, viés de atenção QKV e atenção de consulta em grupo. Ele é capaz de lidar com entradas em larga escala. O modelo se destaca em compreensão de linguagem, geração, capacidade multilíngue, codificação, matemática e raciocínio em vários benchmarks, superando a maioria dos modelos de código aberto e demonstrando competitividade comparável a modelos proprietários em algumas tarefas. O Qwen2-7B-Instruct superou o Qwen1.5-7B-Chat em várias avaliações, mostrando melhorias significativas de desempenho."
  },
  "Pro/Qwen/Qwen2-VL-7B-Instruct": {
    "description": "Qwen2-VL é a versão mais recente do modelo Qwen-VL, alcançando desempenho de ponta em testes de compreensão visual."
  },
  "Pro/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct é um dos mais recentes modelos de linguagem de grande escala lançados pela Alibaba Cloud. Este modelo de 7B apresenta melhorias significativas em áreas como codificação e matemática. O modelo também oferece suporte multilíngue, abrangendo mais de 29 idiomas, incluindo chinês e inglês. O modelo teve melhorias significativas em seguir instruções, entender dados estruturados e gerar saídas estruturadas (especialmente JSON)."
  },
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct é a versão mais recente da série de modelos de linguagem de grande escala específicos para código lançada pela Alibaba Cloud. Este modelo, baseado no Qwen2.5, foi treinado com 55 trilhões de tokens, melhorando significativamente a capacidade de geração, raciocínio e correção de código. Ele não apenas aprimora a capacidade de codificação, mas também mantém as vantagens em matemática e habilidades gerais. O modelo fornece uma base mais abrangente para aplicações práticas, como agentes de código."
  },
  "Pro/Qwen/Qwen2.5-VL-7B-Instruct": {
    "description": "Qwen2.5-VL é o novo membro da série Qwen, com capacidades avançadas de compreensão visual. Ele pode analisar textos, gráficos e layouts em imagens, compreender vídeos longos e capturar eventos. Capaz de realizar raciocínios, manipular ferramentas, suporta localização de objetos em múltiplos formatos e geração de saídas estruturadas. Otimiza a compreensão de vídeos através de treinamento com resolução dinâmica e taxa de quadros, além de melhorar a eficiência do codificador visual."
  },
  "Pro/THUDM/glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat é a versão de código aberto da série de modelos pré-treinados GLM-4 lançada pela Zhipu AI. Este modelo se destaca em semântica, matemática, raciocínio, código e conhecimento. Além de suportar diálogos de múltiplas rodadas, o GLM-4-9B-Chat também possui recursos avançados como navegação na web, execução de código, chamadas de ferramentas personalizadas (Function Call) e raciocínio de longo texto. O modelo suporta 26 idiomas, incluindo chinês, inglês, japonês, coreano e alemão. Em vários benchmarks, o GLM-4-9B-Chat demonstrou desempenho excepcional, como AlignBench-v2, MT-Bench, MMLU e C-Eval. O modelo suporta um comprimento de contexto máximo de 128K, adequado para pesquisa acadêmica e aplicações comerciais."
  },
  "Pro/deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 é um modelo de inferência impulsionado por aprendizado por reforço (RL), que resolve problemas de repetitividade e legibilidade no modelo. Antes do RL, o DeepSeek-R1 introduziu dados de inicialização a frio, otimizando ainda mais o desempenho de inferência. Ele se compara ao OpenAI-o1 em tarefas matemáticas, de código e de inferência, e melhora o desempenho geral por meio de métodos de treinamento cuidadosamente projetados."
  },
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "DeepSeek-R1-Distill-Qwen-1.5B é um modelo obtido por destilação de conhecimento baseado no Qwen2.5-Math-1.5B. Este modelo foi refinado usando 800 mil amostras selecionadas geradas pelo DeepSeek-R1, demonstrando desempenho notável em diversos benchmarks. Como um modelo leve, alcançou 83,9% de precisão no MATH-500, 28,9% de taxa de aprovação no AIME 2024 e uma pontuação de 954 no CodeForces, exibindo capacidades de raciocínio que superam seu tamanho de parâmetros."
  },
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B é um modelo obtido por destilação de conhecimento baseado no Qwen2.5-Math-7B. Este modelo foi refinado usando 800 mil amostras selecionadas geradas pelo DeepSeek-R1, demonstrando excelente capacidade de raciocínio. Apresenta desempenho destacado em diversos benchmarks, alcançando 92,8% de precisão no MATH-500, 55,5% de taxa de aprovação no AIME 2024 e uma pontuação de 1189 no CodeForces, mostrando forte competência em matemática e programação para um modelo de escala 7B."
  },
  "Pro/deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 é um modelo de linguagem com 671 bilhões de parâmetros, utilizando uma arquitetura de especialistas mistos (MoE) com atenção potencial de múltiplas cabeças (MLA) e uma estratégia de balanceamento de carga sem perda auxiliar, otimizando a eficiência de inferência e treinamento. Pré-treinado em 14,8 trilhões de tokens de alta qualidade, e ajustado por supervisão e aprendizado por reforço, o DeepSeek-V3 supera outros modelos de código aberto, aproximando-se de modelos fechados líderes."
  },
  "QwQ-32B-Preview": {
    "description": "O QwQ-32B-Preview é um modelo de processamento de linguagem natural inovador, capaz de lidar eficientemente com tarefas complexas de geração de diálogos e compreensão de contexto."
  },
  "Qwen/QVQ-72B-Preview": {
    "description": "QVQ-72B-Preview é um modelo de pesquisa desenvolvido pela equipe Qwen, focado em capacidades de raciocínio visual, apresentando vantagens únicas na compreensão de cenários complexos e na resolução de problemas matemáticos relacionados à visão."
  },
  "Qwen/QwQ-32B": {
    "description": "QwQ é o modelo de inferência da série Qwen. Em comparação com modelos tradicionais de ajuste de instruções, o QwQ possui habilidades de raciocínio e inferência, permitindo um desempenho significativamente melhorado em tarefas de downstream, especialmente na resolução de problemas difíceis. O QwQ-32B é um modelo de inferência de médio porte, capaz de obter um desempenho competitivo em comparação com modelos de inferência de ponta, como DeepSeek-R1 e o1-mini. Este modelo utiliza tecnologias como RoPE, SwiGLU, RMSNorm e viés de atenção QKV, apresentando uma estrutura de rede de 64 camadas e 40 cabeças de atenção Q (sendo KV 8 no GQA)."
  },
  "Qwen/QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview é o mais recente modelo de pesquisa experimental da Qwen, focado em melhorar a capacidade de raciocínio da IA. Ao explorar mecanismos complexos como mistura de linguagem e raciocínio recursivo, suas principais vantagens incluem forte capacidade de análise de raciocínio, habilidades matemáticas e de programação. Ao mesmo tempo, existem questões de troca de linguagem, ciclos de raciocínio, considerações de segurança e diferenças em outras capacidades."
  },
  "Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct é um modelo de linguagem de grande escala com ajuste fino para instruções na série Qwen2, com um tamanho de parâmetro de 1.5B. Este modelo é baseado na arquitetura Transformer, utilizando funções de ativação SwiGLU, viés de atenção QKV e atenção de consulta em grupo. Ele se destaca em compreensão de linguagem, geração, capacidade multilíngue, codificação, matemática e raciocínio em vários benchmarks, superando a maioria dos modelos de código aberto. Em comparação com o Qwen1.5-1.8B-Chat, o Qwen2-1.5B-Instruct mostrou melhorias significativas de desempenho em testes como MMLU, HumanEval, GSM8K, C-Eval e IFEval, apesar de ter um número de parâmetros ligeiramente menor."
  },
  "Qwen/Qwen2-72B-Instruct": {
    "description": "Qwen2 é um modelo de linguagem universal avançado, suportando diversos tipos de instruções."
  },
  "Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-72B-Instruct é um modelo de linguagem de grande escala com ajuste fino para instruções na série Qwen2, com um tamanho de parâmetro de 72B. Este modelo é baseado na arquitetura Transformer, utilizando funções de ativação SwiGLU, viés de atenção QKV e atenção de consulta em grupo. Ele é capaz de lidar com entradas em larga escala. O modelo se destaca em compreensão de linguagem, geração, capacidade multilíngue, codificação, matemática e raciocínio em vários benchmarks, superando a maioria dos modelos de código aberto e demonstrando competitividade comparável a modelos proprietários em algumas tarefas."
  },
  "Qwen/Qwen2-VL-72B-Instruct": {
    "description": "Qwen2-VL é a versão mais recente do modelo Qwen-VL, alcançando desempenho de ponta em testes de compreensão visual."
  },
  "Qwen/Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5 é uma nova série de modelos de linguagem em larga escala, projetada para otimizar o processamento de tarefas instrucionais."
  },
  "Qwen/Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5 é uma nova série de modelos de linguagem em larga escala, projetada para otimizar o processamento de tarefas instrucionais."
  },
  "Qwen/Qwen2.5-72B-Instruct": {
    "description": "Modelo de linguagem de grande escala desenvolvido pela equipe Qianwen da Alibaba Cloud."
  },
  "Qwen/Qwen2.5-72B-Instruct-128K": {
    "description": "Qwen2.5 é uma nova série de grandes modelos de linguagem, com capacidades de compreensão e geração aprimoradas."
  },
  "Qwen/Qwen2.5-72B-Instruct-Turbo": {
    "description": "Qwen2.5 é uma nova série de grandes modelos de linguagem, projetada para otimizar o processamento de tarefas instrucionais."
  },
  "Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5 é uma nova série de modelos de linguagem em larga escala, projetada para otimizar o processamento de tarefas instrucionais."
  },
  "Qwen/Qwen2.5-7B-Instruct-Turbo": {
    "description": "Qwen2.5 é uma nova série de grandes modelos de linguagem, projetada para otimizar o processamento de tarefas instrucionais."
  },
  "Qwen/Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder foca na escrita de código."
  },
  "Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct é a versão mais recente da série de modelos de linguagem de grande escala específicos para código lançada pela Alibaba Cloud. Este modelo, baseado no Qwen2.5, foi treinado com 55 trilhões de tokens, melhorando significativamente a capacidade de geração, raciocínio e correção de código. Ele não apenas aprimora a capacidade de codificação, mas também mantém as vantagens em matemática e habilidades gerais. O modelo fornece uma base mais abrangente para aplicações práticas, como agentes de código."
  },
  "Qwen/Qwen2.5-VL-32B-Instruct": {
    "description": "Qwen2.5-VL-32B-Instruct é um modelo multimodal de grande escala desenvolvido pela equipe Tongyi Qianwen, parte da série Qwen2.5-VL. Este modelo não apenas domina o reconhecimento de objetos comuns, mas também pode analisar textos, gráficos, ícones, diagramas e layouts em imagens. Ele funciona como um agente visual inteligente, capaz de raciocinar e manipular ferramentas dinamicamente, com habilidades para operar computadores e smartphones. Além disso, o modelo pode localizar objetos em imagens com precisão e gerar saídas estruturadas para documentos como faturas e tabelas. Em comparação com a versão anterior Qwen2-VL, esta versão apresenta melhorias significativas em habilidades matemáticas e de resolução de problemas através de aprendizado por reforço, com um estilo de resposta mais alinhado às preferências humanas."
  },
  "Qwen/Qwen2.5-VL-72B-Instruct": {
    "description": "Qwen2.5-VL é o modelo de linguagem visual da série Qwen2.5. Este modelo apresenta melhorias significativas em vários aspectos: possui capacidade aprimorada de compreensão visual, podendo reconhecer objetos comuns, analisar textos, gráficos e layouts; atua como um agente visual capaz de raciocinar e orientar dinamicamente o uso de ferramentas; suporta a compreensão de vídeos longos com mais de 1 hora de duração, capturando eventos-chave; pode localizar objetos em imagens com precisão através da geração de caixas delimitadoras ou pontos; suporta a geração de saídas estruturadas, sendo especialmente útil para dados digitalizados como faturas e tabelas."
  },
  "Qwen2-72B-Instruct": {
    "description": "Qwen2 é a mais recente série do modelo Qwen, suportando 128k de contexto. Em comparação com os melhores modelos de código aberto atuais, o Qwen2-72B supera significativamente os modelos líderes em várias capacidades, incluindo compreensão de linguagem natural, conhecimento, código, matemática e multilinguismo."
  },
  "Qwen2-7B-Instruct": {
    "description": "Qwen2 é a mais recente série do modelo Qwen, capaz de superar modelos de código aberto de tamanho equivalente e até mesmo modelos de maior escala. O Qwen2 7B obteve vantagens significativas em várias avaliações, especialmente em compreensão de código e chinês."
  },
  "Qwen2-VL-72B": {
    "description": "O Qwen2-VL-72B é um poderoso modelo de linguagem visual, que suporta processamento multimodal de imagens e texto, capaz de reconhecer com precisão o conteúdo das imagens e gerar descrições ou respostas relacionadas."
  },
  "Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5-14B-Instruct é um grande modelo de linguagem com 14 bilhões de parâmetros, com desempenho excelente, otimizado para cenários em chinês e multilíngues, suportando aplicações como perguntas e respostas inteligentes e geração de conteúdo."
  },
  "Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5-32B-Instruct é um grande modelo de linguagem com 32 bilhões de parâmetros, com desempenho equilibrado, otimizado para cenários em chinês e multilíngues, suportando aplicações como perguntas e respostas inteligentes e geração de conteúdo."
  },
  "Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct suporta 16k de contexto, gerando textos longos com mais de 8K. Suporta chamadas de função e interação sem costura com sistemas externos, aumentando significativamente a flexibilidade e escalabilidade. O conhecimento do modelo aumentou consideravelmente, e suas habilidades em codificação e matemática melhoraram muito, com suporte a mais de 29 idiomas."
  },
  "Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct é um grande modelo de linguagem com 7 bilhões de parâmetros, que suporta chamadas de função e interação sem costura com sistemas externos, aumentando significativamente a flexibilidade e escalabilidade. Otimizado para cenários em chinês e multilíngues, suporta aplicações como perguntas e respostas inteligentes e geração de conteúdo."
  },
  "Qwen2.5-Coder-14B-Instruct": {
    "description": "O Qwen2.5-Coder-14B-Instruct é um modelo de instrução de programação baseado em pré-treinamento em larga escala, com forte capacidade de compreensão e geração de código, capaz de lidar eficientemente com diversas tarefas de programação, especialmente adequado para escrita inteligente de código, geração de scripts automatizados e resolução de problemas de programação."
  },
  "Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder-32B-Instruct é um grande modelo de linguagem projetado para geração de código, compreensão de código e cenários de desenvolvimento eficiente, com uma escala de 32 bilhões de parâmetros, atendendo a diversas necessidades de programação."
  },
  "SenseChat": {
    "description": "Modelo da versão básica (V4), com comprimento de contexto de 4K, com capacidades gerais poderosas."
  },
  "SenseChat-128K": {
    "description": "Modelo da versão básica (V4), com comprimento de contexto de 128K, se destaca em tarefas de compreensão e geração de textos longos."
  },
  "SenseChat-32K": {
    "description": "Modelo da versão básica (V4), com comprimento de contexto de 32K, aplicável de forma flexível em diversos cenários."
  },
  "SenseChat-5": {
    "description": "Modelo da versão mais recente (V5.5), com comprimento de contexto de 128K, com capacidades significativamente aprimoradas em raciocínio matemático, diálogos em inglês, seguimento de instruções e compreensão de textos longos, rivalizando com o GPT-4o."
  },
  "SenseChat-5-1202": {
    "description": "É a versão mais recente baseada no V5.5, com melhorias significativas em várias dimensões, como habilidades básicas em chinês e inglês, conversação, conhecimento em ciências exatas, conhecimento em ciências humanas, redação, lógica matemática e controle de contagem de palavras em comparação com a versão anterior."
  },
  "SenseChat-5-Cantonese": {
    "description": "Comprimento de contexto de 32K, superando o GPT-4 na compreensão de diálogos em cantonês, competindo com o GPT-4 Turbo em várias áreas, incluindo conhecimento, raciocínio, matemática e programação."
  },
  "SenseChat-Character": {
    "description": "Modelo padrão, com comprimento de contexto de 8K, alta velocidade de resposta."
  },
  "SenseChat-Character-Pro": {
    "description": "Modelo avançado, com comprimento de contexto de 32K, com capacidades amplamente aprimoradas, suportando diálogos em chinês e inglês."
  },
  "SenseChat-Turbo": {
    "description": "Adequado para perguntas rápidas e cenários de ajuste fino do modelo."
  },
  "SenseChat-Turbo-1202": {
    "description": "É a versão mais recente do modelo leve, alcançando mais de 90% da capacidade do modelo completo, reduzindo significativamente o custo de inferência."
  },
  "SenseChat-Vision": {
    "description": "Modelo da versão mais recente (V5.5), suporta entrada de múltiplas imagens, otimizando completamente as capacidades básicas do modelo, com grandes melhorias em reconhecimento de atributos de objetos, relações espaciais, reconhecimento de eventos, compreensão de cenários, reconhecimento de emoções, raciocínio lógico e compreensão e geração de texto."
  },
  "Skylark2-lite-8k": {
    "description": "Modelo de segunda geração Skylark, o modelo Skylark2-lite possui alta velocidade de resposta, adequado para cenários que exigem alta capacidade de resposta, sensíveis ao custo e com baixa exigência de precisão do modelo, com uma janela de contexto de 8k."
  },
  "Skylark2-pro-32k": {
    "description": "Modelo de segunda geração Skylark, a versão Skylark2-pro possui alta precisão, adequada para cenários de geração de texto mais complexos, como geração de textos em campos especializados, criação de romances e traduções de alta qualidade, com uma janela de contexto de 32k."
  },
  "Skylark2-pro-4k": {
    "description": "Modelo de segunda geração Skylark, o modelo Skylark2-pro possui alta precisão, adequado para cenários de geração de texto mais complexos, como geração de textos em campos especializados, criação de romances e traduções de alta qualidade, com uma janela de contexto de 4k."
  },
  "Skylark2-pro-character-4k": {
    "description": "Modelo de segunda geração Skylark, o modelo Skylark2-pro-character possui excelentes habilidades de interpretação de papéis e chat, especializado em interpretar diferentes papéis com base nas solicitações do usuário e engajar em conversas, apresentando um estilo de personagem distinto e um conteúdo de diálogo natural e fluído, adequado para construir chatbots, assistentes virtuais e atendimento ao cliente online, com alta velocidade de resposta."
  },
  "Skylark2-pro-turbo-8k": {
    "description": "Modelo de segunda geração Skylark, o Skylark2-pro-turbo-8k proporciona raciocínio mais rápido e menor custo, com uma janela de contexto de 8k."
  },
  "THUDM/chatglm3-6b": {
    "description": "ChatGLM3-6B é um modelo de código aberto da série ChatGLM, desenvolvido pela Zhipu AI. Este modelo mantém as excelentes características da geração anterior, como fluência no diálogo e baixo custo de implantação, enquanto introduz novos recursos. Ele utiliza dados de treinamento mais variados, um número de passos de treinamento mais robusto e uma estratégia de treinamento mais razoável, destacando-se entre modelos pré-treinados abaixo de 10B. O ChatGLM3-6B suporta diálogos de múltiplas rodadas, chamadas de ferramentas, execução de código e tarefas de agente em cenários complexos. Além do modelo de diálogo, também foram lançados o modelo base ChatGLM-6B-Base e o modelo de diálogo de longo texto ChatGLM3-6B-32K. Este modelo está completamente aberto para pesquisa acadêmica e permite uso comercial gratuito após registro."
  },
  "THUDM/glm-4-9b-chat": {
    "description": "GLM-4 9B é uma versão de código aberto, oferecendo uma experiência de diálogo otimizada para aplicações de conversa."
  },
  "TeleAI/TeleChat2": {
    "description": "O modelo TeleChat2 é um modelo semântico gerador desenvolvido de forma independente pela China Telecom, que suporta funções como perguntas e respostas enciclopédicas, geração de código e geração de textos longos, oferecendo serviços de consulta de diálogo aos usuários, permitindo interações de diálogo, respondendo perguntas e auxiliando na criação, ajudando os usuários a obter informações, conhecimento e inspiração de forma eficiente e conveniente. O modelo apresenta um desempenho notável em questões de alucinação, geração de textos longos e compreensão lógica."
  },
  "Vendor-A/Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct é um dos mais recentes modelos de linguagem de grande escala lançados pela Alibaba Cloud. Este modelo de 72B apresenta melhorias significativas em áreas como codificação e matemática. O modelo também oferece suporte multilíngue, abrangendo mais de 29 idiomas, incluindo chinês e inglês. O modelo teve melhorias significativas em seguir instruções, entender dados estruturados e gerar saídas estruturadas (especialmente JSON)."
  },
  "Yi-34B-Chat": {
    "description": "Yi-1.5-34B, mantendo as excelentes habilidades linguísticas do modelo original, aumentou significativamente suas capacidades de lógica matemática e codificação através de treinamento incremental com 500 bilhões de tokens de alta qualidade."
  },
  "abab5.5-chat": {
    "description": "Voltado para cenários de produtividade, suportando o processamento de tarefas complexas e geração de texto eficiente, adequado para aplicações em áreas profissionais."
  },
  "abab5.5s-chat": {
    "description": "Projetado para cenários de diálogo de personagens em chinês, oferecendo capacidade de geração de diálogos de alta qualidade em chinês, adequado para várias aplicações."
  },
  "abab6.5g-chat": {
    "description": "Projetado para diálogos de personagens multilíngues, suportando geração de diálogos de alta qualidade em inglês e várias outras línguas."
  },
  "abab6.5s-chat": {
    "description": "Adequado para uma ampla gama de tarefas de processamento de linguagem natural, incluindo geração de texto, sistemas de diálogo, etc."
  },
  "abab6.5t-chat": {
    "description": "Otimizado para cenários de diálogo de personagens em chinês, oferecendo capacidade de geração de diálogos fluentes e que respeitam os hábitos de expressão em chinês."
  },
  "accounts/fireworks/models/deepseek-r1": {
    "description": "DeepSeek-R1 é um modelo de linguagem grande de última geração, otimizado com aprendizado por reforço e dados de inicialização a frio, apresentando desempenho excepcional em raciocínio, matemática e programação."
  },
  "accounts/fireworks/models/deepseek-v3": {
    "description": "Modelo de linguagem poderoso da Deepseek, baseado em Mixture-of-Experts (MoE), com um total de 671B de parâmetros, ativando 37B de parâmetros por token."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct": {
    "description": "O modelo Llama 3 70B Instruct é otimizado para diálogos multilíngues e compreensão de linguagem natural, superando a maioria dos modelos concorrentes."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct": {
    "description": "O modelo Llama 3 8B Instruct é otimizado para diálogos e tarefas multilíngues, apresentando desempenho excepcional e eficiência."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct-hf": {
    "description": "O modelo Llama 3 8B Instruct (versão HF) é consistente com os resultados da implementação oficial, apresentando alta consistência e compatibilidade entre plataformas."
  },
  "accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "description": "O modelo Llama 3.1 405B Instruct possui parâmetros em escala extremamente grande, adequado para seguimento de instruções em tarefas complexas e cenários de alta carga."
  },
  "accounts/fireworks/models/llama-v3p1-70b-instruct": {
    "description": "O modelo Llama 3.1 70B Instruct oferece excelente compreensão e geração de linguagem natural, sendo a escolha ideal para tarefas de diálogo e análise."
  },
  "accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "description": "O modelo Llama 3.1 8B Instruct é otimizado para diálogos multilíngues, superando a maioria dos modelos de código aberto e fechado em benchmarks do setor."
  },
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct": {
    "description": "Modelo de raciocínio visual de 11B parâmetros da Meta, otimizado para reconhecimento visual, raciocínio visual, descrição de imagens e resposta a perguntas gerais sobre imagens. Este modelo é capaz de entender dados visuais, como gráficos e diagramas, e preencher a lacuna entre visão e linguagem gerando descrições textuais dos detalhes das imagens."
  },
  "accounts/fireworks/models/llama-v3p2-3b-instruct": {
    "description": "O modelo de instrução Llama 3.2 3B é um modelo multilíngue leve lançado pela Meta. Este modelo visa aumentar a eficiência, oferecendo melhorias significativas em latência e custo em comparação com modelos maiores. Exemplos de uso incluem consultas, reescrita de prompts e auxílio na redação."
  },
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct": {
    "description": "Modelo de raciocínio visual de 90B parâmetros da Meta, otimizado para reconhecimento visual, raciocínio visual, descrição de imagens e resposta a perguntas gerais sobre imagens. Este modelo é capaz de entender dados visuais, como gráficos e diagramas, e preencher a lacuna entre visão e linguagem gerando descrições textuais dos detalhes das imagens."
  },
  "accounts/fireworks/models/llama-v3p3-70b-instruct": {
    "description": "Llama 3.3 70B Instruct é a versão atualizada de dezembro do Llama 3.1 70B. Este modelo foi aprimorado com base no Llama 3.1 70B (lançado em julho de 2024), melhorando a chamada de ferramentas, suporte a textos multilíngues, habilidades matemáticas e de programação. O modelo alcançou níveis de liderança da indústria em raciocínio, matemática e seguimento de instruções, e é capaz de oferecer desempenho semelhante ao 3.1 405B, ao mesmo tempo em que apresenta vantagens significativas em velocidade e custo."
  },
  "accounts/fireworks/models/mistral-small-24b-instruct-2501": {
    "description": "Modelo com 24B de parâmetros, com capacidades de ponta comparáveis a modelos maiores."
  },
  "accounts/fireworks/models/mixtral-8x22b-instruct": {
    "description": "O modelo Mixtral MoE 8x22B Instruct, com parâmetros em grande escala e arquitetura de múltiplos especialistas, suporta o processamento eficiente de tarefas complexas."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct": {
    "description": "O modelo Mixtral MoE 8x7B Instruct, com uma arquitetura de múltiplos especialistas, oferece seguimento e execução de instruções de forma eficiente."
  },
  "accounts/fireworks/models/mythomax-l2-13b": {
    "description": "O modelo MythoMax L2 13B combina novas técnicas de fusão, sendo especializado em narrativas e interpretação de personagens."
  },
  "accounts/fireworks/models/phi-3-vision-128k-instruct": {
    "description": "O modelo Phi 3 Vision Instruct é um modelo multimodal leve, capaz de processar informações visuais e textuais complexas, com forte capacidade de raciocínio."
  },
  "accounts/fireworks/models/qwen-qwq-32b-preview": {
    "description": "O modelo QwQ é um modelo de pesquisa experimental desenvolvido pela equipe Qwen, focado em aprimorar a capacidade de raciocínio da IA."
  },
  "accounts/fireworks/models/qwen2-vl-72b-instruct": {
    "description": "A versão 72B do modelo Qwen-VL é o resultado da mais recente iteração da Alibaba, representando quase um ano de inovações."
  },
  "accounts/fireworks/models/qwen2p5-72b-instruct": {
    "description": "Qwen2.5 é uma série de modelos de linguagem com apenas decodificadores, desenvolvida pela equipe Qwen da Alibaba Cloud. Estes modelos têm tamanhos variados, incluindo 0.5B, 1.5B, 3B, 7B, 14B, 32B e 72B, com variantes base (base) e de instrução (instruct)."
  },
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "description": "Qwen2.5 Coder 32B Instruct é a versão mais recente da série de modelos de linguagem de grande escala específicos para código lançada pela Alibaba Cloud. Este modelo, baseado no Qwen2.5, foi treinado com 55 trilhões de tokens, melhorando significativamente a capacidade de geração, raciocínio e correção de código. Ele não apenas aprimora a capacidade de codificação, mas também mantém as vantagens em matemática e habilidades gerais. O modelo fornece uma base mais abrangente para aplicações práticas, como agentes de código."
  },
  "accounts/yi-01-ai/models/yi-large": {
    "description": "O modelo Yi-Large oferece excelente capacidade de processamento multilíngue, adequado para diversas tarefas de geração e compreensão de linguagem."
  },
  "ai21-jamba-1.5-large": {
    "description": "Um modelo multilíngue com 398B de parâmetros (94B ativos), oferecendo uma janela de contexto longa de 256K, chamada de função, saída estruturada e geração fundamentada."
  },
  "ai21-jamba-1.5-mini": {
    "description": "Um modelo multilíngue com 52B de parâmetros (12B ativos), oferecendo uma janela de contexto longa de 256K, chamada de função, saída estruturada e geração fundamentada."
  },
  "anthropic.claude-3-5-sonnet-20240620-v1:0": {
    "description": "O Claude 3.5 Sonnet eleva o padrão da indústria, superando modelos concorrentes e o Claude 3 Opus, apresentando um desempenho excepcional em avaliações amplas, ao mesmo tempo que mantém a velocidade e o custo de nossos modelos de nível médio."
  },
  "anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet elevou o padrão da indústria, superando modelos concorrentes e o Claude 3 Opus, apresentando um desempenho excepcional em avaliações amplas, enquanto mantém a velocidade e o custo de nossos modelos de nível médio."
  },
  "anthropic.claude-3-haiku-20240307-v1:0": {
    "description": "O Claude 3 Haiku é o modelo mais rápido e compacto da Anthropic, oferecendo uma velocidade de resposta quase instantânea. Ele pode responder rapidamente a consultas e solicitações simples. Os clientes poderão construir uma experiência de IA sem costura que imita a interação humana. O Claude 3 Haiku pode processar imagens e retornar saídas de texto, com uma janela de contexto de 200K."
  },
  "anthropic.claude-3-opus-20240229-v1:0": {
    "description": "O Claude 3 Opus é o modelo de IA mais poderoso da Anthropic, com desempenho de ponta em tarefas altamente complexas. Ele pode lidar com prompts abertos e cenários não vistos, apresentando fluência excepcional e compreensão semelhante à humana. O Claude 3 Opus demonstra as possibilidades de geração de IA na vanguarda. O Claude 3 Opus pode processar imagens e retornar saídas de texto, com uma janela de contexto de 200K."
  },
  "anthropic.claude-3-sonnet-20240229-v1:0": {
    "description": "O Claude 3 Sonnet da Anthropic alcança um equilíbrio ideal entre inteligência e velocidade — especialmente adequado para cargas de trabalho empresariais. Ele oferece a máxima utilidade a um custo inferior ao dos concorrentes e foi projetado para ser um modelo confiável e durável, adequado para implantações de IA em larga escala. O Claude 3 Sonnet pode processar imagens e retornar saídas de texto, com uma janela de contexto de 200K."
  },
  "anthropic.claude-instant-v1": {
    "description": "Um modelo rápido, econômico e ainda muito capaz, capaz de lidar com uma variedade de tarefas, incluindo diálogos cotidianos, análise de texto, resumos e perguntas e respostas de documentos."
  },
  "anthropic.claude-v2": {
    "description": "O modelo da Anthropic demonstra alta capacidade em uma ampla gama de tarefas, desde diálogos complexos e geração de conteúdo criativo até o seguimento detalhado de instruções."
  },
  "anthropic.claude-v2:1": {
    "description": "A versão atualizada do Claude 2, com o dobro da janela de contexto, além de melhorias na confiabilidade, taxa de alucinação e precisão baseada em evidências em documentos longos e contextos RAG."
  },
  "anthropic/claude-3-haiku": {
    "description": "Claude 3 Haiku é o modelo mais rápido e compacto da Anthropic, projetado para oferecer respostas quase instantâneas. Ele possui desempenho direcionado rápido e preciso."
  },
  "anthropic/claude-3-opus": {
    "description": "Claude 3 Opus é o modelo mais poderoso da Anthropic para lidar com tarefas altamente complexas. Ele se destaca em desempenho, inteligência, fluência e compreensão."
  },
  "anthropic/claude-3.5-haiku": {
    "description": "Claude 3.5 Haiku é o modelo de próxima geração mais rápido da Anthropic. Em comparação com Claude 3 Haiku, Claude 3.5 Haiku apresenta melhorias em várias habilidades e supera o maior modelo da geração anterior, Claude 3 Opus, em muitos testes de inteligência."
  },
  "anthropic/claude-3.5-sonnet": {
    "description": "Claude 3.5 Sonnet oferece capacidades que vão além do Opus e uma velocidade superior ao Sonnet, mantendo o mesmo preço do Sonnet. O Sonnet é especialmente habilidoso em programação, ciência de dados, processamento visual e tarefas de agente."
  },
  "anthropic/claude-3.7-sonnet": {
    "description": "Claude 3.7 Sonnet é o modelo mais inteligente da Anthropic até agora e é o primeiro modelo de raciocínio misto do mercado. Claude 3.7 Sonnet pode gerar respostas quase instantâneas ou um pensamento gradual prolongado, permitindo que os usuários vejam claramente esses processos. Sonnet é especialmente habilidoso em programação, ciência de dados, processamento visual e tarefas de agente."
  },
  "aya": {
    "description": "Aya 23 é um modelo multilíngue lançado pela Cohere, suportando 23 idiomas, facilitando aplicações linguísticas diversificadas."
  },
  "aya:35b": {
    "description": "Aya 23 é um modelo multilíngue lançado pela Cohere, suportando 23 idiomas, facilitando aplicações linguísticas diversificadas."
  },
  "baichuan/baichuan2-13b-chat": {
    "description": "Baichuan-13B é um modelo de linguagem de código aberto e comercializável desenvolvido pela Baichuan Intelligence, contendo 13 bilhões de parâmetros, alcançando os melhores resultados em benchmarks de chinês e inglês na mesma dimensão."
  },
  "c4ai-aya-expanse-32b": {
    "description": "Aya Expanse é um modelo multilíngue de alto desempenho com 32B, projetado para desafiar o desempenho de modelos monolíngues por meio de inovações em ajuste por instrução, arbitragem de dados, treinamento de preferências e fusão de modelos. Ele suporta 23 idiomas."
  },
  "c4ai-aya-expanse-8b": {
    "description": "Aya Expanse é um modelo multilíngue de alto desempenho com 8B, projetado para desafiar o desempenho de modelos monolíngues por meio de inovações em ajuste por instrução, arbitragem de dados, treinamento de preferências e fusão de modelos. Ele suporta 23 idiomas."
  },
  "c4ai-aya-vision-32b": {
    "description": "Aya Vision é um modelo multimodal de ponta, apresentando desempenho excepcional em múltiplos benchmarks críticos de linguagem, texto e imagem. Esta versão de 32 bilhões de parâmetros foca no desempenho multilíngue de ponta."
  },
  "c4ai-aya-vision-8b": {
    "description": "Aya Vision é um modelo multimodal de ponta, apresentando desempenho excepcional em múltiplos benchmarks críticos de linguagem, texto e imagem. Esta versão de 8 bilhões de parâmetros foca em baixa latência e desempenho ideal."
  },
  "charglm-3": {
    "description": "O CharGLM-3 é projetado para interpretação de personagens e companhia emocional, suportando memória de múltiplas rodadas e diálogos personalizados, com ampla aplicação."
  },
  "chatglm3": {
    "description": "ChatGLM3 é um modelo de código fechado desenvolvido pela AI Zhipu em colaboração com o laboratório KEG da Tsinghua. Após um pré-treinamento extenso com identificadores em chinês e inglês, e um alinhamento com as preferências humanas, o modelo apresenta melhorias de 16%, 36% e 280% em MMLU, C-Eval e GSM8K, respectivamente, em comparação com a primeira geração. Ele lidera o ranking de tarefas em chinês C-Eval. É ideal para cenários que exigem alto nível de conhecimento, capacidade de raciocínio e criatividade, como redação de textos publicitários, escrita de romances, redação de conteúdo informativo e geração de código."
  },
  "chatglm3-6b-base": {
    "description": "ChatGLM3-6b-base é o modelo base de 6 bilhões de parâmetros da mais recente geração da série ChatGLM, desenvolvida pela Zhípǔ."
  },
  "chatgpt-4o-latest": {
    "description": "O ChatGPT-4o é um modelo dinâmico, atualizado em tempo real para manter a versão mais atual. Ele combina uma poderosa capacidade de compreensão e geração de linguagem, adequado para cenários de aplicação em larga escala, incluindo atendimento ao cliente, educação e suporte técnico."
  },
  "claude-2.0": {
    "description": "Claude 2 oferece avanços em capacidades críticas para empresas, incluindo um contexto líder do setor de 200K tokens, uma redução significativa na taxa de alucinação do modelo, prompts de sistema e uma nova funcionalidade de teste: chamadas de ferramentas."
  },
  "claude-2.1": {
    "description": "Claude 2 oferece avanços em capacidades críticas para empresas, incluindo um contexto líder do setor de 200K tokens, uma redução significativa na taxa de alucinação do modelo, prompts de sistema e uma nova funcionalidade de teste: chamadas de ferramentas."
  },
  "claude-3-5-haiku-20241022": {
    "description": "Claude 3.5 Haiku é o modelo de próxima geração mais rápido da Anthropic. Em comparação com o Claude 3 Haiku, o Claude 3.5 Haiku apresenta melhorias em várias habilidades e superou o maior modelo da geração anterior, o Claude 3 Opus, em muitos testes de referência de inteligência."
  },
  "claude-3-5-sonnet-20240620": {
    "description": "Claude 3.5 Sonnet oferece capacidades que superam o Opus e uma velocidade mais rápida que o Sonnet, mantendo o mesmo preço. O Sonnet é especialmente bom em programação, ciência de dados, processamento visual e tarefas de agente."
  },
  "claude-3-5-sonnet-20241022": {
    "description": "Claude 3.5 Sonnet oferece capacidades que vão além do Opus e uma velocidade mais rápida do que o Sonnet, mantendo o mesmo preço do Sonnet. O Sonnet é especialmente bom em programação, ciência de dados, processamento visual e tarefas de agente."
  },
  "claude-3-7-sonnet-20250219": {
    "description": "Claude 3.7 Sonnet é o modelo de IA mais poderoso da Anthropic, com desempenho de ponta em tarefas altamente complexas. Ele pode lidar com prompts abertos e cenários não vistos, apresentando fluência excepcional e compreensão semelhante à humana. O Claude 3.7 Sonnet demonstra as possibilidades de geração de IA na vanguarda."
  },
  "claude-3-haiku-20240307": {
    "description": "Claude 3 Haiku é o modelo mais rápido e compacto da Anthropic, projetado para respostas quase instantâneas. Ele possui desempenho direcionado rápido e preciso."
  },
  "claude-3-opus-20240229": {
    "description": "Claude 3 Opus é o modelo mais poderoso da Anthropic para lidar com tarefas altamente complexas. Ele se destaca em desempenho, inteligência, fluência e compreensão."
  },
  "claude-3-sonnet-20240229": {
    "description": "Claude 3 Sonnet oferece um equilíbrio ideal entre inteligência e velocidade para cargas de trabalho empresariais. Ele fornece máxima utilidade a um custo mais baixo, sendo confiável e adequado para implantação em larga escala."
  },
  "codegeex-4": {
    "description": "O CodeGeeX-4 é um poderoso assistente de programação AI, suportando perguntas e respostas inteligentes e autocompletar em várias linguagens de programação, aumentando a eficiência do desenvolvimento."
  },
  "codegeex4-all-9b": {
    "description": "CodeGeeX4-ALL-9B é um modelo de geração de código multilíngue, suportando funcionalidades abrangentes, incluindo completude e geração de código, interpretador de código, busca na web, chamadas de função e perguntas e respostas em nível de repositório, cobrindo diversos cenários de desenvolvimento de software. É um modelo de geração de código de ponta com menos de 10B de parâmetros."
  },
  "codegemma": {
    "description": "CodeGemma é um modelo de linguagem leve especializado em diferentes tarefas de programação, suportando iterações rápidas e integração."
  },
  "codegemma:2b": {
    "description": "CodeGemma é um modelo de linguagem leve especializado em diferentes tarefas de programação, suportando iterações rápidas e integração."
  },
  "codellama": {
    "description": "Code Llama é um LLM focado em geração e discussão de código, combinando suporte a uma ampla gama de linguagens de programação, adequado para ambientes de desenvolvedores."
  },
  "codellama/CodeLlama-34b-Instruct-hf": {
    "description": "Code Llama é um LLM focado em geração e discussão de código, combinando amplo suporte a linguagens de programação, adequado para ambientes de desenvolvedores."
  },
  "codellama:13b": {
    "description": "Code Llama é um LLM focado em geração e discussão de código, combinando suporte a uma ampla gama de linguagens de programação, adequado para ambientes de desenvolvedores."
  },
  "codellama:34b": {
    "description": "Code Llama é um LLM focado em geração e discussão de código, combinando suporte a uma ampla gama de linguagens de programação, adequado para ambientes de desenvolvedores."
  },
  "codellama:70b": {
    "description": "Code Llama é um LLM focado em geração e discussão de código, combinando suporte a uma ampla gama de linguagens de programação, adequado para ambientes de desenvolvedores."
  },
  "codeqwen": {
    "description": "CodeQwen1.5 é um modelo de linguagem de grande escala treinado com uma vasta quantidade de dados de código, projetado para resolver tarefas de programação complexas."
  },
  "codestral": {
    "description": "Codestral é o primeiro modelo de código da Mistral AI, oferecendo suporte excepcional para tarefas de geração de código."
  },
  "codestral-latest": {
    "description": "Codestral é um modelo gerador de ponta focado em geração de código, otimizado para preenchimento intermediário e tarefas de conclusão de código."
  },
  "cognitivecomputations/dolphin-mixtral-8x22b": {
    "description": "Dolphin Mixtral 8x22B é um modelo projetado para seguir instruções, diálogos e programação."
  },
  "cohere-command-r": {
    "description": "Command R é um modelo generativo escalável voltado para RAG e uso de ferramentas, permitindo IA em escala de produção para empresas."
  },
  "cohere-command-r-plus": {
    "description": "Command R+ é um modelo otimizado para RAG de última geração, projetado para lidar com cargas de trabalho de nível empresarial."
  },
  "command": {
    "description": "Um modelo de diálogo que segue instruções, apresentando alta qualidade e confiabilidade em tarefas linguísticas, além de um comprimento de contexto mais longo em comparação com nosso modelo de geração básico."
  },
  "command-a-03-2025": {
    "description": "O Command A é o nosso modelo mais poderoso até agora, apresentando um desempenho excepcional em uso de ferramentas, agentes, geração aumentada por recuperação (RAG) e cenários de aplicação multilíngue. O Command A possui um comprimento de contexto de 256K, pode ser executado com apenas duas GPUs e, em comparação com o Command R+ 08-2024, teve um aumento de 150% na taxa de transferência."
  },
  "command-light": {
    "description": "Uma versão do Command que é menor e mais rápida, quase tão poderosa, mas com maior velocidade."
  },
  "command-light-nightly": {
    "description": "Para reduzir o intervalo entre os lançamentos de versões principais, lançamos versões noturnas do modelo Command. Para a série command-light, essa versão é chamada de command-light-nightly. Observe que o command-light-nightly é a versão mais recente, experimental e (possivelmente) instável. As versões noturnas são atualizadas regularmente e sem aviso prévio, portanto, não são recomendadas para uso em ambientes de produção."
  },
  "command-nightly": {
    "description": "Para reduzir o intervalo entre os lançamentos de versões principais, lançamos versões noturnas do modelo Command. Para a série Command, essa versão é chamada de command-cightly. Observe que o command-nightly é a versão mais recente, experimental e (possivelmente) instável. As versões noturnas são atualizadas regularmente e sem aviso prévio, portanto, não são recomendadas para uso em ambientes de produção."
  },
  "command-r": {
    "description": "Command R é um LLM otimizado para tarefas de diálogo e longos contextos, especialmente adequado para interações dinâmicas e gerenciamento de conhecimento."
  },
  "command-r-03-2024": {
    "description": "O Command R é um modelo de diálogo que segue instruções, apresentando maior qualidade e confiabilidade em tarefas linguísticas, além de um comprimento de contexto mais longo em comparação com modelos anteriores. Ele pode ser utilizado em fluxos de trabalho complexos, como geração de código, geração aumentada por recuperação (RAG), uso de ferramentas e agentes."
  },
  "command-r-08-2024": {
    "description": "O command-r-08-2024 é uma versão atualizada do modelo Command R, lançada em agosto de 2024."
  },
  "command-r-plus": {
    "description": "Command R+ é um modelo de linguagem de grande porte de alto desempenho, projetado para cenários empresariais reais e aplicações complexas."
  },
  "command-r-plus-04-2024": {
    "description": "O Command R+ é um modelo de diálogo que segue instruções, apresentando maior qualidade e confiabilidade em tarefas linguísticas, além de um comprimento de contexto mais longo em comparação com modelos anteriores. É mais adequado para fluxos de trabalho complexos de RAG e uso de ferramentas em múltiplas etapas."
  },
  "command-r7b-12-2024": {
    "description": "O command-r7b-12-2024 é uma versão compacta e eficiente, lançada em dezembro de 2024. Ele se destaca em tarefas que exigem raciocínio complexo e processamento em múltiplas etapas, como RAG, uso de ferramentas e agentes."
  },
  "dall-e-2": {
    "description": "O segundo modelo DALL·E, suporta geração de imagens mais realistas e precisas, com resolução quatro vezes maior que a da primeira geração."
  },
  "dall-e-3": {
    "description": "O mais recente modelo DALL·E, lançado em novembro de 2023. Suporta geração de imagens mais realistas e precisas, com maior capacidade de detalhamento."
  },
  "databricks/dbrx-instruct": {
    "description": "DBRX Instruct oferece capacidade de processamento de instruções altamente confiável, suportando aplicações em diversos setores."
  },
  "deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 é um modelo de inferência impulsionado por aprendizado por reforço (RL), que resolve problemas de repetitividade e legibilidade no modelo. Antes do RL, o DeepSeek-R1 introduziu dados de inicialização a frio, otimizando ainda mais o desempenho da inferência. Ele apresenta desempenho comparável ao OpenAI-o1 em tarefas matemáticas, de código e de inferência, e melhora o resultado geral por meio de métodos de treinamento cuidadosamente projetados."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
    "description": "Modelo de destilação DeepSeek-R1, otimizado para desempenho de inferência através de aprendizado por reforço e dados de inicialização fria, modelo de código aberto que redefine os padrões de múltiplas tarefas."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Modelo de destilação DeepSeek-R1, otimizado para desempenho de inferência através de aprendizado por reforço e dados de inicialização fria, modelo de código aberto que redefine os padrões de múltiplas tarefas."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Modelo de destilação DeepSeek-R1, otimizado para desempenho de inferência através de aprendizado por reforço e dados de inicialização fria, modelo de código aberto que redefine os padrões de múltiplas tarefas."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
    "description": "DeepSeek-R1-Distill-Qwen-32B é um modelo obtido através da destilação do Qwen2.5-32B. Este modelo foi ajustado com 800 mil amostras selecionadas geradas pelo DeepSeek-R1, demonstrando desempenho excepcional em várias áreas, como matemática, programação e raciocínio. Obteve resultados notáveis em vários testes de referência, alcançando uma precisão de 94,3% no MATH-500, demonstrando forte capacidade de raciocínio matemático."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B é um modelo obtido através da destilação do Qwen2.5-Math-7B. Este modelo foi ajustado com 800 mil amostras selecionadas geradas pelo DeepSeek-R1, demonstrando excelente capacidade de inferência. Apresentou desempenho notável em vários testes de referência, alcançando uma precisão de 92,8% no MATH-500, uma taxa de aprovação de 55,5% no AIME 2024 e uma pontuação de 1189 no CodeForces, demonstrando forte capacidade matemática e de programação para um modelo de 7B."
  },
  "deepseek-ai/DeepSeek-V2.5": {
    "description": "DeepSeek V2.5 combina as excelentes características das versões anteriores, aprimorando a capacidade geral e de codificação."
  },
  "deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 é um modelo de linguagem de especialistas mistos (MoE) com 671 bilhões de parâmetros, utilizando atenção latente de múltiplas cabeças (MLA) e a arquitetura DeepSeekMoE, combinando uma estratégia de balanceamento de carga sem perda auxiliar para otimizar a eficiência de inferência e treinamento. Após ser pré-treinado em 14,8 trilhões de tokens de alta qualidade e passar por ajuste fino supervisionado e aprendizado por reforço, o DeepSeek-V3 supera outros modelos de código aberto em desempenho, aproximando-se de modelos fechados líderes."
  },
  "deepseek-ai/deepseek-llm-67b-chat": {
    "description": "DeepSeek 67B é um modelo avançado treinado para diálogos de alta complexidade."
  },
  "deepseek-ai/deepseek-r1": {
    "description": "LLM avançado e eficiente, especializado em raciocínio, matemática e programação."
  },
  "deepseek-ai/deepseek-vl2": {
    "description": "DeepSeek-VL2 é um modelo de linguagem visual baseado no DeepSeekMoE-27B, desenvolvido como um especialista misto (MoE), utilizando uma arquitetura de MoE com ativação esparsa, alcançando desempenho excepcional com apenas 4,5 bilhões de parâmetros ativados. Este modelo se destaca em várias tarefas, incluindo perguntas visuais, reconhecimento óptico de caracteres, compreensão de documentos/tabelas/gráficos e localização visual."
  },
  "deepseek-chat": {
    "description": "Um novo modelo de código aberto que combina capacidades gerais e de codificação, não apenas preservando a capacidade de diálogo geral do modelo Chat original e a poderosa capacidade de processamento de código do modelo Coder, mas também alinhando-se melhor às preferências humanas. Além disso, o DeepSeek-V2.5 também alcançou melhorias significativas em várias áreas, como tarefas de escrita e seguimento de instruções."
  },
  "deepseek-coder-33B-instruct": {
    "description": "DeepSeek Coder 33B é um modelo de linguagem de código, treinado com 20 trilhões de dados, dos quais 87% são código e 13% são em chinês e inglês. O modelo introduz uma janela de 16K e tarefas de preenchimento, oferecendo funcionalidades de completude de código e preenchimento de fragmentos em nível de projeto."
  },
  "deepseek-coder-v2": {
    "description": "DeepSeek Coder V2 é um modelo de código de especialistas abertos, destacando-se em tarefas de codificação, comparável ao GPT4-Turbo."
  },
  "deepseek-coder-v2:236b": {
    "description": "DeepSeek Coder V2 é um modelo de código de especialistas abertos, destacando-se em tarefas de codificação, comparável ao GPT4-Turbo."
  },
  "deepseek-r1": {
    "description": "DeepSeek-R1 é um modelo de inferência impulsionado por aprendizado por reforço (RL), que resolve problemas de repetitividade e legibilidade no modelo. Antes do RL, o DeepSeek-R1 introduziu dados de inicialização a frio, otimizando ainda mais o desempenho da inferência. Ele apresenta desempenho comparável ao OpenAI-o1 em tarefas matemáticas, de código e de inferência, e melhora o resultado geral por meio de métodos de treinamento cuidadosamente projetados."
  },
  "deepseek-r1-70b-fast-online": {
    "description": "DeepSeek R1 70B versão rápida, suporta busca em tempo real, oferecendo maior velocidade de resposta enquanto mantém o desempenho do modelo."
  },
  "deepseek-r1-70b-online": {
    "description": "DeepSeek R1 70B versão padrão, suporta busca em tempo real, adequado para diálogos e tarefas de processamento de texto que requerem informações atualizadas."
  },
  "deepseek-r1-distill-llama": {
    "description": "deepseek-r1-distill-llama é um modelo baseado no Llama, destilado a partir do DeepSeek-R1."
  },
  "deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 — um modelo maior e mais inteligente dentro do pacote DeepSeek — foi destilado para a arquitetura Llama 70B. Com base em testes de referência e avaliações humanas, este modelo é mais inteligente que o Llama 70B original, destacando-se especialmente em tarefas que exigem precisão matemática e factual."
  },
  "deepseek-r1-distill-llama-8b": {
    "description": "O modelo da série DeepSeek-R1-Distill é obtido através da técnica de destilação de conhecimento, ajustando amostras geradas pelo DeepSeek-R1 em modelos de código aberto como Qwen e Llama."
  },
  "deepseek-r1-distill-qianfan-llama-70b": {
    "description": "Lançado pela primeira vez em 14 de fevereiro de 2025, destilado pela equipe de desenvolvimento do modelo Qianfan a partir do modelo base Llama3_70B (Construído com Meta Llama), com dados de destilação que também incluem o corpus do Qianfan."
  },
  "deepseek-r1-distill-qianfan-llama-8b": {
    "description": "Lançado pela primeira vez em 14 de fevereiro de 2025, destilado pela equipe de desenvolvimento do modelo Qianfan a partir do modelo base Llama3_8B (Construído com Meta Llama), com dados de destilação que também incluem o corpus do Qianfan."
  },
  "deepseek-r1-distill-qwen": {
    "description": "deepseek-r1-distill-qwen é um modelo derivado do Qwen, destilado a partir do DeepSeek-R1."
  },
  "deepseek-r1-distill-qwen-1.5b": {
    "description": "O modelo da série DeepSeek-R1-Distill é obtido através da técnica de destilação de conhecimento, ajustando amostras geradas pelo DeepSeek-R1 em modelos de código aberto como Qwen e Llama."
  },
  "deepseek-r1-distill-qwen-14b": {
    "description": "O modelo da série DeepSeek-R1-Distill é obtido através da técnica de destilação de conhecimento, ajustando amostras geradas pelo DeepSeek-R1 em modelos de código aberto como Qwen e Llama."
  },
  "deepseek-r1-distill-qwen-32b": {
    "description": "O modelo da série DeepSeek-R1-Distill é obtido através da técnica de destilação de conhecimento, ajustando amostras geradas pelo DeepSeek-R1 em modelos de código aberto como Qwen e Llama."
  },
  "deepseek-r1-distill-qwen-7b": {
    "description": "O modelo da série DeepSeek-R1-Distill é obtido através da técnica de destilação de conhecimento, ajustando amostras geradas pelo DeepSeek-R1 em modelos de código aberto como Qwen e Llama."
  },
  "deepseek-r1-fast-online": {
    "description": "DeepSeek R1 versão completa rápida, suporta busca em tempo real, combinando a poderosa capacidade de 671B de parâmetros com maior velocidade de resposta."
  },
  "deepseek-r1-online": {
    "description": "DeepSeek R1 versão completa, com 671B de parâmetros, suporta busca em tempo real, apresentando capacidades de compreensão e geração mais robustas."
  },
  "deepseek-reasoner": {
    "description": "Modelo de raciocínio lançado pela DeepSeek. Antes de fornecer a resposta final, o modelo gera uma cadeia de pensamento para aumentar a precisão da resposta final."
  },
  "deepseek-v2": {
    "description": "DeepSeek V2 é um modelo de linguagem eficiente Mixture-of-Experts, adequado para demandas de processamento econômico."
  },
  "deepseek-v2:236b": {
    "description": "DeepSeek V2 236B é o modelo de código projetado do DeepSeek, oferecendo forte capacidade de geração de código."
  },
  "deepseek-v3": {
    "description": "DeepSeek-V3 é um modelo MoE desenvolvido pela Hangzhou DeepSeek Artificial Intelligence Technology Research Co., Ltd., com desempenho destacado em várias avaliações, ocupando o primeiro lugar entre os modelos de código aberto nas principais listas. Em comparação com o modelo V2.5, a velocidade de geração do V3 foi aumentada em 3 vezes, proporcionando uma experiência de uso mais rápida e fluida."
  },
  "deepseek/deepseek-chat": {
    "description": "Um novo modelo de código aberto que integra capacidades gerais e de codificação, não apenas preservando a capacidade de diálogo geral do modelo Chat original e a poderosa capacidade de processamento de código do modelo Coder, mas também alinhando-se melhor às preferências humanas. Além disso, o DeepSeek-V2.5 também alcançou melhorias significativas em várias áreas, como tarefas de escrita e seguimento de instruções."
  },
  "deepseek/deepseek-r1": {
    "description": "DeepSeek-R1 melhorou significativamente a capacidade de raciocínio do modelo com muito poucos dados rotulados. Antes de fornecer a resposta final, o modelo gera uma cadeia de pensamento para aumentar a precisão da resposta final."
  },
  "deepseek/deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 Distill Llama 70B é um grande modelo de linguagem baseado no Llama3.3 70B, que utiliza o ajuste fino da saída do DeepSeek R1 para alcançar um desempenho competitivo comparável aos grandes modelos de ponta."
  },
  "deepseek/deepseek-r1-distill-llama-8b": {
    "description": "DeepSeek R1 Distill Llama 8B é um modelo de linguagem grande destilado baseado no Llama-3.1-8B-Instruct, treinado usando a saída do DeepSeek R1."
  },
  "deepseek/deepseek-r1-distill-qwen-14b": {
    "description": "DeepSeek R1 Distill Qwen 14B é um modelo de linguagem grande destilado baseado no Qwen 2.5 14B, treinado usando a saída do DeepSeek R1. Este modelo superou o o1-mini da OpenAI em vários benchmarks, alcançando os mais recentes avanços tecnológicos em modelos densos (state-of-the-art). Aqui estão alguns resultados de benchmarks:\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nClassificação CodeForces: 1481\nEste modelo, ajustado a partir da saída do DeepSeek R1, demonstrou desempenho competitivo comparável a modelos de ponta de maior escala."
  },
  "deepseek/deepseek-r1-distill-qwen-32b": {
    "description": "DeepSeek R1 Distill Qwen 32B é um modelo de linguagem grande destilado baseado no Qwen 2.5 32B, treinado usando a saída do DeepSeek R1. Este modelo superou o o1-mini da OpenAI em vários benchmarks, alcançando os mais recentes avanços tecnológicos em modelos densos (state-of-the-art). Aqui estão alguns resultados de benchmarks:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nClassificação CodeForces: 1691\nEste modelo, ajustado a partir da saída do DeepSeek R1, demonstrou desempenho competitivo comparável a modelos de ponta de maior escala."
  },
  "deepseek/deepseek-r1/community": {
    "description": "DeepSeek R1 é o mais recente modelo de código aberto lançado pela equipe DeepSeek, com desempenho de inferência extremamente robusto, especialmente em tarefas de matemática, programação e raciocínio, alcançando níveis comparáveis ao modelo o1 da OpenAI."
  },
  "deepseek/deepseek-r1:free": {
    "description": "DeepSeek-R1 melhorou significativamente a capacidade de raciocínio do modelo com muito poucos dados rotulados. Antes de fornecer a resposta final, o modelo gera uma cadeia de pensamento para aumentar a precisão da resposta final."
  },
  "deepseek/deepseek-v3": {
    "description": "DeepSeek-V3 alcançou um avanço significativo na velocidade de inferência em comparação com os modelos anteriores. Classificado como o número um entre os modelos de código aberto, pode competir com os modelos fechados mais avançados do mundo. DeepSeek-V3 utiliza a arquitetura de Atenção Multi-Cabeça (MLA) e DeepSeekMoE, que foram amplamente validadas no DeepSeek-V2. Além disso, DeepSeek-V3 introduziu uma estratégia auxiliar sem perdas para balanceamento de carga e definiu objetivos de treinamento de previsão de múltiplos rótulos para obter um desempenho mais forte."
  },
  "deepseek/deepseek-v3/community": {
    "description": "DeepSeek-V3 alcançou um avanço significativo na velocidade de inferência em comparação com os modelos anteriores. Classificado como o número um entre os modelos de código aberto, pode competir com os modelos fechados mais avançados do mundo. DeepSeek-V3 utiliza a arquitetura de Atenção Multi-Cabeça (MLA) e DeepSeekMoE, que foram amplamente validadas no DeepSeek-V2. Além disso, DeepSeek-V3 introduziu uma estratégia auxiliar sem perdas para balanceamento de carga e definiu objetivos de treinamento de previsão de múltiplos rótulos para obter um desempenho mais forte."
  },
  "doubao-1.5-lite-32k": {
    "description": "Doubao-1.5-lite é a nova geração de modelo leve, com velocidade de resposta extrema, alcançando níveis de desempenho e latência de classe mundial."
  },
  "doubao-1.5-pro-256k": {
    "description": "Doubao-1.5-pro-256k é uma versão totalmente aprimorada do Doubao-1.5-Pro, com um aumento significativo de 10% no desempenho geral. Suporta raciocínio com janelas de contexto de 256k e um comprimento de saída de até 12k tokens. Maior desempenho, janelas maiores e excelente custo-benefício, adequado para uma ampla gama de cenários de aplicação."
  },
  "doubao-1.5-pro-32k": {
    "description": "Doubao-1.5-pro é a nova geração de modelo principal, com desempenho totalmente aprimorado, destacando-se em conhecimento, código, raciocínio, entre outros aspectos."
  },
  "emohaa": {
    "description": "O Emohaa é um modelo psicológico com capacidade de consultoria profissional, ajudando os usuários a entender questões emocionais."
  },
  "ernie-3.5-128k": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, cobrindo uma vasta quantidade de dados em chinês e inglês, com forte capacidade geral, capaz de atender à maioria das demandas de diálogo, geração criativa e aplicações de plugins; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas."
  },
  "ernie-3.5-8k": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, cobrindo uma vasta quantidade de dados em chinês e inglês, com forte capacidade geral, capaz de atender à maioria das demandas de diálogo, geração criativa e aplicações de plugins; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas."
  },
  "ernie-3.5-8k-preview": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, cobrindo uma vasta quantidade de dados em chinês e inglês, com forte capacidade geral, capaz de atender à maioria das demandas de diálogo, geração criativa e aplicações de plugins; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas."
  },
  "ernie-4.0-8k-latest": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, com capacidade de modelo amplamente aprimorada em comparação com o ERNIE 3.5, amplamente aplicável a cenários de tarefas complexas em várias áreas; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas."
  },
  "ernie-4.0-8k-preview": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, com capacidade de modelo amplamente aprimorada em comparação com o ERNIE 3.5, amplamente aplicável a cenários de tarefas complexas em várias áreas; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas."
  },
  "ernie-4.0-turbo-128k": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, com desempenho geral excepcional, amplamente aplicável a cenários de tarefas complexas em várias áreas; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas. Em comparação com o ERNIE 4.0, apresenta desempenho superior."
  },
  "ernie-4.0-turbo-8k-latest": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, com desempenho geral excepcional, amplamente aplicável a cenários de tarefas complexas em várias áreas; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas. Em comparação com o ERNIE 4.0, apresenta desempenho superior."
  },
  "ernie-4.0-turbo-8k-preview": {
    "description": "Modelo de linguagem de grande escala de nível flagship desenvolvido pela Baidu, com desempenho geral excepcional, amplamente aplicável a cenários de tarefas complexas em várias áreas; suporta integração automática com plugins de busca da Baidu, garantindo a atualidade das informações de perguntas e respostas. Em comparação com o ERNIE 4.0, apresenta desempenho superior."
  },
  "ernie-4.5-8k-preview": {
    "description": "O modelo ERNIE 4.5 é a nova geração de modelo de base multimodal nativo desenvolvido pela Baidu, alcançando otimização colaborativa por meio de modelagem conjunta de múltiplos modos, com excelente capacidade de compreensão multimodal; apresenta habilidades linguísticas aprimoradas, com melhorias abrangentes em compreensão, geração, lógica e memória, além de redução de alucinações e melhorias significativas em raciocínio lógico e habilidades de codificação."
  },
  "ernie-char-8k": {
    "description": "Modelo de linguagem de grande escala vertical desenvolvido pela Baidu, adequado para aplicações como NPCs de jogos, diálogos de atendimento ao cliente e interpretação de personagens, com estilo de personagem mais distinto e consistente, capacidade de seguir instruções mais forte e desempenho de inferência superior."
  },
  "ernie-char-fiction-8k": {
    "description": "Modelo de linguagem de grande escala vertical desenvolvido pela Baidu, adequado para aplicações como NPCs de jogos, diálogos de atendimento ao cliente e interpretação de personagens, com estilo de personagem mais distinto e consistente, capacidade de seguir instruções mais forte e desempenho de inferência superior."
  },
  "ernie-lite-8k": {
    "description": "ERNIE Lite é um modelo de linguagem de grande escala leve desenvolvido pela Baidu, equilibrando excelente desempenho do modelo e eficiência de inferência, adequado para uso em placas de aceleração de IA de baixa potência."
  },
  "ernie-lite-pro-128k": {
    "description": "Modelo de linguagem de grande escala leve desenvolvido pela Baidu, equilibrando excelente desempenho do modelo e eficiência de inferência, com desempenho superior ao ERNIE Lite, adequado para uso em placas de aceleração de IA de baixa potência."
  },
  "ernie-novel-8k": {
    "description": "Modelo de linguagem de grande escala geral desenvolvido pela Baidu, com vantagens notáveis na capacidade de continuar histórias, também aplicável em cenários como peças curtas e filmes."
  },
  "ernie-speed-128k": {
    "description": "Modelo de linguagem de alto desempenho desenvolvido pela Baidu, lançado em 2024, com excelente capacidade geral, adequado para ser usado como modelo base para ajuste fino, lidando melhor com problemas de cenários específicos, enquanto apresenta excelente desempenho de inferência."
  },
  "ernie-speed-pro-128k": {
    "description": "Modelo de linguagem de alto desempenho desenvolvido pela Baidu, lançado em 2024, com excelente capacidade geral, desempenho superior ao ERNIE Speed, adequado para ser usado como modelo base para ajuste fino, lidando melhor com problemas de cenários específicos, enquanto apresenta excelente desempenho de inferência."
  },
  "ernie-tiny-8k": {
    "description": "ERNIE Tiny é um modelo de linguagem de grande escala de alto desempenho desenvolvido pela Baidu, com os menores custos de implantação e ajuste entre os modelos da série Wenxin."
  },
  "gemini-1.0-pro-001": {
    "description": "Gemini 1.0 Pro 001 (Ajuste) oferece desempenho estável e ajustável, sendo a escolha ideal para soluções de tarefas complexas."
  },
  "gemini-1.0-pro-002": {
    "description": "Gemini 1.0 Pro 002 (Ajuste) oferece excelente suporte multimodal, focando na resolução eficaz de tarefas complexas."
  },
  "gemini-1.0-pro-latest": {
    "description": "Gemini 1.0 Pro é o modelo de IA de alto desempenho do Google, projetado para expansão em uma ampla gama de tarefas."
  },
  "gemini-1.5-flash": {
    "description": "Gemini 1.5 Flash é o mais recente modelo de IA multimodal do Google, com capacidade de processamento rápido, suportando entradas de texto, imagem e vídeo, adequado para a escalabilidade eficiente de diversas tarefas."
  },
  "gemini-1.5-flash-001": {
    "description": "Gemini 1.5 Flash 001 é um modelo multimodal eficiente, suportando a expansão de aplicações amplas."
  },
  "gemini-1.5-flash-002": {
    "description": "O Gemini 1.5 Flash 002 é um modelo multimodal eficiente, que suporta uma ampla gama de aplicações."
  },
  "gemini-1.5-flash-8b": {
    "description": "O Gemini 1.5 Flash 8B é um modelo multimodal eficiente, com suporte para uma ampla gama de aplicações."
  },
  "gemini-1.5-flash-8b-exp-0924": {
    "description": "O Gemini 1.5 Flash 8B 0924 é o mais recente modelo experimental, com melhorias significativas de desempenho em casos de uso de texto e multimídia."
  },
  "gemini-1.5-flash-8b-latest": {
    "description": "O Gemini 1.5 Flash 8B é um modelo multimodal eficiente que suporta uma ampla gama de aplicações em expansão."
  },
  "gemini-1.5-flash-exp-0827": {
    "description": "Gemini 1.5 Flash 0827 oferece capacidade de processamento multimodal otimizada, adequada para diversos cenários de tarefas complexas."
  },
  "gemini-1.5-flash-latest": {
    "description": "Gemini 1.5 Flash é o mais recente modelo de IA multimodal do Google, com capacidade de processamento rápido, suportando entradas de texto, imagem e vídeo, adequado para uma variedade de tarefas de expansão eficiente."
  },
  "gemini-1.5-pro-001": {
    "description": "Gemini 1.5 Pro 001 é uma solução de IA multimodal escalável, suportando uma ampla gama de tarefas complexas."
  },
  "gemini-1.5-pro-002": {
    "description": "O Gemini 1.5 Pro 002 é o mais recente modelo pronto para produção, oferecendo saídas de maior qualidade, com melhorias significativas em tarefas matemáticas, contextos longos e tarefas visuais."
  },
  "gemini-1.5-pro-exp-0801": {
    "description": "Gemini 1.5 Pro 0801 oferece excelente capacidade de processamento multimodal, proporcionando maior flexibilidade para o desenvolvimento de aplicações."
  },
  "gemini-1.5-pro-exp-0827": {
    "description": "Gemini 1.5 Pro 0827 combina as mais recentes técnicas de otimização, proporcionando uma capacidade de processamento de dados multimodal mais eficiente."
  },
  "gemini-1.5-pro-latest": {
    "description": "Gemini 1.5 Pro suporta até 2 milhões de tokens, sendo a escolha ideal para modelos multimodais de médio porte, adequados para suporte multifacetado em tarefas complexas."
  },
  "gemini-2.0-flash": {
    "description": "Gemini 2.0 Flash oferece funcionalidades e melhorias de próxima geração, incluindo velocidade excepcional, uso nativo de ferramentas, geração multimodal e uma janela de contexto de 1M tokens."
  },
  "gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash oferece funcionalidades e melhorias de próxima geração, incluindo velocidade excepcional, uso nativo de ferramentas, geração multimodal e uma janela de contexto de 1M tokens."
  },
  "gemini-2.0-flash-exp": {
    "description": "Variante do modelo Gemini 2.0 Flash, otimizada para custo-benefício e baixa latência."
  },
  "gemini-2.0-flash-exp-image-generation": {
    "description": "Modelo experimental Gemini 2.0 Flash, suporta geração de imagens"
  },
  "gemini-2.0-flash-lite": {
    "description": "Variante do modelo Gemini 2.0 Flash, otimizada para custo-benefício e baixa latência."
  },
  "gemini-2.0-flash-lite-001": {
    "description": "Variante do modelo Gemini 2.0 Flash, otimizada para custo-benefício e baixa latência."
  },
  "gemini-2.0-flash-lite-preview-02-05": {
    "description": "Um modelo Gemini 2.0 Flash otimizado para custo-benefício e baixa latência."
  },
  "gemini-2.0-flash-thinking-exp-01-21": {
    "description": "O Gemini 2.0 Flash Exp é o mais recente modelo experimental de IA multimodal do Google, com características de próxima geração, velocidade excepcional, chamadas nativas de ferramentas e geração multimodal."
  },
  "gemini-2.0-pro-exp-02-05": {
    "description": "Gemini 2.0 Pro Experimental é o mais recente modelo de IA multimodal experimental do Google, apresentando melhorias de qualidade em comparação com versões anteriores, especialmente em conhecimento mundial, código e contextos longos."
  },
  "gemini-2.5-pro-exp-03-25": {
    "description": "O Gemini 2.5 Pro Experimental é o modelo de pensamento mais avançado do Google, capaz de raciocinar sobre problemas complexos em código, matemática e áreas STEM, além de analisar grandes conjuntos de dados, repositórios de código e documentos utilizando contextos longos."
  },
  "gemma-7b-it": {
    "description": "Gemma 7B é adequado para o processamento de tarefas de pequeno a médio porte, combinando custo e eficiência."
  },
  "gemma2": {
    "description": "Gemma 2 é um modelo eficiente lançado pelo Google, abrangendo uma variedade de cenários de aplicação, desde aplicações pequenas até processamento de dados complexos."
  },
  "gemma2-9b-it": {
    "description": "Gemma 2 9B é um modelo otimizado para integração de tarefas e ferramentas específicas."
  },
  "gemma2:27b": {
    "description": "Gemma 2 é um modelo eficiente lançado pelo Google, abrangendo uma variedade de cenários de aplicação, desde aplicações pequenas até processamento de dados complexos."
  },
  "gemma2:2b": {
    "description": "Gemma 2 é um modelo eficiente lançado pelo Google, abrangendo uma variedade de cenários de aplicação, desde aplicações pequenas até processamento de dados complexos."
  },
  "generalv3": {
    "description": "Spark Pro é um modelo de linguagem de alto desempenho otimizado para áreas profissionais, focando em matemática, programação, medicina, educação e outros campos, e suportando busca online e plugins integrados como clima e data. Seu modelo otimizado apresenta desempenho excepcional e eficiência em perguntas e respostas complexas, compreensão de linguagem e criação de texto de alto nível, sendo a escolha ideal para cenários de aplicação profissional."
  },
  "generalv3.5": {
    "description": "Spark3.5 Max é a versão mais completa, suportando busca online e muitos plugins integrados. Suas capacidades centrais totalmente otimizadas, juntamente com a definição de papéis do sistema e a funcionalidade de chamada de funções, fazem com que seu desempenho em vários cenários de aplicação complexos seja extremamente excepcional."
  },
  "glm-4": {
    "description": "O GLM-4 é a versão antiga lançada em janeiro de 2024, atualmente substituída pelo mais poderoso GLM-4-0520."
  },
  "glm-4-0520": {
    "description": "O GLM-4-0520 é a versão mais recente do modelo, projetada para tarefas altamente complexas e diversificadas, com desempenho excepcional."
  },
  "glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat apresenta alto desempenho em semântica, matemática, raciocínio, código e conhecimento. Também possui navegação na web, execução de código, chamadas de ferramentas personalizadas e raciocínio de texto longo. Suporta 26 idiomas, incluindo japonês, coreano e alemão."
  },
  "glm-4-air": {
    "description": "O GLM-4-Air é uma versão econômica, com desempenho próximo ao GLM-4, oferecendo alta velocidade a um preço acessível."
  },
  "glm-4-airx": {
    "description": "O GLM-4-AirX oferece uma versão eficiente do GLM-4-Air, com velocidade de inferência até 2,6 vezes mais rápida."
  },
  "glm-4-alltools": {
    "description": "O GLM-4-AllTools é um modelo de agente multifuncional, otimizado para suportar planejamento de instruções complexas e chamadas de ferramentas, como navegação na web, interpretação de código e geração de texto, adequado para execução de múltiplas tarefas."
  },
  "glm-4-flash": {
    "description": "O GLM-4-Flash é a escolha ideal para tarefas simples, com a maior velocidade e o preço mais acessível."
  },
  "glm-4-flashx": {
    "description": "GLM-4-FlashX é uma versão aprimorada do Flash, com velocidade de inferência super rápida."
  },
  "glm-4-long": {
    "description": "O GLM-4-Long suporta entradas de texto superlongas, adequado para tarefas de memória e processamento de documentos em larga escala."
  },
  "glm-4-plus": {
    "description": "O GLM-4-Plus, como um modelo de alta inteligência, possui uma forte capacidade de lidar com textos longos e tarefas complexas, com desempenho amplamente aprimorado."
  },
  "glm-4v": {
    "description": "O GLM-4V oferece uma forte capacidade de compreensão e raciocínio de imagens, suportando várias tarefas visuais."
  },
  "glm-4v-flash": {
    "description": "GLM-4V-Flash é focado na compreensão eficiente de uma única imagem, adequado para cenários de análise de imagem rápida, como análise de imagem em tempo real ou processamento em lote de imagens."
  },
  "glm-4v-plus": {
    "description": "O GLM-4V-Plus possui a capacidade de entender conteúdo de vídeo e múltiplas imagens, adequado para tarefas multimodais."
  },
  "glm-zero-preview": {
    "description": "O GLM-Zero-Preview possui uma poderosa capacidade de raciocínio complexo, destacando-se em áreas como raciocínio lógico, matemática e programação."
  },
  "google/gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash oferece funcionalidades e melhorias de próxima geração, incluindo velocidade excepcional, uso nativo de ferramentas, geração multimodal e uma janela de contexto de 1M tokens."
  },
  "google/gemini-2.0-pro-exp-02-05:free": {
    "description": "Gemini 2.0 Pro Experimental é o mais recente modelo de IA multimodal experimental do Google, apresentando melhorias de qualidade em comparação com versões anteriores, especialmente em conhecimento mundial, código e contextos longos."
  },
  "google/gemini-flash-1.5": {
    "description": "Gemini 1.5 Flash oferece capacidades de processamento multimodal otimizadas, adequadas para uma variedade de cenários de tarefas complexas."
  },
  "google/gemini-pro-1.5": {
    "description": "Gemini 1.5 Pro combina as mais recentes tecnologias de otimização, proporcionando uma capacidade de processamento de dados multimodais mais eficiente."
  },
  "google/gemma-2-27b": {
    "description": "Gemma 2 é um modelo eficiente lançado pelo Google, abrangendo uma variedade de cenários de aplicação, desde pequenos aplicativos até processamento de dados complexos."
  },
  "google/gemma-2-27b-it": {
    "description": "Gemma 2 continua a filosofia de design leve e eficiente."
  },
  "google/gemma-2-2b-it": {
    "description": "Modelo leve de ajuste de instruções do Google."
  },
  "google/gemma-2-9b": {
    "description": "Gemma 2 é um modelo eficiente lançado pelo Google, abrangendo uma variedade de cenários de aplicação, desde pequenos aplicativos até processamento de dados complexos."
  },
  "google/gemma-2-9b-it": {
    "description": "Gemma 2 é uma série de modelos de texto de código aberto leve da Google."
  },
  "google/gemma-2-9b-it:free": {
    "description": "Gemma 2 é uma série de modelos de texto de código aberto leve da Google."
  },
  "google/gemma-2b-it": {
    "description": "Gemma Instruct (2B) oferece capacidade básica de processamento de instruções, adequada para aplicações leves."
  },
  "gpt-3.5-turbo": {
    "description": "O GPT 3.5 Turbo é adequado para uma variedade de tarefas de geração e compreensão de texto, atualmente apontando para gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-0125": {
    "description": "O GPT 3.5 Turbo é adequado para uma variedade de tarefas de geração e compreensão de texto, atualmente apontando para gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-1106": {
    "description": "O GPT 3.5 Turbo é adequado para uma variedade de tarefas de geração e compreensão de texto, atualmente apontando para gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-instruct": {
    "description": "O GPT 3.5 Turbo é adequado para uma variedade de tarefas de geração e compreensão de texto, atualmente apontando para gpt-3.5-turbo-0125."
  },
  "gpt-35-turbo": {
    "description": "GPT 3.5 Turbo, um modelo eficiente fornecido pela OpenAI, adequado para tarefas de chat e geração de texto, suportando chamadas de função paralelas."
  },
  "gpt-35-turbo-16k": {
    "description": "GPT 3.5 Turbo 16k, um modelo de geração de texto de alta capacidade, adequado para tarefas complexas."
  },
  "gpt-4": {
    "description": "O GPT-4 oferece uma janela de contexto maior, capaz de lidar com entradas de texto mais longas, adequado para cenários que exigem integração ampla de informações e análise de dados."
  },
  "gpt-4-0125-preview": {
    "description": "O mais recente modelo GPT-4 Turbo possui funcionalidades visuais. Agora, solicitações visuais podem ser feitas usando o modo JSON e chamadas de função. O GPT-4 Turbo é uma versão aprimorada, oferecendo suporte econômico para tarefas multimodais. Ele encontra um equilíbrio entre precisão e eficiência, adequado para aplicações que requerem interação em tempo real."
  },
  "gpt-4-0613": {
    "description": "O GPT-4 oferece uma janela de contexto maior, capaz de lidar com entradas de texto mais longas, adequado para cenários que exigem integração ampla de informações e análise de dados."
  },
  "gpt-4-1106-preview": {
    "description": "O mais recente modelo GPT-4 Turbo possui funcionalidades visuais. Agora, solicitações visuais podem ser feitas usando o modo JSON e chamadas de função. O GPT-4 Turbo é uma versão aprimorada, oferecendo suporte econômico para tarefas multimodais. Ele encontra um equilíbrio entre precisão e eficiência, adequado para aplicações que requerem interação em tempo real."
  },
  "gpt-4-32k": {
    "description": "O GPT-4 oferece uma janela de contexto maior, capaz de lidar com entradas de texto mais longas, adequado para cenários que exigem integração ampla de informações e análise de dados."
  },
  "gpt-4-32k-0613": {
    "description": "O GPT-4 oferece uma janela de contexto maior, capaz de lidar com entradas de texto mais longas, adequado para cenários que exigem integração ampla de informações e análise de dados."
  },
  "gpt-4-turbo": {
    "description": "O mais recente modelo GPT-4 Turbo possui funcionalidades visuais. Agora, solicitações visuais podem ser feitas usando o modo JSON e chamadas de função. O GPT-4 Turbo é uma versão aprimorada, oferecendo suporte econômico para tarefas multimodais. Ele encontra um equilíbrio entre precisão e eficiência, adequado para aplicações que requerem interação em tempo real."
  },
  "gpt-4-turbo-2024-04-09": {
    "description": "O mais recente modelo GPT-4 Turbo possui funcionalidades visuais. Agora, solicitações visuais podem ser feitas usando o modo JSON e chamadas de função. O GPT-4 Turbo é uma versão aprimorada, oferecendo suporte econômico para tarefas multimodais. Ele encontra um equilíbrio entre precisão e eficiência, adequado para aplicações que requerem interação em tempo real."
  },
  "gpt-4-turbo-preview": {
    "description": "O mais recente modelo GPT-4 Turbo possui funcionalidades visuais. Agora, solicitações visuais podem ser feitas usando o modo JSON e chamadas de função. O GPT-4 Turbo é uma versão aprimorada, oferecendo suporte econômico para tarefas multimodais. Ele encontra um equilíbrio entre precisão e eficiência, adequado para aplicações que requerem interação em tempo real."
  },
  "gpt-4-vision-preview": {
    "description": "O mais recente modelo GPT-4 Turbo possui funcionalidades visuais. Agora, solicitações visuais podem ser feitas usando o modo JSON e chamadas de função. O GPT-4 Turbo é uma versão aprimorada, oferecendo suporte econômico para tarefas multimodais. Ele encontra um equilíbrio entre precisão e eficiência, adequado para aplicações que requerem interação em tempo real."
  },
  "gpt-4.5-preview": {
    "description": "Versão de pesquisa do GPT-4.5, que é o nosso maior e mais poderoso modelo GPT até agora. Ele possui um amplo conhecimento sobre o mundo e consegue entender melhor a intenção do usuário, destacando-se em tarefas criativas e planejamento autônomo. O GPT-4.5 aceita entradas de texto e imagem, gerando saídas de texto (incluindo saídas estruturadas). Suporta recursos essenciais para desenvolvedores, como chamadas de função, API em lote e saída em fluxo. O GPT-4.5 se destaca especialmente em tarefas que requerem criatividade, pensamento aberto e diálogo (como escrita, aprendizado ou exploração de novas ideias). A data limite do conhecimento é outubro de 2023."
  },
  "gpt-4o": {
    "description": "O ChatGPT-4o é um modelo dinâmico, atualizado em tempo real para manter a versão mais atual. Ele combina uma poderosa capacidade de compreensão e geração de linguagem, adequado para cenários de aplicação em larga escala, incluindo atendimento ao cliente, educação e suporte técnico."
  },
  "gpt-4o-2024-05-13": {
    "description": "O ChatGPT-4o é um modelo dinâmico, atualizado em tempo real para manter a versão mais atual. Ele combina uma poderosa capacidade de compreensão e geração de linguagem, adequado para cenários de aplicação em larga escala, incluindo atendimento ao cliente, educação e suporte técnico."
  },
  "gpt-4o-2024-08-06": {
    "description": "O ChatGPT-4o é um modelo dinâmico, atualizado em tempo real para manter a versão mais atual. Ele combina uma poderosa capacidade de compreensão e geração de linguagem, adequado para cenários de aplicação em larga escala, incluindo atendimento ao cliente, educação e suporte técnico."
  },
  "gpt-4o-2024-11-20": {
    "description": "ChatGPT-4o é um modelo dinâmico, atualizado em tempo real para manter a versão mais atualizada. Combina uma poderosa compreensão e capacidade de geração de linguagem, adequado para cenários de aplicação em larga escala, incluindo atendimento ao cliente, educação e suporte técnico."
  },
  "gpt-4o-audio-preview": {
    "description": "Modelo de áudio GPT-4o, suporta entrada e saída de áudio."
  },
  "gpt-4o-mini": {
    "description": "O GPT-4o mini é o mais recente modelo lançado pela OpenAI após o GPT-4 Omni, suportando entrada de texto e imagem e gerando texto como saída. Como seu modelo compacto mais avançado, ele é muito mais acessível do que outros modelos de ponta recentes, custando mais de 60% menos que o GPT-3.5 Turbo. Ele mantém uma inteligência de ponta, ao mesmo tempo que oferece um custo-benefício significativo. O GPT-4o mini obteve uma pontuação de 82% no teste MMLU e atualmente está classificado acima do GPT-4 em preferências de chat."
  },
  "gpt-4o-mini-realtime-preview": {
    "description": "Versão em tempo real do GPT-4o-mini, suporta entrada e saída de áudio e texto em tempo real."
  },
  "gpt-4o-mini-tts": {
    "description": "GPT-4o mini TTS é um modelo de texto para fala baseado em GPT-4o mini, oferecendo uma geração de voz de alta qualidade a um custo mais baixo."
  },
  "gpt-4o-realtime-preview": {
    "description": "Versão em tempo real do GPT-4o, suporta entrada e saída de áudio e texto em tempo real."
  },
  "gpt-4o-realtime-preview-2024-10-01": {
    "description": "Versão em tempo real do GPT-4o, suporta entrada e saída de áudio e texto em tempo real."
  },
  "gpt-4o-realtime-preview-2024-12-17": {
    "description": "Versão em tempo real do GPT-4o, suporta entrada e saída de áudio e texto em tempo real."
  },
  "grok-2-1212": {
    "description": "Este modelo apresenta melhorias em precisão, conformidade com instruções e capacidade multilíngue."
  },
  "grok-2-vision-1212": {
    "description": "Este modelo apresenta melhorias em precisão, conformidade com instruções e capacidade multilíngue."
  },
  "grok-beta": {
    "description": "Apresenta desempenho equivalente ao Grok 2, mas com maior eficiência, velocidade e funcionalidades."
  },
  "grok-vision-beta": {
    "description": "O mais recente modelo de compreensão de imagem, capaz de lidar com uma variedade de informações visuais, incluindo documentos, gráficos, capturas de tela e fotos."
  },
  "gryphe/mythomax-l2-13b": {
    "description": "MythoMax l2 13B é um modelo de linguagem que combina criatividade e inteligência, integrando vários modelos de ponta."
  },
  "hunyuan-code": {
    "description": "O mais recente modelo de geração de código Hunyuan, treinado com 200B de dados de código de alta qualidade, com seis meses de treinamento de dados SFT de alta qualidade, aumentando o comprimento da janela de contexto para 8K, destacando-se em métricas automáticas de geração de código em cinco linguagens; em avaliações de qualidade de código em dez aspectos em cinco linguagens, o desempenho está na primeira divisão."
  },
  "hunyuan-functioncall": {
    "description": "O mais recente modelo FunctionCall da arquitetura MOE Hunyuan, treinado com dados de alta qualidade de FunctionCall, com uma janela de contexto de 32K, liderando em várias métricas de avaliação."
  },
  "hunyuan-large": {
    "description": "O modelo Hunyuan-large possui um total de aproximadamente 389B de parâmetros, com cerca de 52B de parâmetros ativados, sendo o modelo MoE de código aberto com a maior escala de parâmetros e melhor desempenho na arquitetura Transformer atualmente disponível no mercado."
  },
  "hunyuan-large-longcontext": {
    "description": "Especializado em tarefas de texto longo, como resumo de documentos e perguntas e respostas de documentos, também possui a capacidade de lidar com tarefas gerais de geração de texto. Apresenta desempenho excepcional na análise e geração de textos longos, conseguindo atender efetivamente às demandas complexas e detalhadas de processamento de conteúdo longo."
  },
  "hunyuan-lite": {
    "description": "Atualizado para uma estrutura MOE, com uma janela de contexto de 256k, liderando em várias avaliações em NLP, código, matemática e setores diversos em comparação com muitos modelos de código aberto."
  },
  "hunyuan-lite-vision": {
    "description": "Modelo multimodal mais recente de 7B da Hunyuan, com janela de contexto de 32K, suporta diálogos multimodais em cenários em chinês e português, reconhecimento de objetos em imagens, compreensão de documentos e tabelas, matemática multimodal, entre outros, superando modelos concorrentes de 7B em várias métricas de avaliação."
  },
  "hunyuan-pro": {
    "description": "Modelo de texto longo MOE-32K com trilhões de parâmetros. Alcança níveis de liderança absoluta em vários benchmarks, com capacidades complexas de instrução e raciocínio, habilidades matemáticas complexas, suporte a chamadas de função, otimizado para áreas como tradução multilíngue, finanças, direito e saúde."
  },
  "hunyuan-role": {
    "description": "O mais recente modelo de interpretação de papéis Hunyuan, um modelo de interpretação de papéis ajustado e treinado oficialmente pela Hunyuan, que combina o modelo Hunyuan com um conjunto de dados de cenários de interpretação de papéis, apresentando um desempenho básico melhor em cenários de interpretação de papéis."
  },
  "hunyuan-standard": {
    "description": "Adota uma estratégia de roteamento superior, ao mesmo tempo que mitiga problemas de balanceamento de carga e convergência de especialistas. Em termos de textos longos, o índice de precisão atinge 99,9%. O MOE-32K oferece uma relação custo-benefício relativamente melhor, equilibrando desempenho e preço, permitindo o processamento de entradas de texto longo."
  },
  "hunyuan-standard-256K": {
    "description": "Adota uma estratégia de roteamento superior, ao mesmo tempo que mitiga problemas de balanceamento de carga e convergência de especialistas. Em termos de textos longos, o índice de precisão atinge 99,9%. O MOE-256K rompe ainda mais em comprimento e desempenho, expandindo significativamente o comprimento de entrada permitido."
  },
  "hunyuan-standard-vision": {
    "description": "Modelo multimodal mais recente da Hunyuan, suporta respostas em múltiplas línguas, com habilidades equilibradas em chinês e português."
  },
  "hunyuan-t1-20250321": {
    "description": "Modelo abrangente que constrói habilidades em ciências exatas e humanas, com forte capacidade de captura de informações em textos longos. Suporta raciocínio para responder a problemas científicos de diversas dificuldades, incluindo matemática, lógica, ciências e código."
  },
  "hunyuan-t1-latest": {
    "description": "O primeiro modelo de inferência Hybrid-Transformer-Mamba em larga escala da indústria, que expande a capacidade de inferência, possui uma velocidade de decodificação excepcional e alinha-se ainda mais às preferências humanas."
  },
  "hunyuan-translation": {
    "description": "Suporta tradução entre 15 idiomas, incluindo chinês, inglês, japonês, francês, português, espanhol, turco, russo, árabe, coreano, italiano, alemão, vietnamita, malaio e indonésio, com avaliação automatizada baseada no conjunto de testes de tradução em múltiplos cenários e pontuação COMET, superando modelos de tamanho semelhante no mercado em termos de capacidade de tradução entre idiomas."
  },
  "hunyuan-translation-lite": {
    "description": "O modelo de tradução Hunyuan suporta tradução em estilo de diálogo em linguagem natural; suporta tradução entre 15 idiomas, incluindo chinês, inglês, japonês, francês, português, espanhol, turco, russo, árabe, coreano, italiano, alemão, vietnamita, malaio e indonésio."
  },
  "hunyuan-turbo": {
    "description": "Versão de pré-visualização do novo modelo de linguagem de próxima geração Hunyuan, utilizando uma nova estrutura de modelo de especialistas mistos (MoE), com eficiência de inferência mais rápida e desempenho superior em comparação ao Hunyuan-Pro."
  },
  "hunyuan-turbo-20241223": {
    "description": "Esta versão otimiza: escalonamento de instruções de dados, aumentando significativamente a capacidade de generalização do modelo; melhoria substancial nas habilidades matemáticas, de codificação e de raciocínio lógico; otimização das capacidades de compreensão de texto e palavras; melhoria na qualidade da geração de conteúdo de criação de texto."
  },
  "hunyuan-turbo-latest": {
    "description": "Otimização da experiência geral, incluindo compreensão de NLP, criação de texto, conversas informais, perguntas e respostas de conhecimento, tradução, entre outros; aumento da humanização, otimização da inteligência emocional do modelo; melhoria na capacidade do modelo de esclarecer ativamente em casos de intenção ambígua; aprimoramento na capacidade de lidar com questões de análise de palavras; melhoria na qualidade e interatividade da criação; aprimoramento da experiência em múltiplas interações."
  },
  "hunyuan-turbo-vision": {
    "description": "Novo modelo de linguagem visual de próxima geração da Hunyuan, adotando uma nova estrutura de modelo de especialistas mistos (MoE), com melhorias abrangentes em relação ao modelo anterior nas capacidades de reconhecimento básico, criação de conteúdo, perguntas e respostas de conhecimento, e análise e raciocínio relacionados à compreensão de texto e imagem."
  },
  "hunyuan-turbos-20250226": {
    "description": "Versão fixa do hunyuan-TurboS pv2.1.2 com atualização do token de treinamento; aprimoramento das capacidades de raciocínio em matemática/lógica/código; melhoria da experiência em chinês e inglês, incluindo criação de texto, compreensão de texto, perguntas e respostas de conhecimento, conversas informais, entre outros."
  },
  "hunyuan-turbos-20250313": {
    "description": "Estilo unificado para etapas de resolução de problemas matemáticos, fortalecendo perguntas e respostas matemáticas em múltiplas rodadas. Otimização do estilo de resposta para criação de texto, removendo o 'sabor' de IA e aumentando a eloquência."
  },
  "hunyuan-turbos-latest": {
    "description": "A versão mais recente do hunyuan-TurboS, o modelo de grande porte da Hunyuan, possui uma capacidade de raciocínio mais forte e uma experiência aprimorada."
  },
  "hunyuan-vision": {
    "description": "O mais recente modelo multimodal Hunyuan, que suporta a entrada de imagens e texto para gerar conteúdo textual."
  },
  "internlm/internlm2_5-20b-chat": {
    "description": "O modelo de código aberto inovador InternLM2.5, com um grande número de parâmetros, melhora a inteligência do diálogo."
  },
  "internlm/internlm2_5-7b-chat": {
    "description": "InternLM2.5 oferece soluções de diálogo inteligente em múltiplos cenários."
  },
  "internlm2-pro-chat": {
    "description": "Modelo mais antigo que ainda estamos mantendo, disponível em opções de 7B e 20B de parâmetros."
  },
  "internlm2.5-latest": {
    "description": "Nossa mais recente série de modelos, com desempenho de raciocínio excepcional, suportando um comprimento de contexto de 1M e capacidades aprimoradas de seguimento de instruções e chamadas de ferramentas."
  },
  "internlm3-latest": {
    "description": "Nossa mais recente série de modelos, com desempenho de inferência excepcional, liderando entre modelos de código aberto de mesma escala. Aponta por padrão para nossa mais recente série de modelos InternLM3."
  },
  "jina-deepsearch-v1": {
    "description": "A busca profunda combina pesquisa na web, leitura e raciocínio para realizar investigações abrangentes. Você pode vê-la como um agente que aceita suas tarefas de pesquisa - ela realizará uma busca extensa e passará por várias iterações antes de fornecer uma resposta. Esse processo envolve pesquisa contínua, raciocínio e resolução de problemas sob diferentes ângulos. Isso é fundamentalmente diferente de gerar respostas diretamente a partir de dados pré-treinados de grandes modelos padrão e de sistemas RAG tradicionais que dependem de buscas superficiais únicas."
  },
  "kimi-latest": {
    "description": "O produto assistente inteligente Kimi utiliza o mais recente modelo Kimi, que pode conter recursos ainda não estáveis. Suporta compreensão de imagens e seleciona automaticamente o modelo de cobrança de 8k/32k/128k com base no comprimento do contexto da solicitação."
  },
  "learnlm-1.5-pro-experimental": {
    "description": "LearnLM é um modelo de linguagem experimental e específico para tarefas, treinado para atender aos princípios da ciência da aprendizagem, podendo seguir instruções sistemáticas em cenários de ensino e aprendizagem, atuando como um mentor especialista, entre outros."
  },
  "lite": {
    "description": "Spark Lite é um modelo de linguagem grande leve, com latência extremamente baixa e alta eficiência de processamento, totalmente gratuito e aberto, suportando funcionalidades de busca online em tempo real. Sua característica de resposta rápida o torna excelente para aplicações de inferência em dispositivos de baixo poder computacional e ajuste fino de modelos, proporcionando aos usuários uma excelente relação custo-benefício e experiência inteligente, especialmente em cenários de perguntas e respostas, geração de conteúdo e busca."
  },
  "llama-2-7b-chat": {
    "description": "Llama2 é uma série de modelos de linguagem grandes (LLM) desenvolvidos e open source pela Meta, que inclui modelos de texto gerativo pré-treinados e finetunados com escalas variando de 7 bilhões a 70 bilhões de parâmetros. Do ponto de vista arquitetural, o Llama2 é um modelo de linguagem autoregressivo que utiliza uma arquitetura de transformador otimizada. As versões ajustadas utilizam micro-treinamento supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para alinhar as preferências de utilidade e segurança humanas. O Llama2 apresenta um desempenho notável em vários conjuntos de dados acadêmicos, fornecendo inspiração para o design e desenvolvimento de muitos outros modelos."
  },
  "llama-3.1-70b-versatile": {
    "description": "Llama 3.1 70B oferece capacidade de raciocínio AI mais poderosa, adequada para aplicações complexas, suportando um processamento computacional extenso e garantindo eficiência e precisão."
  },
  "llama-3.1-8b-instant": {
    "description": "Llama 3.1 8B é um modelo de alto desempenho, oferecendo capacidade de geração de texto rápida, ideal para cenários de aplicação que exigem eficiência em larga escala e custo-benefício."
  },
  "llama-3.1-instruct": {
    "description": "O modelo Llama 3.1 com ajuste fino de instruções foi otimizado para cenários de diálogo, superando muitos modelos de chat de código aberto existentes em benchmarks comuns do setor."
  },
  "llama-3.2-11b-vision-instruct": {
    "description": "Capacidade excepcional de raciocínio visual em imagens de alta resolução, adequada para aplicações de compreensão visual."
  },
  "llama-3.2-11b-vision-preview": {
    "description": "Llama 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "llama-3.2-90b-vision-instruct": {
    "description": "Capacidade avançada de raciocínio visual para aplicações de agentes de compreensão visual."
  },
  "llama-3.2-90b-vision-preview": {
    "description": "Llama 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "llama-3.2-vision-instruct": {
    "description": "O modelo Llama 3.2-Vision com ajuste fino de instruções foi otimizado para reconhecimento visual, raciocínio com imagens, descrição de imagens e respostas a perguntas gerais relacionadas a imagens."
  },
  "llama-3.3-70b-instruct": {
    "description": "Llama 3.3 é o modelo de linguagem de código aberto multilíngue mais avançado da série Llama, oferecendo desempenho comparável ao modelo 405B a um custo extremamente baixo. Baseado na estrutura Transformer, e aprimorado por meio de ajuste fino supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para aumentar a utilidade e a segurança. Sua versão ajustada para instruções é otimizada para diálogos multilíngues, superando muitos modelos de chat de código aberto e fechado em vários benchmarks da indústria. A data limite de conhecimento é dezembro de 2023."
  },
  "llama-3.3-70b-versatile": {
    "description": "O modelo de linguagem multilíngue Meta Llama 3.3 (LLM) é um modelo gerador pré-treinado e ajustado para instruções, com 70B (entrada/saída de texto). O modelo de texto puro ajustado para instruções do Llama 3.3 é otimizado para casos de uso de diálogo multilíngue e supera muitos modelos de chat open source e fechados disponíveis em benchmarks comuns da indústria."
  },
  "llama-3.3-instruct": {
    "description": "O modelo Llama 3.3 com ajuste fino de instruções foi otimizado para cenários de diálogo, superando muitos modelos de chat open-source existentes em benchmarks comuns do setor."
  },
  "llama3-70b-8192": {
    "description": "Meta Llama 3 70B oferece capacidade de processamento incomparável para complexidade, projetado sob medida para projetos de alta demanda."
  },
  "llama3-8b-8192": {
    "description": "Meta Llama 3 8B oferece desempenho de raciocínio de alta qualidade, adequado para uma variedade de necessidades de aplicação."
  },
  "llama3-groq-70b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 70B Tool Use oferece poderosa capacidade de chamada de ferramentas, suportando o processamento eficiente de tarefas complexas."
  },
  "llama3-groq-8b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 8B Tool Use é um modelo otimizado para uso eficiente de ferramentas, suportando cálculos paralelos rápidos."
  },
  "llama3.1": {
    "description": "Llama 3.1 é um modelo líder lançado pela Meta, suportando até 405B de parâmetros, aplicável em diálogos complexos, tradução multilíngue e análise de dados."
  },
  "llama3.1:405b": {
    "description": "Llama 3.1 é um modelo líder lançado pela Meta, suportando até 405B de parâmetros, aplicável em diálogos complexos, tradução multilíngue e análise de dados."
  },
  "llama3.1:70b": {
    "description": "Llama 3.1 é um modelo líder lançado pela Meta, suportando até 405B de parâmetros, aplicável em diálogos complexos, tradução multilíngue e análise de dados."
  },
  "llava": {
    "description": "LLaVA é um modelo multimodal que combina um codificador visual e Vicuna, projetado para forte compreensão visual e linguística."
  },
  "llava-v1.5-7b-4096-preview": {
    "description": "LLaVA 1.5 7B oferece capacidade de processamento visual integrada, gerando saídas complexas a partir de informações visuais."
  },
  "llava:13b": {
    "description": "LLaVA é um modelo multimodal que combina um codificador visual e Vicuna, projetado para forte compreensão visual e linguística."
  },
  "llava:34b": {
    "description": "LLaVA é um modelo multimodal que combina um codificador visual e Vicuna, projetado para forte compreensão visual e linguística."
  },
  "mathstral": {
    "description": "MathΣtral é projetado para pesquisa científica e raciocínio matemático, oferecendo capacidade de cálculo eficaz e interpretação de resultados."
  },
  "max-32k": {
    "description": "Spark Max 32K possui uma capacidade de processamento de contexto grande, com melhor compreensão de contexto e capacidade de raciocínio lógico, suportando entradas de texto de 32K tokens, adequado para leitura de documentos longos, perguntas e respostas de conhecimento privado e outros cenários."
  },
  "megrez-3b-instruct": {
    "description": "Megrez-3B-Instruct é um modelo de linguagem grande treinado de forma totalmente autônoma pela Wúwèn Xīnqióng. O Megrez-3B-Instruct visa criar uma solução de inteligência de borda rápida, compacta e fácil de usar, através do conceito de integração de hardware e software."
  },
  "meta-llama-3-70b-instruct": {
    "description": "Um poderoso modelo com 70 bilhões de parâmetros, destacando-se em raciocínio, codificação e amplas aplicações linguísticas."
  },
  "meta-llama-3-8b-instruct": {
    "description": "Um modelo versátil com 8 bilhões de parâmetros, otimizado para tarefas de diálogo e geração de texto."
  },
  "meta-llama-3.1-405b-instruct": {
    "description": "Os modelos de texto apenas ajustados por instrução Llama 3.1 são otimizados para casos de uso de diálogo multilíngue e superam muitos dos modelos de chat de código aberto e fechado disponíveis em benchmarks comuns da indústria."
  },
  "meta-llama-3.1-70b-instruct": {
    "description": "Os modelos de texto apenas ajustados por instrução Llama 3.1 são otimizados para casos de uso de diálogo multilíngue e superam muitos dos modelos de chat de código aberto e fechado disponíveis em benchmarks comuns da indústria."
  },
  "meta-llama-3.1-8b-instruct": {
    "description": "Os modelos de texto apenas ajustados por instrução Llama 3.1 são otimizados para casos de uso de diálogo multilíngue e superam muitos dos modelos de chat de código aberto e fechado disponíveis em benchmarks comuns da indústria."
  },
  "meta-llama/Llama-2-13b-chat-hf": {
    "description": "LLaMA-2 Chat (13B) oferece excelente capacidade de processamento de linguagem e uma experiência interativa notável."
  },
  "meta-llama/Llama-2-70b-hf": {
    "description": "LLaMA-2 oferece excelente capacidade de processamento de linguagem e uma experiência interativa excepcional."
  },
  "meta-llama/Llama-3-70b-chat-hf": {
    "description": "LLaMA-3 Chat (70B) é um modelo de chat poderoso, suportando necessidades de diálogo complexas."
  },
  "meta-llama/Llama-3-8b-chat-hf": {
    "description": "LLaMA-3 Chat (8B) oferece suporte multilíngue, abrangendo um rico conhecimento em diversas áreas."
  },
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "meta-llama/Llama-3.2-3B-Instruct-Turbo": {
    "description": "LLaMA 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "meta-llama/Llama-3.3-70B-Instruct-Turbo": {
    "description": "O Meta Llama 3.3 é um modelo de linguagem de grande escala multilíngue (LLM) com 70B (entrada/saída de texto) que é um modelo gerado por pré-treinamento e ajuste de instruções. O modelo de texto puro ajustado por instruções do Llama 3.3 foi otimizado para casos de uso de diálogo multilíngue e supera muitos modelos de chat de código aberto e fechados disponíveis em benchmarks de indústria comuns."
  },
  "meta-llama/Llama-Vision-Free": {
    "description": "LLaMA 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
    "description": "Llama 3 70B Instruct Lite é ideal para ambientes que exigem alta eficiência e baixa latência."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": {
    "description": "Llama 3 70B Instruct Turbo oferece uma capacidade excepcional de compreensão e geração de linguagem, adequado para as tarefas computacionais mais exigentes."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite": {
    "description": "Llama 3 8B Instruct Lite é adequado para ambientes com recursos limitados, oferecendo um excelente equilíbrio de desempenho."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": {
    "description": "Llama 3 8B Instruct Turbo é um modelo de linguagem de alto desempenho, suportando uma ampla gama de cenários de aplicação."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "description": "LLaMA 3.1 405B é um modelo poderoso para pré-treinamento e ajuste de instruções."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
    "description": "O modelo Llama 3.1 Turbo 405B oferece suporte a um contexto de capacidade extremamente grande para processamento de grandes volumes de dados, destacando-se em aplicações de inteligência artificial em larga escala."
  },
  "meta-llama/Meta-Llama-3.1-70B": {
    "description": "Llama 3.1 é o modelo líder lançado pela Meta, suportando até 405B de parâmetros, aplicável em diálogos complexos, tradução multilíngue e análise de dados."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
    "description": "O modelo Llama 3.1 70B é ajustado para aplicações de alta carga, quantizado para FP8, oferecendo maior eficiência computacional e precisão, garantindo desempenho excepcional em cenários complexos."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
    "description": "O modelo Llama 3.1 8B utiliza quantização FP8, suportando até 131.072 tokens de contexto, destacando-se entre os modelos de código aberto, ideal para tarefas complexas e superando muitos benchmarks do setor."
  },
  "meta-llama/llama-3-70b-instruct": {
    "description": "Llama 3 70B Instruct é otimizado para cenários de diálogo de alta qualidade, apresentando desempenho excepcional em várias avaliações humanas."
  },
  "meta-llama/llama-3-8b-instruct": {
    "description": "Llama 3 8B Instruct otimiza cenários de diálogo de alta qualidade, com desempenho superior a muitos modelos fechados."
  },
  "meta-llama/llama-3.1-70b-instruct": {
    "description": "Llama 3.1 70B Instruct é projetado para diálogos de alta qualidade, destacando-se em avaliações humanas, especialmente em cenários de alta interação."
  },
  "meta-llama/llama-3.1-8b-instruct": {
    "description": "Llama 3.1 8B Instruct é a versão mais recente lançada pela Meta, otimizada para cenários de diálogo de alta qualidade, superando muitos modelos fechados de ponta."
  },
  "meta-llama/llama-3.1-8b-instruct:free": {
    "description": "LLaMA 3.1 oferece suporte multilíngue e é um dos modelos geradores líderes do setor."
  },
  "meta-llama/llama-3.2-11b-vision-instruct": {
    "description": "LLaMA 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "meta-llama/llama-3.2-3b-instruct": {
    "description": "meta-llama/llama-3.2-3b-instruct"
  },
  "meta-llama/llama-3.2-90b-vision-instruct": {
    "description": "LLaMA 3.2 é projetado para lidar com tarefas que combinam dados visuais e textuais. Ele se destaca em tarefas como descrição de imagens e perguntas visuais, superando a lacuna entre geração de linguagem e raciocínio visual."
  },
  "meta-llama/llama-3.3-70b-instruct": {
    "description": "Llama 3.3 é o modelo de linguagem de código aberto multilíngue mais avançado da série Llama, oferecendo desempenho comparável ao modelo 405B a um custo extremamente baixo. Baseado na estrutura Transformer, e aprimorado por meio de ajuste fino supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para aumentar a utilidade e a segurança. Sua versão ajustada para instruções é otimizada para diálogos multilíngues, superando muitos modelos de chat de código aberto e fechado em vários benchmarks da indústria. A data limite de conhecimento é dezembro de 2023."
  },
  "meta-llama/llama-3.3-70b-instruct:free": {
    "description": "Llama 3.3 é o modelo de linguagem de código aberto multilíngue mais avançado da série Llama, oferecendo desempenho comparável ao modelo 405B a um custo extremamente baixo. Baseado na estrutura Transformer, e aprimorado por meio de ajuste fino supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para aumentar a utilidade e a segurança. Sua versão ajustada para instruções é otimizada para diálogos multilíngues, superando muitos modelos de chat de código aberto e fechado em vários benchmarks da indústria. A data limite de conhecimento é dezembro de 2023."
  },
  "meta.llama3-1-405b-instruct-v1:0": {
    "description": "Meta Llama 3.1 405B Instruct é o maior e mais poderoso modelo da série Llama 3.1 Instruct, sendo um modelo altamente avançado para raciocínio conversacional e geração de dados sintéticos, que também pode ser usado como base para pré-treinamento ou ajuste fino em domínios específicos. Os modelos de linguagem de grande escala (LLMs) multilíngues oferecidos pelo Llama 3.1 são um conjunto de modelos geradores pré-treinados e ajustados por instruções, incluindo tamanhos de 8B, 70B e 405B (entrada/saída de texto). Os modelos de texto ajustados por instruções do Llama 3.1 (8B, 70B, 405B) são otimizados para casos de uso de diálogo multilíngue e superaram muitos modelos de chat de código aberto disponíveis em benchmarks comuns da indústria. O Llama 3.1 é projetado para uso comercial e de pesquisa em várias línguas. Os modelos de texto ajustados por instruções são adequados para chats semelhantes a assistentes, enquanto os modelos pré-treinados podem se adaptar a várias tarefas de geração de linguagem natural. O modelo Llama 3.1 também suporta a utilização de sua saída para melhorar outros modelos, incluindo geração de dados sintéticos e refinamento. O Llama 3.1 é um modelo de linguagem autoregressivo que utiliza uma arquitetura de transformador otimizada. As versões ajustadas utilizam ajuste fino supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para alinhar-se às preferências humanas em relação à utilidade e segurança."
  },
  "meta.llama3-1-70b-instruct-v1:0": {
    "description": "A versão atualizada do Meta Llama 3.1 70B Instruct, incluindo um comprimento de contexto expandido de 128K, multilinguismo e capacidades de raciocínio aprimoradas. Os modelos de linguagem de grande porte (LLMs) do Llama 3.1 são um conjunto de modelos geradores pré-treinados e ajustados por instruções, incluindo tamanhos de 8B, 70B e 405B (entrada/saída de texto). Os modelos de texto ajustados por instruções do Llama 3.1 (8B, 70B, 405B) são otimizados para casos de uso de diálogo multilíngue e superaram muitos modelos de chat de código aberto disponíveis em benchmarks de indústria comuns. O Llama 3.1 é projetado para uso comercial e de pesquisa em várias línguas. Os modelos de texto ajustados por instruções são adequados para chats semelhantes a assistentes, enquanto os modelos pré-treinados podem se adaptar a várias tarefas de geração de linguagem natural. O modelo Llama 3.1 também suporta a utilização de suas saídas para melhorar outros modelos, incluindo geração de dados sintéticos e refinamento. O Llama 3.1 é um modelo de linguagem autoregressivo usando uma arquitetura de transformador otimizada. As versões ajustadas utilizam ajuste fino supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para alinhar-se às preferências humanas por ajuda e segurança."
  },
  "meta.llama3-1-8b-instruct-v1:0": {
    "description": "A versão atualizada do Meta Llama 3.1 8B Instruct, incluindo um comprimento de contexto expandido de 128K, multilinguismo e capacidades de raciocínio aprimoradas. Os modelos de linguagem de grande porte (LLMs) do Llama 3.1 são um conjunto de modelos geradores pré-treinados e ajustados por instruções, incluindo tamanhos de 8B, 70B e 405B (entrada/saída de texto). Os modelos de texto ajustados por instruções do Llama 3.1 (8B, 70B, 405B) são otimizados para casos de uso de diálogo multilíngue e superaram muitos modelos de chat de código aberto disponíveis em benchmarks de indústria comuns. O Llama 3.1 é projetado para uso comercial e de pesquisa em várias línguas. Os modelos de texto ajustados por instruções são adequados para chats semelhantes a assistentes, enquanto os modelos pré-treinados podem se adaptar a várias tarefas de geração de linguagem natural. O modelo Llama 3.1 também suporta a utilização de suas saídas para melhorar outros modelos, incluindo geração de dados sintéticos e refinamento. O Llama 3.1 é um modelo de linguagem autoregressivo usando uma arquitetura de transformador otimizada. As versões ajustadas utilizam ajuste fino supervisionado (SFT) e aprendizado por reforço com feedback humano (RLHF) para alinhar-se às preferências humanas por ajuda e segurança."
  },
  "meta.llama3-70b-instruct-v1:0": {
    "description": "Meta Llama 3 é um modelo de linguagem de grande escala (LLM) aberto voltado para desenvolvedores, pesquisadores e empresas, projetado para ajudá-los a construir, experimentar e expandir suas ideias de IA geradora de forma responsável. Como parte de um sistema de base para inovação da comunidade global, é ideal para criação de conteúdo, IA de diálogo, compreensão de linguagem, P&D e aplicações empresariais."
  },
  "meta.llama3-8b-instruct-v1:0": {
    "description": "Meta Llama 3 é um modelo de linguagem de grande escala (LLM) aberto voltado para desenvolvedores, pesquisadores e empresas, projetado para ajudá-los a construir, experimentar e expandir suas ideias de IA geradora de forma responsável. Como parte de um sistema de base para inovação da comunidade global, é ideal para dispositivos de borda com capacidade de computação e recursos limitados, além de tempos de treinamento mais rápidos."
  },
  "meta/llama-3.1-405b-instruct": {
    "description": "LLM avançado, suporta geração de dados sintéticos, destilação de conhecimento e raciocínio, adequado para chatbots, programação e tarefas de domínio específico."
  },
  "meta/llama-3.1-70b-instruct": {
    "description": "Capacita diálogos complexos, com excelente compreensão de contexto, capacidade de raciocínio e geração de texto."
  },
  "meta/llama-3.1-8b-instruct": {
    "description": "Modelo de ponta avançado, com compreensão de linguagem, excelente capacidade de raciocínio e geração de texto."
  },
  "meta/llama-3.2-11b-vision-instruct": {
    "description": "Modelo de visão-linguagem de ponta, especializado em raciocínio de alta qualidade a partir de imagens."
  },
  "meta/llama-3.2-1b-instruct": {
    "description": "Modelo de linguagem de ponta avançado e compacto, com compreensão de linguagem, excelente capacidade de raciocínio e geração de texto."
  },
  "meta/llama-3.2-3b-instruct": {
    "description": "Modelo de linguagem de ponta avançado e compacto, com compreensão de linguagem, excelente capacidade de raciocínio e geração de texto."
  },
  "meta/llama-3.2-90b-vision-instruct": {
    "description": "Modelo de visão-linguagem de ponta, especializado em raciocínio de alta qualidade a partir de imagens."
  },
  "meta/llama-3.3-70b-instruct": {
    "description": "Modelo LLM avançado, especializado em raciocínio, matemática, conhecimento geral e chamadas de função."
  },
  "microsoft/WizardLM-2-8x22B": {
    "description": "WizardLM 2 é um modelo de linguagem fornecido pela Microsoft AI, que se destaca em diálogos complexos, multilíngue, raciocínio e assistentes inteligentes."
  },
  "microsoft/wizardlm-2-8x22b": {
    "description": "WizardLM-2 8x22B é o modelo Wizard mais avançado da Microsoft, demonstrando um desempenho extremamente competitivo."
  },
  "minicpm-v": {
    "description": "MiniCPM-V é a nova geração de grandes modelos multimodais lançada pela OpenBMB, com excelente capacidade de reconhecimento de OCR e compreensão multimodal, suportando uma ampla gama de cenários de aplicação."
  },
  "ministral-3b-latest": {
    "description": "Ministral 3B é o modelo de ponta da Mistral para aplicações de edge computing."
  },
  "ministral-8b-latest": {
    "description": "Ministral 8B é o modelo de edge computing altamente custo-efetivo da Mistral."
  },
  "mistral": {
    "description": "Mistral é um modelo de 7B lançado pela Mistral AI, adequado para demandas de processamento de linguagem variáveis."
  },
  "mistral-large": {
    "description": "Mixtral Large é o modelo de destaque da Mistral, combinando capacidades de geração de código, matemática e raciocínio, suportando uma janela de contexto de 128k."
  },
  "mistral-large-instruct": {
    "description": "Mistral-Large-Instruct-2407 é um modelo avançado de linguagem densa (LLM) com 123 bilhões de parâmetros, oferecendo capacidades de raciocínio, conhecimento e codificação de última geração."
  },
  "mistral-large-latest": {
    "description": "Mistral Large é o modelo de destaque, especializado em tarefas multilíngues, raciocínio complexo e geração de código, sendo a escolha ideal para aplicações de alto nível."
  },
  "mistral-nemo": {
    "description": "Mistral Nemo é um modelo de 12B desenvolvido em colaboração entre a Mistral AI e a NVIDIA, oferecendo desempenho eficiente."
  },
  "mistral-nemo-instruct": {
    "description": "Mistral-Nemo-Instruct-2407 é um modelo de linguagem grande (LLM) ajustado para instruções, baseado no Mistral-Nemo-Base-2407."
  },
  "mistral-small": {
    "description": "Mistral Small pode ser usado em qualquer tarefa baseada em linguagem que exija alta eficiência e baixa latência."
  },
  "mistral-small-latest": {
    "description": "Mistral Small é uma opção de alto custo-benefício, rápida e confiável, adequada para casos de uso como tradução, resumo e análise de sentimentos."
  },
  "mistralai/Mistral-7B-Instruct-v0.1": {
    "description": "Mistral (7B) Instruct é conhecido por seu alto desempenho, adequado para diversas tarefas de linguagem."
  },
  "mistralai/Mistral-7B-Instruct-v0.2": {
    "description": "Mistral 7B é um modelo ajustado sob demanda, oferecendo respostas otimizadas para tarefas."
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "description": "Mistral (7B) Instruct v0.3 oferece capacidade computacional eficiente e compreensão de linguagem natural, adequada para uma ampla gama de aplicações."
  },
  "mistralai/Mistral-7B-v0.1": {
    "description": "Mistral 7B é um modelo compacto, mas de alto desempenho, especializado em processamento em lote e tarefas simples, como classificação e geração de texto, com boa capacidade de raciocínio."
  },
  "mistralai/Mixtral-8x22B-Instruct-v0.1": {
    "description": "Mixtral-8x22B Instruct (141B) é um super modelo de linguagem, suportando demandas de processamento extremamente altas."
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "description": "Mixtral 8x7B é um modelo de especialistas esparsos pré-treinados, utilizado para tarefas de texto de uso geral."
  },
  "mistralai/Mixtral-8x7B-v0.1": {
    "description": "Mixtral 8x7B é um modelo de especialistas esparsos, que utiliza múltiplos parâmetros para aumentar a velocidade de raciocínio, ideal para tarefas de geração de código e multilíngues."
  },
  "mistralai/mistral-7b-instruct": {
    "description": "Mistral 7B Instruct é um modelo de padrão industrial de alto desempenho, com otimização de velocidade e suporte a longos contextos."
  },
  "mistralai/mistral-nemo": {
    "description": "Mistral Nemo é um modelo de 7.3B parâmetros com suporte multilíngue e programação de alto desempenho."
  },
  "mixtral": {
    "description": "Mixtral é o modelo de especialistas da Mistral AI, com pesos de código aberto, oferecendo suporte em geração de código e compreensão de linguagem."
  },
  "mixtral-8x7b-32768": {
    "description": "Mixtral 8x7B oferece alta capacidade de computação paralela com tolerância a falhas, adequado para tarefas complexas."
  },
  "mixtral:8x22b": {
    "description": "Mixtral é o modelo de especialistas da Mistral AI, com pesos de código aberto, oferecendo suporte em geração de código e compreensão de linguagem."
  },
  "moonshot-v1-128k": {
    "description": "Moonshot V1 128K é um modelo com capacidade de processamento de contexto ultra longo, adequado para gerar textos muito longos, atendendo a demandas complexas de geração, capaz de lidar com até 128.000 tokens, ideal para pesquisa, acadêmicos e geração de documentos extensos."
  },
  "moonshot-v1-128k-vision-preview": {
    "description": "O modelo visual Kimi (incluindo moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, etc.) é capaz de entender o conteúdo das imagens, incluindo texto, cores e formas dos objetos."
  },
  "moonshot-v1-32k": {
    "description": "Moonshot V1 32K oferece capacidade de processamento de contexto de comprimento médio, capaz de lidar com 32.768 tokens, especialmente adequado para gerar vários documentos longos e diálogos complexos, aplicável em criação de conteúdo, geração de relatórios e sistemas de diálogo."
  },
  "moonshot-v1-32k-vision-preview": {
    "description": "O modelo visual Kimi (incluindo moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, etc.) é capaz de entender o conteúdo das imagens, incluindo texto, cores e formas dos objetos."
  },
  "moonshot-v1-8k": {
    "description": "Moonshot V1 8K é projetado para tarefas de geração de texto curto, com desempenho de processamento eficiente, capaz de lidar com 8.192 tokens, ideal para diálogos curtos, anotações e geração rápida de conteúdo."
  },
  "moonshot-v1-8k-vision-preview": {
    "description": "O modelo visual Kimi (incluindo moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, etc.) é capaz de entender o conteúdo das imagens, incluindo texto, cores e formas dos objetos."
  },
  "moonshot-v1-auto": {
    "description": "O Moonshot V1 Auto pode escolher o modelo adequado com base na quantidade de Tokens ocupados pelo contexto atual."
  },
  "nousresearch/hermes-2-pro-llama-3-8b": {
    "description": "Hermes 2 Pro Llama 3 8B é uma versão aprimorada do Nous Hermes 2, contendo os conjuntos de dados mais recentes desenvolvidos internamente."
  },
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
    "description": "Llama 3.1 Nemotron 70B é um modelo de linguagem em larga escala personalizado pela NVIDIA, projetado para aumentar a utilidade das respostas geradas pelo LLM em relação às consultas dos usuários. Este modelo se destacou em benchmarks como Arena Hard, AlpacaEval 2 LC e GPT-4-Turbo MT-Bench, ocupando o primeiro lugar em todos os três benchmarks de alinhamento automático até 1º de outubro de 2024. O modelo foi treinado usando RLHF (especialmente REINFORCE), Llama-3.1-Nemotron-70B-Reward e HelpSteer2-Preference prompts, com base no modelo Llama-3.1-70B-Instruct."
  },
  "nvidia/llama-3.1-nemotron-51b-instruct": {
    "description": "Modelo de linguagem único, oferecendo precisão e eficiência incomparáveis."
  },
  "nvidia/llama-3.1-nemotron-70b-instruct": {
    "description": "Llama-3.1-Nemotron-70B-Instruct é um modelo de linguagem de grande porte personalizado pela NVIDIA, projetado para melhorar a utilidade das respostas geradas pelo LLM."
  },
  "o1": {
    "description": "Focado em raciocínio avançado e resolução de problemas complexos, incluindo tarefas matemáticas e científicas. Muito adequado para aplicativos que exigem compreensão profunda do contexto e gerenciamento de fluxos de trabalho."
  },
  "o1-mini": {
    "description": "o1-mini é um modelo de raciocínio rápido e econômico, projetado para cenários de programação, matemática e ciências. Este modelo possui um contexto de 128K e uma data limite de conhecimento em outubro de 2023."
  },
  "o1-preview": {
    "description": "o1 é o novo modelo de raciocínio da OpenAI, adequado para tarefas complexas que exigem amplo conhecimento geral. Este modelo possui um contexto de 128K e uma data limite de conhecimento em outubro de 2023."
  },
  "o3-mini": {
    "description": "o3-mini é nosso mais recente modelo de inferência em miniatura, oferecendo alta inteligência com os mesmos custos e metas de latência que o o1-mini."
  },
  "open-codestral-mamba": {
    "description": "Codestral Mamba é um modelo de linguagem Mamba 2 focado em geração de código, oferecendo forte suporte para tarefas avançadas de codificação e raciocínio."
  },
  "open-mistral-7b": {
    "description": "Mistral 7B é um modelo compacto, mas de alto desempenho, especializado em processamento em lote e tarefas simples, como classificação e geração de texto, com boa capacidade de raciocínio."
  },
  "open-mistral-nemo": {
    "description": "Mistral Nemo é um modelo de 12B desenvolvido em colaboração com a Nvidia, oferecendo excelente desempenho em raciocínio e codificação, fácil de integrar e substituir."
  },
  "open-mixtral-8x22b": {
    "description": "Mixtral 8x22B é um modelo de especialistas maior, focado em tarefas complexas, oferecendo excelente capacidade de raciocínio e maior taxa de transferência."
  },
  "open-mixtral-8x7b": {
    "description": "Mixtral 8x7B é um modelo de especialistas esparsos, utilizando múltiplos parâmetros para aumentar a velocidade de raciocínio, adequado para tarefas de geração de linguagem e código."
  },
  "openai/gpt-4o": {
    "description": "ChatGPT-4o é um modelo dinâmico, atualizado em tempo real para manter a versão mais recente. Combina uma poderosa capacidade de compreensão e geração de linguagem, adequado para cenários de aplicação em larga escala, incluindo atendimento ao cliente, educação e suporte técnico."
  },
  "openai/gpt-4o-mini": {
    "description": "GPT-4o mini é o mais recente modelo da OpenAI, lançado após o GPT-4 Omni, que suporta entrada de texto e imagem e saída de texto. Como seu modelo compacto mais avançado, é muito mais barato do que outros modelos de ponta recentes e custa mais de 60% menos que o GPT-3.5 Turbo. Ele mantém inteligência de ponta, ao mesmo tempo que oferece uma relação custo-benefício significativa. O GPT-4o mini obteve uma pontuação de 82% no teste MMLU e atualmente está classificado acima do GPT-4 em preferências de chat."
  },
  "openai/o1-mini": {
    "description": "o1-mini é um modelo de raciocínio rápido e econômico, projetado para cenários de programação, matemática e ciências. Este modelo possui um contexto de 128K e uma data limite de conhecimento em outubro de 2023."
  },
  "openai/o1-preview": {
    "description": "o1 é o novo modelo de raciocínio da OpenAI, adequado para tarefas complexas que exigem amplo conhecimento geral. Este modelo possui um contexto de 128K e uma data limite de conhecimento em outubro de 2023."
  },
  "openchat/openchat-7b": {
    "description": "OpenChat 7B é uma biblioteca de modelos de linguagem de código aberto ajustada com a estratégia de 'C-RLFT (refinamento de aprendizado por reforço condicional)'."
  },
  "openrouter/auto": {
    "description": "Com base no comprimento do contexto, tema e complexidade, sua solicitação será enviada para Llama 3 70B Instruct, Claude 3.5 Sonnet (autoajustável) ou GPT-4o."
  },
  "phi3": {
    "description": "Phi-3 é um modelo leve e aberto lançado pela Microsoft, adequado para integração eficiente e raciocínio de conhecimento em larga escala."
  },
  "phi3:14b": {
    "description": "Phi-3 é um modelo leve e aberto lançado pela Microsoft, adequado para integração eficiente e raciocínio de conhecimento em larga escala."
  },
  "pixtral-12b-2409": {
    "description": "O modelo Pixtral demonstra forte capacidade em tarefas de compreensão de gráficos e imagens, perguntas e respostas de documentos, raciocínio multimodal e seguimento de instruções, podendo ingerir imagens em resolução natural e proporções, além de processar um número arbitrário de imagens em uma janela de contexto longa de até 128K tokens."
  },
  "pixtral-large-latest": {
    "description": "Pixtral Large é um modelo multimodal de código aberto com 124 bilhões de parâmetros, baseado no Mistral Large 2. Este é o segundo modelo da nossa família multimodal, demonstrando capacidades de compreensão de imagem de nível avançado."
  },
  "pro-128k": {
    "description": "Spark Pro 128K possui uma capacidade de processamento de contexto extremamente grande, capaz de lidar com até 128K de informações contextuais, especialmente adequado para análise completa e processamento de associações lógicas de longo prazo em conteúdos longos, podendo oferecer lógica fluida e consistente e suporte a diversas citações em comunicações textuais complexas."
  },
  "qvq-72b-preview": {
    "description": "O modelo QVQ é um modelo de pesquisa experimental desenvolvido pela equipe Qwen, focado em melhorar a capacidade de raciocínio visual, especialmente na área de raciocínio matemático."
  },
  "qwen-coder-plus-latest": {
    "description": "Modelo de código Qwen."
  },
  "qwen-coder-turbo-latest": {
    "description": "Modelo de código Qwen."
  },
  "qwen-long": {
    "description": "O Qwen é um modelo de linguagem em larga escala que suporta contextos de texto longos e funcionalidades de diálogo baseadas em documentos longos e múltiplos cenários."
  },
  "qwen-math-plus-latest": {
    "description": "O modelo de matemática Qwen é especificamente projetado para resolver problemas matemáticos."
  },
  "qwen-math-turbo-latest": {
    "description": "O modelo de matemática Qwen é especificamente projetado para resolver problemas matemáticos."
  },
  "qwen-max": {
    "description": "Modelo de linguagem em larga escala com trilhões de parâmetros do Qwen, suportando entradas em diferentes idiomas, como português e inglês, atualmente a versão API por trás do produto Qwen 2.5."
  },
  "qwen-max-latest": {
    "description": "O modelo de linguagem em larga escala Qwen Max, com trilhões de parâmetros, que suporta entradas em diferentes idiomas, incluindo chinês e inglês, e é o modelo de API por trás da versão do produto Qwen 2.5."
  },
  "qwen-omni-turbo-latest": {
    "description": "A série de modelos Qwen-Omni suporta a entrada de dados em várias modalidades, incluindo vídeo, áudio, imagens e texto, e produz saídas em áudio e texto."
  },
  "qwen-plus": {
    "description": "Versão aprimorada do modelo de linguagem em larga escala Qwen, que suporta entradas em diferentes idiomas, como português e inglês."
  },
  "qwen-plus-latest": {
    "description": "A versão aprimorada do modelo de linguagem em larga escala Qwen Plus, que suporta entradas em diferentes idiomas, incluindo chinês e inglês."
  },
  "qwen-turbo": {
    "description": "O modelo de linguagem em larga escala Qwen suporta entradas em diferentes idiomas, como português e inglês."
  },
  "qwen-turbo-latest": {
    "description": "O modelo de linguagem em larga escala Qwen Turbo, que suporta entradas em diferentes idiomas, incluindo chinês e inglês."
  },
  "qwen-vl-chat-v1": {
    "description": "O Qwen VL suporta uma maneira de interação flexível, incluindo múltiplas imagens, perguntas e respostas em várias rodadas, e capacidades criativas."
  },
  "qwen-vl-max-latest": {
    "description": "Modelo de linguagem visual em escala ultra grande Qwen. Em comparação com a versão aprimorada, melhora ainda mais a capacidade de raciocínio visual e de seguir instruções, oferecendo um nível mais alto de percepção e cognição visual."
  },
  "qwen-vl-ocr-latest": {
    "description": "O OCR Qwen é um modelo especializado em extração de texto, focado na capacidade de extrair texto de imagens de documentos, tabelas, questões de exames, escrita manual, entre outros. Ele pode reconhecer vários idiomas, atualmente suportando: chinês, inglês, francês, japonês, coreano, alemão, russo, italiano, vietnamita e árabe."
  },
  "qwen-vl-plus-latest": {
    "description": "Versão aprimorada do modelo de linguagem visual em larga escala Qwen. Aumenta significativamente a capacidade de reconhecimento de detalhes e de texto, suportando resolução de mais de um milhão de pixels e imagens de qualquer proporção."
  },
  "qwen-vl-v1": {
    "description": "Inicializado com o modelo de linguagem Qwen-7B, adicionando um modelo de imagem, um modelo pré-treinado com resolução de entrada de imagem de 448."
  },
  "qwen/qwen-2-7b-instruct": {
    "description": "Qwen2 é uma nova série de modelos de linguagem grande Qwen. Qwen2 7B é um modelo baseado em transformer, com excelente desempenho em compreensão de linguagem, capacidade multilíngue, programação, matemática e raciocínio."
  },
  "qwen/qwen-2-7b-instruct:free": {
    "description": "Qwen2 é uma nova série de grandes modelos de linguagem, com capacidades de compreensão e geração mais robustas."
  },
  "qwen/qwen-2-vl-72b-instruct": {
    "description": "Qwen2-VL é a versão mais recente do modelo Qwen-VL, alcançando desempenho de ponta em benchmarks de compreensão visual, incluindo MathVista, DocVQA, RealWorldQA e MTVQA. Qwen2-VL é capaz de entender vídeos de mais de 20 minutos, permitindo perguntas e respostas, diálogos e criação de conteúdo de alta qualidade baseados em vídeo. Ele também possui capacidades complexas de raciocínio e tomada de decisão, podendo ser integrado a dispositivos móveis, robôs, etc., para operações automáticas baseadas em ambientes visuais e instruções textuais. Além do inglês e do chinês, o Qwen2-VL agora também suporta a compreensão de texto em diferentes idiomas em imagens, incluindo a maioria das línguas europeias, japonês, coreano, árabe e vietnamita."
  },
  "qwen/qwen-2.5-72b-instruct": {
    "description": "Qwen2.5-72B-Instruct é uma das mais recentes séries de modelos de linguagem grande lançadas pela Alibaba Cloud. Este modelo de 72B apresenta capacidades significativamente aprimoradas em áreas como codificação e matemática. O modelo também oferece suporte a múltiplas línguas, cobrindo mais de 29 idiomas, incluindo chinês e inglês. O modelo teve melhorias significativas em seguir instruções, entender dados estruturados e gerar saídas estruturadas (especialmente JSON)."
  },
  "qwen/qwen2.5-32b-instruct": {
    "description": "Qwen2.5-32B-Instruct é uma das mais recentes séries de modelos de linguagem grande lançadas pela Alibaba Cloud. Este modelo de 32B apresenta capacidades significativamente aprimoradas em áreas como codificação e matemática. O modelo oferece suporte a múltiplas línguas, cobrindo mais de 29 idiomas, incluindo chinês e inglês. O modelo teve melhorias significativas em seguir instruções, entender dados estruturados e gerar saídas estruturadas (especialmente JSON)."
  },
  "qwen/qwen2.5-7b-instruct": {
    "description": "LLM voltado para chinês e inglês, focado em linguagem, programação, matemática, raciocínio e outras áreas."
  },
  "qwen/qwen2.5-coder-32b-instruct": {
    "description": "LLM avançado, suporta geração de código, raciocínio e correção, abrangendo linguagens de programação populares."
  },
  "qwen/qwen2.5-coder-7b-instruct": {
    "description": "Modelo de código de médio porte poderoso, suporta comprimento de contexto de 32K, especializado em programação multilíngue."
  },
  "qwen2": {
    "description": "Qwen2 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwen2-72b-instruct": {
    "description": "Qwen2 é a nova série de modelos de linguagem grandes desenvolvida pela equipe Qwen. Baseia-se na arquitetura Transformer e utiliza funções de ativação SwiGLU, vieses de atenção QKV (attention QKV bias), atenção de consulta em grupo (group query attention), uma mistura de atenção de janela deslizante (mixture of sliding window attention) e atenção completa. Além disso, a equipe Qwen também aprimorou o tokenizador para adaptar-se a múltiplas línguas naturais e códigos."
  },
  "qwen2-7b-instruct": {
    "description": "Qwen2 é uma nova série de modelos de linguagem grandes desenvolvida pela equipe Qwen. Baseia-se na arquitetura Transformer e utiliza funções de ativação SwiGLU, viés de atenção QKV (attention QKV bias), atenção de consulta em grupo (group query attention), uma mistura de atenção de janela deslizante e atenção completa (mixture of sliding window attention and full attention). Além disso, a equipe Qwen também aprimorou o tokenizador para adaptar-se a várias línguas naturais e códigos."
  },
  "qwen2.5": {
    "description": "Qwen2.5 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwen2.5-14b-instruct": {
    "description": "Modelo de 14B parâmetros do Qwen 2.5, disponível como código aberto."
  },
  "qwen2.5-14b-instruct-1m": {
    "description": "Modelo de 72B de código aberto do Qwen2.5."
  },
  "qwen2.5-32b-instruct": {
    "description": "Modelo de 32B parâmetros do Qwen 2.5, disponível como código aberto."
  },
  "qwen2.5-72b-instruct": {
    "description": "Modelo de 72B parâmetros do Qwen 2.5, disponível como código aberto."
  },
  "qwen2.5-7b-instruct": {
    "description": "Modelo de 7B parâmetros do Qwen 2.5, disponível como código aberto."
  },
  "qwen2.5-coder-1.5b-instruct": {
    "description": "Versão open source do modelo de código do Qwen."
  },
  "qwen2.5-coder-32b-instruct": {
    "description": "Versão open source do modelo de código Qwen."
  },
  "qwen2.5-coder-7b-instruct": {
    "description": "Versão de código aberto do modelo de código Qwen."
  },
  "qwen2.5-coder-instruct": {
    "description": "Qwen2.5-Coder é o mais recente modelo de linguagem de grande escala especializado em código da série Qwen (anteriormente conhecido como CodeQwen)."
  },
  "qwen2.5-instruct": {
    "description": "Qwen2.5 é a mais recente série de modelos de linguagem de grande escala Qwen. Para o Qwen2.5, lançamos diversos modelos de linguagem base e modelos de linguagem ajustados por instrução, com parâmetros variando de 500 milhões a 7,2 bilhões."
  },
  "qwen2.5-math-1.5b-instruct": {
    "description": "O modelo Qwen-Math possui poderosas capacidades de resolução de problemas matemáticos."
  },
  "qwen2.5-math-72b-instruct": {
    "description": "O modelo Qwen-Math possui uma forte capacidade de resolução de problemas matemáticos."
  },
  "qwen2.5-math-7b-instruct": {
    "description": "O modelo Qwen-Math possui uma forte capacidade de resolução de problemas matemáticos."
  },
  "qwen2.5-vl-32b-instruct": {
    "description": "A série de modelos Qwen2.5-VL aprimorou o nível de inteligência, praticidade e aplicabilidade dos modelos, proporcionando um desempenho superior em cenários como conversação natural, criação de conteúdo, serviços de conhecimento especializado e desenvolvimento de código. A versão 32B utiliza técnicas de aprendizado por reforço para otimizar o modelo, oferecendo, em comparação com outros modelos da série Qwen2.5 VL, um estilo de saída mais alinhado com as preferências humanas, capacidade de raciocínio para problemas matemáticos complexos e compreensão detalhada e raciocínio sobre imagens."
  },
  "qwen2.5-vl-72b-instruct": {
    "description": "Aprimoramento geral em seguimento de instruções, matemática, resolução de problemas e código, com capacidade de reconhecimento de objetos aprimorada, suporte a formatos diversos para localização precisa de elementos visuais, compreensão de arquivos de vídeo longos (até 10 minutos) e localização de eventos em segundos, capaz de entender a sequência e a velocidade do tempo, suportando controle de agentes em OS ou Mobile com forte capacidade de extração de informações e saída em formato Json. Esta versão é a de 72B, a mais poderosa da série."
  },
  "qwen2.5-vl-7b-instruct": {
    "description": "Aprimoramento geral em seguimento de instruções, matemática, resolução de problemas e código, com capacidade de reconhecimento de objetos aprimorada, suporte a formatos diversos para localização precisa de elementos visuais, compreensão de arquivos de vídeo longos (até 10 minutos) e localização de eventos em segundos, capaz de entender a sequência e a velocidade do tempo, suportando controle de agentes em OS ou Mobile com forte capacidade de extração de informações e saída em formato Json. Esta versão é a de 72B, a mais poderosa da série."
  },
  "qwen2.5-vl-instruct": {
    "description": "Qwen2.5-VL é a versão mais recente do modelo de linguagem visual da família de modelos Qwen."
  },
  "qwen2.5:0.5b": {
    "description": "Qwen2.5 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwen2.5:1.5b": {
    "description": "Qwen2.5 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwen2.5:72b": {
    "description": "Qwen2.5 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwen2:0.5b": {
    "description": "Qwen2 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwen2:1.5b": {
    "description": "Qwen2 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwen2:72b": {
    "description": "Qwen2 é a nova geração de modelo de linguagem em larga escala da Alibaba, oferecendo desempenho excepcional para atender a diversas necessidades de aplicação."
  },
  "qwq": {
    "description": "QwQ é um modelo de pesquisa experimental, focado em melhorar a capacidade de raciocínio da IA."
  },
  "qwq-32b": {
    "description": "Modelo de inferência QwQ treinado com base no modelo Qwen2.5-32B, que melhorou significativamente a capacidade de inferência do modelo através de aprendizado por reforço. Os indicadores principais do modelo, como código matemático (AIME 24/25, LiveCodeBench) e alguns indicadores gerais (IFEval, LiveBench, etc.), alcançaram o nível do DeepSeek-R1 versão completa, com todos os indicadores superando significativamente o DeepSeek-R1-Distill-Qwen-32B, que também é baseado no Qwen2.5-32B."
  },
  "qwq-32b-preview": {
    "description": "O modelo QwQ é um modelo de pesquisa experimental desenvolvido pela equipe Qwen, focado em aprimorar a capacidade de raciocínio da IA."
  },
  "qwq-plus-latest": {
    "description": "Modelo de inferência QwQ treinado com base no modelo Qwen2.5, que melhorou significativamente a capacidade de inferência do modelo através de aprendizado por reforço. Os indicadores principais do modelo, como código matemático (AIME 24/25, LiveCodeBench) e alguns indicadores gerais (IFEval, LiveBench, etc.), alcançaram o nível do DeepSeek-R1 versão completa."
  },
  "r1-1776": {
    "description": "R1-1776 é uma versão do modelo DeepSeek R1, treinada posteriormente para fornecer informações factuais não filtradas e imparciais."
  },
  "solar-mini": {
    "description": "Solar Mini é um LLM compacto, com desempenho superior ao GPT-3.5, possuindo forte capacidade multilíngue, suportando inglês e coreano, oferecendo uma solução eficiente e compacta."
  },
  "solar-mini-ja": {
    "description": "Solar Mini (Ja) expande as capacidades do Solar Mini, focando no japonês, enquanto mantém eficiência e desempenho excepcional no uso de inglês e coreano."
  },
  "solar-pro": {
    "description": "Solar Pro é um LLM de alta inteligência lançado pela Upstage, focado na capacidade de seguir instruções em um único GPU, com pontuação IFEval acima de 80. Atualmente suporta inglês, com uma versão oficial planejada para lançamento em novembro de 2024, que expandirá o suporte a idiomas e comprimento de contexto."
  },
  "sonar": {
    "description": "Produto de busca leve baseado em contexto de busca, mais rápido e mais barato que o Sonar Pro."
  },
  "sonar-deep-research": {
    "description": "A Pesquisa Profunda realiza uma pesquisa abrangente de nível especialista e a sintetiza em relatórios acessíveis e acionáveis."
  },
  "sonar-pro": {
    "description": "Produto de busca avançada que suporta contexto de busca, consultas avançadas e acompanhamento."
  },
  "sonar-reasoning": {
    "description": "Novo produto API suportado pelo modelo de raciocínio da DeepSeek."
  },
  "sonar-reasoning-pro": {
    "description": "Um novo produto de API suportado pelo modelo de raciocínio DeepSeek."
  },
  "step-1-128k": {
    "description": "Equilibra desempenho e custo, adequado para cenários gerais."
  },
  "step-1-256k": {
    "description": "Possui capacidade de processamento de contexto ultra longo, especialmente adequado para análise de documentos longos."
  },
  "step-1-32k": {
    "description": "Suporta diálogos de comprimento médio, adequado para diversas aplicações."
  },
  "step-1-8k": {
    "description": "Modelo pequeno, adequado para tarefas leves."
  },
  "step-1-flash": {
    "description": "Modelo de alta velocidade, adequado para diálogos em tempo real."
  },
  "step-1.5v-mini": {
    "description": "Este modelo possui uma poderosa capacidade de compreensão de vídeo."
  },
  "step-1o-turbo-vision": {
    "description": "Este modelo possui uma poderosa capacidade de compreensão de imagens, superando o 1o em áreas de matemática e programação. O modelo é menor que o 1o e oferece uma velocidade de saída mais rápida."
  },
  "step-1o-vision-32k": {
    "description": "Este modelo possui uma poderosa capacidade de compreensão de imagens. Em comparação com a série de modelos step-1v, apresenta um desempenho visual superior."
  },
  "step-1v-32k": {
    "description": "Suporta entradas visuais, aprimorando a experiência de interação multimodal."
  },
  "step-1v-8k": {
    "description": "Modelo visual compacto, adequado para tarefas básicas de texto e imagem."
  },
  "step-2-16k": {
    "description": "Suporta interações de contexto em larga escala, adequado para cenários de diálogo complexos."
  },
  "step-2-mini": {
    "description": "Um modelo de grande escala de alta velocidade baseado na nova arquitetura de atenção auto-desenvolvida MFA, alcançando resultados semelhantes ao step1 com um custo muito baixo, enquanto mantém uma maior taxa de transferência e um tempo de resposta mais rápido. Capaz de lidar com tarefas gerais, possui especialização em habilidades de codificação."
  },
  "taichu_llm": {
    "description": "O modelo de linguagem Taichu possui uma forte capacidade de compreensão de linguagem, além de habilidades em criação de texto, perguntas e respostas, programação de código, cálculos matemáticos, raciocínio lógico, análise de sentimentos e resumo de texto. Inova ao combinar pré-treinamento com grandes dados e conhecimento rico de múltiplas fontes, aprimorando continuamente a tecnologia de algoritmos e absorvendo novos conhecimentos de vocabulário, estrutura, gramática e semântica de grandes volumes de dados textuais, proporcionando aos usuários informações e serviços mais convenientes e uma experiência mais inteligente."
  },
  "taichu_vl": {
    "description": "Integra capacidades de compreensão de imagens, transferência de conhecimento e atribuição lógica, destacando-se na área de perguntas e respostas baseadas em texto e imagem."
  },
  "text-embedding-3-large": {
    "description": "O modelo de vetorização mais poderoso, adequado para tarefas em inglês e não inglês."
  },
  "text-embedding-3-small": {
    "description": "Modelo de Embedding de nova geração, eficiente e econômico, adequado para recuperação de conhecimento, aplicações RAG e outros cenários."
  },
  "thudm/glm-4-9b-chat": {
    "description": "Versão de código aberto da última geração do modelo pré-treinado GLM-4, lançado pela Zhizhu AI."
  },
  "togethercomputer/StripedHyena-Nous-7B": {
    "description": "StripedHyena Nous (7B) oferece capacidade de computação aprimorada através de estratégias e arquiteturas de modelo eficientes."
  },
  "tts-1": {
    "description": "O mais recente modelo de texto para fala, otimizado para velocidade em cenários em tempo real."
  },
  "tts-1-hd": {
    "description": "O mais recente modelo de texto para fala, otimizado para qualidade."
  },
  "upstage/SOLAR-10.7B-Instruct-v1.0": {
    "description": "Upstage SOLAR Instruct v1 (11B) é adequado para tarefas de instrução refinadas, oferecendo excelente capacidade de processamento de linguagem."
  },
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet eleva o padrão da indústria, superando modelos concorrentes e Claude 3 Opus, apresentando um desempenho excepcional em uma ampla gama de avaliações, enquanto mantém a velocidade e o custo de nossos modelos de nível médio."
  },
  "us.anthropic.claude-3-7-sonnet-20250219-v1:0": {
    "description": "Claude 3.7 sonnet é o modelo de próxima geração mais rápido da Anthropic. Em comparação com o Claude 3 Haiku, o Claude 3.7 Sonnet apresenta melhorias em várias habilidades e supera o maior modelo da geração anterior, o Claude 3 Opus, em muitos testes de referência de inteligência."
  },
  "whisper-1": {
    "description": "Modelo de reconhecimento de voz universal, suporta reconhecimento de voz multilíngue, tradução de voz e identificação de idiomas."
  },
  "wizardlm2": {
    "description": "WizardLM 2 é um modelo de linguagem fornecido pela Microsoft AI, destacando-se em diálogos complexos, multilíngue, raciocínio e assistentes inteligentes."
  },
  "wizardlm2:8x22b": {
    "description": "WizardLM 2 é um modelo de linguagem fornecido pela Microsoft AI, destacando-se em diálogos complexos, multilíngue, raciocínio e assistentes inteligentes."
  },
  "yi-1.5-34b-chat": {
    "description": "Yi-1.5 é uma versão aprimorada do Yi. Ele usa um corpus de alta qualidade com 500B tokens para continuar o pré-treinamento do Yi e é refinado com 3M amostras de ajuste fino diversificadas."
  },
  "yi-large": {
    "description": "Modelo de nova geração com trilhões de parâmetros, oferecendo capacidades excepcionais de perguntas e respostas e geração de texto."
  },
  "yi-large-fc": {
    "description": "Baseado no modelo yi-large, suporta e aprimora a capacidade de chamadas de ferramentas, adequado para diversos cenários de negócios que exigem a construção de agentes ou fluxos de trabalho."
  },
  "yi-large-preview": {
    "description": "Versão inicial, recomenda-se o uso do yi-large (nova versão)."
  },
  "yi-large-rag": {
    "description": "Serviço de alto nível baseado no modelo yi-large, combinando técnicas de recuperação e geração para fornecer respostas precisas, com serviços de busca em tempo real na web."
  },
  "yi-large-turbo": {
    "description": "Excelente relação custo-benefício e desempenho excepcional. Ajuste de alta precisão baseado em desempenho, velocidade de raciocínio e custo."
  },
  "yi-lightning": {
    "description": "Modelo de alto desempenho mais recente, garantindo saída de alta qualidade enquanto a velocidade de raciocínio é significativamente aprimorada."
  },
  "yi-lightning-lite": {
    "description": "Versão leve, recomendada para uso com yi-lightning."
  },
  "yi-medium": {
    "description": "Modelo de tamanho médio com ajuste fino, equilibrando capacidades e custo. Otimização profunda da capacidade de seguir instruções."
  },
  "yi-medium-200k": {
    "description": "Janela de contexto ultra longa de 200K, oferecendo compreensão e geração de texto em profundidade."
  },
  "yi-spark": {
    "description": "Modelo leve e ágil. Oferece capacidades aprimoradas de cálculos matemáticos e escrita de código."
  },
  "yi-vision": {
    "description": "Modelo para tarefas visuais complexas, oferecendo alta performance em compreensão e análise de imagens."
  },
  "yi-vision-v2": {
    "description": "Modelo para tarefas visuais complexas, oferecendo alta performance em compreensão e análise baseadas em múltiplas imagens."
  }
}
