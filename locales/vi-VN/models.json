{
  "01-ai/yi-1.5-34b-chat": {
    "description": "Zero One Vạn Vật, mô hình tinh chỉnh mã nguồn mở mới nhất với 34 tỷ tham số, hỗ trợ nhiều tình huống đối thoại, dữ liệu đào tạo chất lượng cao, phù hợp với sở thích của con người."
  },
  "01-ai/yi-1.5-9b-chat": {
    "description": "Zero One Vạn Vật, mô hình tinh chỉnh mã nguồn mở mới nhất với 9 tỷ tham số, hỗ trợ nhiều tình huống đối thoại, dữ liệu đào tạo chất lượng cao, phù hợp với sở thích của con người."
  },
  "360/deepseek-r1": {
    "description": "【Phiên bản triển khai 360】DeepSeek-R1 đã sử dụng công nghệ học tăng cường quy mô lớn trong giai đoạn huấn luyện sau, nâng cao khả năng suy luận của mô hình một cách đáng kể với rất ít dữ liệu được gán nhãn. Hiệu suất trong các nhiệm vụ toán học, mã, suy luận ngôn ngữ tự nhiên tương đương với phiên bản chính thức OpenAI o1."
  },
  "360gpt-pro": {
    "description": "360GPT Pro là thành viên quan trọng trong dòng mô hình AI của 360, đáp ứng nhu cầu đa dạng của các ứng dụng ngôn ngữ tự nhiên với khả năng xử lý văn bản hiệu quả, hỗ trợ hiểu văn bản dài và đối thoại nhiều vòng."
  },
  "360gpt-pro-trans": {
    "description": "Mô hình chuyên dụng cho dịch thuật, được tối ưu hóa bằng cách tinh chỉnh sâu, mang lại hiệu quả dịch thuật hàng đầu."
  },
  "360gpt-turbo": {
    "description": "360GPT Turbo cung cấp khả năng tính toán và đối thoại mạnh mẽ, có khả năng hiểu ngữ nghĩa và hiệu suất tạo ra xuất sắc, là giải pháp trợ lý thông minh lý tưởng cho doanh nghiệp và nhà phát triển."
  },
  "360gpt-turbo-responsibility-8k": {
    "description": "360GPT Turbo Responsibility 8K nhấn mạnh an toàn ngữ nghĩa và định hướng trách nhiệm, được thiết kế đặc biệt cho các tình huống ứng dụng có yêu cầu cao về an toàn nội dung, đảm bảo độ chính xác và độ ổn định trong trải nghiệm người dùng."
  },
  "360gpt2-o1": {
    "description": "360gpt2-o1 sử dụng tìm kiếm cây để xây dựng chuỗi tư duy, và đưa vào cơ chế phản hồi, sử dụng học tăng cường để đào tạo, mô hình có khả năng tự phản hồi và sửa lỗi."
  },
  "360gpt2-pro": {
    "description": "360GPT2 Pro là mô hình xử lý ngôn ngữ tự nhiên cao cấp do công ty 360 phát hành, có khả năng tạo và hiểu văn bản xuất sắc, đặc biệt trong lĩnh vực tạo ra và sáng tạo, có thể xử lý các nhiệm vụ chuyển đổi ngôn ngữ phức tạp và diễn xuất vai trò."
  },
  "360zhinao2-o1": {
    "description": "360zhinao2-o1 sử dụng tìm kiếm cây để xây dựng chuỗi tư duy, và giới thiệu cơ chế phản hồi, sử dụng học tăng cường để đào tạo, mô hình có khả năng tự phản hồi và sửa lỗi."
  },
  "4.0Ultra": {
    "description": "Spark4.0 Ultra là phiên bản mạnh mẽ nhất trong dòng mô hình lớn Xinghuo, nâng cao khả năng hiểu và tóm tắt nội dung văn bản trong khi nâng cấp liên kết tìm kiếm trực tuyến. Đây là giải pháp toàn diện nhằm nâng cao năng suất văn phòng và đáp ứng chính xác nhu cầu, là sản phẩm thông minh dẫn đầu ngành."
  },
  "AnimeSharp": {
    "description": "AnimeSharp (còn gọi là “4x‑AnimeSharp”) là mô hình siêu phân giải mã nguồn mở do Kim2091 phát triển dựa trên kiến trúc ESRGAN, tập trung vào phóng to và làm sắc nét hình ảnh phong cách anime. Nó được đổi tên từ “4x-TextSharpV1” vào tháng 2 năm 2022, ban đầu cũng phù hợp với hình ảnh văn bản nhưng đã được tối ưu đáng kể cho nội dung anime."
  },
  "Baichuan2-Turbo": {
    "description": "Sử dụng công nghệ tăng cường tìm kiếm để kết nối toàn diện giữa mô hình lớn và kiến thức lĩnh vực, kiến thức toàn cầu. Hỗ trợ tải lên nhiều loại tài liệu như PDF, Word và nhập URL, thông tin được thu thập kịp thời và toàn diện, kết quả đầu ra chính xác và chuyên nghiệp."
  },
  "Baichuan3-Turbo": {
    "description": "Tối ưu hóa cho các tình huống doanh nghiệp thường xuyên, hiệu quả được cải thiện đáng kể, chi phí hiệu quả cao. So với mô hình Baichuan2, sáng tạo nội dung tăng 20%, trả lời câu hỏi kiến thức tăng 17%, khả năng đóng vai tăng 40%. Hiệu quả tổng thể tốt hơn GPT3.5."
  },
  "Baichuan3-Turbo-128k": {
    "description": "Có cửa sổ ngữ cảnh siêu dài 128K, tối ưu hóa cho các tình huống doanh nghiệp thường xuyên, hiệu quả được cải thiện đáng kể, chi phí hiệu quả cao. So với mô hình Baichuan2, sáng tạo nội dung tăng 20%, trả lời câu hỏi kiến thức tăng 17%, khả năng đóng vai tăng 40%. Hiệu quả tổng thể tốt hơn GPT3.5."
  },
  "Baichuan4": {
    "description": "Mô hình có khả năng hàng đầu trong nước, vượt trội hơn các mô hình chính thống nước ngoài trong các nhiệm vụ tiếng Trung như bách khoa toàn thư, văn bản dài, sáng tạo nội dung. Cũng có khả năng đa phương tiện hàng đầu trong ngành, thể hiện xuất sắc trong nhiều tiêu chuẩn đánh giá uy tín."
  },
  "Baichuan4-Air": {
    "description": "Mô hình có khả năng hàng đầu trong nước, vượt trội hơn các mô hình chính thống nước ngoài trong các nhiệm vụ tiếng Trung như bách khoa toàn thư, văn bản dài và sáng tạo nội dung. Cũng có khả năng đa phương tiện hàng đầu trong ngành, thể hiện xuất sắc trong nhiều tiêu chuẩn đánh giá uy tín."
  },
  "Baichuan4-Turbo": {
    "description": "Mô hình có khả năng hàng đầu trong nước, vượt trội hơn các mô hình chính thống nước ngoài trong các nhiệm vụ tiếng Trung như bách khoa toàn thư, văn bản dài và sáng tạo nội dung. Cũng có khả năng đa phương tiện hàng đầu trong ngành, thể hiện xuất sắc trong nhiều tiêu chuẩn đánh giá uy tín."
  },
  "ByteDance-Seed/Seed-OSS-36B-Instruct": {
    "description": "Seed-OSS là một loạt các mô hình ngôn ngữ lớn mã nguồn mở do nhóm Seed của ByteDance phát triển, được thiết kế đặc biệt cho khả năng xử lý ngữ cảnh dài mạnh mẽ, suy luận, tác nhân (agent) và năng lực tổng quát. Trong loạt này, Seed-OSS-36B-Instruct là một mô hình tinh chỉnh chỉ thị với 36 tỷ tham số, hỗ trợ ngữ cảnh siêu dài nguyên bản, cho phép xử lý một lượng lớn tài liệu hoặc kho mã phức tạp trong một lần. Mô hình được tối ưu đặc biệt cho các tác vụ suy luận, tạo mã và tác nhân (như sử dụng công cụ), đồng thời duy trì năng lực tổng quát cân bằng và xuất sắc. Một điểm nổi bật của mô hình này là tính năng “Ngân sách suy nghĩ” (Thinking Budget), cho phép người dùng điều chỉnh linh hoạt độ dài suy luận theo nhu cầu, từ đó nâng cao hiệu quả suy luận trong ứng dụng thực tế."
  },
  "DeepSeek-R1": {
    "description": "Mô hình LLM hiệu quả tiên tiến nhất, xuất sắc trong suy luận, toán học và lập trình."
  },
  "DeepSeek-R1-Distill-Llama-70B": {
    "description": "DeepSeek R1 - mô hình lớn hơn và thông minh hơn trong bộ công cụ DeepSeek - đã được chưng cất vào kiến trúc Llama 70B. Dựa trên các bài kiểm tra và đánh giá của con người, mô hình này thông minh hơn so với Llama 70B gốc, đặc biệt thể hiện xuất sắc trong các nhiệm vụ yêu cầu độ chính xác về toán học và sự thật."
  },
  "DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Mô hình chưng cất DeepSeek-R1 dựa trên Qwen2.5-Math-1.5B, tối ưu hóa hiệu suất suy luận thông qua học tăng cường và dữ liệu khởi động lạnh, mô hình mã nguồn mở làm mới tiêu chuẩn đa nhiệm."
  },
  "DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Mô hình chưng cất DeepSeek-R1 dựa trên Qwen2.5-14B, tối ưu hóa hiệu suất suy luận thông qua học tăng cường và dữ liệu khởi động lạnh, mô hình mã nguồn mở làm mới tiêu chuẩn đa nhiệm."
  },
  "DeepSeek-R1-Distill-Qwen-32B": {
    "description": "Dòng DeepSeek-R1 tối ưu hóa hiệu suất suy luận thông qua học tăng cường và dữ liệu khởi động lạnh, mô hình mã nguồn mở làm mới tiêu chuẩn đa nhiệm, vượt qua mức OpenAI-o1-mini."
  },
  "DeepSeek-R1-Distill-Qwen-7B": {
    "description": "Mô hình chưng cất DeepSeek-R1 dựa trên Qwen2.5-Math-7B, tối ưu hóa hiệu suất suy luận thông qua học tăng cường và dữ liệu khởi động lạnh, mô hình mã nguồn mở làm mới tiêu chuẩn đa nhiệm."
  },
  "DeepSeek-V3": {
    "description": "DeepSeek-V3 là một mô hình MoE do công ty DeepSeek tự phát triển. Nhiều kết quả đánh giá của DeepSeek-V3 đã vượt qua các mô hình mã nguồn mở khác như Qwen2.5-72B và Llama-3.1-405B, và về hiệu suất không thua kém các mô hình đóng nguồn hàng đầu thế giới như GPT-4o và Claude-3.5-Sonnet."
  },
  "DeepSeek-V3-1": {
    "description": "DeepSeek V3.1: Mô hình suy luận thế hệ tiếp theo, nâng cao khả năng suy luận phức tạp và tư duy chuỗi, phù hợp cho các nhiệm vụ cần phân tích sâu."
  },
  "DeepSeek-V3-Fast": {
    "description": "Nhà cung cấp mô hình: nền tảng sophnet. DeepSeek V3 Fast là phiên bản tốc độ cao TPS của DeepSeek V3 0324, không lượng tử hóa, có khả năng mã hóa và toán học mạnh mẽ hơn, phản hồi nhanh hơn!"
  },
  "DeepSeek-V3.1": {
    "description": "DeepSeek-V3.1 - chế độ không suy nghĩ; DeepSeek-V3.1 là mô hình suy luận lai mới của DeepSeek, hỗ trợ hai chế độ suy luận là suy nghĩ và không suy nghĩ, hiệu quả suy nghĩ cao hơn so với DeepSeek-R1-0528. Qua tối ưu hậu huấn luyện, hiệu suất sử dụng công cụ Agent và các tác vụ tác nhân được cải thiện đáng kể."
  },
  "DeepSeek-V3.1-Fast": {
    "description": "DeepSeek V3.1 Fast là phiên bản tốc độ cao TPS của DeepSeek V3.1. Chế độ suy nghĩ lai: thông qua thay đổi mẫu trò chuyện, một mô hình có thể đồng thời hỗ trợ cả chế độ suy nghĩ và không suy nghĩ. Gọi công cụ thông minh hơn: nhờ tối ưu hậu huấn luyện, mô hình thể hiện rõ rệt sự cải thiện trong việc sử dụng công cụ và các tác vụ đại lý."
  },
  "DeepSeek-V3.1-Think": {
    "description": "DeepSeek-V3.1 - chế độ suy nghĩ; DeepSeek-V3.1 là mô hình suy luận lai mới của DeepSeek, hỗ trợ hai chế độ suy luận là suy nghĩ và không suy nghĩ, hiệu quả suy nghĩ cao hơn so với DeepSeek-R1-0528. Qua tối ưu hậu huấn luyện, hiệu suất sử dụng công cụ Agent và các tác vụ tác nhân được cải thiện đáng kể."
  },
  "DeepSeek-V3.2-Exp": {
    "description": "DeepSeek V3.2 là mô hình lớn chung mới nhất của DeepSeek, hỗ trợ kiến trúc suy luận hỗn hợp và có khả năng Agent mạnh mẽ hơn."
  },
  "DeepSeek-V3.2-Exp-Think": {
    "description": "Chế độ suy nghĩ của DeepSeek V3.2. Trước khi đưa ra câu trả lời cuối cùng, mô hình sẽ xuất ra một chuỗi suy nghĩ nhằm nâng cao độ chính xác của câu trả lời."
  },
  "Doubao-lite-128k": {
    "description": "Doubao-lite sở hữu tốc độ phản hồi tối ưu, hiệu quả chi phí tốt hơn, cung cấp lựa chọn linh hoạt hơn cho các kịch bản khác nhau của khách hàng. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 128k."
  },
  "Doubao-lite-32k": {
    "description": "Doubao-lite sở hữu tốc độ phản hồi tối ưu, hiệu quả chi phí tốt hơn, cung cấp lựa chọn linh hoạt hơn cho các kịch bản khác nhau của khách hàng. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 32k."
  },
  "Doubao-lite-4k": {
    "description": "Doubao-lite sở hữu tốc độ phản hồi tối ưu, hiệu quả chi phí tốt hơn, cung cấp lựa chọn linh hoạt hơn cho các kịch bản khác nhau của khách hàng. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 4k."
  },
  "Doubao-pro-128k": {
    "description": "Mô hình chủ lực với hiệu quả tốt nhất, phù hợp xử lý các nhiệm vụ phức tạp, có hiệu quả xuất sắc trong các kịch bản như hỏi đáp tham khảo, tóm tắt, sáng tạo, phân loại văn bản, nhập vai. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 128k."
  },
  "Doubao-pro-32k": {
    "description": "Mô hình chủ lực với hiệu quả tốt nhất, phù hợp xử lý các nhiệm vụ phức tạp, có hiệu quả xuất sắc trong các kịch bản như hỏi đáp tham khảo, tóm tắt, sáng tạo, phân loại văn bản, nhập vai. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 32k."
  },
  "Doubao-pro-4k": {
    "description": "Mô hình chủ lực với hiệu quả tốt nhất, phù hợp xử lý các nhiệm vụ phức tạp, có hiệu quả xuất sắc trong các kịch bản như hỏi đáp tham khảo, tóm tắt, sáng tạo, phân loại văn bản, nhập vai. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 4k."
  },
  "DreamO": {
    "description": "DreamO là mô hình tạo hình ảnh tùy chỉnh mã nguồn mở do ByteDance và Đại học Bắc Kinh hợp tác phát triển, nhằm hỗ trợ tạo hình ảnh đa nhiệm thông qua kiến trúc thống nhất. Nó sử dụng phương pháp mô hình hóa kết hợp hiệu quả, có thể tạo ra hình ảnh nhất quán và tùy chỉnh cao dựa trên các điều kiện như danh tính, chủ thể, phong cách, nền do người dùng chỉ định."
  },
  "ERNIE-3.5-128K": {
    "description": "Mô hình ngôn ngữ quy mô lớn hàng đầu do Baidu tự phát triển, bao phủ một lượng lớn tài liệu tiếng Trung và tiếng Anh, có khả năng tổng quát mạnh mẽ, có thể đáp ứng hầu hết các yêu cầu về đối thoại, hỏi đáp, sáng tạo nội dung và các tình huống ứng dụng plugin; hỗ trợ tự động kết nối với plugin tìm kiếm của Baidu, đảm bảo thông tin hỏi đáp luôn được cập nhật kịp thời."
  },
  "ERNIE-3.5-8K": {
    "description": "Mô hình ngôn ngữ quy mô lớn hàng đầu do Baidu tự phát triển, bao phủ một lượng lớn tài liệu tiếng Trung và tiếng Anh, có khả năng tổng quát mạnh mẽ, có thể đáp ứng hầu hết các yêu cầu về đối thoại, hỏi đáp, sáng tạo nội dung và các tình huống ứng dụng plugin; hỗ trợ tự động kết nối với plugin tìm kiếm của Baidu, đảm bảo thông tin hỏi đáp luôn được cập nhật kịp thời."
  },
  "ERNIE-3.5-8K-Preview": {
    "description": "Mô hình ngôn ngữ quy mô lớn hàng đầu do Baidu tự phát triển, bao phủ một lượng lớn tài liệu tiếng Trung và tiếng Anh, có khả năng tổng quát mạnh mẽ, có thể đáp ứng hầu hết các yêu cầu về đối thoại, hỏi đáp, sáng tạo nội dung và các tình huống ứng dụng plugin; hỗ trợ tự động kết nối với plugin tìm kiếm của Baidu, đảm bảo thông tin hỏi đáp luôn được cập nhật kịp thời."
  },
  "ERNIE-4.0-8K-Latest": {
    "description": "Mô hình ngôn ngữ quy mô siêu lớn hàng đầu do Baidu tự phát triển, so với ERNIE 3.5 đã nâng cấp toàn diện khả năng của mô hình, phù hợp rộng rãi với các nhiệm vụ phức tạp trong nhiều lĩnh vực; hỗ trợ tự động kết nối với plugin tìm kiếm Baidu, đảm bảo thông tin hỏi đáp luôn cập nhật."
  },
  "ERNIE-4.0-8K-Preview": {
    "description": "Mô hình ngôn ngữ quy mô siêu lớn hàng đầu do Baidu tự phát triển, so với ERNIE 3.5 đã nâng cấp toàn diện khả năng của mô hình, phù hợp rộng rãi với các nhiệm vụ phức tạp trong nhiều lĩnh vực; hỗ trợ tự động kết nối với plugin tìm kiếm Baidu, đảm bảo thông tin hỏi đáp luôn cập nhật."
  },
  "ERNIE-4.0-Turbo-8K-Latest": {
    "description": "Mô hình ngôn ngữ quy mô siêu lớn tự phát triển của Baidu, có hiệu suất tổng thể xuất sắc, phù hợp rộng rãi cho các tình huống tác vụ phức tạp trong nhiều lĩnh vực; hỗ trợ tự động kết nối với plugin tìm kiếm của Baidu, đảm bảo tính kịp thời của thông tin câu hỏi đáp. So với ERNIE 4.0, nó có hiệu suất tốt hơn."
  },
  "ERNIE-4.0-Turbo-8K-Preview": {
    "description": "Mô hình ngôn ngữ quy mô siêu lớn hàng đầu do Baidu tự phát triển, có hiệu suất tổng thể xuất sắc, phù hợp rộng rãi với các nhiệm vụ phức tạp trong nhiều lĩnh vực; hỗ trợ tự động kết nối với plugin tìm kiếm Baidu, đảm bảo thông tin hỏi đáp luôn cập nhật. So với ERNIE 4.0, hiệu suất tốt hơn."
  },
  "ERNIE-Character-8K": {
    "description": "Mô hình ngôn ngữ quy mô lớn cho các tình huống chuyên biệt do Baidu tự phát triển, phù hợp cho các ứng dụng như NPC trong game, đối thoại dịch vụ khách hàng, và vai trò trong đối thoại, phong cách nhân vật rõ ràng và nhất quán hơn, khả năng tuân thủ chỉ dẫn mạnh mẽ, hiệu suất suy diễn tốt hơn."
  },
  "ERNIE-Lite-Pro-128K": {
    "description": "Mô hình ngôn ngữ quy mô lớn nhẹ do Baidu tự phát triển, kết hợp hiệu suất mô hình xuất sắc với khả năng suy diễn, hiệu quả tốt hơn ERNIE Lite, phù hợp cho việc suy diễn trên thẻ tăng tốc AI có công suất thấp."
  },
  "ERNIE-Speed-128K": {
    "description": "Mô hình ngôn ngữ quy mô lớn hiệu suất cao do Baidu phát hành vào năm 2024, có khả năng tổng quát xuất sắc, phù hợp làm mô hình nền để tinh chỉnh, xử lý tốt hơn các vấn đề trong các tình huống cụ thể, đồng thời có khả năng suy diễn tuyệt vời."
  },
  "ERNIE-Speed-Pro-128K": {
    "description": "Mô hình ngôn ngữ quy mô lớn hiệu suất cao do Baidu phát hành vào năm 2024, có khả năng tổng quát xuất sắc, hiệu quả tốt hơn ERNIE Speed, phù hợp làm mô hình nền để tinh chỉnh, xử lý tốt hơn các vấn đề trong các tình huống cụ thể, đồng thời có khả năng suy diễn tuyệt vời."
  },
  "FLUX-1.1-pro": {
    "description": "FLUX.1.1 Pro"
  },
  "FLUX.1-Kontext-dev": {
    "description": "FLUX.1-Kontext-dev là mô hình tạo và chỉnh sửa hình ảnh đa phương thức dựa trên kiến trúc Rectified Flow Transformer do Black Forest Labs phát triển, với quy mô 12 tỷ tham số, tập trung vào việc tạo, tái cấu trúc, nâng cao hoặc chỉnh sửa hình ảnh dựa trên điều kiện ngữ cảnh cho trước. Mô hình kết hợp ưu điểm tạo có kiểm soát của mô hình khuếch tán và khả năng mô hình hóa ngữ cảnh của Transformer, hỗ trợ xuất hình ảnh chất lượng cao, ứng dụng rộng rãi trong sửa chữa hình ảnh, hoàn thiện hình ảnh, tái cấu trúc cảnh quan trực quan."
  },
  "FLUX.1-Kontext-pro": {
    "description": "FLUX.1 Kontext [pro]"
  },
  "FLUX.1-dev": {
    "description": "FLUX.1-dev là mô hình ngôn ngữ đa phương thức mã nguồn mở do Black Forest Labs phát triển, tối ưu cho các tác vụ kết hợp hình ảnh và văn bản. Nó tích hợp khả năng hiểu và tạo hình ảnh cùng văn bản, xây dựng trên nền tảng các mô hình ngôn ngữ lớn tiên tiến như Mistral-7B, thông qua bộ mã hóa thị giác thiết kế tinh vi và điều chỉnh chỉ dẫn đa giai đoạn, đạt được khả năng xử lý phối hợp hình ảnh-văn bản và suy luận tác vụ phức tạp."
  },
  "Gryphe/MythoMax-L2-13b": {
    "description": "MythoMax-L2 (13B) là một mô hình sáng tạo, phù hợp cho nhiều lĩnh vực ứng dụng và nhiệm vụ phức tạp."
  },
  "HelloMeme": {
    "description": "HelloMeme là công cụ AI có thể tự động tạo meme, ảnh động hoặc video ngắn dựa trên hình ảnh hoặc hành động bạn cung cấp. Bạn không cần có kỹ năng vẽ hay lập trình, chỉ cần chuẩn bị hình ảnh tham khảo, nó sẽ giúp bạn tạo ra nội dung đẹp mắt, thú vị và đồng nhất về phong cách."
  },
  "HiDream-I1-Full": {
    "description": "HiDream-E1-Full là mô hình chỉnh sửa hình ảnh đa phương thức mã nguồn mở do HiDream.ai phát triển, dựa trên kiến trúc Diffusion Transformer tiên tiến và kết hợp khả năng hiểu ngôn ngữ mạnh mẽ (tích hợp LLaMA 3.1-8B-Instruct). Mô hình hỗ trợ tạo hình ảnh, chuyển đổi phong cách, chỉnh sửa cục bộ và vẽ lại nội dung qua chỉ dẫn ngôn ngữ tự nhiên, có khả năng hiểu và thực thi tốt giữa hình ảnh và văn bản."
  },
  "HunyuanDiT-v1.2-Diffusers-Distilled": {
    "description": "hunyuandit-v1.2-distilled là mô hình tạo hình ảnh từ văn bản nhẹ, được tối ưu qua kỹ thuật chưng cất, có khả năng tạo hình ảnh chất lượng cao nhanh chóng, đặc biệt phù hợp với môi trường tài nguyên thấp và các tác vụ tạo hình ảnh thời gian thực."
  },
  "InstantCharacter": {
    "description": "InstantCharacter là mô hình tạo nhân vật cá nhân hóa không cần tinh chỉnh do đội AI Tencent phát hành năm 2025, nhằm đạt được tạo nhân vật nhất quán, độ trung thực cao và đa cảnh. Mô hình hỗ trợ xây dựng nhân vật chỉ dựa trên một hình ảnh tham khảo và có thể linh hoạt chuyển nhân vật đó sang nhiều phong cách, hành động và nền khác nhau."
  },
  "InternVL2-8B": {
    "description": "InternVL2-8B là một mô hình ngôn ngữ hình ảnh mạnh mẽ, hỗ trợ xử lý đa phương tiện giữa hình ảnh và văn bản, có khả năng nhận diện chính xác nội dung hình ảnh và tạo ra mô tả hoặc câu trả lời liên quan."
  },
  "InternVL2.5-26B": {
    "description": "InternVL2.5-26B là một mô hình ngôn ngữ hình ảnh mạnh mẽ, hỗ trợ xử lý đa phương tiện giữa hình ảnh và văn bản, có khả năng nhận diện chính xác nội dung hình ảnh và tạo ra mô tả hoặc câu trả lời liên quan."
  },
  "Kolors": {
    "description": "Kolors là mô hình tạo hình ảnh từ văn bản do nhóm Kolors của Kuaishou phát triển. Được huấn luyện trên hàng tỷ tham số, nổi bật về chất lượng hình ảnh, hiểu ngữ nghĩa tiếng Trung và khả năng hiển thị văn bản."
  },
  "Kwai-Kolors/Kolors": {
    "description": "Kolors là mô hình tạo hình ảnh từ văn bản quy mô lớn dựa trên khuếch tán tiềm ẩn do nhóm Kolors của Kuaishou phát triển. Mô hình được huấn luyện trên hàng tỷ cặp văn bản-hình ảnh, thể hiện ưu thế rõ rệt về chất lượng hình ảnh, độ chính xác ngữ nghĩa phức tạp và khả năng hiển thị ký tự tiếng Trung và tiếng Anh. Nó hỗ trợ đầu vào tiếng Trung và tiếng Anh, đồng thời thể hiện xuất sắc trong việc hiểu và tạo nội dung đặc thù tiếng Trung."
  },
  "Kwaipilot/KAT-Dev": {
    "description": "KAT-Dev (32B) là một mô hình mã nguồn mở với 32 tỷ tham số, được thiết kế đặc biệt cho các tác vụ kỹ thuật phần mềm. Trong bài kiểm tra chuẩn SWE-Bench Verified, mô hình đạt tỷ lệ giải quyết 62,4%, xếp hạng thứ năm trong số các mô hình mã nguồn mở có quy mô khác nhau. Mô hình này được tối ưu hóa qua nhiều giai đoạn, bao gồm huấn luyện trung gian, tinh chỉnh có giám sát (SFT) và học tăng cường (RL), nhằm cung cấp hỗ trợ mạnh mẽ cho các nhiệm vụ lập trình phức tạp như hoàn thành mã, sửa lỗi, đánh giá mã và nhiều hơn nữa."
  },
  "Llama-3.2-11B-Vision-Instruct": {
    "description": "Khả năng suy luận hình ảnh xuất sắc trên hình ảnh độ phân giải cao, phù hợp cho các ứng dụng hiểu biết thị giác."
  },
  "Llama-3.2-90B-Vision-Instruct\t": {
    "description": "Khả năng suy luận hình ảnh cao cấp cho các ứng dụng đại lý hiểu biết thị giác."
  },
  "Meta-Llama-3-3-70B-Instruct": {
    "description": "Llama 3.3 70B: Mô hình Transformer đa năng, thích hợp cho các nhiệm vụ đối thoại và tạo nội dung."
  },
  "Meta-Llama-3.1-405B-Instruct": {
    "description": "Mô hình văn bản được tinh chỉnh theo chỉ dẫn Llama 3.1, được tối ưu hóa cho các trường hợp sử dụng đối thoại đa ngôn ngữ, thể hiện xuất sắc trong nhiều mô hình trò chuyện mã nguồn mở và đóng có sẵn trên nhiều tiêu chuẩn ngành."
  },
  "Meta-Llama-3.1-70B-Instruct": {
    "description": "Mô hình văn bản được tinh chỉnh theo chỉ dẫn Llama 3.1, được tối ưu hóa cho các trường hợp sử dụng đối thoại đa ngôn ngữ, thể hiện xuất sắc trong nhiều mô hình trò chuyện mã nguồn mở và đóng có sẵn trên nhiều tiêu chuẩn ngành."
  },
  "Meta-Llama-3.1-8B-Instruct": {
    "description": "Mô hình văn bản được tinh chỉnh theo chỉ dẫn Llama 3.1, được tối ưu hóa cho các trường hợp sử dụng đối thoại đa ngôn ngữ, thể hiện xuất sắc trong nhiều mô hình trò chuyện mã nguồn mở và đóng có sẵn trên nhiều tiêu chuẩn ngành."
  },
  "Meta-Llama-3.2-1B-Instruct": {
    "description": "Mô hình ngôn ngữ nhỏ tiên tiến nhất, có khả năng hiểu ngôn ngữ, khả năng suy luận xuất sắc và khả năng sinh văn bản."
  },
  "Meta-Llama-3.2-3B-Instruct": {
    "description": "Mô hình ngôn ngữ nhỏ tiên tiến nhất, có khả năng hiểu ngôn ngữ, khả năng suy luận xuất sắc và khả năng sinh văn bản."
  },
  "Meta-Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 là mô hình ngôn ngữ lớn mã nguồn mở đa ngôn ngữ tiên tiến nhất trong dòng Llama, mang đến trải nghiệm hiệu suất tương đương mô hình 405B với chi phí cực thấp. Dựa trên cấu trúc Transformer, và được cải thiện tính hữu ích và an toàn thông qua tinh chỉnh giám sát (SFT) và học tăng cường từ phản hồi của con người (RLHF). Phiên bản tinh chỉnh theo chỉ dẫn của nó được tối ưu hóa cho các cuộc đối thoại đa ngôn ngữ, thể hiện tốt hơn nhiều mô hình trò chuyện mã nguồn mở và đóng trong nhiều tiêu chuẩn ngành. Ngày cắt đứt kiến thức là tháng 12 năm 2023."
  },
  "Meta-Llama-4-Maverick-17B-128E-Instruct-FP8": {
    "description": "Llama 4 Maverick: Mô hình quy mô lớn dựa trên Mixture-of-Experts, cung cấp chiến lược kích hoạt chuyên gia hiệu quả để đạt hiệu suất xuất sắc trong suy luận."
  },
  "MiniMax-M1": {
    "description": "Mô hình suy luận tự phát triển hoàn toàn mới. Dẫn đầu toàn cầu: 80K chuỗi tư duy x 1M đầu vào, hiệu quả sánh ngang với các mô hình hàng đầu quốc tế"
  },
  "MiniMax-M2": {
    "description": "Được thiết kế đặc biệt cho lập trình hiệu quả và quy trình làm việc của Agent"
  },
  "MiniMax-Text-01": {
    "description": "Trong dòng mô hình MiniMax-01, chúng tôi đã thực hiện những đổi mới táo bạo: lần đầu tiên hiện thực hóa quy mô lớn cơ chế chú ý tuyến tính, kiến trúc Transformer truyền thống không còn là lựa chọn duy nhất. Mô hình này có số lượng tham số lên tới 4560 tỷ, trong đó kích hoạt một lần là 45,9 tỷ. Hiệu suất tổng hợp của mô hình tương đương với các mô hình hàng đầu quốc tế, đồng thời có khả năng xử lý hiệu quả ngữ cảnh dài nhất toàn cầu lên tới 4 triệu token, gấp 32 lần GPT-4o và 20 lần Claude-3.5-Sonnet."
  },
  "MiniMaxAI/MiniMax-M1-80k": {
    "description": "MiniMax-M1 là mô hình suy luận chú ý hỗn hợp quy mô lớn với trọng số mã nguồn mở, sở hữu 456 tỷ 600 triệu tham số, mỗi Token có thể kích hoạt khoảng 45,9 tỷ tham số. Mô hình hỗ trợ ngữ cảnh siêu dài lên đến 1 triệu Token một cách nguyên bản, và thông qua cơ chế chú ý chớp nhoáng, trong các tác vụ sinh 100.000 Token tiết kiệm 75% lượng phép tính dấu chấm động so với DeepSeek R1. Đồng thời, MiniMax-M1 áp dụng kiến trúc MoE (chuyên gia hỗn hợp), kết hợp thuật toán CISPO và thiết kế chú ý hỗn hợp trong huấn luyện tăng cường hiệu quả, đạt hiệu suất hàng đầu trong ngành khi suy luận đầu vào dài và các kịch bản kỹ thuật phần mềm thực tế."
  },
  "MiniMaxAI/MiniMax-M2": {
    "description": "MiniMax-M2 tái định nghĩa hiệu suất cho các tác nhân AI. Đây là một mô hình MoE nhỏ gọn, nhanh chóng và tiết kiệm chi phí, với tổng số 230 tỷ tham số và 10 tỷ tham số kích hoạt, được thiết kế để đạt hiệu năng hàng đầu trong các tác vụ mã hóa và tác nhân, đồng thời duy trì trí tuệ nhân tạo tổng quát mạnh mẽ. Chỉ với 10 tỷ tham số kích hoạt, MiniMax-M2 có thể mang lại hiệu suất tương đương với các mô hình quy mô lớn, khiến nó trở thành lựa chọn lý tưởng cho các ứng dụng hiệu suất cao."
  },
  "Moonshot-Kimi-K2-Instruct": {
    "description": "Tổng tham số 1T, tham số kích hoạt 32B. Trong các mô hình không suy nghĩ, đạt trình độ hàng đầu về kiến thức tiên tiến, toán học và lập trình, đặc biệt phù hợp với các tác vụ đại lý chung. Được tối ưu kỹ lưỡng cho tác vụ đại lý, không chỉ trả lời câu hỏi mà còn có thể thực hiện hành động. Phù hợp nhất cho trò chuyện ứng biến, trải nghiệm đại lý chung, là mô hình phản xạ không cần suy nghĩ lâu."
  },
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) là mô hình chỉ dẫn chính xác cao, phù hợp cho tính toán phức tạp."
  },
  "OmniConsistency": {
    "description": "OmniConsistency nâng cao tính nhất quán phong cách và khả năng tổng quát hóa trong các tác vụ hình ảnh sang hình ảnh (Image-to-Image) bằng cách giới thiệu các Diffusion Transformers (DiTs) quy mô lớn và dữ liệu phong cách ghép đôi, tránh suy giảm phong cách."
  },
  "Phi-3-medium-128k-instruct": {
    "description": "Mô hình Phi-3-medium giống nhau, nhưng với kích thước ngữ cảnh lớn hơn cho RAG hoặc gợi ý ít."
  },
  "Phi-3-medium-4k-instruct": {
    "description": "Mô hình 14B tham số, chứng minh chất lượng tốt hơn Phi-3-mini, tập trung vào dữ liệu dày đặc lý luận chất lượng cao."
  },
  "Phi-3-mini-128k-instruct": {
    "description": "Mô hình Phi-3-mini giống nhau, nhưng với kích thước ngữ cảnh lớn hơn cho RAG hoặc gợi ý ít."
  },
  "Phi-3-mini-4k-instruct": {
    "description": "Thành viên nhỏ nhất của gia đình Phi-3. Tối ưu hóa cho cả chất lượng và độ trễ thấp."
  },
  "Phi-3-small-128k-instruct": {
    "description": "Mô hình Phi-3-small giống nhau, nhưng với kích thước ngữ cảnh lớn hơn cho RAG hoặc gợi ý ít."
  },
  "Phi-3-small-8k-instruct": {
    "description": "Mô hình 7B tham số, chứng minh chất lượng tốt hơn Phi-3-mini, tập trung vào dữ liệu dày đặc lý luận chất lượng cao."
  },
  "Phi-3.5-mini-instruct": {
    "description": "Phi-3-mini là phiên bản cập nhật của mô hình."
  },
  "Phi-3.5-vision-instrust": {
    "description": "Phi-3-vision là phiên bản cập nhật của mô hình."
  },
  "Pro/Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-7B-Instruct là mô hình ngôn ngữ lớn được tinh chỉnh theo chỉ dẫn trong loạt Qwen2, với quy mô tham số là 7B. Mô hình này dựa trên kiến trúc Transformer, sử dụng hàm kích hoạt SwiGLU, độ lệch QKV trong chú ý và chú ý theo nhóm. Nó có khả năng xử lý đầu vào quy mô lớn. Mô hình thể hiện xuất sắc trong nhiều bài kiểm tra chuẩn về hiểu ngôn ngữ, sinh ngôn ngữ, khả năng đa ngôn ngữ, mã hóa, toán học và suy luận, vượt qua hầu hết các mô hình mã nguồn mở và thể hiện sức cạnh tranh tương đương với các mô hình độc quyền trong một số nhiệm vụ. Qwen2-7B-Instruct đã thể hiện sự cải thiện đáng kể về hiệu suất trong nhiều bài kiểm tra so với Qwen1.5-7B-Chat."
  },
  "Pro/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct là một trong những mô hình ngôn ngữ lớn mới nhất do Alibaba Cloud phát hành. Mô hình 7B này có khả năng cải thiện đáng kể trong các lĩnh vực mã hóa và toán học. Mô hình cũng cung cấp hỗ trợ đa ngôn ngữ, bao gồm hơn 29 ngôn ngữ, bao gồm tiếng Trung, tiếng Anh, v.v. Mô hình đã có sự cải thiện đáng kể trong việc tuân theo chỉ dẫn, hiểu dữ liệu có cấu trúc và tạo ra đầu ra có cấu trúc (đặc biệt là JSON)."
  },
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct là phiên bản mới nhất trong loạt mô hình ngôn ngữ lớn chuyên biệt cho mã do Alibaba Cloud phát hành. Mô hình này được cải thiện đáng kể khả năng tạo mã, suy luận và sửa chữa thông qua việc đào tạo trên 5.5 triệu tỷ tokens, không chỉ nâng cao khả năng lập trình mà còn duy trì lợi thế về khả năng toán học và tổng quát. Mô hình cung cấp nền tảng toàn diện hơn cho các ứng dụng thực tế như tác nhân mã."
  },
  "Pro/Qwen/Qwen2.5-VL-7B-Instruct": {
    "description": "Qwen2.5-VL là thành viên mới của series Qwen, sở hữu khả năng hiểu thị giác mạnh mẽ, có thể phân tích văn bản, biểu đồ và bố cục trong hình ảnh, cũng như hiểu video dài và bắt các sự kiện, có thể suy luận, thao tác công cụ, hỗ trợ định vị vật thể đa định dạng và tạo ra đầu ra có cấu trúc, tối ưu hóa việc huấn luyện độ phân giải và tốc độ khung hình động cho việc hiểu video, đồng thời cải thiện hiệu suất của bộ mã hóa thị giác."
  },
  "Pro/THUDM/GLM-4.1V-9B-Thinking": {
    "description": "GLM-4.1V-9B-Thinking là một mô hình ngôn ngữ thị giác (VLM) mã nguồn mở được phát hành chung bởi Zhipu AI và Phòng thí nghiệm KEG của Đại học Thanh Hoa, được thiết kế đặc biệt để xử lý các nhiệm vụ nhận thức đa phương thức phức tạp. Mô hình này dựa trên mô hình cơ sở GLM-4-9B-0414, thông qua việc giới thiệu cơ chế suy luận “Chuỗi tư duy” (Chain-of-Thought) và áp dụng chiến lược học tăng cường, đã nâng cao đáng kể khả năng suy luận đa phương thức và tính ổn định của nó."
  },
  "Pro/THUDM/glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat là phiên bản mã nguồn mở trong loạt mô hình tiền huấn luyện GLM-4 do Zhizhu AI phát hành. Mô hình này thể hiện xuất sắc trong nhiều lĩnh vực như ngữ nghĩa, toán học, suy luận, mã và kiến thức. Ngoài việc hỗ trợ đối thoại nhiều vòng, GLM-4-9B-Chat còn có các tính năng nâng cao như duyệt web, thực thi mã, gọi công cụ tùy chỉnh (Function Call) và suy luận văn bản dài. Mô hình hỗ trợ 26 ngôn ngữ, bao gồm tiếng Trung, tiếng Anh, tiếng Nhật, tiếng Hàn và tiếng Đức. Trong nhiều bài kiểm tra chuẩn, GLM-4-9B-Chat đã thể hiện hiệu suất xuất sắc, như AlignBench-v2, MT-Bench, MMLU và C-Eval. Mô hình hỗ trợ độ dài ngữ cảnh tối đa 128K, phù hợp cho nghiên cứu học thuật và ứng dụng thương mại."
  },
  "Pro/deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 là một mô hình suy diễn được điều khiển bởi học tăng cường (RL), giải quyết các vấn đề về tính lặp lại và khả năng đọc trong mô hình. Trước khi áp dụng RL, DeepSeek-R1 đã giới thiệu dữ liệu khởi động lạnh, tối ưu hóa thêm hiệu suất suy diễn. Nó thể hiện hiệu suất tương đương với OpenAI-o1 trong các nhiệm vụ toán học, mã và suy diễn, và thông qua phương pháp đào tạo được thiết kế cẩn thận, nâng cao hiệu quả tổng thể."
  },
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B là mô hình được tạo ra từ Qwen2.5-Math-7B thông qua quá trình chưng cất kiến thức. Mô hình này được tinh chỉnh bằng 800.000 mẫu được chọn lọc từ DeepSeek-R1, thể hiện khả năng suy luận xuất sắc. Nó đã đạt được hiệu suất tốt trong nhiều bài kiểm tra chuẩn, trong đó có độ chính xác 92,8% trên MATH-500, tỷ lệ vượt qua 55,5% trên AIME 2024, và điểm số 1189 trên CodeForces, thể hiện khả năng toán học và lập trình mạnh mẽ cho một mô hình có quy mô 7B."
  },
  "Pro/deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 là một mô hình ngôn ngữ hỗn hợp chuyên gia (MoE) với 6710 tỷ tham số, sử dụng chú ý tiềm ẩn đa đầu (MLA) và kiến trúc DeepSeekMoE, kết hợp chiến lược cân bằng tải không có tổn thất phụ trợ, tối ưu hóa hiệu suất suy diễn và đào tạo. Thông qua việc được tiền huấn luyện trên 14.8 triệu tỷ token chất lượng cao, và thực hiện tinh chỉnh giám sát và học tăng cường, DeepSeek-V3 vượt trội hơn các mô hình mã nguồn mở khác, gần với các mô hình đóng kín hàng đầu."
  },
  "Pro/deepseek-ai/DeepSeek-V3.1-Terminus": {
    "description": "DeepSeek-V3.1-Terminus là phiên bản cập nhật của mô hình V3.1 do DeepSeek phát hành, được định vị là mô hình ngôn ngữ lớn với trí tuệ hỗn hợp. Bản cập nhật này tập trung sửa các vấn đề phản hồi từ người dùng và nâng cao độ ổn định trong khi vẫn giữ nguyên khả năng của mô hình. Nó cải thiện đáng kể tính nhất quán ngôn ngữ, giảm thiểu việc sử dụng lẫn lộn tiếng Trung và tiếng Anh cũng như các ký tự bất thường. Mô hình tích hợp \"Chế độ suy nghĩ\" (Thinking Mode) và \"Chế độ không suy nghĩ\" (Non-thinking Mode), người dùng có thể linh hoạt chuyển đổi qua các mẫu trò chuyện để phù hợp với các nhiệm vụ khác nhau. Một tối ưu quan trọng là V3.1-Terminus tăng cường hiệu suất của Agent mã (Code Agent) và Agent tìm kiếm (Search Agent), giúp chúng đáng tin cậy hơn trong việc gọi công cụ và thực hiện các nhiệm vụ phức tạp nhiều bước."
  },
  "Pro/deepseek-ai/DeepSeek-V3.2-Exp": {
    "description": "DeepSeek-V3.2-Exp là phiên bản thử nghiệm V3.2 do DeepSeek phát hành, đóng vai trò là bước chuyển tiếp trong hành trình hướng tới kiến trúc thế hệ tiếp theo. Dựa trên nền tảng của V3.1-Terminus, phiên bản này tích hợp cơ chế Chú ý Thưa (DeepSeek Sparse Attention - DSA) nhằm nâng cao hiệu quả huấn luyện và suy luận trong ngữ cảnh dài. Nó được tối ưu hóa đặc biệt cho việc gọi công cụ, hiểu tài liệu dài và suy luận nhiều bước. V3.2-Exp là cầu nối giữa nghiên cứu và ứng dụng thực tế, phù hợp với người dùng mong muốn khám phá hiệu suất suy luận cao hơn trong các tình huống có ngân sách ngữ cảnh lớn."
  },
  "Pro/moonshotai/Kimi-K2-Instruct-0905": {
    "description": "Kimi K2-Instruct-0905 là phiên bản mới nhất và mạnh mẽ nhất của Kimi K2. Đây là một mô hình ngôn ngữ chuyên gia hỗn hợp (MoE) hàng đầu với tổng số tham số lên đến 1 nghìn tỷ và 32 tỷ tham số kích hoạt. Các đặc điểm chính của mô hình bao gồm: tăng cường trí tuệ mã hóa tác nhân, thể hiện sự cải thiện đáng kể trong các bài kiểm tra chuẩn công khai và các nhiệm vụ mã hóa tác nhân trong thế giới thực; cải tiến trải nghiệm mã hóa giao diện người dùng, nâng cao cả về tính thẩm mỹ và tính thực tiễn trong lập trình giao diện."
  },
  "QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview là một mô hình xử lý ngôn ngữ tự nhiên độc đáo, có khả năng xử lý hiệu quả các nhiệm vụ tạo đối thoại phức tạp và hiểu ngữ cảnh."
  },
  "Qwen/QVQ-72B-Preview": {
    "description": "QVQ-72B-Preview là một mô hình nghiên cứu do đội ngũ Qwen phát triển, tập trung vào khả năng suy diễn hình ảnh, có lợi thế độc đáo trong việc hiểu các cảnh phức tạp và giải quyết các vấn đề toán học liên quan đến hình ảnh."
  },
  "Qwen/QwQ-32B": {
    "description": "QwQ là mô hình suy diễn của dòng Qwen. So với các mô hình tinh chỉnh theo chỉ dẫn truyền thống, QwQ có khả năng tư duy và suy diễn, có thể đạt được hiệu suất được cải thiện đáng kể trong các nhiệm vụ hạ nguồn, đặc biệt là trong việc giải quyết các vấn đề khó khăn. QwQ-32B là mô hình suy diễn trung bình, có thể đạt được hiệu suất cạnh tranh khi so sánh với các mô hình suy diễn tiên tiến nhất (như DeepSeek-R1, o1-mini). Mô hình này sử dụng các công nghệ như RoPE, SwiGLU, RMSNorm và Attention QKV bias, có cấu trúc mạng 64 lớp và 40 đầu chú ý Q (trong kiến trúc GQA, KV là 8)."
  },
  "Qwen/QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview là mô hình nghiên cứu thử nghiệm mới nhất của Qwen, tập trung vào việc nâng cao khả năng suy luận của AI. Thông qua việc khám phá các cơ chế phức tạp như trộn ngôn ngữ và suy luận đệ quy, những lợi thế chính bao gồm khả năng phân tích suy luận mạnh mẽ, khả năng toán học và lập trình. Tuy nhiên, cũng có những vấn đề về chuyển đổi ngôn ngữ, vòng lặp suy luận, các vấn đề an toàn và sự khác biệt về các khả năng khác."
  },
  "Qwen/Qwen-Image": {
    "description": "Qwen-Image là mô hình nền tạo ảnh do đội ngũ Tongyi Qianwen của Alibaba phát triển, với 20 tỷ tham số. Mô hình này đạt được những tiến bộ đáng kể trong việc hiển thị văn bản phức tạp và chỉnh sửa hình ảnh chính xác, đặc biệt xuất sắc trong việc tạo ra hình ảnh chứa văn bản tiếng Trung và tiếng Anh với độ trung thực cao. Qwen-Image không chỉ xử lý tốt bố cục nhiều dòng và văn bản cấp đoạn, mà còn duy trì sự nhất quán trong bố cục và hài hòa về ngữ cảnh khi tạo ảnh. Bên cạnh khả năng hiển thị văn bản vượt trội, mô hình còn hỗ trợ nhiều phong cách nghệ thuật, từ ảnh hiện thực đến thẩm mỹ anime, linh hoạt đáp ứng các nhu cầu sáng tạo khác nhau. Đồng thời, nó cũng sở hữu khả năng chỉnh sửa và hiểu hình ảnh mạnh mẽ, hỗ trợ các thao tác nâng cao như chuyển đổi phong cách, thêm hoặc xóa đối tượng, tăng cường chi tiết, chỉnh sửa văn bản và điều khiển tư thế cơ thể người, hướng tới việc trở thành một mô hình nền thông minh toàn diện cho sáng tạo và xử lý hình ảnh tích hợp ngôn ngữ, bố cục và thị giác."
  },
  "Qwen/Qwen-Image-Edit-2509": {
    "description": "Qwen-Image-Edit-2509 là phiên bản chỉnh sửa hình ảnh mới nhất của Qwen-Image, được phát hành bởi đội ngũ Tongyi Qianwen của Alibaba. Mô hình này được huấn luyện chuyên sâu dựa trên Qwen-Image với 20 tỷ tham số, mở rộng thành công khả năng hiển thị văn bản độc đáo sang lĩnh vực chỉnh sửa hình ảnh, cho phép chỉnh sửa chính xác văn bản trong ảnh. Ngoài ra, Qwen-Image-Edit áp dụng kiến trúc sáng tạo, đưa hình ảnh đầu vào đồng thời vào Qwen2.5-VL (để kiểm soát ngữ nghĩa thị giác) và VAE Encoder (để kiểm soát diện mạo thị giác), từ đó đạt được khả năng chỉnh sửa kép về ngữ nghĩa và diện mạo. Điều này có nghĩa là mô hình không chỉ hỗ trợ chỉnh sửa cục bộ như thêm, xóa hoặc thay đổi các yếu tố, mà còn hỗ trợ chỉnh sửa ngữ nghĩa thị giác nâng cao như sáng tạo IP, chuyển đổi phong cách mà vẫn giữ được tính nhất quán về ngữ nghĩa. Mô hình đã thể hiện hiệu suất hàng đầu (SOTA) trên nhiều bộ đánh giá công khai, trở thành một mô hình nền chỉnh sửa hình ảnh mạnh mẽ."
  },
  "Qwen/Qwen2-72B-Instruct": {
    "description": "Qwen2 là mô hình ngôn ngữ tổng quát tiên tiến, hỗ trợ nhiều loại chỉ dẫn."
  },
  "Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-72B-Instruct là mô hình ngôn ngữ lớn được tinh chỉnh theo chỉ dẫn trong loạt Qwen2, với quy mô tham số là 72B. Mô hình này dựa trên kiến trúc Transformer, sử dụng hàm kích hoạt SwiGLU, độ lệch QKV trong chú ý và chú ý theo nhóm. Nó có khả năng xử lý đầu vào quy mô lớn. Mô hình thể hiện xuất sắc trong nhiều bài kiểm tra chuẩn về hiểu ngôn ngữ, sinh ngôn ngữ, khả năng đa ngôn ngữ, mã hóa, toán học và suy luận, vượt qua hầu hết các mô hình mã nguồn mở và thể hiện sức cạnh tranh tương đương với các mô hình độc quyền trong một số nhiệm vụ."
  },
  "Qwen/Qwen2-VL-72B-Instruct": {
    "description": "Qwen2-VL là phiên bản mới nhất của mô hình Qwen-VL, đạt được hiệu suất hàng đầu trong các thử nghiệm chuẩn hiểu biết hình ảnh."
  },
  "Qwen/Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5 là một loạt mô hình ngôn ngữ lớn hoàn toàn mới, nhằm tối ưu hóa việc xử lý các nhiệm vụ theo hướng dẫn."
  },
  "Qwen/Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5 là một loạt mô hình ngôn ngữ lớn hoàn toàn mới, nhằm tối ưu hóa việc xử lý các nhiệm vụ theo hướng dẫn."
  },
  "Qwen/Qwen2.5-72B-Instruct": {
    "description": "Mô hình ngôn ngữ lớn được phát triển bởi đội ngũ Qianwen của Alibaba Cloud"
  },
  "Qwen/Qwen2.5-72B-Instruct-128K": {
    "description": "Qwen2.5 là một loạt mô hình ngôn ngữ lớn hoàn toàn mới, sở hữu khả năng hiểu và tạo ra mạnh mẽ hơn."
  },
  "Qwen/Qwen2.5-72B-Instruct-Turbo": {
    "description": "Qwen2.5 là một loạt mô hình ngôn ngữ lớn hoàn toàn mới, được thiết kế để tối ưu hóa việc xử lý các tác vụ chỉ dẫn."
  },
  "Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5 là một loạt mô hình ngôn ngữ lớn hoàn toàn mới, nhằm tối ưu hóa việc xử lý các nhiệm vụ theo hướng dẫn."
  },
  "Qwen/Qwen2.5-7B-Instruct-Turbo": {
    "description": "Qwen2.5 là một loạt mô hình ngôn ngữ lớn hoàn toàn mới, được thiết kế để tối ưu hóa việc xử lý các tác vụ chỉ dẫn."
  },
  "Qwen/Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder tập trung vào việc viết mã."
  },
  "Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct là phiên bản mới nhất trong loạt mô hình ngôn ngữ lớn chuyên biệt cho mã do Alibaba Cloud phát hành. Mô hình này được cải thiện đáng kể khả năng tạo mã, suy luận và sửa chữa thông qua việc đào tạo trên 5.5 triệu tỷ tokens, không chỉ nâng cao khả năng lập trình mà còn duy trì lợi thế về khả năng toán học và tổng quát. Mô hình cung cấp nền tảng toàn diện hơn cho các ứng dụng thực tế như tác nhân mã."
  },
  "Qwen/Qwen2.5-VL-32B-Instruct": {
    "description": "Qwen2.5-VL-32B-Instruct là mô hình đa phương thức do đội ngũ Qwen2.5-VL phát triển, là một phần của loạt Qwen2.5-VL. Mô hình này không chỉ giỏi nhận diện các vật thể thông thường, mà còn có thể phân tích văn bản, biểu đồ, biểu tượng, hình vẽ và bố cục trong hình ảnh. Nó có thể hoạt động như một đại lý thị giác, có khả năng suy luận và điều khiển công cụ một cách động, bao gồm cả việc sử dụng máy tính và điện thoại. Ngoài ra, mô hình này có thể xác định chính xác vị trí của các đối tượng trong hình ảnh và tạo ra đầu ra có cấu trúc cho hóa đơn, bảng biểu, v.v. So với mô hình tiền nhiệm Qwen2-VL, phiên bản này đã được cải thiện đáng kể về khả năng giải toán và giải quyết vấn đề thông qua học tăng cường, và phong cách phản hồi cũng phù hợp hơn với sở thích của con người."
  },
  "Qwen/Qwen2.5-VL-72B-Instruct": {
    "description": "Qwen2.5-VL là mô hình ngôn ngữ thị giác trong loạt Qwen2.5. Mô hình này có những cải tiến đáng kể: có khả năng hiểu thị giác mạnh hơn, có thể nhận diện các vật thể thông thường, phân tích văn bản, biểu đồ và bố cục; hoạt động như một đại lý thị giác có thể suy luận và hướng dẫn sử dụng công cụ một cách động; hỗ trợ hiểu các video dài hơn 1 giờ và bắt các sự kiện quan trọng; có thể định vị chính xác các vật thể trong hình ảnh thông qua việc tạo khung giới hạn hoặc điểm; hỗ trợ tạo ra đầu ra có cấu trúc, đặc biệt phù hợp với dữ liệu quét như hóa đơn, bảng biểu."
  },
  "Qwen/Qwen3-14B": {
    "description": "Qwen3 là một mô hình lớn thế hệ mới của Tongyi Qianwen với khả năng nâng cao đáng kể, đạt được trình độ hàng đầu trong nhiều khả năng cốt lõi như suy luận, tổng quát, đại lý và đa ngôn ngữ, đồng thời hỗ trợ chuyển đổi chế độ suy nghĩ."
  },
  "Qwen/Qwen3-235B-A22B": {
    "description": "Qwen3 là một mô hình lớn thế hệ mới của Tongyi Qianwen với khả năng nâng cao đáng kể, đạt được trình độ hàng đầu trong nhiều khả năng cốt lõi như suy luận, tổng quát, đại lý và đa ngôn ngữ, đồng thời hỗ trợ chuyển đổi chế độ suy nghĩ."
  },
  "Qwen/Qwen3-235B-A22B-Instruct-2507": {
    "description": "Qwen3-235B-A22B-Instruct-2507 là mô hình ngôn ngữ lớn chuyên gia hỗn hợp (MoE) hàng đầu trong dòng Qwen3 do đội ngũ Aliyun Tongyi Qianwen phát triển. Mô hình có tổng 235 tỷ tham số, mỗi lần suy luận kích hoạt 22 tỷ tham số. Đây là phiên bản cập nhật của Qwen3-235B-A22B không ở chế độ suy nghĩ, tập trung cải thiện đáng kể khả năng tuân thủ chỉ dẫn, suy luận logic, hiểu văn bản, toán học, khoa học, lập trình và sử dụng công cụ. Ngoài ra, mô hình tăng cường bao phủ kiến thức đa ngôn ngữ và điều chỉnh tốt hơn sở thích người dùng trong các tác vụ chủ quan và mở, tạo ra văn bản hữu ích và chất lượng cao hơn."
  },
  "Qwen/Qwen3-235B-A22B-Thinking-2507": {
    "description": "Qwen3-235B-A22B-Thinking-2507 là thành viên trong dòng mô hình ngôn ngữ lớn Qwen3 do đội ngũ Alibaba Tongyi Qianwen phát triển, tập trung vào các tác vụ suy luận phức tạp và khó khăn. Mô hình dựa trên kiến trúc chuyên gia hỗn hợp (MoE), tổng tham số 235 tỷ, mỗi token kích hoạt khoảng 22 tỷ tham số, giúp tăng hiệu quả tính toán trong khi duy trì hiệu suất mạnh mẽ. Là mô hình “suy nghĩ” chuyên biệt, nó cải thiện đáng kể khả năng suy luận logic, toán học, khoa học, lập trình và các bài kiểm tra học thuật, đạt trình độ hàng đầu trong các mô hình suy nghĩ mã nguồn mở. Mô hình cũng tăng cường khả năng chung như tuân thủ chỉ dẫn, sử dụng công cụ và tạo văn bản, hỗ trợ ngữ cảnh dài 256K token, rất phù hợp cho các kịch bản cần suy luận sâu và xử lý tài liệu dài."
  },
  "Qwen/Qwen3-30B-A3B": {
    "description": "Qwen3 là một mô hình lớn thế hệ mới của Tongyi Qianwen với khả năng nâng cao đáng kể, đạt được trình độ hàng đầu trong nhiều khả năng cốt lõi như suy luận, tổng quát, đại lý và đa ngôn ngữ, đồng thời hỗ trợ chuyển đổi chế độ suy nghĩ."
  },
  "Qwen/Qwen3-30B-A3B-Instruct-2507": {
    "description": "Qwen3-30B-A3B-Instruct-2507 là phiên bản cập nhật của Qwen3-30B-A3B ở chế độ không suy nghĩ. Đây là một mô hình chuyên gia hỗn hợp (MoE) với tổng cộng 30,5 tỷ tham số và 3,3 tỷ tham số kích hoạt. Mô hình này đã được cải tiến quan trọng ở nhiều khía cạnh, bao gồm nâng cao đáng kể khả năng tuân thủ chỉ dẫn, suy luận logic, hiểu văn bản, toán học, khoa học, lập trình và sử dụng công cụ. Đồng thời, nó đạt được tiến bộ thực chất trong việc bao phủ kiến thức đa ngôn ngữ và có khả năng điều chỉnh tốt hơn với sở thích của người dùng trong các nhiệm vụ chủ quan và mở, từ đó tạo ra các phản hồi hữu ích hơn và văn bản chất lượng cao hơn. Ngoài ra, khả năng hiểu văn bản dài của mô hình cũng được nâng lên đến 256K. Mô hình này chỉ hỗ trợ chế độ không suy nghĩ và không tạo ra thẻ `<think></think>` trong đầu ra."
  },
  "Qwen/Qwen3-30B-A3B-Thinking-2507": {
    "description": "Qwen3-30B-A3B-Thinking-2507 là mô hình \"suy nghĩ\" mới nhất trong dòng Qwen3, được phát hành bởi nhóm Tongyi Qianwen của Alibaba. Là một mô hình chuyên gia hỗn hợp (MoE) với tổng cộng 305亿 (30,5 tỷ) tham số và 33亿 (3,3 tỷ) tham số kích hoạt, mô hình tập trung vào nâng cao khả năng xử lý các nhiệm vụ phức tạp. Mô hình này thể hiện hiệu năng cải thiện rõ rệt trên các chuẩn đánh giá học thuật về suy luận logic, toán học, khoa học, lập trình và những bài toán đòi hỏi chuyên môn của con người. Đồng thời, các năng lực chung như tuân thủ hướng dẫn, sử dụng công cụ, sinh văn bản và căn chỉnh theo sở thích con người cũng được tăng cường đáng kể. Mô hình hỗ trợ nguyên sinh khả năng hiểu ngữ cảnh dài 256K và có thể mở rộng lên tới 1 triệu token. Phiên bản này được thiết kế dành cho \"chế độ suy nghĩ\", nhằm giải quyết các nhiệm vụ có độ phức tạp cao thông qua quá trình suy luận từng bước chi tiết, đồng thời năng lực tác nhân (Agent) của nó cũng thể hiện xuất sắc."
  },
  "Qwen/Qwen3-32B": {
    "description": "Qwen3 là một mô hình lớn thế hệ mới của Tongyi Qianwen với khả năng nâng cao đáng kể, đạt được trình độ hàng đầu trong nhiều khả năng cốt lõi như suy luận, tổng quát, đại lý và đa ngôn ngữ, đồng thời hỗ trợ chuyển đổi chế độ suy nghĩ."
  },
  "Qwen/Qwen3-8B": {
    "description": "Qwen3 là một mô hình lớn thế hệ mới của Tongyi Qianwen với khả năng nâng cao đáng kể, đạt được trình độ hàng đầu trong nhiều khả năng cốt lõi như suy luận, tổng quát, đại lý và đa ngôn ngữ, đồng thời hỗ trợ chuyển đổi chế độ suy nghĩ."
  },
  "Qwen/Qwen3-Coder-30B-A3B-Instruct": {
    "description": "Qwen3-Coder-30B-A3B-Instruct là một mô hình mã trong dòng Qwen3 được phát triển bởi đội ngũ Tongyi Qianwen của Alibaba. Là một mô hình được tinh giản và tối ưu hóa, nó tập trung nâng cao khả năng xử lý mã nguồn trong khi vẫn duy trì hiệu năng và hiệu suất cao. Mô hình này thể hiện ưu thế hiệu năng nổi bật so với các mô hình mã nguồn mở trong các tác vụ phức tạp như lập trình tác nhân (Agentic Coding), tự động hóa thao tác trình duyệt và gọi công cụ. Nó hỗ trợ ngữ cảnh dài 256K token một cách nguyên bản và có thể mở rộng tới 1M token, giúp hiểu và xử lý ở mức độ toàn bộ kho mã tốt hơn. Ngoài ra, mô hình còn cung cấp hỗ trợ lập trình tác nhân mạnh mẽ cho các nền tảng như Qwen Code, CLINE và được thiết kế với định dạng gọi hàm chuyên biệt."
  },
  "Qwen/Qwen3-Coder-480B-A35B-Instruct": {
    "description": "Qwen3-Coder-480B-A35B-Instruct là mô hình mã do Alibaba phát hành, được đánh giá là có khả năng tác nhân (agentic) mạnh mẽ nhất tính đến nay. Đây là một mô hình chuyên gia hỗn hợp (Mixture of Experts, MoE) với tổng cộng 480 tỷ tham số và 35 tỷ tham số kích hoạt, cân bằng giữa hiệu suất và hiệu quả. Mô hình này hỗ trợ ngữ cảnh gốc dài 256K (khoảng 260 nghìn) token và có thể được mở rộng tới 1 triệu token thông qua các phương pháp ngoại suy như YaRN, giúp nó xử lý các kho mã quy mô lớn và các nhiệm vụ lập trình phức tạp. Qwen3-Coder được thiết kế cho quy trình làm việc lập trình theo mô hình tác nhân, không chỉ sinh mã mà còn có khả năng tương tác tự chủ với các công cụ và môi trường phát triển để giải quyết những vấn đề lập trình phức tạp. Trong nhiều bộ đánh giá chuẩn về mã nguồn và nhiệm vụ tác nhân, mô hình này đạt thứ hạng dẫn đầu trong các mô hình mã nguồn mở, với hiệu năng có thể sánh ngang các mô hình hàng đầu như Claude Sonnet 4."
  },
  "Qwen/Qwen3-Next-80B-A3B-Instruct": {
    "description": "Qwen3-Next-80B-A3B-Instruct là mô hình nền tảng thế hệ tiếp theo do đội ngũ Alibaba Tongyi Qianwen phát hành. Nó dựa trên kiến trúc Qwen3-Next hoàn toàn mới, nhằm đạt được hiệu quả tối ưu trong huấn luyện và suy luận. Mô hình này áp dụng cơ chế chú ý hỗn hợp sáng tạo (Gated DeltaNet và Gated Attention), cấu trúc chuyên gia hỗn hợp có độ thưa cao (MoE) cùng nhiều tối ưu hóa về độ ổn định trong huấn luyện. Là một mô hình thưa với tổng số 80 tỷ tham số, nó chỉ kích hoạt khoảng 3 tỷ tham số trong quá trình suy luận, giúp giảm đáng kể chi phí tính toán và khi xử lý các tác vụ ngữ cảnh dài trên 32K token, thông lượng suy luận cao hơn mô hình Qwen3-32B hơn 10 lần. Mô hình này là phiên bản tinh chỉnh theo chỉ dẫn, thiết kế cho các tác vụ chung và không hỗ trợ chế độ Chuỗi suy nghĩ (Thinking). Về hiệu năng, nó tương đương với mô hình chủ lực Qwen3-235B của Tongyi Qianwen trong một số bài kiểm tra chuẩn, đặc biệt thể hiện ưu thế rõ rệt trong các tác vụ ngữ cảnh siêu dài."
  },
  "Qwen/Qwen3-Next-80B-A3B-Thinking": {
    "description": "Qwen3-Next-80B-A3B-Thinking là mô hình nền tảng thế hệ tiếp theo do đội ngũ Alibaba Tongyi Qianwen phát hành, được thiết kế chuyên biệt cho các tác vụ suy luận phức tạp. Nó dựa trên kiến trúc sáng tạo Qwen3-Next, kết hợp cơ chế chú ý hỗn hợp (Gated DeltaNet và Gated Attention) và cấu trúc chuyên gia hỗn hợp có độ thưa cao (MoE), nhằm đạt hiệu quả tối ưu trong huấn luyện và suy luận. Là mô hình thưa với tổng số 80 tỷ tham số, nó chỉ kích hoạt khoảng 3 tỷ tham số trong quá trình suy luận, giảm đáng kể chi phí tính toán, và khi xử lý các tác vụ ngữ cảnh dài trên 32K token, thông lượng cao hơn mô hình Qwen3-32B hơn 10 lần. Phiên bản “Thinking” này được tối ưu để thực hiện các tác vụ đa bước khó như chứng minh toán học, tổng hợp mã, phân tích logic và lập kế hoạch, và mặc định xuất ra quá trình suy luận dưới dạng chuỗi suy nghĩ có cấu trúc. Về hiệu năng, nó không chỉ vượt trội so với các mô hình có chi phí cao hơn như Qwen3-32B-Thinking mà còn vượt qua Gemini-2.5-Flash-Thinking trong nhiều bài kiểm tra chuẩn."
  },
  "Qwen/Qwen3-Omni-30B-A3B-Captioner": {
    "description": "Qwen3-Omni-30B-A3B-Captioner là một mô hình ngôn ngữ thị giác (VLM) thuộc dòng Qwen3 do nhóm Tongyi Qianwen của Alibaba phát triển. Mô hình này chuyên dùng để tạo ra các mô tả hình ảnh chất lượng cao, chi tiết và chính xác. Dựa trên kiến trúc chuyên gia hỗn hợp (MoE) với tổng cộng 30 tỷ tham số, nó có khả năng hiểu sâu nội dung hình ảnh và chuyển đổi thành mô tả ngôn ngữ tự nhiên mượt mà. Mô hình thể hiện xuất sắc trong việc nắm bắt chi tiết hình ảnh, hiểu cảnh vật, nhận diện đối tượng và suy luận mối quan hệ, đặc biệt phù hợp với các ứng dụng yêu cầu hiểu và mô tả hình ảnh chính xác."
  },
  "Qwen/Qwen3-Omni-30B-A3B-Instruct": {
    "description": "Qwen3-Omni-30B-A3B-Instruct là một thành viên trong dòng Qwen3 mới nhất do nhóm Tongyi Qianwen của Alibaba phát triển. Đây là mô hình chuyên gia hỗn hợp (MoE) với tổng cộng 30 tỷ tham số và 3 tỷ tham số kích hoạt, giúp duy trì hiệu suất mạnh mẽ trong khi giảm chi phí suy luận. Mô hình được huấn luyện trên dữ liệu chất lượng cao, đa nguồn và đa ngôn ngữ, sở hữu năng lực tổng quát vượt trội, hỗ trợ xử lý đầu vào toàn bộ các dạng thức như văn bản, hình ảnh, âm thanh và video, có khả năng hiểu và tạo nội dung xuyên mô thức."
  },
  "Qwen/Qwen3-Omni-30B-A3B-Thinking": {
    "description": "Qwen3-Omni-30B-A3B-Thinking là thành phần \"người suy nghĩ\" (Thinker) cốt lõi trong mô hình toàn mô thức Qwen3-Omni. Nó chuyên xử lý các đầu vào đa mô thức bao gồm văn bản, âm thanh, hình ảnh và video, thực hiện suy luận chuỗi tư duy phức tạp. Là bộ não của quá trình suy luận, mô hình này thống nhất tất cả đầu vào vào không gian biểu diễn chung, từ đó đạt được khả năng hiểu sâu và suy luận phức tạp xuyên mô thức. Dựa trên kiến trúc chuyên gia hỗn hợp (MoE) với 30 tỷ tham số và 3 tỷ tham số kích hoạt, mô hình tối ưu hóa hiệu quả tính toán trong khi vẫn duy trì năng lực suy luận mạnh mẽ."
  },
  "Qwen/Qwen3-VL-235B-A22B-Instruct": {
    "description": "Qwen3-VL-235B-A22B-Instruct là mô hình tinh chỉnh theo chỉ dẫn quy mô lớn thuộc dòng Qwen3-VL, dựa trên kiến trúc chuyên gia hỗn hợp (MoE), sở hữu khả năng hiểu và tạo nội dung đa phương tiện vượt trội, hỗ trợ nguyên bản ngữ cảnh lên đến 256K, thích hợp cho các dịch vụ đa phương tiện cấp độ sản xuất với yêu cầu đồng thời cao."
  },
  "Qwen/Qwen3-VL-235B-A22B-Thinking": {
    "description": "Qwen3-VL-235B-A22B-Thinking là phiên bản tư duy hàng đầu trong dòng Qwen3-VL, được tối ưu hóa đặc biệt cho suy luận đa phương tiện phức tạp, suy luận ngữ cảnh dài và tương tác với tác tử thông minh, phù hợp với các tình huống doanh nghiệp đòi hỏi khả năng tư duy sâu và suy luận hình ảnh."
  },
  "Qwen/Qwen3-VL-30B-A3B-Instruct": {
    "description": "Qwen3-VL-30B-A3B-Instruct là phiên bản tinh chỉnh theo chỉ dẫn của dòng Qwen3-VL, có khả năng hiểu và tạo nội dung ngôn ngữ - hình ảnh mạnh mẽ, hỗ trợ nguyên bản độ dài ngữ cảnh lên đến 256K, phù hợp cho các tác vụ đối thoại đa phương tiện và tạo nội dung có điều kiện hình ảnh."
  },
  "Qwen/Qwen3-VL-30B-A3B-Thinking": {
    "description": "Qwen3-VL-30B-A3B-Thinking là phiên bản tăng cường suy luận (Thinking) của Qwen3-VL, được tối ưu hóa cho các tác vụ suy luận đa phương tiện, chuyển đổi hình ảnh thành mã và hiểu hình ảnh phức tạp, hỗ trợ ngữ cảnh lên đến 256K và có khả năng tư duy chuỗi mạnh mẽ hơn."
  },
  "Qwen/Qwen3-VL-32B-Instruct": {
    "description": "Qwen3-VL-32B-Instruct là một mô hình ngôn ngữ thị giác do nhóm Tongyi Qianwen của Alibaba phát triển, đạt hiệu suất SOTA hàng đầu trong nhiều bài kiểm tra chuẩn ngôn ngữ thị giác. Mô hình hỗ trợ đầu vào hình ảnh độ phân giải cao cấp độ megapixel, sở hữu năng lực hiểu thị giác tổng quát mạnh mẽ, nhận diện ký tự đa ngôn ngữ (OCR), định vị thị giác chi tiết và đối thoại thị giác. Là một mô hình ngôn ngữ thị giác trong dòng Qwen3, nó có thể xử lý các nhiệm vụ đa mô thức phức tạp, hỗ trợ gọi công cụ và tiếp tục tiền tố."
  },
  "Qwen/Qwen3-VL-32B-Thinking": {
    "description": "Qwen3-VL-32B-Thinking là phiên bản được tối ưu đặc biệt cho các nhiệm vụ suy luận thị giác phức tạp trong dòng mô hình ngôn ngữ thị giác do nhóm Tongyi Qianwen của Alibaba phát triển. Mô hình tích hợp chế độ \"suy nghĩ\", cho phép tạo ra các bước suy luận trung gian chi tiết trước khi trả lời câu hỏi, từ đó nâng cao đáng kể hiệu suất trong các nhiệm vụ đòi hỏi logic nhiều bước, lập kế hoạch và suy luận phức tạp. Mô hình hỗ trợ đầu vào hình ảnh độ phân giải cao cấp độ megapixel, có năng lực hiểu thị giác tổng quát mạnh mẽ, nhận diện ký tự đa ngôn ngữ (OCR), định vị thị giác chi tiết và đối thoại thị giác, đồng thời hỗ trợ gọi công cụ và tiếp tục tiền tố."
  },
  "Qwen/Qwen3-VL-8B-Instruct": {
    "description": "Qwen3-VL-8B-Instruct là mô hình ngôn ngữ thị giác thuộc dòng Qwen3, được phát triển dựa trên Qwen3-8B-Instruct và huấn luyện trên lượng lớn dữ liệu hình ảnh và văn bản. Mô hình này có thế mạnh trong hiểu thị giác tổng quát, đối thoại xoay quanh hình ảnh và nhận diện văn bản đa ngôn ngữ trong ảnh. Phù hợp với các tình huống như hỏi đáp thị giác, mô tả hình ảnh, tuân theo chỉ dẫn đa phương thức và gọi công cụ."
  },
  "Qwen/Qwen3-VL-8B-Thinking": {
    "description": "Qwen3-VL-8B-Thinking là phiên bản tư duy thị giác thuộc dòng Qwen3, được tối ưu hóa cho các nhiệm vụ suy luận đa bước phức tạp. Mặc định mô hình sẽ tạo ra chuỗi suy nghĩ (thinking chain) trước khi trả lời nhằm nâng cao độ chính xác trong suy luận. Phù hợp với các tình huống yêu cầu suy luận sâu như hỏi đáp thị giác, đánh giá nội dung hình ảnh và đưa ra phân tích chi tiết."
  },
  "Qwen2-72B-Instruct": {
    "description": "Qwen2 là dòng mô hình mới nhất của Qwen, hỗ trợ ngữ cảnh 128k, so với các mô hình mã nguồn mở tốt nhất hiện tại, Qwen2-72B vượt trội hơn hẳn trong nhiều khả năng như hiểu ngôn ngữ tự nhiên, kiến thức, mã, toán học và đa ngôn ngữ."
  },
  "Qwen2-7B-Instruct": {
    "description": "Qwen2 là dòng mô hình mới nhất của Qwen, có khả năng vượt qua các mô hình mã nguồn mở cùng quy mô hoặc thậm chí lớn hơn, Qwen2 7B đạt được lợi thế đáng kể trong nhiều bài kiểm tra, đặc biệt là trong việc hiểu mã và tiếng Trung."
  },
  "Qwen2-VL-72B": {
    "description": "Qwen2-VL-72B là một mô hình ngôn ngữ hình ảnh mạnh mẽ, hỗ trợ xử lý đa phương thức giữa hình ảnh và văn bản, có khả năng nhận diện chính xác nội dung hình ảnh và sinh ra mô tả hoặc câu trả lời liên quan."
  },
  "Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5-14B-Instruct là một mô hình ngôn ngữ lớn với 14 tỷ tham số, có hiệu suất xuất sắc, tối ưu cho các tình huống tiếng Trung và đa ngôn ngữ, hỗ trợ các ứng dụng như hỏi đáp thông minh, tạo nội dung."
  },
  "Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5-32B-Instruct là một mô hình ngôn ngữ lớn với 32 tỷ tham số, có hiệu suất cân bằng, tối ưu cho các tình huống tiếng Trung và đa ngôn ngữ, hỗ trợ các ứng dụng như hỏi đáp thông minh, tạo nội dung."
  },
  "Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct hỗ trợ ngữ cảnh 16k, tạo ra văn bản dài hơn 8K. Hỗ trợ gọi hàm và tương tác liền mạch với hệ thống bên ngoài, nâng cao đáng kể tính linh hoạt và khả năng mở rộng. Kiến thức của mô hình đã tăng lên rõ rệt và khả năng mã hóa cũng như toán học được cải thiện đáng kể, hỗ trợ hơn 29 ngôn ngữ."
  },
  "Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct là một mô hình ngôn ngữ lớn với 7 tỷ tham số, hỗ trợ gọi hàm và tương tác liền mạch với các hệ thống bên ngoài, nâng cao tính linh hoạt và khả năng mở rộng. Tối ưu cho các tình huống tiếng Trung và đa ngôn ngữ, hỗ trợ các ứng dụng như hỏi đáp thông minh, tạo nội dung."
  },
  "Qwen2.5-Coder-14B-Instruct": {
    "description": "Qwen2.5-Coder-14B-Instruct là một mô hình hướng dẫn lập trình dựa trên đào tạo trước quy mô lớn, có khả năng hiểu và sinh mã mạnh mẽ, có thể xử lý hiệu quả các nhiệm vụ lập trình khác nhau, đặc biệt phù hợp cho việc viết mã thông minh, tạo kịch bản tự động và giải đáp các vấn đề lập trình."
  },
  "Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder-32B-Instruct là một mô hình ngôn ngữ lớn được thiết kế đặc biệt cho việc tạo mã, hiểu mã và các tình huống phát triển hiệu quả, với quy mô 32B tham số hàng đầu trong ngành, có thể đáp ứng nhu cầu lập trình đa dạng."
  },
  "Qwen3-235B": {
    "description": "Qwen3-235B-A22B là mô hình MoE (mô hình chuyên gia hỗn hợp), giới thiệu “chế độ suy luận hỗn hợp”, cho phép người dùng chuyển đổi liền mạch giữa “chế độ suy nghĩ” và “chế độ không suy nghĩ”. Mô hình hỗ trợ hiểu và suy luận bằng 119 ngôn ngữ và phương ngữ, đồng thời có khả năng gọi công cụ mạnh mẽ. Trong các bài kiểm tra chuẩn về năng lực tổng hợp, mã hóa và toán học, đa ngôn ngữ, kiến thức và suy luận, mô hình có thể cạnh tranh với các mô hình lớn hàng đầu trên thị trường hiện nay như DeepSeek R1, OpenAI o1, o3-mini, Grok 3 và Google Gemini 2.5 Pro."
  },
  "Qwen3-235B-A22B-Instruct-2507-FP8": {
    "description": "Qwen3 235B A22B Instruct 2507: Mô hình tối ưu hóa cho suy luận nâng cao và chỉ dẫn đối thoại, kiến trúc chuyên gia hỗn hợp giúp duy trì hiệu quả suy luận với số lượng tham số lớn."
  },
  "Qwen3-32B": {
    "description": "Qwen3-32B là mô hình đặc (Dense Model), giới thiệu “chế độ suy luận hỗn hợp”, cho phép người dùng chuyển đổi liền mạch giữa “chế độ suy nghĩ” và “chế độ không suy nghĩ”. Nhờ cải tiến kiến trúc mô hình, tăng dữ liệu huấn luyện và phương pháp huấn luyện hiệu quả hơn, hiệu suất tổng thể tương đương với Qwen2.5-72B."
  },
  "SenseChat": {
    "description": "Mô hình phiên bản cơ bản (V4), độ dài ngữ cảnh 4K, khả năng tổng quát mạnh mẽ."
  },
  "SenseChat-128K": {
    "description": "Mô hình phiên bản cơ bản (V4), độ dài ngữ cảnh 128K, thể hiện xuất sắc trong các nhiệm vụ hiểu và sinh văn bản dài."
  },
  "SenseChat-32K": {
    "description": "Mô hình phiên bản cơ bản (V4), độ dài ngữ cảnh 32K, linh hoạt áp dụng trong nhiều tình huống."
  },
  "SenseChat-5": {
    "description": "Phiên bản mô hình mới nhất (V5.5), độ dài ngữ cảnh 128K, khả năng cải thiện đáng kể trong suy luận toán học, đối thoại tiếng Anh, theo dõi chỉ dẫn và hiểu biết văn bản dài, ngang tầm với GPT-4o."
  },
  "SenseChat-5-1202": {
    "description": "Phiên bản mới nhất dựa trên V5.5, cải thiện đáng kể về năng lực cơ bản tiếng Trung và tiếng Anh, trò chuyện, kiến thức khoa học tự nhiên, khoa học xã hội, viết lách, logic toán học và kiểm soát số lượng từ so với phiên bản trước."
  },
  "SenseChat-5-Cantonese": {
    "description": "Độ dài ngữ cảnh 32K, vượt qua GPT-4 trong hiểu biết đối thoại tiếng Quảng Đông, có thể so sánh với GPT-4 Turbo trong nhiều lĩnh vực như kiến thức, suy luận, toán học và lập trình mã."
  },
  "SenseChat-5-beta": {
    "description": "Một số hiệu suất vượt trội hơn SenseCat-5-1202"
  },
  "SenseChat-Character": {
    "description": "Mô hình phiên bản tiêu chuẩn, độ dài ngữ cảnh 8K, tốc độ phản hồi cao."
  },
  "SenseChat-Character-Pro": {
    "description": "Mô hình phiên bản cao cấp, độ dài ngữ cảnh 32K, khả năng được cải thiện toàn diện, hỗ trợ đối thoại tiếng Trung/tiếng Anh."
  },
  "SenseChat-Turbo": {
    "description": "Phù hợp cho các tình huống hỏi đáp nhanh và tinh chỉnh mô hình."
  },
  "SenseChat-Turbo-1202": {
    "description": "Là phiên bản nhẹ mới nhất của mô hình, đạt được hơn 90% khả năng của mô hình đầy đủ, giảm đáng kể chi phí suy diễn."
  },
  "SenseChat-Vision": {
    "description": "Mô hình phiên bản mới nhất (V5.5), hỗ trợ đầu vào nhiều hình ảnh, hoàn thiện khả năng cơ bản của mô hình, đạt được sự cải thiện lớn trong nhận diện thuộc tính đối tượng, mối quan hệ không gian, nhận diện sự kiện hành động, hiểu cảnh, nhận diện cảm xúc, suy luận kiến thức logic và hiểu sinh ra văn bản."
  },
  "SenseNova-V6-5-Pro": {
    "description": "Thông qua việc cập nhật toàn diện dữ liệu đa phương thức, ngôn ngữ và suy luận cùng với tối ưu hóa chiến lược huấn luyện, mô hình mới đạt được sự cải thiện đáng kể trong suy luận đa phương thức và khả năng tuân theo chỉ dẫn tổng quát, hỗ trợ cửa sổ ngữ cảnh lên đến 128k và thể hiện xuất sắc trong các nhiệm vụ chuyên biệt như nhận dạng OCR và nhận diện IP du lịch văn hóa."
  },
  "SenseNova-V6-5-Turbo": {
    "description": "Thông qua việc cập nhật toàn diện dữ liệu đa phương thức, ngôn ngữ và suy luận cùng với tối ưu hóa chiến lược huấn luyện, mô hình mới đạt được sự cải thiện đáng kể trong suy luận đa phương thức và khả năng tuân theo chỉ dẫn tổng quát, hỗ trợ cửa sổ ngữ cảnh lên đến 128k và thể hiện xuất sắc trong các nhiệm vụ chuyên biệt như nhận dạng OCR và nhận diện IP du lịch văn hóa."
  },
  "SenseNova-V6-Pro": {
    "description": "Thực hiện sự thống nhất nguyên bản giữa hình ảnh, văn bản và video, vượt qua giới hạn phân tách đa phương thức truyền thống, giành được hai giải vô địch trong các đánh giá OpenCompass và SuperCLUE."
  },
  "SenseNova-V6-Reasoner": {
    "description": "Kết hợp giữa lý luận sâu sắc về thị giác và ngôn ngữ, thực hiện tư duy chậm và lý luận sâu, trình bày quy trình chuỗi tư duy hoàn chỉnh."
  },
  "SenseNova-V6-Turbo": {
    "description": "Thực hiện sự thống nhất nguyên bản giữa hình ảnh, văn bản và video, vượt qua giới hạn phân tách đa phương thức truyền thống, dẫn đầu toàn diện trong các khía cạnh cốt lõi như khả năng đa phương thức và khả năng ngôn ngữ, vừa văn vừa lý, nhiều lần đứng đầu trong các đánh giá trong và ngoài nước."
  },
  "Skylark2-lite-8k": {
    "description": "Mô hình thế hệ thứ hai Skylark, mô hình Skylark2-lite có tốc độ phản hồi cao, phù hợp cho các tình huống yêu cầu tính thời gian thực cao, nhạy cảm với chi phí, không yêu cầu độ chính xác mô hình cao, chiều dài cửa sổ ngữ cảnh là 8k."
  },
  "Skylark2-pro-32k": {
    "description": "Mô hình thế hệ thứ hai Skylark, phiên bản Skylark2-pro có độ chính xác cao hơn, phù hợp cho các tình huống tạo văn bản phức tạp, như tạo nội dung chuyên ngành, sáng tác tiểu thuyết, dịch thuật chất lượng cao, chiều dài cửa sổ ngữ cảnh là 32k."
  },
  "Skylark2-pro-4k": {
    "description": "Mô hình thế hệ thứ hai Skylark, mô hình Skylark2-pro có độ chính xác cao hơn, phù hợp cho các tình huống tạo văn bản phức tạp, như tạo nội dung chuyên ngành, sáng tác tiểu thuyết, dịch thuật chất lượng cao, chiều dài cửa sổ ngữ cảnh là 4k."
  },
  "Skylark2-pro-character-4k": {
    "description": "Mô hình thế hệ thứ hai Skylark, mô hình Skylark2-pro-character có khả năng nhập vai và trò chuyện xuất sắc, giỏi nhập vai theo yêu cầu của người dùng, tạo ra những cuộc trò chuyện tự nhiên, phù hợp để xây dựng chatbot, trợ lý ảo và dịch vụ khách hàng trực tuyến, có tốc độ phản hồi cao."
  },
  "Skylark2-pro-turbo-8k": {
    "description": "Mô hình thế hệ thứ hai Skylark, mô hình Skylark2-pro-turbo-8k có tốc độ suy diễn nhanh hơn, chi phí thấp hơn, chiều dài cửa sổ ngữ cảnh là 8k."
  },
  "THUDM/GLM-4-32B-0414": {
    "description": "GLM-4-32B-0414 là mô hình mã nguồn mở thế hệ mới trong dòng GLM, với 32 tỷ tham số. Mô hình này có hiệu suất tương đương với các dòng GPT của OpenAI và các dòng V3/R1 của DeepSeek."
  },
  "THUDM/GLM-4-9B-0414": {
    "description": "GLM-4-9B-0414 là mô hình nhỏ trong dòng GLM, với 9 tỷ tham số. Mô hình này kế thừa các đặc điểm kỹ thuật của dòng GLM-4-32B, nhưng cung cấp lựa chọn triển khai nhẹ hơn. Mặc dù quy mô nhỏ, GLM-4-9B-0414 vẫn thể hiện khả năng xuất sắc trong các nhiệm vụ như tạo mã, thiết kế trang web, tạo đồ họa SVG và viết dựa trên tìm kiếm."
  },
  "THUDM/GLM-4.1V-9B-Thinking": {
    "description": "GLM-4.1V-9B-Thinking là một mô hình ngôn ngữ thị giác (VLM) mã nguồn mở được phát hành chung bởi Zhipu AI và Phòng thí nghiệm KEG của Đại học Thanh Hoa, được thiết kế đặc biệt để xử lý các nhiệm vụ nhận thức đa phương thức phức tạp. Mô hình này dựa trên mô hình cơ sở GLM-4-9B-0414, thông qua việc giới thiệu cơ chế suy luận “Chuỗi tư duy” (Chain-of-Thought) và áp dụng chiến lược học tăng cường, đã nâng cao đáng kể khả năng suy luận đa phương thức và tính ổn định của nó."
  },
  "THUDM/GLM-Z1-32B-0414": {
    "description": "GLM-Z1-32B-0414 là một mô hình suy luận có khả năng suy tư sâu. Mô hình này được phát triển dựa trên GLM-4-32B-0414 thông qua khởi động lạnh và tăng cường học tập, và đã được huấn luyện thêm trong các nhiệm vụ toán học, mã và logic. So với mô hình cơ sở, GLM-Z1-32B-0414 đã nâng cao đáng kể khả năng toán học và khả năng giải quyết các nhiệm vụ phức tạp."
  },
  "THUDM/GLM-Z1-9B-0414": {
    "description": "GLM-Z1-9B-0414 là mô hình nhỏ trong dòng GLM, chỉ có 9 tỷ tham số, nhưng vẫn thể hiện khả năng đáng kinh ngạc trong khi duy trì truyền thống mã nguồn mở. Mặc dù quy mô nhỏ, mô hình này vẫn thể hiện xuất sắc trong suy luận toán học và các nhiệm vụ chung, với hiệu suất tổng thể đứng đầu trong các mô hình mã nguồn mở cùng quy mô."
  },
  "THUDM/GLM-Z1-Rumination-32B-0414": {
    "description": "GLM-Z1-Rumination-32B-0414 là một mô hình suy luận sâu có khả năng suy tư (đối thủ của Deep Research của OpenAI). Khác với các mô hình suy tư sâu điển hình, mô hình suy tư này sử dụng thời gian suy tư sâu hơn để giải quyết các vấn đề mở và phức tạp hơn."
  },
  "THUDM/glm-4-9b-chat": {
    "description": "GLM-4 9B là phiên bản mã nguồn mở, cung cấp trải nghiệm đối thoại tối ưu cho các ứng dụng hội thoại."
  },
  "Tongyi-Zhiwen/QwenLong-L1-32B": {
    "description": "QwenLong-L1-32B là mô hình suy luận lớn có ngữ cảnh dài đầu tiên được huấn luyện bằng học tăng cường (LRM), tối ưu hóa cho các nhiệm vụ suy luận văn bản dài. Mô hình sử dụng khung học tăng cường mở rộng ngữ cảnh tiến dần, đạt được chuyển đổi ổn định từ ngữ cảnh ngắn sang dài. Trong bảy bài kiểm tra chuẩn hỏi đáp tài liệu ngữ cảnh dài, QwenLong-L1-32B vượt qua các mô hình hàng đầu như OpenAI-o3-mini và Qwen3-235B-A22B, hiệu suất tương đương Claude-3.7-Sonnet-Thinking. Mô hình đặc biệt mạnh về suy luận toán học, logic và suy luận đa bước."
  },
  "Yi-34B-Chat": {
    "description": "Yi-1.5-34B, trong khi vẫn giữ được khả năng ngôn ngữ chung xuất sắc của dòng mô hình gốc, đã tăng cường đào tạo với 500 tỷ token chất lượng cao, nâng cao đáng kể khả năng logic toán học và mã."
  },
  "abab5.5-chat": {
    "description": "Hướng đến các tình huống sản xuất, hỗ trợ xử lý nhiệm vụ phức tạp và sinh văn bản hiệu quả, phù hợp cho các ứng dụng trong lĩnh vực chuyên môn."
  },
  "abab5.5s-chat": {
    "description": "Được thiết kế đặc biệt cho các tình huống đối thoại bằng tiếng Trung, cung cấp khả năng sinh đối thoại chất lượng cao bằng tiếng Trung, phù hợp cho nhiều tình huống ứng dụng."
  },
  "abab6.5g-chat": {
    "description": "Được thiết kế đặc biệt cho các cuộc đối thoại đa ngôn ngữ, hỗ trợ sinh đối thoại chất lượng cao bằng tiếng Anh và nhiều ngôn ngữ khác."
  },
  "abab6.5s-chat": {
    "description": "Phù hợp cho nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên, bao gồm sinh văn bản, hệ thống đối thoại, v.v."
  },
  "abab6.5t-chat": {
    "description": "Tối ưu hóa cho các tình huống đối thoại bằng tiếng Trung, cung cấp khả năng sinh đối thoại mượt mà và phù hợp với thói quen diễn đạt tiếng Trung."
  },
  "accounts/fireworks/models/deepseek-r1": {
    "description": "DeepSeek-R1 là một mô hình ngôn ngữ lớn tiên tiến, được tối ưu hóa thông qua học tăng cường và dữ liệu khởi động lạnh, có hiệu suất suy luận, toán học và lập trình xuất sắc."
  },
  "accounts/fireworks/models/deepseek-v3": {
    "description": "Mô hình ngôn ngữ Mixture-of-Experts (MoE) mạnh mẽ do Deepseek cung cấp, với tổng số tham số là 671B, mỗi ký hiệu kích hoạt 37B tham số."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct": {
    "description": "Mô hình chỉ dẫn Llama 3 70B, được tối ưu hóa cho đối thoại đa ngôn ngữ và hiểu ngôn ngữ tự nhiên, hiệu suất vượt trội hơn nhiều mô hình cạnh tranh."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct": {
    "description": "Mô hình chỉ dẫn Llama 3 8B, được tối ưu hóa cho đối thoại và các nhiệm vụ đa ngôn ngữ, thể hiện hiệu suất xuất sắc và hiệu quả."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct-hf": {
    "description": "Mô hình chỉ dẫn Llama 3 8B (phiên bản HF), kết quả nhất quán với thực hiện chính thức, có tính nhất quán cao và tương thích đa nền tảng."
  },
  "accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "description": "Mô hình chỉ dẫn Llama 3.1 405B, có số lượng tham số cực lớn, phù hợp cho các nhiệm vụ phức tạp và theo dõi chỉ dẫn trong các tình huống tải cao."
  },
  "accounts/fireworks/models/llama-v3p1-70b-instruct": {
    "description": "Mô hình chỉ dẫn Llama 3.1 70B, cung cấp khả năng hiểu và sinh ngôn ngữ tự nhiên xuất sắc, là lựa chọn lý tưởng cho các nhiệm vụ đối thoại và phân tích."
  },
  "accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "description": "Mô hình chỉ dẫn Llama 3.1 8B, được tối ưu hóa cho đối thoại đa ngôn ngữ, có thể vượt qua hầu hết các mô hình mã nguồn mở và đóng trong các tiêu chuẩn ngành phổ biến."
  },
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct": {
    "description": "Mô hình suy luận hình ảnh chỉ dẫn với 11B tham số của Meta. Mô hình này được tối ưu hóa cho nhận diện hình ảnh, suy luận hình ảnh, mô tả hình ảnh và trả lời các câu hỏi chung liên quan đến hình ảnh. Mô hình có khả năng hiểu dữ liệu hình ảnh như biểu đồ và đồ thị, và thu hẹp khoảng cách giữa hình ảnh và ngôn ngữ thông qua việc tạo mô tả văn bản về chi tiết hình ảnh."
  },
  "accounts/fireworks/models/llama-v3p2-3b-instruct": {
    "description": "Mô hình chỉ dẫn Llama 3.2 3B là một mô hình đa ngôn ngữ nhẹ mà Meta phát hành. Mô hình này được thiết kế để tăng cường hiệu quả, mang lại cải tiến đáng kể về độ trễ và chi phí so với các mô hình lớn hơn. Các trường hợp sử dụng ví dụ của mô hình này bao gồm truy vấn, viết lại thông báo và hỗ trợ viết."
  },
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct": {
    "description": "Mô hình suy luận hình ảnh chỉ dẫn với 90B tham số của Meta. Mô hình này được tối ưu hóa cho nhận diện hình ảnh, suy luận hình ảnh, mô tả hình ảnh và trả lời các câu hỏi chung liên quan đến hình ảnh. Mô hình có khả năng hiểu dữ liệu hình ảnh như biểu đồ và đồ thị, và thu hẹp khoảng cách giữa hình ảnh và ngôn ngữ thông qua việc tạo mô tả văn bản về chi tiết hình ảnh."
  },
  "accounts/fireworks/models/llama-v3p3-70b-instruct": {
    "description": "Llama 3.3 70B Instruct là phiên bản cập nhật tháng 12 của Llama 3.1 70B. Mô hình này được cải tiến dựa trên Llama 3.1 70B (ra mắt vào tháng 7 năm 2024), nâng cao khả năng gọi công cụ, hỗ trợ văn bản đa ngôn ngữ, toán học và lập trình. Mô hình này đạt được trình độ hàng đầu trong ngành về suy luận, toán học và tuân thủ hướng dẫn, đồng thời có thể cung cấp hiệu suất tương tự như 3.1 405B, với lợi thế đáng kể về tốc độ và chi phí."
  },
  "accounts/fireworks/models/mistral-small-24b-instruct-2501": {
    "description": "Mô hình 24B tham số, có khả năng tiên tiến tương đương với các mô hình lớn hơn."
  },
  "accounts/fireworks/models/mixtral-8x22b-instruct": {
    "description": "Mô hình chỉ dẫn Mixtral MoE 8x22B, với số lượng tham số lớn và kiến trúc nhiều chuyên gia, hỗ trợ toàn diện cho việc xử lý hiệu quả các nhiệm vụ phức tạp."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct": {
    "description": "Mô hình chỉ dẫn Mixtral MoE 8x7B, kiến trúc nhiều chuyên gia cung cấp khả năng theo dõi và thực hiện chỉ dẫn hiệu quả."
  },
  "accounts/fireworks/models/mythomax-l2-13b": {
    "description": "Mô hình MythoMax L2 13B, kết hợp công nghệ hợp nhất mới, xuất sắc trong việc kể chuyện và đóng vai."
  },
  "accounts/fireworks/models/phi-3-vision-128k-instruct": {
    "description": "Mô hình chỉ dẫn Phi 3 Vision, mô hình đa mô hình nhẹ, có khả năng xử lý thông tin hình ảnh và văn bản phức tạp, với khả năng suy luận mạnh mẽ."
  },
  "accounts/fireworks/models/qwen-qwq-32b-preview": {
    "description": "Mô hình QwQ là một mô hình nghiên cứu thử nghiệm được phát triển bởi đội ngũ Qwen, tập trung vào việc nâng cao khả năng suy luận của AI."
  },
  "accounts/fireworks/models/qwen2-vl-72b-instruct": {
    "description": "Phiên bản 72B của mô hình Qwen-VL là thành quả mới nhất của Alibaba, đại diện cho gần một năm đổi mới."
  },
  "accounts/fireworks/models/qwen2p5-72b-instruct": {
    "description": "Qwen2.5 là một loạt mô hình ngôn ngữ chỉ chứa bộ giải mã do đội ngũ Qwen của Alibaba Cloud phát triển. Những mô hình này cung cấp các kích thước khác nhau, bao gồm 0.5B, 1.5B, 3B, 7B, 14B, 32B và 72B, và có hai biến thể: phiên bản cơ sở (base) và phiên bản chỉ dẫn (instruct)."
  },
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "description": "Qwen2.5 Coder 32B Instruct là phiên bản mới nhất trong loạt mô hình ngôn ngữ lớn chuyên biệt cho mã do Alibaba Cloud phát hành. Mô hình này được cải thiện đáng kể khả năng tạo mã, suy luận và sửa chữa thông qua việc đào tạo trên 5.5 triệu tỷ tokens, không chỉ nâng cao khả năng lập trình mà còn duy trì lợi thế về khả năng toán học và tổng quát. Mô hình cung cấp nền tảng toàn diện hơn cho các ứng dụng thực tế như tác nhân mã."
  },
  "accounts/yi-01-ai/models/yi-large": {
    "description": "Mô hình Yi-Large, có khả năng xử lý đa ngôn ngữ xuất sắc, có thể được sử dụng cho nhiều nhiệm vụ sinh và hiểu ngôn ngữ."
  },
  "ai21-jamba-1.5-large": {
    "description": "Mô hình đa ngôn ngữ với 398B tham số (94B hoạt động), cung cấp cửa sổ ngữ cảnh dài 256K, gọi hàm, đầu ra có cấu trúc và tạo ra nội dung có căn cứ."
  },
  "ai21-jamba-1.5-mini": {
    "description": "Mô hình đa ngôn ngữ với 52B tham số (12B hoạt động), cung cấp cửa sổ ngữ cảnh dài 256K, gọi hàm, đầu ra có cấu trúc và tạo ra nội dung có căn cứ."
  },
  "ai21-labs/AI21-Jamba-1.5-Large": {
    "description": "Một mô hình đa ngôn ngữ với 398 tỷ tham số (94 tỷ tham số hoạt động), cung cấp cửa sổ ngữ cảnh dài 256K, gọi hàm, đầu ra có cấu trúc và sinh dựa trên sự thật."
  },
  "ai21-labs/AI21-Jamba-1.5-Mini": {
    "description": "Một mô hình đa ngôn ngữ với 52 tỷ tham số (12 tỷ tham số hoạt động), cung cấp cửa sổ ngữ cảnh dài 256K, gọi hàm, đầu ra có cấu trúc và sinh dựa trên sự thật."
  },
  "alibaba/qwen-3-14b": {
    "description": "Qwen3 là thế hệ mới nhất trong dòng mô hình ngôn ngữ lớn Qwen, cung cấp một bộ mô hình chuyên gia dày đặc và hỗn hợp (MoE) toàn diện. Được xây dựng dựa trên đào tạo rộng rãi, Qwen3 mang lại bước đột phá trong suy luận, tuân thủ chỉ dẫn, khả năng đại lý và hỗ trợ đa ngôn ngữ."
  },
  "alibaba/qwen-3-235b": {
    "description": "Qwen3 là thế hệ mới nhất trong dòng mô hình ngôn ngữ lớn Qwen, cung cấp một bộ mô hình chuyên gia dày đặc và hỗn hợp (MoE) toàn diện. Được xây dựng dựa trên đào tạo rộng rãi, Qwen3 mang lại bước đột phá trong suy luận, tuân thủ chỉ dẫn, khả năng đại lý và hỗ trợ đa ngôn ngữ."
  },
  "alibaba/qwen-3-30b": {
    "description": "Qwen3 là thế hệ mới nhất trong dòng mô hình ngôn ngữ lớn Qwen, cung cấp một bộ mô hình chuyên gia dày đặc và hỗn hợp (MoE) toàn diện. Được xây dựng dựa trên đào tạo rộng rãi, Qwen3 mang lại bước đột phá trong suy luận, tuân thủ chỉ dẫn, khả năng đại lý và hỗ trợ đa ngôn ngữ."
  },
  "alibaba/qwen-3-32b": {
    "description": "Qwen3 là thế hệ mới nhất trong dòng mô hình ngôn ngữ lớn Qwen, cung cấp một bộ mô hình chuyên gia dày đặc và hỗn hợp (MoE) toàn diện. Được xây dựng dựa trên đào tạo rộng rãi, Qwen3 mang lại bước đột phá trong suy luận, tuân thủ chỉ dẫn, khả năng đại lý và hỗ trợ đa ngôn ngữ."
  },
  "alibaba/qwen3-coder": {
    "description": "Qwen3-Coder-480B-A35B-Instruct là mô hình mã hóa có khả năng đại lý cao nhất của Qwen, thể hiện hiệu suất nổi bật trong mã hóa đại lý, sử dụng trình duyệt đại lý và các nhiệm vụ mã hóa cơ bản khác, đạt kết quả tương đương với Claude Sonnet."
  },
  "amazon/nova-lite": {
    "description": "Một mô hình đa phương thức với chi phí rất thấp, xử lý đầu vào hình ảnh, video và văn bản với tốc độ cực nhanh."
  },
  "amazon/nova-micro": {
    "description": "Một mô hình chỉ văn bản, cung cấp phản hồi với độ trễ thấp nhất ở chi phí rất thấp."
  },
  "amazon/nova-pro": {
    "description": "Một mô hình đa phương thức rất năng lực, kết hợp tối ưu giữa độ chính xác, tốc độ và chi phí, phù hợp cho nhiều nhiệm vụ đa dạng."
  },
  "amazon/titan-embed-text-v2": {
    "description": "Amazon Titan Text Embeddings V2 là mô hình nhúng đa ngôn ngữ nhẹ, hiệu quả, hỗ trợ các chiều 1024, 512 và 256."
  },
  "anthropic.claude-3-5-sonnet-20240620-v1:0": {
    "description": "Claude 3.5 Sonnet nâng cao tiêu chuẩn ngành, hiệu suất vượt trội hơn các mô hình cạnh tranh và Claude 3 Opus, thể hiện xuất sắc trong nhiều đánh giá, đồng thời có tốc độ và chi phí của mô hình tầm trung của chúng tôi."
  },
  "anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet nâng cao tiêu chuẩn ngành, hiệu suất vượt trội so với các mô hình đối thủ và Claude 3 Opus, thể hiện xuất sắc trong các đánh giá rộng rãi, đồng thời có tốc độ và chi phí tương đương với các mô hình tầm trung của chúng tôi."
  },
  "anthropic.claude-3-haiku-20240307-v1:0": {
    "description": "Claude 3 Haiku là mô hình nhanh nhất và gọn nhẹ nhất của Anthropic, cung cấp tốc độ phản hồi gần như ngay lập tức. Nó có thể nhanh chóng trả lời các truy vấn và yêu cầu đơn giản. Khách hàng sẽ có thể xây dựng trải nghiệm AI liền mạch mô phỏng tương tác của con người. Claude 3 Haiku có thể xử lý hình ảnh và trả về đầu ra văn bản, với cửa sổ ngữ cảnh 200K."
  },
  "anthropic.claude-3-opus-20240229-v1:0": {
    "description": "Claude 3 Opus là mô hình AI mạnh nhất của Anthropic, có hiệu suất tiên tiến trong các nhiệm vụ phức tạp. Nó có thể xử lý các gợi ý mở và các tình huống chưa thấy, với độ trôi chảy và khả năng hiểu giống con người xuất sắc. Claude 3 Opus thể hiện những khả năng tiên tiến của AI sinh. Claude 3 Opus có thể xử lý hình ảnh và trả về đầu ra văn bản, với cửa sổ ngữ cảnh 200K."
  },
  "anthropic.claude-3-sonnet-20240229-v1:0": {
    "description": "Claude 3 Sonnet của Anthropic đạt được sự cân bằng lý tưởng giữa trí thông minh và tốc độ - đặc biệt phù hợp cho khối lượng công việc doanh nghiệp. Nó cung cấp hiệu quả tối đa với giá thấp hơn đối thủ, được thiết kế để trở thành một máy chủ đáng tin cậy và bền bỉ, phù hợp cho triển khai AI quy mô lớn. Claude 3 Sonnet có thể xử lý hình ảnh và trả về đầu ra văn bản, với cửa sổ ngữ cảnh 200K."
  },
  "anthropic.claude-instant-v1": {
    "description": "Một mô hình nhanh chóng, kinh tế nhưng vẫn rất mạnh mẽ, có thể xử lý một loạt các nhiệm vụ bao gồm đối thoại hàng ngày, phân tích văn bản, tóm tắt và hỏi đáp tài liệu."
  },
  "anthropic.claude-v2": {
    "description": "Mô hình của Anthropic thể hiện khả năng cao trong nhiều nhiệm vụ từ đối thoại phức tạp và sinh nội dung sáng tạo đến tuân thủ chỉ dẫn chi tiết."
  },
  "anthropic.claude-v2:1": {
    "description": "Phiên bản cập nhật của Claude 2, có cửa sổ ngữ cảnh gấp đôi, cùng với độ tin cậy, tỷ lệ ảo giác và độ chính xác dựa trên bằng chứng được cải thiện trong các tài liệu dài và ngữ cảnh RAG."
  },
  "anthropic/claude-3-haiku": {
    "description": "Claude 3 Haiku là mô hình nhanh nhất của Anthropic cho đến nay, được thiết kế cho các khối lượng công việc doanh nghiệp thường liên quan đến các lời nhắc dài. Haiku có thể phân tích nhanh lượng lớn tài liệu như báo cáo quý, hợp đồng hoặc vụ kiện pháp lý với chi phí chỉ bằng một nửa so với các mô hình cùng cấp hiệu suất."
  },
  "anthropic/claude-3-opus": {
    "description": "Claude 3 Opus là mô hình thông minh nhất của Anthropic, dẫn đầu thị trường trong các nhiệm vụ phức tạp cao. Nó có khả năng xử lý các lời nhắc mở và các tình huống chưa từng thấy với độ trôi chảy xuất sắc và hiểu biết gần như con người."
  },
  "anthropic/claude-3.5-haiku": {
    "description": "Claude 3.5 Haiku là thế hệ tiếp theo của mô hình nhanh nhất của chúng tôi. Với tốc độ tương đương Claude 3 Haiku, Claude 3.5 Haiku được cải thiện trên mọi kỹ năng và vượt qua mô hình lớn nhất thế hệ trước là Claude 3 Opus trong nhiều bài kiểm tra trí tuệ."
  },
  "anthropic/claude-3.5-sonnet": {
    "description": "Claude 3.5 Sonnet đạt sự cân bằng lý tưởng giữa trí tuệ và tốc độ — đặc biệt phù hợp cho khối lượng công việc doanh nghiệp. So với các sản phẩm cùng loại, nó cung cấp hiệu suất mạnh mẽ với chi phí thấp hơn và được thiết kế cho độ bền cao trong triển khai AI quy mô lớn."
  },
  "anthropic/claude-3.7-sonnet": {
    "description": "Claude 3.7 Sonnet là mô hình suy luận hỗn hợp đầu tiên và là mô hình thông minh nhất của Anthropic cho đến nay. Nó cung cấp hiệu suất tiên tiến trong mã hóa, tạo nội dung, phân tích dữ liệu và lập kế hoạch, xây dựng trên nền tảng khả năng kỹ thuật phần mềm và sử dụng máy tính của Claude 3.5 Sonnet."
  },
  "anthropic/claude-opus-4": {
    "description": "Claude Opus 4 là mô hình mạnh mẽ nhất của Anthropic cho đến nay và là mô hình mã hóa tốt nhất thế giới, dẫn đầu trên các bảng đánh giá SWE-bench (72,5%) và Terminal-bench (43,2%). Nó cung cấp hiệu suất liên tục cho các nhiệm vụ dài hạn đòi hỏi sự tập trung cao và hàng nghìn bước, có thể làm việc liên tục trong nhiều giờ — mở rộng đáng kể khả năng của các đại lý AI."
  },
  "anthropic/claude-opus-4.1": {
    "description": "Claude Opus 4.1 là phiên bản thay thế plug-and-play của Opus 4, cung cấp hiệu suất và độ chính xác vượt trội cho các nhiệm vụ mã hóa và đại lý thực tế. Opus 4.1 nâng cao hiệu suất mã hóa tiên tiến lên 74,5% trên SWE-bench Verified và xử lý các vấn đề phức tạp nhiều bước với độ nghiêm ngặt và chú ý đến chi tiết cao hơn."
  },
  "anthropic/claude-sonnet-4": {
    "description": "Claude Sonnet 4 cải tiến đáng kể dựa trên khả năng dẫn đầu ngành của Sonnet 3.7, thể hiện xuất sắc trong mã hóa với điểm số tiên tiến 72,7% trên SWE-bench. Mô hình cân bằng giữa hiệu suất và hiệu quả, phù hợp cho các trường hợp sử dụng nội bộ và bên ngoài, đồng thời cung cấp kiểm soát lớn hơn thông qua khả năng điều khiển nâng cao."
  },
  "anthropic/claude-sonnet-4.5": {
    "description": "Claude Sonnet 4.5 là mô hình thông minh nhất của Anthropic cho đến nay."
  },
  "ascend-tribe/pangu-pro-moe": {
    "description": "Pangu-Pro-MoE 72B-A16B là một mô hình ngôn ngữ lớn thưa thớt với 72 tỷ tham số và 16 tỷ tham số kích hoạt, dựa trên kiến trúc chuyên gia hỗn hợp theo nhóm (MoGE). Nó phân nhóm các chuyên gia trong giai đoạn lựa chọn chuyên gia và giới hạn token kích hoạt số lượng chuyên gia bằng nhau trong mỗi nhóm, từ đó đạt được cân bằng tải chuyên gia và cải thiện đáng kể hiệu quả triển khai mô hình trên nền tảng Ascend."
  },
  "aya": {
    "description": "Aya 23 là mô hình đa ngôn ngữ do Cohere phát hành, hỗ trợ 23 ngôn ngữ, tạo điều kiện thuận lợi cho các ứng dụng ngôn ngữ đa dạng."
  },
  "aya:35b": {
    "description": "Aya 23 là mô hình đa ngôn ngữ do Cohere phát hành, hỗ trợ 23 ngôn ngữ, tạo điều kiện thuận lợi cho các ứng dụng ngôn ngữ đa dạng."
  },
  "azure-DeepSeek-R1-0528": {
    "description": "Được triển khai và cung cấp bởi Microsoft; mô hình DeepSeek R1 đã được nâng cấp phiên bản nhỏ, phiên bản hiện tại là DeepSeek-R1-0528. Trong bản cập nhật mới nhất, DeepSeek R1 đã cải thiện đáng kể độ sâu suy luận và khả năng suy đoán bằng cách tăng tài nguyên tính toán và giới thiệu cơ chế tối ưu thuật toán giai đoạn hậu huấn luyện. Mô hình này thể hiện xuất sắc trong nhiều bài kiểm tra chuẩn về toán học, lập trình và logic tổng quát, hiệu suất tổng thể đã gần đạt đến các mô hình hàng đầu như O3 và Gemini 2.5 Pro."
  },
  "baichuan-m2-32b": {
    "description": "Baichuan M2 32B là mô hình chuyên gia hỗn hợp do Baichuan Intelligence phát triển, sở hữu khả năng suy luận mạnh mẽ."
  },
  "baichuan/baichuan2-13b-chat": {
    "description": "Baichuan-13B là mô hình ngôn ngữ lớn mã nguồn mở có thể thương mại hóa với 130 tỷ tham số, được phát triển bởi Baichuan Intelligence, đã đạt được hiệu suất tốt nhất trong cùng kích thước trên các benchmark tiếng Trung và tiếng Anh."
  },
  "baidu/ERNIE-4.5-300B-A47B": {
    "description": "ERNIE-4.5-300B-A47B là một mô hình ngôn ngữ lớn dựa trên kiến trúc chuyên gia hỗn hợp (MoE) do công ty Baidu phát triển. Mô hình có tổng số 300 tỷ tham số, nhưng trong quá trình suy luận mỗi token chỉ kích hoạt 47 tỷ tham số, đảm bảo hiệu suất mạnh mẽ đồng thời tối ưu hóa hiệu quả tính toán. Là một trong những mô hình cốt lõi của dòng ERNIE 4.5, nó thể hiện khả năng xuất sắc trong các nhiệm vụ hiểu, tạo văn bản, suy luận và lập trình. Mô hình áp dụng phương pháp tiền huấn luyện MoE dị thể đa phương thức sáng tạo, thông qua huấn luyện kết hợp văn bản và hình ảnh, nâng cao hiệu quả tổng thể, đặc biệt nổi bật trong việc tuân thủ chỉ dẫn và ghi nhớ kiến thức thế giới."
  },
  "c4ai-aya-expanse-32b": {
    "description": "Aya Expanse là một mô hình đa ngôn ngữ hiệu suất cao 32B, được thiết kế để thách thức hiệu suất của các mô hình đơn ngôn ngữ thông qua việc tinh chỉnh theo chỉ dẫn, khai thác dữ liệu, đào tạo theo sở thích và hợp nhất mô hình. Nó hỗ trợ 23 ngôn ngữ."
  },
  "c4ai-aya-expanse-8b": {
    "description": "Aya Expanse là một mô hình đa ngôn ngữ hiệu suất cao 8B, được thiết kế để thách thức hiệu suất của các mô hình đơn ngôn ngữ thông qua việc tinh chỉnh theo chỉ dẫn, khai thác dữ liệu, đào tạo theo sở thích và hợp nhất mô hình. Nó hỗ trợ 23 ngôn ngữ."
  },
  "c4ai-aya-vision-32b": {
    "description": "Aya Vision là một mô hình đa phương tiện tiên tiến, thể hiện xuất sắc trên nhiều tiêu chuẩn chính về khả năng ngôn ngữ, văn bản và hình ảnh. Phiên bản 32 tỷ tham số này tập trung vào hiệu suất đa ngôn ngữ tiên tiến."
  },
  "c4ai-aya-vision-8b": {
    "description": "Aya Vision là một mô hình đa phương tiện tiên tiến, thể hiện xuất sắc trên nhiều tiêu chuẩn chính về khả năng ngôn ngữ, văn bản và hình ảnh. Phiên bản 8 tỷ tham số này tập trung vào độ trễ thấp và hiệu suất tối ưu."
  },
  "charglm-3": {
    "description": "CharGLM-3 được thiết kế đặc biệt cho vai trò và đồng hành cảm xúc, hỗ trợ trí nhớ nhiều vòng siêu dài và đối thoại cá nhân hóa, ứng dụng rộng rãi."
  },
  "charglm-4": {
    "description": "CharGLM-4 được thiết kế đặc biệt cho vai trò và sự đồng hành cảm xúc, hỗ trợ trí nhớ đa vòng dài và đối thoại cá nhân hóa, ứng dụng rộng rãi."
  },
  "chatgpt-4o-latest": {
    "description": "ChatGPT-4o là một mô hình động, được cập nhật theo thời gian thực để giữ phiên bản mới nhất. Nó kết hợp khả năng hiểu và sinh ngôn ngữ mạnh mẽ, phù hợp cho các ứng dụng quy mô lớn, bao gồm dịch vụ khách hàng, giáo dục và hỗ trợ kỹ thuật."
  },
  "claude-2.0": {
    "description": "Claude 2 cung cấp những tiến bộ quan trọng trong khả năng cho doanh nghiệp, bao gồm ngữ cảnh 200K token hàng đầu trong ngành, giảm đáng kể tỷ lệ ảo giác của mô hình, nhắc nhở hệ thống và một tính năng kiểm tra mới: gọi công cụ."
  },
  "claude-2.1": {
    "description": "Claude 2 cung cấp những tiến bộ quan trọng trong khả năng cho doanh nghiệp, bao gồm ngữ cảnh 200K token hàng đầu trong ngành, giảm đáng kể tỷ lệ ảo giác của mô hình, nhắc nhở hệ thống và một tính năng kiểm tra mới: gọi công cụ."
  },
  "claude-3-5-haiku-20241022": {
    "description": "Claude 3.5 Haiku là mô hình thế hệ tiếp theo nhanh nhất của Anthropic. So với Claude 3 Haiku, Claude 3.5 Haiku đã cải thiện ở nhiều kỹ năng và vượt qua mô hình lớn nhất thế hệ trước là Claude 3 Opus trong nhiều bài kiểm tra trí tuệ."
  },
  "claude-3-5-haiku-latest": {
    "description": "Claude 3.5 Haiku cung cấp phản hồi nhanh, phù hợp cho các tác vụ nhẹ."
  },
  "claude-3-7-sonnet-20250219": {
    "description": "Claude 3.7 Sonnet là mô hình AI mạnh nhất của Anthropic, với hiệu suất vượt trội so với các mô hình đối thủ và Claude 3 Opus, thể hiện xuất sắc trong nhiều đánh giá rộng rãi, đồng thời có tốc độ và chi phí tương đương với các mô hình tầm trung của chúng tôi."
  },
  "claude-3-7-sonnet-latest": {
    "description": "Claude 3.7 Sonnet là mô hình mạnh mẽ nhất mới nhất của Anthropic dành cho các tác vụ phức tạp cao. Nó thể hiện xuất sắc về hiệu suất, trí tuệ, sự mượt mà và khả năng hiểu biết."
  },
  "claude-3-haiku-20240307": {
    "description": "Claude 3 Haiku là mô hình nhanh nhất và gọn nhẹ nhất của Anthropic, được thiết kế để đạt được phản hồi gần như ngay lập tức. Nó có hiệu suất định hướng nhanh và chính xác."
  },
  "claude-3-opus-20240229": {
    "description": "Claude 3 Opus là mô hình mạnh mẽ nhất của Anthropic để xử lý các nhiệm vụ phức tạp. Nó thể hiện xuất sắc về hiệu suất, trí thông minh, sự trôi chảy và khả năng hiểu biết."
  },
  "claude-3-sonnet-20240229": {
    "description": "Claude 3 Sonnet cung cấp sự cân bằng lý tưởng giữa trí thông minh và tốc độ cho khối lượng công việc doanh nghiệp. Nó cung cấp hiệu suất tối đa với mức giá thấp hơn, đáng tin cậy và phù hợp cho triển khai quy mô lớn."
  },
  "claude-haiku-4-5-20251001": {
    "description": "Claude Haiku 4.5 là mô hình Haiku nhanh nhất và thông minh nhất của Anthropic, với tốc độ như chớp và khả năng tư duy mở rộng."
  },
  "claude-opus-4-1-20250805": {
    "description": "Claude Opus 4.1 là mô hình mạnh mẽ nhất mới nhất của Anthropic dành cho xử lý các nhiệm vụ phức tạp cao. Nó thể hiện xuất sắc về hiệu suất, trí tuệ, sự mượt mà và khả năng hiểu biết."
  },
  "claude-opus-4-1-20250805-thinking": {
    "description": "Mô hình suy nghĩ Claude Opus 4.1, phiên bản nâng cao có thể trình bày quá trình suy luận của nó."
  },
  "claude-opus-4-20250514": {
    "description": "Claude Opus 4 là mô hình mạnh mẽ nhất của Anthropic được sử dụng để xử lý các nhiệm vụ phức tạp cao. Nó thể hiện xuất sắc về hiệu suất, trí tuệ, sự trôi chảy và khả năng hiểu biết."
  },
  "claude-sonnet-4-20250514": {
    "description": "Claude Sonnet 4 có thể tạo ra phản hồi gần như tức thì hoặc suy nghĩ từng bước kéo dài, người dùng có thể thấy rõ các quá trình này."
  },
  "claude-sonnet-4-20250514-thinking": {
    "description": "Mô hình suy nghĩ Claude Sonnet 4 có thể tạo ra phản hồi gần như tức thì hoặc suy nghĩ từng bước kéo dài, người dùng có thể thấy rõ các quá trình này."
  },
  "claude-sonnet-4-5-20250929": {
    "description": "Claude Sonnet 4.5 là mô hình thông minh nhất của Anthropic cho đến nay."
  },
  "codegeex-4": {
    "description": "CodeGeeX-4 là trợ lý lập trình AI mạnh mẽ, hỗ trợ nhiều ngôn ngữ lập trình với câu hỏi thông minh và hoàn thành mã, nâng cao hiệu suất phát triển."
  },
  "codegeex4-all-9b": {
    "description": "CodeGeeX4-ALL-9B là một mô hình tạo mã đa ngôn ngữ, hỗ trợ đầy đủ các chức năng như hoàn thành và tạo mã, trình giải thích mã, tìm kiếm trên mạng, gọi hàm, và hỏi đáp mã cấp kho, bao phủ nhiều tình huống trong phát triển phần mềm. Đây là mô hình tạo mã hàng đầu với số tham số dưới 10B."
  },
  "codegemma": {
    "description": "CodeGemma là mô hình ngôn ngữ nhẹ chuyên dụng cho các nhiệm vụ lập trình khác nhau, hỗ trợ lặp lại và tích hợp nhanh chóng."
  },
  "codegemma:2b": {
    "description": "CodeGemma là mô hình ngôn ngữ nhẹ chuyên dụng cho các nhiệm vụ lập trình khác nhau, hỗ trợ lặp lại và tích hợp nhanh chóng."
  },
  "codellama": {
    "description": "Code Llama là một LLM tập trung vào việc sinh và thảo luận mã, kết hợp hỗ trợ cho nhiều ngôn ngữ lập trình, phù hợp cho môi trường phát triển."
  },
  "codellama/CodeLlama-34b-Instruct-hf": {
    "description": "Code Llama là một LLM tập trung vào việc tạo mã và thảo luận, kết hợp hỗ trợ nhiều ngôn ngữ lập trình, phù hợp cho môi trường phát triển."
  },
  "codellama:13b": {
    "description": "Code Llama là một LLM tập trung vào việc sinh và thảo luận mã, kết hợp hỗ trợ cho nhiều ngôn ngữ lập trình, phù hợp cho môi trường phát triển."
  },
  "codellama:34b": {
    "description": "Code Llama là một LLM tập trung vào việc sinh và thảo luận mã, kết hợp hỗ trợ cho nhiều ngôn ngữ lập trình, phù hợp cho môi trường phát triển."
  },
  "codellama:70b": {
    "description": "Code Llama là một LLM tập trung vào việc sinh và thảo luận mã, kết hợp hỗ trợ cho nhiều ngôn ngữ lập trình, phù hợp cho môi trường phát triển."
  },
  "codeqwen": {
    "description": "CodeQwen1.5 là mô hình ngôn ngữ quy mô lớn được đào tạo trên một lượng lớn dữ liệu mã, chuyên giải quyết các nhiệm vụ lập trình phức tạp."
  },
  "codestral": {
    "description": "Codestral là mô hình mã đầu tiên của Mistral AI, cung cấp hỗ trợ xuất sắc cho các nhiệm vụ sinh mã."
  },
  "codestral-latest": {
    "description": "Codestral là mô hình sinh mã tiên tiến tập trung vào việc sinh mã, tối ưu hóa cho các nhiệm vụ điền vào khoảng trống và hoàn thiện mã."
  },
  "codex-mini-latest": {
    "description": "codex-mini-latest là phiên bản tinh chỉnh của o4-mini, được thiết kế đặc biệt cho Codex CLI. Đối với việc sử dụng trực tiếp qua API, chúng tôi khuyến nghị bắt đầu từ gpt-4.1."
  },
  "cogview-4": {
    "description": "CogView-4 là mô hình tạo hình ảnh văn bản mã nguồn mở đầu tiên của Zhipu hỗ trợ tạo ký tự Trung Hoa, với sự cải tiến toàn diện về hiểu ngữ nghĩa, chất lượng tạo hình ảnh, khả năng tạo ký tự tiếng Trung và tiếng Anh, hỗ trợ đầu vào song ngữ Trung-Anh với độ dài tùy ý, có thể tạo hình ảnh với độ phân giải bất kỳ trong phạm vi cho phép."
  },
  "cohere-command-r": {
    "description": "Command R là một mô hình sinh tạo có thể mở rộng, nhắm đến RAG và Sử dụng Công cụ để cho phép AI quy mô sản xuất cho doanh nghiệp."
  },
  "cohere-command-r-plus": {
    "description": "Command R+ là mô hình tối ưu hóa RAG hiện đại, được thiết kế để xử lý khối lượng công việc cấp doanh nghiệp."
  },
  "cohere/Cohere-command-r": {
    "description": "Command R là một mô hình sinh có thể mở rộng, được thiết kế cho việc sử dụng RAG và công cụ, giúp doanh nghiệp triển khai AI cấp sản xuất."
  },
  "cohere/Cohere-command-r-plus": {
    "description": "Command R+ là mô hình tối ưu RAG tiên tiến nhất, được thiết kế để xử lý khối lượng công việc cấp doanh nghiệp."
  },
  "cohere/command-a": {
    "description": "Command A là mô hình hiệu suất cao nhất của Cohere cho đến nay, xuất sắc trong việc sử dụng công cụ, đại lý, tạo tăng cường truy xuất (RAG) và các trường hợp đa ngôn ngữ. Command A có độ dài ngữ cảnh 256K, chỉ cần hai GPU để vận hành, tăng thông lượng 150% so với Command R+ 08-2024."
  },
  "cohere/command-r": {
    "description": "Command R là mô hình ngôn ngữ lớn được tối ưu cho tương tác hội thoại và các nhiệm vụ ngữ cảnh dài. Nó thuộc loại mô hình \"có thể mở rộng\", cân bằng giữa hiệu suất cao và độ chính xác mạnh mẽ, giúp các công ty vượt qua giai đoạn chứng minh khái niệm và tiến vào sản xuất."
  },
  "cohere/command-r-plus": {
    "description": "Command R+ là mô hình ngôn ngữ lớn mới nhất của Cohere, được tối ưu cho tương tác hội thoại và các nhiệm vụ ngữ cảnh dài. Mục tiêu của nó là đạt hiệu suất xuất sắc, giúp các công ty vượt qua giai đoạn chứng minh khái niệm và tiến vào sản xuất."
  },
  "cohere/embed-v4.0": {
    "description": "Mô hình cho phép phân loại hoặc chuyển đổi văn bản, hình ảnh hoặc nội dung hỗn hợp thành các vector nhúng."
  },
  "comfyui/flux-dev": {
    "description": "FLUX.1 Dev - Mô hình tạo ảnh từ văn bản chất lượng cao, tạo ảnh trong 10-50 bước, phù hợp cho sáng tác chất lượng cao và tác phẩm nghệ thuật."
  },
  "comfyui/flux-kontext-dev": {
    "description": "FLUX.1 Kontext-dev - Mô hình chỉnh sửa hình ảnh, hỗ trợ chỉnh sửa hình ảnh hiện có dựa trên hướng dẫn văn bản, bao gồm chỉnh sửa cục bộ và chuyển đổi phong cách."
  },
  "comfyui/flux-krea-dev": {
    "description": "FLUX.1 Krea-dev - Mô hình tạo ảnh từ văn bản với tính năng an toàn nâng cao, phát triển hợp tác với Krea, tích hợp bộ lọc an toàn."
  },
  "comfyui/flux-schnell": {
    "description": "FLUX.1 Schnell - Mô hình tạo ảnh từ văn bản siêu nhanh, tạo ảnh chất lượng cao chỉ trong 1-4 bước, lý tưởng cho ứng dụng thời gian thực và tạo nguyên mẫu nhanh."
  },
  "comfyui/stable-diffusion-15": {
    "description": "Stable Diffusion 1.5 - Mô hình tạo ảnh từ văn bản cổ điển với độ phân giải 512x512, phù hợp cho tạo nguyên mẫu nhanh và thử nghiệm sáng tạo."
  },
  "comfyui/stable-diffusion-35": {
    "description": "Stable Diffusion 3.5 - Mô hình tạo ảnh từ văn bản thế hệ mới, hỗ trợ hai phiên bản Large và Medium, yêu cầu tệp mã hóa CLIP bên ngoài, mang lại chất lượng hình ảnh vượt trội và độ khớp cao với từ khóa."
  },
  "comfyui/stable-diffusion-35-inclclip": {
    "description": "Stable Diffusion 3.5 - Phiên bản tích hợp mã hóa CLIP/T5, không cần tệp mã hóa bên ngoài, phù hợp với các mô hình như sd3.5_medium_incl_clips, sử dụng ít tài nguyên hơn."
  },
  "comfyui/stable-diffusion-custom": {
    "description": "Mô hình tạo ảnh từ văn bản SD tùy chỉnh, tên tệp mô hình nên là custom_sd_lobe.safetensors, nếu có VAE thì dùng custom_sd_vae_lobe.safetensors, tệp mô hình cần được đặt đúng thư mục theo yêu cầu của Comfy."
  },
  "comfyui/stable-diffusion-custom-refiner": {
    "description": "Mô hình chuyển ảnh thành ảnh SDXL tùy chỉnh, tên tệp mô hình nên là custom_sd_lobe.safetensors, nếu có VAE thì dùng custom_sd_vae_lobe.safetensors, tệp mô hình cần được đặt đúng thư mục theo yêu cầu của Comfy."
  },
  "comfyui/stable-diffusion-refiner": {
    "description": "Mô hình chuyển ảnh thành ảnh SDXL, chuyển đổi hình ảnh đầu vào thành hình ảnh chất lượng cao, hỗ trợ chuyển đổi phong cách, phục hồi hình ảnh và biến đổi sáng tạo."
  },
  "comfyui/stable-diffusion-xl": {
    "description": "Mô hình tạo ảnh từ văn bản SDXL, hỗ trợ tạo ảnh độ phân giải cao 1024x1024 từ văn bản, mang lại chất lượng hình ảnh và chi tiết vượt trội."
  },
  "command": {
    "description": "Một mô hình đối thoại tuân theo chỉ dẫn, thể hiện chất lượng cao và đáng tin cậy trong các nhiệm vụ ngôn ngữ, đồng thời có độ dài ngữ cảnh dài hơn so với mô hình sinh cơ bản của chúng tôi."
  },
  "command-a-03-2025": {
    "description": "Command A là mô hình mạnh nhất mà chúng tôi đã phát triển cho đến nay, thể hiện xuất sắc trong việc sử dụng công cụ, đại lý, tạo ra thông tin tăng cường (RAG) và các ứng dụng đa ngôn ngữ. Command A có độ dài ngữ cảnh 256K, chỉ cần hai GPU để vận hành, và so với Command R+ 08-2024, hiệu suất tăng 150%."
  },
  "command-light": {
    "description": "Một phiên bản Command nhỏ hơn, nhanh hơn, gần như mạnh mẽ tương đương nhưng có tốc độ nhanh hơn."
  },
  "command-light-nightly": {
    "description": "Để rút ngắn khoảng cách thời gian giữa các phiên bản chính, chúng tôi đã phát hành phiên bản hàng đêm của mô hình Command. Đối với dòng command-light, phiên bản này được gọi là command-light-nightly. Xin lưu ý rằng command-light-nightly là phiên bản mới nhất, mang tính thử nghiệm cao và (có thể) không ổn định. Phiên bản hàng đêm sẽ được cập nhật định kỳ mà không có thông báo trước, vì vậy không nên sử dụng trong môi trường sản xuất."
  },
  "command-nightly": {
    "description": "Để rút ngắn khoảng cách thời gian giữa các phiên bản chính, chúng tôi đã phát hành phiên bản hàng đêm của mô hình Command. Đối với dòng Command, phiên bản này được gọi là command-cightly. Xin lưu ý rằng command-nightly là phiên bản mới nhất, mang tính thử nghiệm cao và (có thể) không ổn định. Phiên bản hàng đêm sẽ được cập nhật định kỳ mà không có thông báo trước, vì vậy không nên sử dụng trong môi trường sản xuất."
  },
  "command-r": {
    "description": "Command R là LLM được tối ưu hóa cho các nhiệm vụ đối thoại và ngữ cảnh dài, đặc biệt phù hợp cho tương tác động và quản lý kiến thức."
  },
  "command-r-03-2024": {
    "description": "Command R là một mô hình đối thoại tuân theo chỉ dẫn, thể hiện chất lượng cao hơn và đáng tin cậy hơn trong các nhiệm vụ ngôn ngữ, đồng thời có độ dài ngữ cảnh dài hơn so với các mô hình trước đây. Nó có thể được sử dụng cho các quy trình phức tạp như tạo mã, tạo ra thông tin tăng cường (RAG), sử dụng công cụ và đại lý."
  },
  "command-r-08-2024": {
    "description": "command-r-08-2024 là phiên bản cập nhật của mô hình Command R, được phát hành vào tháng 8 năm 2024."
  },
  "command-r-plus": {
    "description": "Command R+ là một mô hình ngôn ngữ lớn hiệu suất cao, được thiết kế cho các tình huống doanh nghiệp thực tế và ứng dụng phức tạp."
  },
  "command-r-plus-04-2024": {
    "description": "Command R+ là một mô hình đối thoại tuân theo chỉ dẫn, thể hiện chất lượng cao hơn và đáng tin cậy hơn trong các nhiệm vụ ngôn ngữ, đồng thời có độ dài ngữ cảnh dài hơn so với các mô hình trước đây. Nó phù hợp nhất cho các quy trình RAG phức tạp và việc sử dụng công cụ nhiều bước."
  },
  "command-r-plus-08-2024": {
    "description": "Command R+ là một mô hình đối thoại tuân theo hướng dẫn, thể hiện chất lượng cao hơn trong các nhiệm vụ ngôn ngữ, đáng tin cậy hơn và có độ dài ngữ cảnh dài hơn so với các mô hình trước đây. Nó phù hợp nhất cho các quy trình làm việc RAG phức tạp và việc sử dụng công cụ nhiều bước."
  },
  "command-r7b-12-2024": {
    "description": "command-r7b-12-2024 là một phiên bản cập nhật nhỏ gọn và hiệu quả, được phát hành vào tháng 12 năm 2024. Nó thể hiện xuất sắc trong các nhiệm vụ cần suy luận phức tạp và xử lý nhiều bước như RAG, sử dụng công cụ và đại lý."
  },
  "computer-use-preview": {
    "description": "Mô hình computer-use-preview được thiết kế chuyên biệt cho “công cụ sử dụng máy tính”, được huấn luyện để hiểu và thực hiện các nhiệm vụ liên quan đến máy tính."
  },
  "dall-e-2": {
    "description": "Mô hình DALL·E thế hệ thứ hai, hỗ trợ tạo hình ảnh chân thực và chính xác hơn, với độ phân giải gấp 4 lần thế hệ đầu tiên."
  },
  "dall-e-3": {
    "description": "Mô hình DALL·E mới nhất, phát hành vào tháng 11 năm 2023. Hỗ trợ tạo hình ảnh chân thực và chính xác hơn, với khả năng thể hiện chi tiết mạnh mẽ hơn."
  },
  "databricks/dbrx-instruct": {
    "description": "DBRX Instruct cung cấp khả năng xử lý chỉ dẫn đáng tin cậy, hỗ trợ nhiều ứng dụng trong ngành."
  },
  "deepseek-ai/DeepSeek-OCR": {
    "description": "DeepSeek-OCR là một mô hình ngôn ngữ thị giác do DeepSeek AI phát triển, tập trung vào nhận diện ký tự quang học (OCR) và \"nén quang học theo ngữ cảnh\". Mô hình này nhằm khám phá giới hạn của việc nén thông tin ngữ cảnh từ hình ảnh, có khả năng xử lý tài liệu hiệu quả và chuyển đổi chúng thành các định dạng văn bản có cấu trúc như Markdown. Nó có thể nhận diện chính xác nội dung văn bản trong hình ảnh, đặc biệt phù hợp với các ứng dụng số hóa tài liệu, trích xuất văn bản và xử lý có cấu trúc."
  },
  "deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 là một mô hình suy diễn được điều khiển bởi học tăng cường (RL), giải quyết các vấn đề về tính lặp lại và khả năng đọc hiểu trong mô hình. Trước khi áp dụng RL, DeepSeek-R1 đã giới thiệu dữ liệu khởi động lạnh, tối ưu hóa thêm hiệu suất suy diễn. Nó thể hiện hiệu suất tương đương với OpenAI-o1 trong các nhiệm vụ toán học, mã và suy diễn, và thông qua phương pháp đào tạo được thiết kế cẩn thận, nâng cao hiệu quả tổng thể."
  },
  "deepseek-ai/DeepSeek-R1-0528": {
    "description": "DeepSeek R1 đã nâng cao đáng kể chiều sâu khả năng suy luận và phán đoán nhờ tận dụng tài nguyên tính toán tăng thêm và cơ chế tối ưu thuật toán trong quá trình huấn luyện sau. Mô hình thể hiện xuất sắc trong nhiều bài đánh giá chuẩn, bao gồm toán học, lập trình và logic chung. Hiệu suất tổng thể hiện gần đạt các mô hình hàng đầu như O3 và Gemini 2.5 Pro."
  },
  "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": {
    "description": "DeepSeek-R1-0528-Qwen3-8B là mô hình được chưng cất chuỗi suy nghĩ từ DeepSeek-R1-0528 sang Qwen3 8B Base. Mô hình đạt hiệu suất tiên tiến nhất (SOTA) trong các mô hình mã nguồn mở, vượt Qwen3 8B 10% trong bài kiểm tra AIME 2024 và đạt mức hiệu suất của Qwen3-235B-thinking. Mô hình thể hiện xuất sắc trong suy luận toán học, lập trình và logic chung, có kiến trúc giống Qwen3-8B nhưng dùng chung cấu hình tokenizer của DeepSeek-R1-0528."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
    "description": "Mô hình chưng cất DeepSeek-R1, tối ưu hóa hiệu suất suy luận thông qua học tăng cường và dữ liệu khởi động lạnh, mô hình mã nguồn mở làm mới tiêu chuẩn đa nhiệm."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Mô hình chưng cất DeepSeek-R1, tối ưu hóa hiệu suất suy luận thông qua học tăng cường và dữ liệu khởi động lạnh, mô hình mã nguồn mở làm mới tiêu chuẩn đa nhiệm."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Mô hình chưng cất DeepSeek-R1, tối ưu hóa hiệu suất suy luận thông qua học tăng cường và dữ liệu khởi động lạnh, mô hình mã nguồn mở làm mới tiêu chuẩn đa nhiệm."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
    "description": "DeepSeek-R1-Distill-Qwen-32B là mô hình được tạo ra từ Qwen2.5-32B thông qua chưng cất kiến thức. Mô hình này sử dụng 800.000 mẫu được chọn lọc từ DeepSeek-R1 để tinh chỉnh, thể hiện hiệu suất xuất sắc trong nhiều lĩnh vực như toán học, lập trình và suy luận. Trong nhiều bài kiểm tra chuẩn như AIME 2024, MATH-500, GPQA Diamond, nó đã đạt được kết quả xuất sắc, trong đó đạt 94.3% độ chính xác trên MATH-500, thể hiện khả năng suy luận toán học mạnh mẽ."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B là mô hình được tạo ra từ Qwen2.5-Math-7B thông qua chưng cất kiến thức. Mô hình này sử dụng 800.000 mẫu được chọn lọc từ DeepSeek-R1 để tinh chỉnh, thể hiện khả năng suy luận xuất sắc. Trong nhiều bài kiểm tra chuẩn, nó đã thể hiện xuất sắc, trong đó đạt 92.8% độ chính xác trên MATH-500, đạt 55.5% tỷ lệ vượt qua trên AIME 2024, và đạt điểm 1189 trên CodeForces, thể hiện khả năng toán học và lập trình mạnh mẽ cho mô hình quy mô 7B."
  },
  "deepseek-ai/DeepSeek-V2.5": {
    "description": "DeepSeek V2.5 kết hợp các đặc điểm xuất sắc của các phiên bản trước, tăng cường khả năng tổng quát và mã hóa."
  },
  "deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 là một mô hình ngôn ngữ hỗn hợp chuyên gia (MoE) với 6710 tỷ tham số, sử dụng chú ý tiềm ẩn đa đầu (MLA) và kiến trúc DeepSeekMoE, kết hợp với chiến lược cân bằng tải không có tổn thất phụ trợ, tối ưu hóa hiệu suất suy diễn và đào tạo. Thông qua việc được tiền huấn luyện trên 14.8 triệu tỷ token chất lượng cao, và thực hiện tinh chỉnh giám sát và học tăng cường, DeepSeek-V3 vượt trội về hiệu suất so với các mô hình mã nguồn mở khác, gần gũi với các mô hình đóng nguồn hàng đầu."
  },
  "deepseek-ai/DeepSeek-V3.1": {
    "description": "Mô hình DeepSeek V3.1 là mô hình kiến trúc suy luận hỗn hợp, hỗ trợ cả chế độ tư duy và không tư duy."
  },
  "deepseek-ai/DeepSeek-V3.1-Terminus": {
    "description": "DeepSeek-V3.1-Terminus là phiên bản cập nhật của mô hình V3.1 do DeepSeek phát hành, được định vị là mô hình ngôn ngữ lớn với trí tuệ hỗn hợp. Bản cập nhật này tập trung sửa các vấn đề phản hồi từ người dùng và nâng cao độ ổn định trong khi vẫn giữ nguyên khả năng của mô hình. Nó cải thiện đáng kể tính nhất quán ngôn ngữ, giảm thiểu việc sử dụng lẫn lộn tiếng Trung và tiếng Anh cũng như các ký tự bất thường. Mô hình tích hợp \"Chế độ suy nghĩ\" (Thinking Mode) và \"Chế độ không suy nghĩ\" (Non-thinking Mode), người dùng có thể linh hoạt chuyển đổi qua các mẫu trò chuyện để phù hợp với các nhiệm vụ khác nhau. Một tối ưu quan trọng là V3.1-Terminus tăng cường hiệu suất của Agent mã (Code Agent) và Agent tìm kiếm (Search Agent), giúp chúng đáng tin cậy hơn trong việc gọi công cụ và thực hiện các nhiệm vụ phức tạp nhiều bước."
  },
  "deepseek-ai/DeepSeek-V3.2-Exp": {
    "description": "DeepSeek-V3.2-Exp là phiên bản thử nghiệm V3.2 do DeepSeek phát hành, đóng vai trò là bước chuyển tiếp trong hành trình hướng tới kiến trúc thế hệ tiếp theo. Dựa trên nền tảng của V3.1-Terminus, phiên bản này tích hợp cơ chế Chú ý Thưa (DeepSeek Sparse Attention - DSA) nhằm nâng cao hiệu quả huấn luyện và suy luận trong ngữ cảnh dài. Nó được tối ưu hóa đặc biệt cho việc gọi công cụ, hiểu tài liệu dài và suy luận nhiều bước. V3.2-Exp là cầu nối giữa nghiên cứu và ứng dụng thực tế, phù hợp với người dùng mong muốn khám phá hiệu suất suy luận cao hơn trong các tình huống có ngân sách ngữ cảnh lớn."
  },
  "deepseek-ai/deepseek-llm-67b-chat": {
    "description": "DeepSeek 67B là mô hình tiên tiến được huấn luyện cho các cuộc đối thoại phức tạp."
  },
  "deepseek-ai/deepseek-r1": {
    "description": "LLM hiệu quả tiên tiến, xuất sắc trong suy luận, toán học và lập trình."
  },
  "deepseek-ai/deepseek-v3.1": {
    "description": "DeepSeek V3.1: Mô hình suy luận thế hệ tiếp theo, nâng cao khả năng suy luận phức tạp và tư duy chuỗi, phù hợp cho các tác vụ cần phân tích sâu."
  },
  "deepseek-ai/deepseek-v3.1-terminus": {
    "description": "DeepSeek V3.1: Mô hình suy luận thế hệ mới, nâng cao khả năng suy luận phức tạp và tư duy chuỗi, phù hợp với các nhiệm vụ cần phân tích chuyên sâu."
  },
  "deepseek-ai/deepseek-vl2": {
    "description": "DeepSeek-VL2 là một mô hình ngôn ngữ hình ảnh hỗn hợp chuyên gia (MoE) được phát triển dựa trên DeepSeekMoE-27B, sử dụng kiến trúc MoE với kích hoạt thưa, đạt được hiệu suất xuất sắc chỉ với 4.5B tham số được kích hoạt. Mô hình này thể hiện xuất sắc trong nhiều nhiệm vụ như hỏi đáp hình ảnh, nhận diện ký tự quang học, hiểu tài liệu/bảng/biểu đồ và định vị hình ảnh."
  },
  "deepseek-chat": {
    "description": "Mô hình mã nguồn mở mới kết hợp khả năng tổng quát và mã, không chỉ giữ lại khả năng đối thoại tổng quát của mô hình Chat ban đầu và khả năng xử lý mã mạnh mẽ của mô hình Coder, mà còn tốt hơn trong việc phù hợp với sở thích của con người. Hơn nữa, DeepSeek-V2.5 cũng đã đạt được sự cải thiện lớn trong nhiều khía cạnh như nhiệm vụ viết, theo dõi chỉ dẫn."
  },
  "deepseek-coder-33B-instruct": {
    "description": "DeepSeek Coder 33B là một mô hình ngôn ngữ mã, được đào tạo trên 20 triệu tỷ dữ liệu, trong đó 87% là mã và 13% là ngôn ngữ Trung và Anh. Mô hình này giới thiệu kích thước cửa sổ 16K và nhiệm vụ điền chỗ trống, cung cấp chức năng hoàn thành mã và điền đoạn mã ở cấp độ dự án."
  },
  "deepseek-coder-v2": {
    "description": "DeepSeek Coder V2 là mô hình mã nguồn mở hỗn hợp chuyên gia, thể hiện xuất sắc trong các nhiệm vụ mã, tương đương với GPT4-Turbo."
  },
  "deepseek-coder-v2:236b": {
    "description": "DeepSeek Coder V2 là mô hình mã nguồn mở hỗn hợp chuyên gia, thể hiện xuất sắc trong các nhiệm vụ mã, tương đương với GPT4-Turbo."
  },
  "deepseek-r1": {
    "description": "DeepSeek-R1 là một mô hình suy diễn được điều khiển bởi học tăng cường (RL), giải quyết các vấn đề về tính lặp lại và khả năng đọc hiểu trong mô hình. Trước khi áp dụng RL, DeepSeek-R1 đã giới thiệu dữ liệu khởi động lạnh, tối ưu hóa thêm hiệu suất suy diễn. Nó thể hiện hiệu suất tương đương với OpenAI-o1 trong các nhiệm vụ toán học, mã và suy diễn, và thông qua phương pháp đào tạo được thiết kế cẩn thận, nâng cao hiệu quả tổng thể."
  },
  "deepseek-r1-0528": {
    "description": "Mô hình phiên bản đầy đủ 685 tỷ tham số, phát hành ngày 28 tháng 5 năm 2025. DeepSeek-R1 sử dụng rộng rãi kỹ thuật học tăng cường trong giai đoạn huấn luyện sau, nâng cao đáng kể khả năng suy luận của mô hình dù có rất ít dữ liệu gán nhãn. Hiệu suất cao và năng lực mạnh mẽ trong các nhiệm vụ toán học, lập trình, suy luận ngôn ngữ tự nhiên."
  },
  "deepseek-r1-250528": {
    "description": "DeepSeek R1 250528, phiên bản đầy đủ của mô hình suy luận DeepSeek-R1, phù hợp với các nhiệm vụ toán học và logic phức tạp."
  },
  "deepseek-r1-70b-fast-online": {
    "description": "DeepSeek R1 70B phiên bản nhanh, hỗ trợ tìm kiếm trực tuyến theo thời gian thực, cung cấp tốc độ phản hồi nhanh hơn trong khi vẫn giữ hiệu suất của mô hình."
  },
  "deepseek-r1-70b-online": {
    "description": "DeepSeek R1 70B phiên bản tiêu chuẩn, hỗ trợ tìm kiếm trực tuyến theo thời gian thực, phù hợp cho các nhiệm vụ đối thoại và xử lý văn bản cần thông tin mới nhất."
  },
  "deepseek-r1-distill-llama": {
    "description": "deepseek-r1-distill-llama là mô hình được chưng cất từ DeepSeek-R1 dựa trên Llama."
  },
  "deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 Distill Llama 70B, mô hình chưng cất kết hợp khả năng suy luận R1 với hệ sinh thái Llama."
  },
  "deepseek-r1-distill-llama-8b": {
    "description": "DeepSeek-R1-Distill-Llama-8B là mô hình ngôn ngữ lớn chưng cất dựa trên Llama-3.1-8B, sử dụng đầu ra từ DeepSeek R1."
  },
  "deepseek-r1-distill-qianfan-70b": {
    "description": "DeepSeek R1 Distill Qianfan 70B, mô hình chưng cất R1 dựa trên Qianfan-70B, hiệu quả về chi phí."
  },
  "deepseek-r1-distill-qianfan-8b": {
    "description": "DeepSeek R1 Distill Qianfan 8B, mô hình chưng cất R1 dựa trên Qianfan-8B, phù hợp với các ứng dụng vừa và nhỏ."
  },
  "deepseek-r1-distill-qianfan-llama-70b": {
    "description": "DeepSeek R1 Distill Qianfan Llama 70B, mô hình chưng cất R1 dựa trên Llama-70B."
  },
  "deepseek-r1-distill-qwen": {
    "description": "deepseek-r1-distill-qwen là mô hình được chưng cất từ DeepSeek-R1 dựa trên Qwen."
  },
  "deepseek-r1-distill-qwen-1.5b": {
    "description": "DeepSeek R1 Distill Qwen 1.5B, mô hình chưng cất R1 siêu nhẹ, phù hợp với môi trường tài nguyên cực thấp."
  },
  "deepseek-r1-distill-qwen-14b": {
    "description": "DeepSeek R1 Distill Qwen 14B, mô hình chưng cất R1 quy mô trung bình, phù hợp với triển khai đa kịch bản."
  },
  "deepseek-r1-distill-qwen-32b": {
    "description": "DeepSeek R1 Distill Qwen 32B, mô hình chưng cất R1 dựa trên Qwen-32B, cân bằng giữa hiệu năng và chi phí."
  },
  "deepseek-r1-distill-qwen-7b": {
    "description": "DeepSeek R1 Distill Qwen 7B, mô hình chưng cất R1 nhẹ, phù hợp với môi trường biên và triển khai nội bộ doanh nghiệp."
  },
  "deepseek-r1-fast-online": {
    "description": "DeepSeek R1 phiên bản nhanh đầy đủ, hỗ trợ tìm kiếm trực tuyến theo thời gian thực, kết hợp sức mạnh của 671B tham số với tốc độ phản hồi nhanh hơn."
  },
  "deepseek-r1-online": {
    "description": "DeepSeek R1 phiên bản đầy đủ, có 671B tham số, hỗ trợ tìm kiếm trực tuyến theo thời gian thực, có khả năng hiểu và tạo ra mạnh mẽ hơn."
  },
  "deepseek-reasoner": {
    "description": "Chế độ suy nghĩ của DeepSeek V3.2. Trước khi đưa ra câu trả lời cuối cùng, mô hình sẽ xuất ra một chuỗi suy nghĩ nhằm nâng cao độ chính xác của câu trả lời."
  },
  "deepseek-v2": {
    "description": "DeepSeek V2 là mô hình ngôn ngữ Mixture-of-Experts hiệu quả, phù hợp cho các nhu cầu xử lý tiết kiệm."
  },
  "deepseek-v2:236b": {
    "description": "DeepSeek V2 236B là mô hình mã thiết kế của DeepSeek, cung cấp khả năng sinh mã mạnh mẽ."
  },
  "deepseek-v3": {
    "description": "DeepSeek-V3 là mô hình MoE tự phát triển của Công ty Nghiên cứu Công nghệ AI Độ Sâu Hàng Châu, có nhiều thành tích xuất sắc trong các bài kiểm tra, đứng đầu bảng xếp hạng mô hình mã nguồn mở. V3 so với mô hình V2.5 đã cải thiện tốc độ tạo ra gấp 3 lần, mang đến trải nghiệm sử dụng nhanh chóng và mượt mà hơn cho người dùng."
  },
  "deepseek-v3-0324": {
    "description": "DeepSeek-V3-0324 là mô hình MoE với 671B tham số, nổi bật trong khả năng lập trình và kỹ thuật, hiểu ngữ cảnh và xử lý văn bản dài."
  },
  "deepseek-v3.1": {
    "description": "DeepSeek-V3.1 là mô hình suy luận hỗn hợp hoàn toàn mới do DeepSeek phát hành, hỗ trợ hai chế độ suy luận: suy nghĩ và không suy nghĩ, với hiệu quả suy nghĩ cao hơn so với DeepSeek-R1-0528. Sau khi tối ưu hóa Post-Training, việc sử dụng công cụ Agent và hiệu suất nhiệm vụ của tác nhân được cải thiện đáng kể. Hỗ trợ cửa sổ ngữ cảnh 128k, độ dài đầu ra tối đa lên đến 64k tokens."
  },
  "deepseek-v3.1-terminus": {
    "description": "DeepSeek-V3.1-Terminus là phiên bản tối ưu hóa cho thiết bị đầu cuối của mô hình ngôn ngữ lớn do DeepSeek phát triển, được thiết kế đặc biệt cho các thiết bị đầu cuối."
  },
  "deepseek-v3.1-think-250821": {
    "description": "DeepSeek V3.1 Think 250821, mô hình tư duy sâu phiên bản Terminus, phù hợp với các tình huống suy luận hiệu năng cao."
  },
  "deepseek-v3.1:671b": {
    "description": "DeepSeek V3.1: Mô hình suy luận thế hệ tiếp theo, nâng cao khả năng suy luận phức tạp và tư duy chuỗi, phù hợp cho các tác vụ cần phân tích sâu."
  },
  "deepseek-v3.2-exp": {
    "description": "deepseek-v3.2-exp giới thiệu cơ chế chú ý thưa thớt, nhằm nâng cao hiệu quả đào tạo và suy luận khi xử lý văn bản dài, với giá thấp hơn deepseek-v3.1."
  },
  "deepseek-v3.2-think": {
    "description": "DeepSeek V3.2 Think, phiên bản đầy đủ của mô hình tư duy sâu, tăng cường khả năng suy luận chuỗi dài."
  },
  "deepseek-vl2": {
    "description": "DeepSeek VL2, mô hình đa phương thức, hỗ trợ hiểu hình ảnh và văn bản cùng hỏi đáp thị giác chi tiết."
  },
  "deepseek-vl2-small": {
    "description": "DeepSeek VL2 Small, phiên bản đa phương thức nhẹ, phù hợp với môi trường tài nguyên hạn chế và yêu cầu đồng thời cao."
  },
  "deepseek/deepseek-chat-v3-0324": {
    "description": "DeepSeek V3 là một mô hình hỗn hợp chuyên gia với 685B tham số, là phiên bản mới nhất trong dòng mô hình trò chuyện flagship của đội ngũ DeepSeek.\n\nNó kế thừa mô hình [DeepSeek V3](/deepseek/deepseek-chat-v3) và thể hiện xuất sắc trong nhiều nhiệm vụ."
  },
  "deepseek/deepseek-chat-v3-0324:free": {
    "description": "DeepSeek V3 là một mô hình hỗn hợp chuyên gia với 685B tham số, là phiên bản mới nhất trong dòng mô hình trò chuyện flagship của đội ngũ DeepSeek.\n\nNó kế thừa mô hình [DeepSeek V3](/deepseek/deepseek-chat-v3) và thể hiện xuất sắc trong nhiều nhiệm vụ."
  },
  "deepseek/deepseek-chat-v3.1": {
    "description": "DeepSeek-V3.1 là mô hình suy luận hỗn hợp lớn hỗ trợ ngữ cảnh dài 128K và chuyển đổi chế độ hiệu quả, đạt hiệu suất và tốc độ xuất sắc trong việc gọi công cụ, tạo mã và các nhiệm vụ suy luận phức tạp."
  },
  "deepseek/deepseek-r1": {
    "description": "Mô hình DeepSeek R1 đã được nâng cấp phiên bản nhỏ, hiện tại là DeepSeek-R1-0528. Trong bản cập nhật mới nhất, DeepSeek R1 đã cải thiện đáng kể độ sâu và khả năng suy luận bằng cách tận dụng tài nguyên tính toán tăng và cơ chế tối ưu thuật toán sau đào tạo. Mô hình thể hiện xuất sắc trong các bài đánh giá chuẩn về toán học, lập trình và logic chung, hiệu suất tổng thể hiện gần bằng các mô hình hàng đầu như O3 và Gemini 2.5 Pro."
  },
  "deepseek/deepseek-r1-0528": {
    "description": "DeepSeek-R1 đã cải thiện đáng kể khả năng suy luận của mô hình ngay cả khi có rất ít dữ liệu gán nhãn. Trước khi đưa ra câu trả lời cuối cùng, mô hình sẽ xuất ra một chuỗi suy nghĩ nhằm nâng cao độ chính xác của câu trả lời cuối."
  },
  "deepseek/deepseek-r1-0528:free": {
    "description": "DeepSeek-R1 đã cải thiện đáng kể khả năng suy luận của mô hình ngay cả khi có rất ít dữ liệu gán nhãn. Trước khi đưa ra câu trả lời cuối cùng, mô hình sẽ xuất ra một chuỗi suy nghĩ nhằm nâng cao độ chính xác của câu trả lời cuối."
  },
  "deepseek/deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 Distill Llama 70B là một mô hình ngôn ngữ lớn dựa trên Llama3.3 70B, được tinh chỉnh bằng đầu ra từ DeepSeek R1, mang lại hiệu suất cạnh tranh tương đương với các mô hình tiên tiến quy mô lớn."
  },
  "deepseek/deepseek-r1-distill-llama-8b": {
    "description": "DeepSeek R1 Distill Llama 8B là một mô hình ngôn ngữ lớn đã được tinh chế dựa trên Llama-3.1-8B-Instruct, được đào tạo bằng cách sử dụng đầu ra từ DeepSeek R1."
  },
  "deepseek/deepseek-r1-distill-qwen-14b": {
    "description": "DeepSeek R1 Distill Qwen 14B là một mô hình ngôn ngữ lớn đã được tinh chế dựa trên Qwen 2.5 14B, được đào tạo bằng cách sử dụng đầu ra từ DeepSeek R1. Mô hình này đã vượt qua o1-mini của OpenAI trong nhiều bài kiểm tra chuẩn, đạt được những thành tựu công nghệ tiên tiến nhất trong các mô hình dày đặc (dense models). Dưới đây là một số kết quả từ các bài kiểm tra chuẩn:\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nMô hình này đã thể hiện hiệu suất cạnh tranh tương đương với các mô hình tiên tiến lớn hơn thông qua việc tinh chỉnh từ đầu ra của DeepSeek R1."
  },
  "deepseek/deepseek-r1-distill-qwen-32b": {
    "description": "DeepSeek R1 Distill Qwen 32B là một mô hình ngôn ngữ lớn đã được tinh chế dựa trên Qwen 2.5 32B, được đào tạo bằng cách sử dụng đầu ra từ DeepSeek R1. Mô hình này đã vượt qua o1-mini của OpenAI trong nhiều bài kiểm tra chuẩn, đạt được những thành tựu công nghệ tiên tiến nhất trong các mô hình dày đặc (dense models). Dưới đây là một số kết quả từ các bài kiểm tra chuẩn:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nMô hình này đã thể hiện hiệu suất cạnh tranh tương đương với các mô hình tiên tiến lớn hơn thông qua việc tinh chỉnh từ đầu ra của DeepSeek R1."
  },
  "deepseek/deepseek-r1/community": {
    "description": "DeepSeek R1 là mô hình mã nguồn mở mới nhất được phát hành bởi đội ngũ DeepSeek, có hiệu suất suy diễn rất mạnh mẽ, đặc biệt trong các nhiệm vụ toán học, lập trình và suy luận, đạt được mức độ tương đương với mô hình o1 của OpenAI."
  },
  "deepseek/deepseek-r1:free": {
    "description": "DeepSeek-R1 đã nâng cao khả năng suy luận của mô hình một cách đáng kể với rất ít dữ liệu được gán nhãn. Trước khi đưa ra câu trả lời cuối cùng, mô hình sẽ xuất ra một chuỗi suy nghĩ để nâng cao độ chính xác của câu trả lời cuối cùng."
  },
  "deepseek/deepseek-v3": {
    "description": "Mô hình ngôn ngữ lớn đa năng nhanh với khả năng suy luận nâng cao."
  },
  "deepseek/deepseek-v3.1-base": {
    "description": "DeepSeek V3.1 Base là phiên bản cải tiến của mô hình DeepSeek V3."
  },
  "deepseek/deepseek-v3/community": {
    "description": "DeepSeek-V3 đã đạt được bước đột phá lớn về tốc độ suy diễn so với các mô hình trước đó. Nó đứng đầu trong số các mô hình mã nguồn mở và có thể so sánh với các mô hình đóng nguồn tiên tiến nhất trên toàn cầu. DeepSeek-V3 sử dụng kiến trúc Attention đa đầu (MLA) và DeepSeekMoE, những kiến trúc này đã được xác thực toàn diện trong DeepSeek-V2. Hơn nữa, DeepSeek-V3 đã sáng tạo ra một chiến lược phụ trợ không mất mát cho cân bằng tải và thiết lập mục tiêu đào tạo dự đoán đa nhãn để đạt được hiệu suất mạnh mẽ hơn."
  },
  "deepseek_r1": {
    "description": "DeepSeek-R1 là một mô hình suy luận được điều khiển bởi học tăng cường (RL), giải quyết các vấn đề về tính lặp lại và khả năng đọc hiểu trong mô hình. Trước khi áp dụng RL, DeepSeek-R1 đã giới thiệu dữ liệu khởi động lạnh, tối ưu hóa thêm hiệu suất suy luận. Nó thể hiện hiệu suất tương đương với OpenAI-o1 trong các nhiệm vụ toán học, mã và suy luận, và đã nâng cao hiệu quả tổng thể thông qua phương pháp huấn luyện được thiết kế cẩn thận."
  },
  "deepseek_r1_distill_llama_70b": {
    "description": "DeepSeek-R1-Distill-Llama-70B là mô hình được phát triển từ Llama-3.3-70B-Instruct thông qua quá trình tinh chế. Mô hình này là một phần của dòng DeepSeek-R1, thể hiện hiệu suất xuất sắc trong nhiều lĩnh vực như toán học, lập trình và suy luận thông qua việc tinh chỉnh bằng các mẫu được tạo ra từ DeepSeek-R1."
  },
  "deepseek_r1_distill_qwen_14b": {
    "description": "DeepSeek-R1-Distill-Qwen-14B là mô hình được phát triển từ Qwen2.5-14B thông qua quá trình tinh chế kiến thức. Mô hình này được tinh chỉnh bằng 800.000 mẫu được chọn từ DeepSeek-R1, thể hiện khả năng suy luận xuất sắc."
  },
  "deepseek_r1_distill_qwen_32b": {
    "description": "DeepSeek-R1-Distill-Qwen-32B là mô hình được phát triển từ Qwen2.5-32B thông qua quá trình tinh chế kiến thức. Mô hình này được tinh chỉnh bằng 800.000 mẫu được chọn từ DeepSeek-R1, thể hiện hiệu suất xuất sắc trong nhiều lĩnh vực như toán học, lập trình và suy luận."
  },
  "doubao-1.5-lite-32k": {
    "description": "Doubao-1.5-lite là mô hình phiên bản nhẹ thế hệ mới, tốc độ phản hồi cực nhanh, hiệu quả và độ trễ đạt tiêu chuẩn hàng đầu thế giới."
  },
  "doubao-1.5-pro-256k": {
    "description": "Doubao-1.5-pro-256k là phiên bản nâng cấp toàn diện dựa trên Doubao-1.5-Pro, hiệu quả tổng thể tăng 10%. Hỗ trợ suy luận với cửa sổ ngữ cảnh 256k, độ dài đầu ra tối đa lên đến 12k tokens. Hiệu suất cao hơn, cửa sổ lớn hơn, giá trị vượt trội, phù hợp với nhiều ứng dụng khác nhau."
  },
  "doubao-1.5-pro-32k": {
    "description": "Doubao-1.5-pro là mô hình chủ lực thế hệ mới, hiệu suất được nâng cấp toàn diện, thể hiện xuất sắc trong các lĩnh vực kiến thức, mã nguồn, suy luận, và nhiều hơn nữa."
  },
  "doubao-1.5-thinking-pro": {
    "description": "Mô hình tư duy sâu mới Doubao-1.5, nổi bật trong các lĩnh vực chuyên môn như toán học, lập trình, suy luận khoa học và các nhiệm vụ viết sáng tạo, đạt hoặc gần đạt trình độ hàng đầu trong ngành trên nhiều tiêu chuẩn uy tín như AIME 2024, Codeforces, GPQA. Hỗ trợ cửa sổ ngữ cảnh 128k, đầu ra 16k."
  },
  "doubao-1.5-thinking-pro-m": {
    "description": "Doubao-1.5 là mô hình tư duy sâu hoàn toàn mới (phiên bản m có khả năng suy luận đa phương thức sâu nguyên bản), thể hiện xuất sắc trong các lĩnh vực chuyên môn như toán học, lập trình, suy luận khoa học và các nhiệm vụ sáng tạo chung. Đạt hoặc gần đạt trình độ hàng đầu ngành trên nhiều chuẩn đánh giá uy tín như AIME 2024, Codeforces, GPQA. Hỗ trợ cửa sổ ngữ cảnh 128k, đầu ra 16k."
  },
  "doubao-1.5-thinking-vision-pro": {
    "description": "Mô hình tư duy sâu đa phương thức hoàn toàn mới, có khả năng hiểu và suy luận đa phương thức tổng quát mạnh mẽ, đạt hiệu suất SOTA trên 37 trong số 59 chuẩn đánh giá công khai."
  },
  "doubao-1.5-ui-tars": {
    "description": "Doubao-1.5-UI-TARS là mô hình Agent nguyên bản hướng tới tương tác giao diện đồ họa (GUI). Thông qua khả năng nhận thức, suy luận và hành động giống con người, tương tác liền mạch với GUI."
  },
  "doubao-1.5-vision-lite": {
    "description": "Doubao-1.5-vision-lite là mô hình đa phương tiện lớn được nâng cấp mới, hỗ trợ nhận diện hình ảnh với bất kỳ độ phân giải nào và tỷ lệ dài rộng cực đoan, tăng cường khả năng suy luận hình ảnh, nhận diện tài liệu, hiểu thông tin chi tiết và tuân thủ hướng dẫn. Hỗ trợ cửa sổ ngữ cảnh 128k, độ dài đầu ra tối đa 16k tokens."
  },
  "doubao-1.5-vision-pro": {
    "description": "Doubao-1.5-vision-pro là mô hình đa phương thức lớn được nâng cấp hoàn toàn mới, hỗ trợ nhận dạng hình ảnh với độ phân giải tùy ý và tỷ lệ khung hình cực đoan, tăng cường khả năng suy luận thị giác, nhận dạng tài liệu, hiểu thông tin chi tiết và tuân thủ chỉ dẫn."
  },
  "doubao-1.5-vision-pro-32k": {
    "description": "Doubao-1.5-vision-pro là mô hình đa phương thức lớn được nâng cấp hoàn toàn mới, hỗ trợ nhận dạng hình ảnh với độ phân giải tùy ý và tỷ lệ khung hình cực đoan, tăng cường khả năng suy luận thị giác, nhận dạng tài liệu, hiểu thông tin chi tiết và tuân thủ chỉ dẫn."
  },
  "doubao-lite-128k": {
    "description": "Sở hữu tốc độ phản hồi tối ưu, hiệu quả chi phí tốt hơn, cung cấp lựa chọn linh hoạt hơn cho các kịch bản khác nhau của khách hàng. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 128k."
  },
  "doubao-lite-32k": {
    "description": "Sở hữu tốc độ phản hồi tối ưu, hiệu quả chi phí tốt hơn, cung cấp lựa chọn linh hoạt hơn cho các kịch bản khác nhau của khách hàng. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 32k."
  },
  "doubao-lite-4k": {
    "description": "Sở hữu tốc độ phản hồi tối ưu, hiệu quả chi phí tốt hơn, cung cấp lựa chọn linh hoạt hơn cho các kịch bản khác nhau của khách hàng. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 4k."
  },
  "doubao-pro-256k": {
    "description": "Mô hình chủ lực với hiệu quả tốt nhất, phù hợp xử lý các nhiệm vụ phức tạp, có hiệu quả xuất sắc trong các kịch bản như hỏi đáp tham khảo, tóm tắt, sáng tạo, phân loại văn bản, nhập vai. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 256k."
  },
  "doubao-pro-32k": {
    "description": "Mô hình chủ lực với hiệu quả tốt nhất, phù hợp xử lý các nhiệm vụ phức tạp, có hiệu quả xuất sắc trong các kịch bản như hỏi đáp tham khảo, tóm tắt, sáng tạo, phân loại văn bản, nhập vai. Hỗ trợ suy luận và tinh chỉnh với cửa sổ ngữ cảnh 32k."
  },
  "doubao-seed-1.6": {
    "description": "Doubao-Seed-1.6 là mô hình suy nghĩ sâu đa phương thức hoàn toàn mới, hỗ trợ ba chế độ suy nghĩ auto/thinking/non-thinking. Ở chế độ non-thinking, hiệu quả mô hình cải thiện đáng kể so với Doubao-1.5-pro/250115. Hỗ trợ cửa sổ ngữ cảnh 256k, độ dài đầu ra tối đa 16k tokens."
  },
  "doubao-seed-1.6-flash": {
    "description": "Doubao-Seed-1.6-flash là mô hình suy nghĩ sâu đa phương thức với tốc độ suy luận tối ưu, TPOT chỉ cần 10ms; đồng thời hỗ trợ hiểu văn bản và hình ảnh, khả năng hiểu văn bản vượt trội so với thế hệ lite trước, khả năng hiểu hình ảnh sánh ngang với các mô hình pro của đối thủ. Hỗ trợ cửa sổ ngữ cảnh 256k, độ dài đầu ra tối đa 16k tokens."
  },
  "doubao-seed-1.6-lite": {
    "description": "Doubao-Seed-1.6-lite là mô hình tư duy sâu đa phương thức hoàn toàn mới, hỗ trợ điều chỉnh mức độ suy luận (reasoning effort) với bốn chế độ: Tối thiểu, Thấp, Trung bình và Cao. Đây là lựa chọn tối ưu cho các tác vụ phổ biến với hiệu suất vượt trội và cửa sổ ngữ cảnh lên đến 256k."
  },
  "doubao-seed-1.6-thinking": {
    "description": "Mô hình Doubao-Seed-1.6-thinking có khả năng suy nghĩ được tăng cường đáng kể, so với Doubao-1.5-thinking-pro, nâng cao hơn nữa các năng lực cơ bản như lập trình, toán học, suy luận logic, đồng thời hỗ trợ hiểu hình ảnh. Hỗ trợ cửa sổ ngữ cảnh 256k, độ dài đầu ra tối đa 16k tokens."
  },
  "doubao-seed-1.6-vision": {
    "description": "Doubao-Seed-1.6-vision là mô hình suy nghĩ sâu về thị giác, thể hiện khả năng hiểu và suy luận đa phương thức tổng quát mạnh mẽ hơn trong các kịch bản như giáo dục, kiểm duyệt hình ảnh, kiểm tra và an ninh, cũng như tìm kiếm và hỏi đáp AI. Hỗ trợ cửa sổ ngữ cảnh 256k, độ dài đầu ra tối đa lên đến 64k token."
  },
  "doubao-seededit-3-0-i2i-250628": {
    "description": "Mô hình tạo ảnh Doubao do đội Seed của ByteDance phát triển, hỗ trợ đầu vào văn bản và hình ảnh, cung cấp trải nghiệm tạo ảnh chất lượng cao và kiểm soát tốt. Hỗ trợ chỉnh sửa hình ảnh qua chỉ dẫn văn bản, kích thước ảnh tạo ra từ 512 đến 1536 pixel."
  },
  "doubao-seedream-3-0-t2i-250415": {
    "description": "Mô hình tạo ảnh Seedream 3.0 do đội Seed của ByteDance phát triển, hỗ trợ đầu vào văn bản và hình ảnh, cung cấp trải nghiệm tạo ảnh chất lượng cao và kiểm soát tốt. Tạo ảnh dựa trên từ khóa văn bản."
  },
  "doubao-seedream-4-0-250828": {
    "description": "Mô hình tạo ảnh Seedream 4.0 do đội Seed của ByteDance phát triển, hỗ trợ đầu vào văn bản và hình ảnh, cung cấp trải nghiệm tạo ảnh chất lượng cao và kiểm soát tốt. Tạo ảnh dựa trên từ khóa văn bản."
  },
  "doubao-vision-lite-32k": {
    "description": "Mô hình Doubao-vision là mô hình đa phương thức lớn do Doubao phát triển, có khả năng hiểu và suy luận hình ảnh mạnh mẽ, cùng khả năng hiểu chỉ dẫn chính xác. Mô hình thể hiện hiệu suất vượt trội trong việc trích xuất thông tin văn bản từ hình ảnh và các nhiệm vụ suy luận dựa trên hình ảnh, có thể ứng dụng trong các nhiệm vụ hỏi đáp thị giác phức tạp và đa dạng hơn."
  },
  "doubao-vision-pro-32k": {
    "description": "Mô hình Doubao-vision là mô hình đa phương thức lớn do Doubao phát triển, có khả năng hiểu và suy luận hình ảnh mạnh mẽ, cùng khả năng hiểu chỉ dẫn chính xác. Mô hình thể hiện hiệu suất vượt trội trong việc trích xuất thông tin văn bản từ hình ảnh và các nhiệm vụ suy luận dựa trên hình ảnh, có thể ứng dụng trong các nhiệm vụ hỏi đáp thị giác phức tạp và đa dạng hơn."
  },
  "emohaa": {
    "description": "Emohaa là mô hình tâm lý, có khả năng tư vấn chuyên nghiệp, giúp người dùng hiểu các vấn đề cảm xúc."
  },
  "ernie-4.5-0.3b": {
    "description": "ERNIE 4.5 0.3B, mô hình mã nguồn mở nhẹ, phù hợp cho triển khai cục bộ và tùy chỉnh."
  },
  "ernie-4.5-21b-a3b": {
    "description": "ERNIE 4.5 21B A3B, mô hình lớn mã nguồn mở, thể hiện hiệu suất vượt trội trong các nhiệm vụ hiểu và sinh văn bản."
  },
  "ernie-4.5-300b-a47b": {
    "description": "ERNIE 4.5 300B A47B là mô hình chuyên gia hỗn hợp quy mô siêu lớn do Wenxin của Baidu phát triển, nổi bật với khả năng suy luận vượt trội."
  },
  "ernie-4.5-8k-preview": {
    "description": "ERNIE 4.5 8K Preview, mô hình xem trước ngữ cảnh 8K, dùng để trải nghiệm và kiểm thử khả năng của Wenxin 4.5."
  },
  "ernie-4.5-turbo-128k": {
    "description": "ERNIE 4.5 Turbo 128K, mô hình đa năng hiệu suất cao, hỗ trợ tăng cường tìm kiếm và gọi công cụ, phù hợp với hỏi đáp, lập trình, tác tử thông minh và nhiều tình huống khác."
  },
  "ernie-4.5-turbo-128k-preview": {
    "description": "ERNIE 4.5 Turbo 128K Preview, phiên bản xem trước với trải nghiệm tương đương bản chính thức, phù hợp cho kiểm thử và tích hợp."
  },
  "ernie-4.5-turbo-32k": {
    "description": "ERNIE 4.5 Turbo 32K, phiên bản ngữ cảnh trung bình đến dài, phù hợp với hỏi đáp, truy vấn tri thức và hội thoại nhiều lượt."
  },
  "ernie-4.5-turbo-latest": {
    "description": "ERNIE 4.5 Turbo Phiên bản mới nhất, tối ưu hóa toàn diện hiệu suất, phù hợp làm mô hình chính trong môi trường sản xuất."
  },
  "ernie-4.5-turbo-vl": {
    "description": "ERNIE 4.5 Turbo VL, mô hình đa phương thức trưởng thành, phù hợp với các nhiệm vụ hiểu và nhận diện hình ảnh-văn bản trong môi trường sản xuất."
  },
  "ernie-4.5-turbo-vl-32k": {
    "description": "ERNIE 4.5 Turbo VL 32K, phiên bản đa phương thức văn bản dài, phù hợp với hiểu kết hợp tài liệu dài và hình ảnh."
  },
  "ernie-4.5-turbo-vl-32k-preview": {
    "description": "ERNIE 4.5 Turbo VL 32K Preview, phiên bản xem trước đa phương thức 32K, thuận tiện để đánh giá khả năng thị giác ngữ cảnh dài."
  },
  "ernie-4.5-turbo-vl-latest": {
    "description": "ERNIE 4.5 Turbo VL Phiên bản mới nhất, phiên bản đa phương thức mới nhất, cung cấp khả năng hiểu và suy luận hình ảnh-văn bản tốt hơn."
  },
  "ernie-4.5-turbo-vl-preview": {
    "description": "ERNIE 4.5 Turbo VL Preview, mô hình đa phương thức xem trước, hỗ trợ hiểu và sinh hình ảnh-văn bản, phù hợp với hỏi đáp thị giác và trải nghiệm hiểu nội dung."
  },
  "ernie-4.5-vl-28b-a3b": {
    "description": "ERNIE 4.5 VL 28B A3B, mô hình đa phương thức mã nguồn mở, hỗ trợ các nhiệm vụ hiểu và suy luận hình ảnh-văn bản."
  },
  "ernie-5.0-thinking-preview": {
    "description": "Wenxin 5.0 Thinking Preview, mô hình hàng đầu toàn phương thức nguyên bản, hỗ trợ mô hình hóa thống nhất văn bản, hình ảnh, âm thanh và video, nâng cấp toàn diện năng lực tổng hợp, phù hợp với hỏi đáp phức tạp, sáng tạo và tác tử thông minh."
  },
  "ernie-char-8k": {
    "description": "ERNIE Character 8K, mô hình hội thoại nhân cách nhân vật, phù hợp với xây dựng nhân vật IP và hội thoại đồng hành dài hạn."
  },
  "ernie-char-fiction-8k": {
    "description": "ERNIE Character Fiction 8K, mô hình nhân cách dành cho sáng tác tiểu thuyết và cốt truyện, phù hợp với sinh truyện dài."
  },
  "ernie-char-fiction-8k-preview": {
    "description": "ERNIE Character Fiction 8K Preview, phiên bản xem trước mô hình sáng tác nhân vật và cốt truyện, dùng để trải nghiệm và kiểm thử tính năng."
  },
  "ernie-irag-edit": {
    "description": "ERNIE iRAG Edit, mô hình chỉnh sửa hình ảnh hỗ trợ xóa, vẽ lại và tạo biến thể hình ảnh."
  },
  "ernie-lite-8k": {
    "description": "ERNIE Lite 8K, mô hình đa năng nhẹ, phù hợp với các tình huống hỏi đáp hàng ngày và sinh nội dung nhạy cảm về chi phí."
  },
  "ernie-lite-pro-128k": {
    "description": "ERNIE Lite Pro 128K, mô hình nhẹ hiệu suất cao, phù hợp với các tình huống kinh doanh nhạy cảm về độ trễ và chi phí."
  },
  "ernie-novel-8k": {
    "description": "ERNIE Novel 8K, mô hình sáng tác tiểu thuyết dài và cốt truyện IP, giỏi trong kể chuyện đa nhân vật và đa tuyến."
  },
  "ernie-speed-128k": {
    "description": "ERNIE Speed 128K, mô hình lớn không tính phí nhập/xuất, phù hợp với hiểu văn bản dài và thử nghiệm quy mô lớn."
  },
  "ernie-speed-8k": {
    "description": "ERNIE Speed 8K, mô hình miễn phí và nhanh, phù hợp với hội thoại hàng ngày và nhiệm vụ văn bản nhẹ."
  },
  "ernie-speed-pro-128k": {
    "description": "ERNIE Speed Pro 128K, mô hình hiệu suất cao và chi phí thấp, phù hợp với dịch vụ trực tuyến quy mô lớn và ứng dụng doanh nghiệp."
  },
  "ernie-tiny-8k": {
    "description": "ERNIE Tiny 8K, mô hình siêu nhẹ, phù hợp với các tình huống suy luận chi phí thấp như hỏi đáp đơn giản và phân loại."
  },
  "ernie-x1-turbo-32k": {
    "description": "ERNIE X1 Turbo 32K, mô hình tư duy tốc độ cao, ngữ cảnh dài 32K, phù hợp với suy luận phức tạp và hội thoại nhiều lượt."
  },
  "ernie-x1.1-preview": {
    "description": "ERNIE X1.1 Preview, phiên bản xem trước mô hình tư duy ERNIE X1.1, phù hợp với kiểm thử và xác minh năng lực."
  },
  "fal-ai/bytedance/seedream/v4": {
    "description": "Mô hình tạo ảnh Seedream 4.0 do đội Seed của ByteDance phát triển, hỗ trợ đầu vào văn bản và hình ảnh, cung cấp trải nghiệm tạo ảnh chất lượng cao và kiểm soát tốt. Tạo ảnh dựa trên từ khóa văn bản."
  },
  "fal-ai/flux-kontext/dev": {
    "description": "Mô hình FLUX.1 tập trung vào nhiệm vụ chỉnh sửa hình ảnh, hỗ trợ đầu vào văn bản và hình ảnh."
  },
  "fal-ai/flux-pro/kontext": {
    "description": "FLUX.1 Kontext [pro] có khả năng xử lý đầu vào là văn bản và hình ảnh tham khảo, thực hiện chỉnh sửa cục bộ mục tiêu và biến đổi cảnh phức tạp một cách liền mạch."
  },
  "fal-ai/flux/krea": {
    "description": "Flux Krea [dev] là mô hình tạo ảnh có sở thích thẩm mỹ, nhằm tạo ra hình ảnh chân thực và tự nhiên hơn."
  },
  "fal-ai/flux/schnell": {
    "description": "FLUX.1 [schnell] là mô hình tạo ảnh với 12 tỷ tham số, tập trung vào việc tạo ảnh chất lượng cao nhanh chóng."
  },
  "fal-ai/hunyuan-image/v3": {
    "description": "Một mô hình tạo hình ảnh đa phương thức gốc mạnh mẽ"
  },
  "fal-ai/imagen4/preview": {
    "description": "Mô hình tạo ảnh chất lượng cao do Google cung cấp."
  },
  "fal-ai/nano-banana": {
    "description": "Nano Banana là mô hình đa phương thức nguyên bản mới nhất, nhanh nhất và hiệu quả nhất của Google, cho phép bạn tạo và chỉnh sửa hình ảnh qua đối thoại."
  },
  "fal-ai/qwen-image": {
    "description": "Mô hình ảnh thô mạnh mẽ do đội Qwen phát triển, có khả năng tạo văn bản tiếng Trung ấn tượng và đa dạng phong cách hình ảnh."
  },
  "fal-ai/qwen-image-edit": {
    "description": "Mô hình chỉnh sửa hình ảnh chuyên nghiệp do đội Qwen phát hành, hỗ trợ chỉnh sửa ngữ nghĩa và ngoại hình, có thể chỉnh sửa chính xác văn bản tiếng Trung và tiếng Anh, thực hiện chuyển đổi phong cách, xoay đối tượng và các chỉnh sửa hình ảnh chất lượng cao khác."
  },
  "flux-1-schnell": {
    "description": "Mô hình tạo hình ảnh từ văn bản 12 tỷ tham số do Black Forest Labs phát triển, sử dụng kỹ thuật chưng cất khuếch tán đối kháng tiềm ẩn, có thể tạo hình ảnh chất lượng cao trong 1 đến 4 bước. Mô hình có hiệu suất tương đương các sản phẩm đóng nguồn và được phát hành dưới giấy phép Apache-2.0, phù hợp cho cá nhân, nghiên cứu và thương mại."
  },
  "flux-dev": {
    "description": "FLUX.1 [dev] là mô hình tinh luyện mã nguồn mở dành cho ứng dụng phi thương mại. FLUX.1 [dev] duy trì chất lượng hình ảnh và khả năng tuân thủ chỉ dẫn gần tương đương phiên bản chuyên nghiệp FLUX, đồng thời có hiệu suất vận hành cao hơn. So với mô hình chuẩn cùng kích thước, nó sử dụng tài nguyên hiệu quả hơn."
  },
  "flux-kontext-max": {
    "description": "Tạo và chỉnh sửa hình ảnh theo ngữ cảnh tiên tiến nhất — kết hợp văn bản và hình ảnh để có kết quả chính xác, mạch lạc."
  },
  "flux-kontext-pro": {
    "description": "Tạo và chỉnh sửa hình ảnh theo ngữ cảnh tiên tiến nhất — kết hợp văn bản và hình ảnh để đạt được kết quả chính xác, mạch lạc."
  },
  "flux-merged": {
    "description": "Mô hình FLUX.1-merged kết hợp các đặc tính sâu sắc được khám phá trong giai đoạn phát triển của \"DEV\" và ưu thế thực thi nhanh của \"Schnell\". Qua đó, FLUX.1-merged không chỉ nâng cao giới hạn hiệu suất mà còn mở rộng phạm vi ứng dụng."
  },
  "flux-pro": {
    "description": "Mô hình tạo ảnh AI thương mại hàng đầu — chất lượng hình ảnh vô song và độ đa dạng đầu ra vượt trội."
  },
  "flux-pro-1.1": {
    "description": "Mô hình tạo ảnh AI chuyên nghiệp phiên bản nâng cấp — cung cấp chất lượng hình ảnh vượt trội và khả năng tuân thủ chính xác các gợi ý."
  },
  "flux-pro-1.1-ultra": {
    "description": "Tạo ảnh AI độ phân giải cực cao — hỗ trợ xuất ảnh 4 megapixel, tạo ảnh siêu nét trong vòng 10 giây."
  },
  "flux-schnell": {
    "description": "FLUX.1 [schnell] là mô hình ít bước tiên tiến nhất mã nguồn mở hiện nay, vượt trội so với các đối thủ cùng loại và thậm chí hơn cả các mô hình không tinh luyện mạnh như Midjourney v6.0 và DALL·E 3 (HD). Mô hình được tinh chỉnh đặc biệt để giữ lại toàn bộ đa dạng đầu ra giai đoạn tiền huấn luyện, so với các mô hình tiên tiến trên thị trường, FLUX.1 [schnell] cải thiện đáng kể chất lượng hình ảnh, tuân thủ chỉ dẫn, thay đổi kích thước/tỷ lệ, xử lý phông chữ và đa dạng đầu ra, mang đến trải nghiệm tạo hình ảnh sáng tạo phong phú hơn cho người dùng."
  },
  "flux.1-schnell": {
    "description": "FLUX.1-schnell, mô hình tạo hình ảnh hiệu suất cao, phù hợp với tạo nhanh hình ảnh đa phong cách."
  },
  "gemini-1.0-pro-001": {
    "description": "Gemini 1.0 Pro 001 (Tuning) cung cấp hiệu suất ổn định và có thể điều chỉnh, là lựa chọn lý tưởng cho các giải pháp nhiệm vụ phức tạp."
  },
  "gemini-1.0-pro-002": {
    "description": "Gemini 1.0 Pro 002 (Tuning) cung cấp hỗ trợ đa phương thức xuất sắc, tập trung vào việc giải quyết hiệu quả các nhiệm vụ phức tạp."
  },
  "gemini-1.0-pro-latest": {
    "description": "Gemini 1.0 Pro là mô hình AI hiệu suất cao của Google, được thiết kế để mở rộng cho nhiều nhiệm vụ."
  },
  "gemini-1.5-flash-001": {
    "description": "Gemini 1.5 Flash 001 là một mô hình đa phương thức hiệu quả, hỗ trợ mở rộng cho nhiều ứng dụng."
  },
  "gemini-1.5-flash-002": {
    "description": "Gemini 1.5 Flash 002 là một mô hình đa phương thức hiệu quả, hỗ trợ mở rộng cho nhiều ứng dụng."
  },
  "gemini-1.5-flash-8b": {
    "description": "Gemini 1.5 Flash 8B là một mô hình đa phương thức hiệu quả, hỗ trợ mở rộng cho nhiều ứng dụng."
  },
  "gemini-1.5-flash-8b-exp-0924": {
    "description": "Gemini 1.5 Flash 8B 0924 là mô hình thử nghiệm mới nhất, có sự cải thiện đáng kể về hiệu suất trong các trường hợp sử dụng văn bản và đa phương thức."
  },
  "gemini-1.5-flash-8b-latest": {
    "description": "Gemini 1.5 Flash 8B là một mô hình đa chế độ hiệu quả, hỗ trợ mở rộng ứng dụng rộng rãi."
  },
  "gemini-1.5-flash-exp-0827": {
    "description": "Gemini 1.5 Flash 0827 cung cấp khả năng xử lý đa phương tiện tối ưu, áp dụng cho nhiều tình huống tác vụ phức tạp."
  },
  "gemini-1.5-flash-latest": {
    "description": "Gemini 1.5 Flash là mô hình AI đa phương thức mới nhất của Google, có khả năng xử lý nhanh, hỗ trợ đầu vào văn bản, hình ảnh và video, phù hợp cho việc mở rộng hiệu quả cho nhiều nhiệm vụ."
  },
  "gemini-1.5-pro-001": {
    "description": "Gemini 1.5 Pro 001 là giải pháp AI đa phương thức có thể mở rộng, hỗ trợ nhiều nhiệm vụ phức tạp."
  },
  "gemini-1.5-pro-002": {
    "description": "Gemini 1.5 Pro 002 là mô hình sẵn sàng cho sản xuất mới nhất, cung cấp đầu ra chất lượng cao hơn, đặc biệt là trong các nhiệm vụ toán học, ngữ cảnh dài và thị giác."
  },
  "gemini-1.5-pro-exp-0801": {
    "description": "Gemini 1.5 Pro 0801 cung cấp khả năng xử lý đa phương tiện xuất sắc, mang lại tính linh hoạt cao hơn cho việc phát triển ứng dụng."
  },
  "gemini-1.5-pro-exp-0827": {
    "description": "Gemini 1.5 Pro 0827 kết hợp công nghệ tối ưu hóa mới nhất, mang lại khả năng xử lý dữ liệu đa phương tiện hiệu quả hơn."
  },
  "gemini-1.5-pro-latest": {
    "description": "Gemini 1.5 Pro hỗ trợ lên đến 2 triệu tokens, là lựa chọn lý tưởng cho mô hình đa phương thức trung bình, phù hợp cho hỗ trợ đa diện cho các nhiệm vụ phức tạp."
  },
  "gemini-2.0-flash": {
    "description": "Gemini 2.0 Flash cung cấp các tính năng và cải tiến thế hệ tiếp theo, bao gồm tốc độ vượt trội, sử dụng công cụ bản địa, tạo đa phương tiện và cửa sổ ngữ cảnh 1M token."
  },
  "gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash cung cấp các tính năng và cải tiến thế hệ tiếp theo, bao gồm tốc độ vượt trội, sử dụng công cụ bản địa, tạo đa phương tiện và cửa sổ ngữ cảnh 1M token."
  },
  "gemini-2.0-flash-exp": {
    "description": "Biến thể mô hình Gemini 2.0 Flash, được tối ưu hóa cho hiệu quả chi phí và độ trễ thấp."
  },
  "gemini-2.0-flash-exp-image-generation": {
    "description": "Mô hình thử nghiệm Gemini 2.0 Flash, hỗ trợ tạo hình ảnh"
  },
  "gemini-2.0-flash-lite": {
    "description": "Biến thể mô hình Gemini 2.0 Flash được tối ưu hóa cho hiệu quả chi phí và độ trễ thấp."
  },
  "gemini-2.0-flash-lite-001": {
    "description": "Biến thể mô hình Gemini 2.0 Flash được tối ưu hóa cho hiệu quả chi phí và độ trễ thấp."
  },
  "gemini-2.5-flash": {
    "description": "Gemini 2.5 Flash là mô hình có hiệu suất chi phí tốt nhất của Google, cung cấp đầy đủ các chức năng."
  },
  "gemini-2.5-flash-image": {
    "description": "Nano Banana là mô hình đa phương thức nguyên bản mới nhất, nhanh nhất và hiệu quả nhất của Google, cho phép bạn tạo và chỉnh sửa hình ảnh thông qua đối thoại."
  },
  "gemini-2.5-flash-image-preview": {
    "description": "Nano Banana là mô hình đa phương thức nguyên bản mới nhất, nhanh nhất và hiệu quả nhất của Google, cho phép bạn tạo và chỉnh sửa hình ảnh thông qua đối thoại."
  },
  "gemini-2.5-flash-image-preview:image": {
    "description": "Nano Banana là mô hình đa phương thức nguyên bản mới nhất, nhanh nhất và hiệu quả nhất của Google, cho phép bạn tạo và chỉnh sửa hình ảnh thông qua đối thoại."
  },
  "gemini-2.5-flash-image:image": {
    "description": "Nano Banana là mô hình đa phương thức nguyên bản mới nhất, nhanh nhất và hiệu quả nhất của Google, cho phép bạn tạo và chỉnh sửa hình ảnh thông qua đối thoại."
  },
  "gemini-2.5-flash-lite": {
    "description": "Gemini 2.5 Flash-Lite là mô hình nhỏ nhất và có hiệu suất chi phí tốt nhất của Google, được thiết kế dành cho việc sử dụng quy mô lớn."
  },
  "gemini-2.5-flash-lite-preview-06-17": {
    "description": "Gemini 2.5 Flash-Lite Preview là mô hình nhỏ nhất và có hiệu suất chi phí tốt nhất của Google, được thiết kế dành cho sử dụng quy mô lớn."
  },
  "gemini-2.5-flash-lite-preview-09-2025": {
    "description": "Phiên bản xem trước (25 tháng 9 năm 2025) của Gemini 2.5 Flash-Lite"
  },
  "gemini-2.5-flash-preview-04-17": {
    "description": "Gemini 2.5 Flash Preview là mô hình có giá trị tốt nhất của Google, cung cấp đầy đủ các tính năng."
  },
  "gemini-2.5-flash-preview-09-2025": {
    "description": "Phiên bản xem trước (25 tháng 9 năm 2025) của Gemini 2.5 Flash"
  },
  "gemini-2.5-pro": {
    "description": "Gemini 2.5 Pro là mô hình tư duy tiên tiến nhất của Google, có khả năng suy luận các vấn đề phức tạp trong lĩnh vực mã nguồn, toán học và STEM, cũng như phân tích các bộ dữ liệu lớn, kho mã và tài liệu bằng ngữ cảnh dài."
  },
  "gemini-2.5-pro-preview-03-25": {
    "description": "Gemini 2.5 Pro Preview là mô hình tư duy tiên tiến nhất của Google, có khả năng suy luận về mã, toán học và các vấn đề phức tạp trong lĩnh vực STEM, cũng như phân tích các tập dữ liệu lớn, kho mã và tài liệu bằng cách sử dụng ngữ cảnh dài."
  },
  "gemini-2.5-pro-preview-05-06": {
    "description": "Gemini 2.5 Pro Preview là mô hình tư duy tiên tiến nhất của Google, có khả năng suy luận về mã, toán học và các vấn đề phức tạp trong lĩnh vực STEM, cũng như phân tích các tập dữ liệu lớn, kho mã và tài liệu bằng cách sử dụng ngữ cảnh dài."
  },
  "gemini-2.5-pro-preview-06-05": {
    "description": "Gemini 2.5 Pro Preview là mô hình tư duy tiên tiến nhất của Google, có khả năng suy luận các vấn đề phức tạp trong lĩnh vực mã nguồn, toán học và STEM, cũng như phân tích dữ liệu lớn, kho mã và tài liệu với ngữ cảnh dài."
  },
  "gemini-3-pro-preview": {
    "description": "Gemini 3 Pro là mô hình thông minh nhất của Google, với khả năng suy luận tiên tiến hàng đầu và hiểu đa phương thức, cùng với các tính năng đại lý mạnh mẽ và mã hóa ngữ cảnh vượt trội."
  },
  "gemini-flash-latest": {
    "description": "Phiên bản mới nhất của Gemini Flash"
  },
  "gemini-flash-lite-latest": {
    "description": "Phiên bản mới nhất của Gemini Flash-Lite"
  },
  "gemini-pro-latest": {
    "description": "Phiên bản mới nhất của Gemini Pro"
  },
  "gemma-7b-it": {
    "description": "Gemma 7B phù hợp cho việc xử lý các nhiệm vụ quy mô vừa và nhỏ, đồng thời mang lại hiệu quả chi phí."
  },
  "gemma2": {
    "description": "Gemma 2 là mô hình hiệu quả do Google phát hành, bao gồm nhiều ứng dụng từ nhỏ đến xử lý dữ liệu phức tạp."
  },
  "gemma2-9b-it": {
    "description": "Gemma 2 9B là một mô hình được tối ưu hóa cho các nhiệm vụ cụ thể và tích hợp công cụ."
  },
  "gemma2:27b": {
    "description": "Gemma 2 là mô hình hiệu quả do Google phát hành, bao gồm nhiều ứng dụng từ nhỏ đến xử lý dữ liệu phức tạp."
  },
  "gemma2:2b": {
    "description": "Gemma 2 là mô hình hiệu quả do Google phát hành, bao gồm nhiều ứng dụng từ nhỏ đến xử lý dữ liệu phức tạp."
  },
  "generalv3": {
    "description": "Spark Pro là một mô hình ngôn ngữ lớn hiệu suất cao được tối ưu hóa cho các lĩnh vực chuyên môn, tập trung vào toán học, lập trình, y tế, giáo dục và nhiều lĩnh vực khác, đồng thời hỗ trợ tìm kiếm trực tuyến và các plugin tích hợp như thời tiết, ngày tháng. Mô hình đã được tối ưu hóa thể hiện xuất sắc và hiệu suất cao trong các nhiệm vụ hỏi đáp kiến thức phức tạp, hiểu ngôn ngữ và sáng tạo văn bản cấp cao, là lựa chọn lý tưởng cho các tình huống ứng dụng chuyên nghiệp."
  },
  "generalv3.5": {
    "description": "Spark3.5 Max là phiên bản toàn diện nhất, hỗ trợ tìm kiếm trực tuyến và nhiều plugin tích hợp. Khả năng cốt lõi đã được tối ưu hóa toàn diện cùng với thiết lập vai trò hệ thống và chức năng gọi hàm, giúp nó thể hiện xuất sắc và nổi bật trong nhiều tình huống ứng dụng phức tạp."
  },
  "glm-4": {
    "description": "GLM-4 là phiên bản flagship cũ phát hành vào tháng 1 năm 2024, hiện đã được GLM-4-0520 mạnh mẽ hơn thay thế."
  },
  "glm-4-0520": {
    "description": "GLM-4-0520 là phiên bản mô hình mới nhất, được thiết kế cho các nhiệm vụ phức tạp và đa dạng, thể hiện xuất sắc."
  },
  "glm-4-32b-0414": {
    "description": "GLM-4 32B 0414, phiên bản mô hình lớn đa năng dòng GLM, hỗ trợ sinh và hiểu văn bản đa nhiệm vụ."
  },
  "glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat thể hiện hiệu suất cao trong các lĩnh vực như ngữ nghĩa, toán học, suy luận, lập trình và tri thức. Ngoài ra còn hỗ trợ duyệt web, thực thi mã, gọi công cụ tùy chỉnh và suy luận văn bản dài. Hỗ trợ 26 ngôn ngữ bao gồm tiếng Nhật, tiếng Hàn, tiếng Đức."
  },
  "glm-4-air": {
    "description": "GLM-4-Air là phiên bản có giá trị sử dụng cao, hiệu suất gần giống GLM-4, cung cấp tốc độ nhanh và giá cả phải chăng."
  },
  "glm-4-air-250414": {
    "description": "GLM-4-Air là phiên bản có giá trị cao, hiệu suất gần tương đương với GLM-4, cung cấp tốc độ nhanh và giá cả phải chăng."
  },
  "glm-4-airx": {
    "description": "GLM-4-AirX cung cấp phiên bản hiệu quả của GLM-4-Air, tốc độ suy luận có thể đạt 2.6 lần."
  },
  "glm-4-alltools": {
    "description": "GLM-4-AllTools là một mô hình tác nhân đa chức năng, được tối ưu hóa để hỗ trợ lập kế hoạch chỉ dẫn phức tạp và gọi công cụ, như duyệt web, giải thích mã và sinh văn bản, phù hợp cho thực hiện nhiều nhiệm vụ."
  },
  "glm-4-flash": {
    "description": "GLM-4-Flash là lựa chọn lý tưởng cho các nhiệm vụ đơn giản, tốc độ nhanh nhất và giá cả phải chăng nhất."
  },
  "glm-4-flash-250414": {
    "description": "GLM-4-Flash là lựa chọn lý tưởng cho các nhiệm vụ đơn giản, nhanh nhất và miễn phí."
  },
  "glm-4-flashx": {
    "description": "GLM-4-FlashX là phiên bản nâng cấp của Flash, với tốc độ suy diễn siêu nhanh."
  },
  "glm-4-long": {
    "description": "GLM-4-Long hỗ trợ đầu vào văn bản siêu dài, phù hợp cho các nhiệm vụ ghi nhớ và xử lý tài liệu quy mô lớn."
  },
  "glm-4-plus": {
    "description": "GLM-4-Plus là mô hình flagship thông minh cao, có khả năng xử lý văn bản dài và nhiệm vụ phức tạp, hiệu suất được nâng cao toàn diện."
  },
  "glm-4.1v-thinking-flash": {
    "description": "Dòng mô hình GLM-4.1V-Thinking là mô hình VLM cấp 10 tỷ tham số mạnh nhất hiện biết, tích hợp các nhiệm vụ ngôn ngữ thị giác SOTA cùng cấp, bao gồm hiểu video, hỏi đáp hình ảnh, giải bài tập chuyên ngành, nhận dạng ký tự quang học (OCR), phân tích tài liệu và biểu đồ, tác nhân GUI, lập trình giao diện web frontend, định vị (Grounding) và nhiều nhiệm vụ khác, với khả năng vượt trội so với Qwen2.5-VL-72B có tham số gấp 8 lần. Thông qua công nghệ học tăng cường tiên tiến, mô hình nắm vững phương pháp suy luận chuỗi tư duy để nâng cao độ chính xác và sự phong phú của câu trả lời, vượt trội rõ rệt so với các mô hình truyền thống không có tính năng thinking về hiệu quả cuối cùng và khả năng giải thích."
  },
  "glm-4.1v-thinking-flashx": {
    "description": "Dòng mô hình GLM-4.1V-Thinking là mô hình VLM cấp 10 tỷ tham số mạnh nhất hiện biết, tích hợp các nhiệm vụ ngôn ngữ thị giác SOTA cùng cấp, bao gồm hiểu video, hỏi đáp hình ảnh, giải bài tập chuyên ngành, nhận dạng ký tự quang học (OCR), phân tích tài liệu và biểu đồ, tác nhân GUI, lập trình giao diện web frontend, định vị (Grounding) và nhiều nhiệm vụ khác, với khả năng vượt trội so với Qwen2.5-VL-72B có tham số gấp 8 lần. Thông qua công nghệ học tăng cường tiên tiến, mô hình nắm vững phương pháp suy luận chuỗi tư duy để nâng cao độ chính xác và sự phong phú của câu trả lời, vượt trội rõ rệt so với các mô hình truyền thống không có tính năng thinking về hiệu quả cuối cùng và khả năng giải thích."
  },
  "glm-4.5": {
    "description": "Mô hình chủ lực của Zhipu, hỗ trợ chuyển đổi chế độ suy nghĩ, năng lực tổng hợp đạt mức SOTA của các mô hình mã nguồn mở, độ dài ngữ cảnh lên đến 128K."
  },
  "glm-4.5-air": {
    "description": "Phiên bản nhẹ của GLM-4.5, cân bằng giữa hiệu suất và chi phí, có thể linh hoạt chuyển đổi mô hình suy nghĩ hỗn hợp."
  },
  "glm-4.5-airx": {
    "description": "Phiên bản tốc độ cao của GLM-4.5-Air, phản hồi nhanh hơn, thiết kế cho nhu cầu quy mô lớn và tốc độ cao."
  },
  "glm-4.5-flash": {
    "description": "Phiên bản miễn phí của GLM-4.5, thể hiện tốt trong các tác vụ suy luận, lập trình và tác nhân."
  },
  "glm-4.5-x": {
    "description": "Phiên bản tốc độ cao của GLM-4.5, vừa mạnh mẽ về hiệu suất, vừa đạt tốc độ tạo 100 token/giây."
  },
  "glm-4.5v": {
    "description": "Mô hình suy luận thị giác thế hệ mới của Zhipu dựa trên kiến trúc MOE, với tổng số tham số 106B và 12B tham số kích hoạt, đạt SOTA trong số các mô hình đa phương thức mã nguồn mở cùng cấp trên toàn cầu trên nhiều bộ đánh giá, bao gồm các nhiệm vụ phổ biến như hiểu ảnh, video, tài liệu và giao diện người dùng (GUI)."
  },
  "glm-4.6": {
    "description": "Mô hình chủ lực mới nhất của Zhipu GLM-4.6 (355B) vượt trội toàn diện so với thế hệ trước về mã hóa nâng cao, xử lý văn bản dài, suy luận và khả năng tác nhân, đặc biệt về năng lực lập trình đã đạt chuẩn Claude Sonnet 4, trở thành mô hình Coding hàng đầu trong nước."
  },
  "glm-4v": {
    "description": "GLM-4V cung cấp khả năng hiểu và suy luận hình ảnh mạnh mẽ, hỗ trợ nhiều nhiệm vụ hình ảnh."
  },
  "glm-4v-flash": {
    "description": "GLM-4V-Flash tập trung vào hiểu hình ảnh đơn lẻ một cách hiệu quả, phù hợp cho các tình huống phân tích hình ảnh nhanh chóng, chẳng hạn như phân tích hình ảnh theo thời gian thực hoặc xử lý hình ảnh hàng loạt."
  },
  "glm-4v-plus": {
    "description": "GLM-4V-Plus có khả năng hiểu nội dung video và nhiều hình ảnh, phù hợp cho các nhiệm vụ đa phương tiện."
  },
  "glm-4v-plus-0111": {
    "description": "GLM-4V-Plus có khả năng hiểu nội dung video và nhiều hình ảnh, phù hợp cho các nhiệm vụ đa phương tiện."
  },
  "glm-z1-air": {
    "description": "Mô hình suy luận: có khả năng suy luận mạnh mẽ, phù hợp cho các nhiệm vụ cần suy luận sâu."
  },
  "glm-z1-airx": {
    "description": "Suy luận siêu tốc: có tốc độ suy luận cực nhanh và hiệu quả suy luận mạnh mẽ."
  },
  "glm-z1-flash": {
    "description": "Dòng GLM-Z1 có khả năng suy luận phức tạp mạnh mẽ, thể hiện xuất sắc trong các lĩnh vực suy luận logic, toán học và lập trình."
  },
  "glm-z1-flashx": {
    "description": "Tốc độ cao, giá thấp: Phiên bản tăng cường Flash, tốc độ suy luận siêu nhanh, đảm bảo đồng thời nhanh hơn."
  },
  "glm-zero-preview": {
    "description": "GLM-Zero-Preview có khả năng suy luận phức tạp mạnh mẽ, thể hiện xuất sắc trong các lĩnh vực suy luận logic, toán học, lập trình."
  },
  "google/gemini-2.0-flash": {
    "description": "Gemini 2.0 Flash cung cấp các tính năng thế hệ tiếp theo và cải tiến, bao gồm tốc độ vượt trội, sử dụng công cụ tích hợp, tạo đa phương thức và cửa sổ ngữ cảnh 1 triệu token."
  },
  "google/gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash cung cấp các tính năng và cải tiến thế hệ tiếp theo, bao gồm tốc độ vượt trội, sử dụng công cụ bản địa, tạo đa phương tiện và cửa sổ ngữ cảnh 1M token."
  },
  "google/gemini-2.0-flash-exp:free": {
    "description": "Gemini 2.0 Flash Experimental là mô hình AI đa phương tiện thử nghiệm mới nhất của Google, có sự cải thiện về chất lượng so với các phiên bản trước, đặc biệt là đối với kiến thức thế giới, mã và ngữ cảnh dài."
  },
  "google/gemini-2.0-flash-lite": {
    "description": "Gemini 2.0 Flash Lite cung cấp các tính năng thế hệ tiếp theo và cải tiến, bao gồm tốc độ vượt trội, sử dụng công cụ tích hợp, tạo đa phương thức và cửa sổ ngữ cảnh 1 triệu token."
  },
  "google/gemini-2.5-flash": {
    "description": "Gemini 2.5 Flash là mô hình tư duy cung cấp khả năng toàn diện xuất sắc. Nó được thiết kế để cân bằng giữa giá cả và hiệu suất, hỗ trợ đa phương thức và cửa sổ ngữ cảnh 1 triệu token."
  },
  "google/gemini-2.5-flash-image-preview": {
    "description": "Mô hình thử nghiệm Gemini 2.5 Flash, hỗ trợ tạo hình ảnh."
  },
  "google/gemini-2.5-flash-lite": {
    "description": "Gemini 2.5 Flash-Lite là mô hình cân bằng, độ trễ thấp với ngân sách tư duy và kết nối công cụ có thể cấu hình (ví dụ: Google Search có căn cứ và thực thi mã). Nó hỗ trợ đầu vào đa phương thức và cung cấp cửa sổ ngữ cảnh 1 triệu token."
  },
  "google/gemini-2.5-flash-preview": {
    "description": "Gemini 2.5 Flash là mô hình chủ lực tiên tiến nhất của Google, được thiết kế cho suy luận nâng cao, lập trình, toán học và các nhiệm vụ khoa học. Nó bao gồm khả năng 'suy nghĩ' tích hợp, cho phép nó cung cấp phản hồi với độ chính xác cao hơn và xử lý ngữ cảnh chi tiết hơn.\n\nLưu ý: Mô hình này có hai biến thể: suy nghĩ và không suy nghĩ. Giá đầu ra có sự khác biệt đáng kể tùy thuộc vào việc khả năng suy nghĩ có được kích hoạt hay không. Nếu bạn chọn biến thể tiêu chuẩn (không có hậu tố ':thinking'), mô hình sẽ rõ ràng tránh việc tạo ra các token suy nghĩ.\n\nĐể tận dụng khả năng suy nghĩ và nhận các token suy nghĩ, bạn phải chọn biến thể ':thinking', điều này sẽ tạo ra giá đầu ra suy nghĩ cao hơn.\n\nNgoài ra, Gemini 2.5 Flash có thể được cấu hình thông qua tham số 'số token tối đa cho suy luận', như đã mô tả trong tài liệu (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."
  },
  "google/gemini-2.5-flash-preview:thinking": {
    "description": "Gemini 2.5 Flash là mô hình chủ lực tiên tiến nhất của Google, được thiết kế cho suy luận nâng cao, lập trình, toán học và các nhiệm vụ khoa học. Nó bao gồm khả năng 'suy nghĩ' tích hợp, cho phép nó cung cấp phản hồi với độ chính xác cao hơn và xử lý ngữ cảnh chi tiết hơn.\n\nLưu ý: Mô hình này có hai biến thể: suy nghĩ và không suy nghĩ. Giá đầu ra có sự khác biệt đáng kể tùy thuộc vào việc khả năng suy nghĩ có được kích hoạt hay không. Nếu bạn chọn biến thể tiêu chuẩn (không có hậu tố ':thinking'), mô hình sẽ rõ ràng tránh việc tạo ra các token suy nghĩ.\n\nĐể tận dụng khả năng suy nghĩ và nhận các token suy nghĩ, bạn phải chọn biến thể ':thinking', điều này sẽ tạo ra giá đầu ra suy nghĩ cao hơn.\n\nNgoài ra, Gemini 2.5 Flash có thể được cấu hình thông qua tham số 'số token tối đa cho suy luận', như đã mô tả trong tài liệu (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."
  },
  "google/gemini-2.5-pro": {
    "description": "Gemini 2.5 Pro là mô hình Gemini suy luận tiên tiến nhất của chúng tôi, có khả năng giải quyết các vấn đề phức tạp. Nó có cửa sổ ngữ cảnh 2 triệu token, hỗ trợ đầu vào đa phương thức bao gồm văn bản, hình ảnh, âm thanh, video và tài liệu PDF."
  },
  "google/gemini-2.5-pro-preview": {
    "description": "Gemini 2.5 Pro Preview là mô hình tư duy tiên tiến nhất của Google, có khả năng suy luận các vấn đề phức tạp trong lĩnh vực mã hóa, toán học và STEM, cũng như phân tích các bộ dữ liệu lớn, kho mã và tài liệu bằng ngữ cảnh dài."
  },
  "google/gemini-embedding-001": {
    "description": "Mô hình nhúng tiên tiến, thể hiện hiệu suất xuất sắc trong các nhiệm vụ tiếng Anh, đa ngôn ngữ và mã hóa."
  },
  "google/gemini-flash-1.5": {
    "description": "Gemini 1.5 Flash cung cấp khả năng xử lý đa phương thức được tối ưu hóa, phù hợp cho nhiều tình huống nhiệm vụ phức tạp."
  },
  "google/gemini-pro-1.5": {
    "description": "Gemini 1.5 Pro kết hợp công nghệ tối ưu hóa mới nhất, mang lại khả năng xử lý dữ liệu đa phương thức hiệu quả hơn."
  },
  "google/gemma-2-27b": {
    "description": "Gemma 2 là mô hình hiệu quả do Google phát hành, bao gồm nhiều ứng dụng từ ứng dụng nhỏ đến xử lý dữ liệu phức tạp."
  },
  "google/gemma-2-27b-it": {
    "description": "Gemma 2 tiếp tục triết lý thiết kế nhẹ và hiệu quả."
  },
  "google/gemma-2-2b-it": {
    "description": "Mô hình tinh chỉnh hướng dẫn nhẹ của Google"
  },
  "google/gemma-2-9b": {
    "description": "Gemma 2 là mô hình hiệu quả do Google phát hành, bao gồm nhiều ứng dụng từ ứng dụng nhỏ đến xử lý dữ liệu phức tạp."
  },
  "google/gemma-2-9b-it": {
    "description": "Gemma 2 là một loạt mô hình văn bản mã nguồn mở nhẹ của Google."
  },
  "google/gemma-2-9b-it:free": {
    "description": "Gemma 2 là loạt mô hình văn bản mã nguồn mở nhẹ của Google."
  },
  "google/gemma-2b-it": {
    "description": "Gemma Instruct (2B) cung cấp khả năng xử lý chỉ dẫn cơ bản, phù hợp cho các ứng dụng nhẹ."
  },
  "google/gemma-3-12b-it": {
    "description": "Gemma 3 12B là một mô hình ngôn ngữ mã nguồn mở của Google, thiết lập tiêu chuẩn mới về hiệu quả và hiệu suất."
  },
  "google/gemma-3-27b-it": {
    "description": "Gemma 3 27B là một mô hình ngôn ngữ mã nguồn mở của Google, thiết lập tiêu chuẩn mới về hiệu suất và hiệu quả."
  },
  "google/text-embedding-005": {
    "description": "Mô hình nhúng văn bản tập trung vào tiếng Anh, được tối ưu cho các nhiệm vụ mã hóa và ngôn ngữ tiếng Anh."
  },
  "google/text-multilingual-embedding-002": {
    "description": "Mô hình nhúng văn bản đa ngôn ngữ được tối ưu cho các nhiệm vụ đa ngôn ngữ, hỗ trợ nhiều ngôn ngữ."
  },
  "gpt-3.5-turbo": {
    "description": "GPT 3.5 Turbo, phù hợp cho nhiều nhiệm vụ sinh và hiểu văn bản, hiện tại trỏ đến gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-0125": {
    "description": "GPT 3.5 Turbo, phù hợp cho nhiều nhiệm vụ sinh và hiểu văn bản, hiện tại trỏ đến gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-1106": {
    "description": "GPT 3.5 Turbo, phù hợp cho nhiều nhiệm vụ sinh và hiểu văn bản, hiện tại trỏ đến gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-instruct": {
    "description": "GPT 3.5 Turbo, phù hợp cho nhiều nhiệm vụ sinh và hiểu văn bản, hiện tại trỏ đến gpt-3.5-turbo-0125."
  },
  "gpt-35-turbo": {
    "description": "GPT 3.5 Turbo, mô hình hiệu quả do OpenAI cung cấp, phù hợp cho các tác vụ trò chuyện và tạo văn bản, hỗ trợ gọi hàm song song."
  },
  "gpt-35-turbo-16k": {
    "description": "GPT 3.5 Turbo 16k, mô hình tạo văn bản dung lượng cao, phù hợp cho các nhiệm vụ phức tạp."
  },
  "gpt-4": {
    "description": "GPT-4 cung cấp một cửa sổ ngữ cảnh lớn hơn, có khả năng xử lý các đầu vào văn bản dài hơn, phù hợp cho các tình huống cần tích hợp thông tin rộng rãi và phân tích dữ liệu."
  },
  "gpt-4-0125-preview": {
    "description": "Mô hình GPT-4 Turbo mới nhất có chức năng hình ảnh. Hiện tại, các yêu cầu hình ảnh có thể sử dụng chế độ JSON và gọi hàm. GPT-4 Turbo là một phiên bản nâng cao, cung cấp hỗ trợ chi phí hiệu quả cho các nhiệm vụ đa phương tiện. Nó tìm thấy sự cân bằng giữa độ chính xác và hiệu quả, phù hợp cho các ứng dụng cần tương tác theo thời gian thực."
  },
  "gpt-4-0613": {
    "description": "GPT-4 cung cấp một cửa sổ ngữ cảnh lớn hơn, có khả năng xử lý các đầu vào văn bản dài hơn, phù hợp cho các tình huống cần tích hợp thông tin rộng rãi và phân tích dữ liệu."
  },
  "gpt-4-1106-preview": {
    "description": "Mô hình GPT-4 Turbo mới nhất có chức năng hình ảnh. Hiện tại, các yêu cầu hình ảnh có thể sử dụng chế độ JSON và gọi hàm. GPT-4 Turbo là một phiên bản nâng cao, cung cấp hỗ trợ chi phí hiệu quả cho các nhiệm vụ đa phương tiện. Nó tìm thấy sự cân bằng giữa độ chính xác và hiệu quả, phù hợp cho các ứng dụng cần tương tác theo thời gian thực."
  },
  "gpt-4-32k": {
    "description": "GPT-4 cung cấp một cửa sổ ngữ cảnh lớn hơn, có khả năng xử lý các đầu vào văn bản dài hơn, phù hợp cho các tình huống cần tích hợp thông tin rộng rãi và phân tích dữ liệu."
  },
  "gpt-4-32k-0613": {
    "description": "GPT-4 cung cấp một cửa sổ ngữ cảnh lớn hơn, có khả năng xử lý các đầu vào văn bản dài hơn, phù hợp cho các tình huống cần tích hợp thông tin rộng rãi và phân tích dữ liệu."
  },
  "gpt-4-turbo": {
    "description": "Mô hình GPT-4 Turbo mới nhất có chức năng hình ảnh. Hiện tại, các yêu cầu hình ảnh có thể sử dụng chế độ JSON và gọi hàm. GPT-4 Turbo là một phiên bản nâng cao, cung cấp hỗ trợ chi phí hiệu quả cho các nhiệm vụ đa phương tiện. Nó tìm thấy sự cân bằng giữa độ chính xác và hiệu quả, phù hợp cho các ứng dụng cần tương tác theo thời gian thực."
  },
  "gpt-4-turbo-2024-04-09": {
    "description": "Mô hình GPT-4 Turbo mới nhất có chức năng hình ảnh. Hiện tại, các yêu cầu hình ảnh có thể sử dụng chế độ JSON và gọi hàm. GPT-4 Turbo là một phiên bản nâng cao, cung cấp hỗ trợ chi phí hiệu quả cho các nhiệm vụ đa phương tiện. Nó tìm thấy sự cân bằng giữa độ chính xác và hiệu quả, phù hợp cho các ứng dụng cần tương tác theo thời gian thực."
  },
  "gpt-4-turbo-preview": {
    "description": "Mô hình GPT-4 Turbo mới nhất có chức năng hình ảnh. Hiện tại, các yêu cầu hình ảnh có thể sử dụng chế độ JSON và gọi hàm. GPT-4 Turbo là một phiên bản nâng cao, cung cấp hỗ trợ chi phí hiệu quả cho các nhiệm vụ đa phương tiện. Nó tìm thấy sự cân bằng giữa độ chính xác và hiệu quả, phù hợp cho các ứng dụng cần tương tác theo thời gian thực."
  },
  "gpt-4-vision-preview": {
    "description": "Mô hình GPT-4 Turbo mới nhất có chức năng hình ảnh. Hiện tại, các yêu cầu hình ảnh có thể sử dụng chế độ JSON và gọi hàm. GPT-4 Turbo là một phiên bản nâng cao, cung cấp hỗ trợ chi phí hiệu quả cho các nhiệm vụ đa phương tiện. Nó tìm thấy sự cân bằng giữa độ chính xác và hiệu quả, phù hợp cho các ứng dụng cần tương tác theo thời gian thực."
  },
  "gpt-4.1": {
    "description": "GPT-4.1 là mô hình hàng đầu của chúng tôi cho các nhiệm vụ phức tạp. Nó rất phù hợp để giải quyết vấn đề đa lĩnh vực."
  },
  "gpt-4.1-mini": {
    "description": "GPT-4.1 mini cung cấp sự cân bằng giữa trí tuệ, tốc độ và chi phí, khiến nó trở thành mô hình hấp dẫn cho nhiều trường hợp sử dụng."
  },
  "gpt-4.1-nano": {
    "description": "GPT-4.1 mini cung cấp sự cân bằng giữa trí tuệ, tốc độ và chi phí, khiến nó trở thành mô hình hấp dẫn cho nhiều trường hợp sử dụng."
  },
  "gpt-4.5-preview": {
    "description": "GPT-4.5-preview là mô hình tổng quát mới nhất, sở hữu kiến thức toàn cầu sâu rộng và khả năng hiểu ý định người dùng tốt hơn, mạnh trong các nhiệm vụ sáng tạo và trong việc lập kế hoạch cho các tác nhân. Kiến thức của mô hình được cập nhật đến tháng 10 năm 2023."
  },
  "gpt-4o": {
    "description": "ChatGPT-4o là một mô hình động, được cập nhật theo thời gian thực để giữ phiên bản mới nhất. Nó kết hợp khả năng hiểu và sinh ngôn ngữ mạnh mẽ, phù hợp cho các ứng dụng quy mô lớn, bao gồm dịch vụ khách hàng, giáo dục và hỗ trợ kỹ thuật."
  },
  "gpt-4o-2024-05-13": {
    "description": "ChatGPT-4o là một mô hình động, được cập nhật theo thời gian thực để giữ phiên bản mới nhất. Nó kết hợp khả năng hiểu và sinh ngôn ngữ mạnh mẽ, phù hợp cho các ứng dụng quy mô lớn, bao gồm dịch vụ khách hàng, giáo dục và hỗ trợ kỹ thuật."
  },
  "gpt-4o-2024-08-06": {
    "description": "ChatGPT-4o là một mô hình động, được cập nhật theo thời gian thực để giữ phiên bản mới nhất. Nó kết hợp khả năng hiểu và sinh ngôn ngữ mạnh mẽ, phù hợp cho các ứng dụng quy mô lớn, bao gồm dịch vụ khách hàng, giáo dục và hỗ trợ kỹ thuật."
  },
  "gpt-4o-2024-11-20": {
    "description": "ChatGPT-4o là một mô hình động, được cập nhật liên tục để giữ phiên bản mới nhất. Nó kết hợp khả năng hiểu và tạo ngôn ngữ mạnh mẽ, phù hợp cho nhiều ứng dụng quy mô lớn, bao gồm dịch vụ khách hàng, giáo dục và hỗ trợ kỹ thuật."
  },
  "gpt-4o-audio-preview": {
    "description": "Mô hình GPT-4o Audio Preview, hỗ trợ đầu vào và đầu ra âm thanh."
  },
  "gpt-4o-mini": {
    "description": "GPT-4o mini là mô hình mới nhất do OpenAI phát hành sau GPT-4 Omni, hỗ trợ đầu vào hình ảnh và đầu ra văn bản. Là mô hình nhỏ gọn tiên tiến nhất của họ, nó rẻ hơn nhiều so với các mô hình tiên tiến gần đây khác và rẻ hơn hơn 60% so với GPT-3.5 Turbo. Nó giữ lại trí thông minh tiên tiến nhất trong khi có giá trị sử dụng đáng kể. GPT-4o mini đạt 82% điểm trong bài kiểm tra MMLU và hiện đứng cao hơn GPT-4 về sở thích trò chuyện."
  },
  "gpt-4o-mini-audio-preview": {
    "description": "Mô hình GPT-4o mini Audio, hỗ trợ đầu vào và đầu ra âm thanh."
  },
  "gpt-4o-mini-realtime-preview": {
    "description": "Phiên bản thời gian thực của GPT-4o-mini, hỗ trợ đầu vào và đầu ra âm thanh và văn bản theo thời gian thực."
  },
  "gpt-4o-mini-search-preview": {
    "description": "GPT-4o mini phiên bản xem trước tìm kiếm là mô hình được huấn luyện chuyên biệt để hiểu và thực thi các truy vấn tìm kiếm trên web, sử dụng API Chat Completions. Ngoài phí token, truy vấn tìm kiếm trên web còn tính phí theo mỗi lần gọi công cụ."
  },
  "gpt-4o-mini-transcribe": {
    "description": "GPT-4o Mini Transcribe là mô hình chuyển đổi giọng nói thành văn bản sử dụng GPT-4o để phiên âm âm thanh. So với mô hình Whisper gốc, nó cải thiện tỷ lệ lỗi từ và nâng cao khả năng nhận diện ngôn ngữ cũng như độ chính xác. Sử dụng nó để có bản phiên âm chính xác hơn."
  },
  "gpt-4o-mini-tts": {
    "description": "GPT-4o mini TTS là mô hình chuyển văn bản thành giọng nói dựa trên GPT-4o mini, cung cấp sinh âm thanh cao cấp với chi phí thấp hơn."
  },
  "gpt-4o-realtime-preview": {
    "description": "Phiên bản thời gian thực của GPT-4o, hỗ trợ đầu vào và đầu ra âm thanh và văn bản theo thời gian thực."
  },
  "gpt-4o-realtime-preview-2024-10-01": {
    "description": "Phiên bản thời gian thực của GPT-4o, hỗ trợ đầu vào và đầu ra âm thanh và văn bản theo thời gian thực."
  },
  "gpt-4o-realtime-preview-2025-06-03": {
    "description": "Phiên bản thời gian thực của GPT-4o, hỗ trợ nhập xuất âm thanh và văn bản theo thời gian thực."
  },
  "gpt-4o-search-preview": {
    "description": "GPT-4o phiên bản xem trước tìm kiếm là mô hình được huấn luyện chuyên biệt để hiểu và thực thi các truy vấn tìm kiếm trên web, sử dụng API Chat Completions. Ngoài phí token, truy vấn tìm kiếm trên web còn tính phí theo mỗi lần gọi công cụ."
  },
  "gpt-4o-transcribe": {
    "description": "GPT-4o Transcribe là mô hình chuyển đổi giọng nói thành văn bản sử dụng GPT-4o để phiên âm âm thanh. So với mô hình Whisper gốc, nó cải thiện tỷ lệ lỗi từ và nâng cao khả năng nhận diện ngôn ngữ cũng như độ chính xác. Sử dụng nó để có bản phiên âm chính xác hơn."
  },
  "gpt-5": {
    "description": "Mô hình tốt nhất cho các nhiệm vụ mã hóa và đại diện đa lĩnh vực. GPT-5 đạt bước tiến vượt bậc về độ chính xác, tốc độ, suy luận, nhận diện ngữ cảnh, tư duy có cấu trúc và giải quyết vấn đề."
  },
  "gpt-5-chat": {
    "description": "GPT-5 Chat là phiên bản xem trước được tối ưu hóa cho các tình huống hội thoại. Hỗ trợ đầu vào văn bản và hình ảnh, chỉ xuất ra văn bản, phù hợp cho chatbot và các ứng dụng AI đối thoại."
  },
  "gpt-5-chat-latest": {
    "description": "Mô hình GPT-5 được sử dụng trong ChatGPT. Kết hợp khả năng hiểu và tạo ngôn ngữ mạnh mẽ, phù hợp cho các ứng dụng tương tác đối thoại."
  },
  "gpt-5-codex": {
    "description": "GPT-5 Codex là phiên bản GPT-5 được tối ưu cho các tác vụ mã hóa đại diện trong môi trường Codex hoặc tương tự."
  },
  "gpt-5-mini": {
    "description": "Phiên bản GPT-5 nhanh hơn và tiết kiệm chi phí hơn, phù hợp cho các nhiệm vụ được xác định rõ ràng. Cung cấp tốc độ phản hồi nhanh hơn trong khi vẫn giữ chất lượng đầu ra cao."
  },
  "gpt-5-nano": {
    "description": "Phiên bản GPT-5 nhanh nhất và tiết kiệm chi phí nhất. Rất phù hợp cho các ứng dụng cần phản hồi nhanh và nhạy cảm về chi phí."
  },
  "gpt-5-pro": {
    "description": "GPT-5 pro sử dụng nhiều tài nguyên tính toán hơn để suy nghĩ sâu sắc hơn và liên tục cung cấp các câu trả lời tốt hơn."
  },
  "gpt-5.1": {
    "description": "GPT-5.1 — Mô hình hàng đầu được tối ưu hóa cho các tác vụ lập trình và agent, hỗ trợ cường độ suy luận có thể cấu hình và ngữ cảnh dài hơn."
  },
  "gpt-5.1-chat-latest": {
    "description": "GPT-5.1 Chat: Biến thể GPT-5.1 dành cho ChatGPT, phù hợp với các tình huống trò chuyện."
  },
  "gpt-5.1-codex": {
    "description": "GPT-5.1 Codex: Phiên bản GPT-5.1 được tối ưu hóa cho các tác vụ lập trình mang tính agent, có thể sử dụng trong Responses API cho các quy trình làm việc mã hóa/đại lý phức tạp hơn."
  },
  "gpt-5.1-codex-mini": {
    "description": "GPT-5.1 Codex mini: Biến thể Codex nhỏ gọn hơn và tiết kiệm chi phí hơn, được tối ưu hóa cho các tác vụ lập trình mang tính agent."
  },
  "gpt-audio": {
    "description": "GPT Audio là mô hình trò chuyện chung hỗ trợ đầu vào và đầu ra âm thanh, có thể sử dụng âm thanh I/O trong API Chat Completions."
  },
  "gpt-image-1": {
    "description": "Mô hình tạo hình ảnh đa phương thức nguyên bản của ChatGPT"
  },
  "gpt-image-1-mini": {
    "description": "Phiên bản tiết kiệm chi phí hơn của GPT Image 1, hỗ trợ gốc đầu vào văn bản và hình ảnh, đồng thời tạo đầu ra hình ảnh."
  },
  "gpt-oss-120b": {
    "description": "Cần đăng ký để trải nghiệm. GPT-OSS-120B là mô hình ngôn ngữ mã nguồn mở quy mô lớn do OpenAI phát hành, có khả năng tạo văn bản mạnh mẽ."
  },
  "gpt-oss-20b": {
    "description": "Cần đăng ký để trải nghiệm. GPT-OSS-20B là mô hình ngôn ngữ mã nguồn mở quy mô trung bình do OpenAI phát hành, có khả năng tạo văn bản hiệu quả."
  },
  "gpt-oss:120b": {
    "description": "GPT-OSS 120B là mô hình ngôn ngữ lớn mã nguồn mở do OpenAI phát hành, sử dụng công nghệ lượng tử hóa MXFP4, thuộc dòng mô hình hàng đầu. Cần môi trường đa GPU hoặc máy trạm hiệu năng cao để vận hành, có hiệu suất vượt trội trong suy luận phức tạp, tạo mã và xử lý đa ngôn ngữ, hỗ trợ gọi hàm nâng cao và tích hợp bộ công cụ."
  },
  "gpt-oss:20b": {
    "description": "GPT-OSS 20B là mô hình ngôn ngữ lớn mã nguồn mở do OpenAI phát hành, sử dụng kỹ thuật lượng tử hóa MXFP4, phù hợp chạy trên GPU tiêu dùng cao cấp hoặc Apple Silicon Mac. Mô hình này thể hiện xuất sắc trong tạo hội thoại, viết mã và các tác vụ suy luận, hỗ trợ gọi hàm và sử dụng công cụ."
  },
  "gpt-realtime": {
    "description": "Mô hình thời gian thực chung, hỗ trợ đầu vào và đầu ra văn bản, âm thanh theo thời gian thực, đồng thời hỗ trợ đầu vào hình ảnh."
  },
  "grok-2-image-1212": {
    "description": "Mô hình tạo hình ảnh mới nhất của chúng tôi có thể tạo ra hình ảnh sống động và chân thực dựa trên gợi ý văn bản. Nó thể hiện xuất sắc trong các lĩnh vực marketing, mạng xã hội và giải trí."
  },
  "grok-2-vision-1212": {
    "description": "Mô hình này đã được cải thiện về độ chính xác, khả năng tuân thủ hướng dẫn và khả năng đa ngôn ngữ."
  },
  "grok-3": {
    "description": "Mô hình chủ lực, xuất sắc trong trích xuất dữ liệu, lập trình và tóm tắt văn bản cho các ứng dụng doanh nghiệp, sở hữu kiến thức sâu rộng trong các lĩnh vực tài chính, y tế, pháp lý và khoa học."
  },
  "grok-3-mini": {
    "description": "Mô hình nhẹ, suy nghĩ trước khi trả lời. Chạy nhanh, thông minh, phù hợp cho các nhiệm vụ logic không đòi hỏi kiến thức chuyên sâu và có thể truy xuất được chuỗi suy nghĩ gốc."
  },
  "grok-4": {
    "description": "Mô hình hàng đầu mới nhất và mạnh mẽ nhất của chúng tôi, thể hiện xuất sắc trong xử lý ngôn ngữ tự nhiên, tính toán toán học và suy luận — một lựa chọn toàn diện hoàn hảo."
  },
  "grok-4-0709": {
    "description": "Grok 4 của xAI, có khả năng suy luận mạnh mẽ."
  },
  "grok-4-1-fast-non-reasoning": {
    "description": "Mô hình đa phương thức tiên tiến, được tối ưu hóa đặc biệt để gọi công cụ đại diện hiệu suất cao."
  },
  "grok-4-1-fast-reasoning": {
    "description": "Mô hình đa phương thức tiên tiến, được tối ưu hóa đặc biệt để gọi công cụ đại diện hiệu suất cao."
  },
  "grok-4-fast-non-reasoning": {
    "description": "Chúng tôi rất vui mừng giới thiệu Grok 4 Fast, bước tiến mới nhất của chúng tôi trong các mô hình suy luận hiệu quả về chi phí."
  },
  "grok-4-fast-reasoning": {
    "description": "Chúng tôi rất vui mừng giới thiệu Grok 4 Fast, bước tiến mới nhất của chúng tôi trong các mô hình suy luận hiệu quả về chi phí."
  },
  "grok-code-fast-1": {
    "description": "Chúng tôi rất vui mừng giới thiệu grok-code-fast-1, một mô hình suy luận nhanh và tiết kiệm chi phí, thể hiện xuất sắc trong việc mã hóa đại lý."
  },
  "groq/compound": {
    "description": "Compound là một hệ thống AI tổng hợp, được hỗ trợ bởi nhiều mô hình công khai có sẵn trong GroqCloud, có khả năng sử dụng công cụ một cách thông minh và chọn lọc để trả lời các truy vấn của người dùng."
  },
  "groq/compound-mini": {
    "description": "Compound-mini là một hệ thống AI tổng hợp, được hỗ trợ bởi các mô hình công khai có sẵn trong GroqCloud, có khả năng sử dụng công cụ một cách thông minh và chọn lọc để trả lời các truy vấn của người dùng."
  },
  "gryphe/mythomax-l2-13b": {
    "description": "MythoMax l2 13B là mô hình ngôn ngữ kết hợp giữa sáng tạo và trí thông minh, kết hợp nhiều mô hình hàng đầu."
  },
  "hunyuan-a13b": {
    "description": "Hunyuan là mô hình suy luận hỗn hợp đầu tiên, phiên bản nâng cấp của hunyuan-standard-256K, với tổng số tham số 80 tỷ và 13 tỷ tham số kích hoạt. Mặc định ở chế độ suy nghĩ chậm, hỗ trợ chuyển đổi giữa chế độ suy nghĩ nhanh và chậm qua tham số hoặc chỉ thị, cách chuyển đổi là thêm / no_think trước truy vấn; năng lực tổng thể được cải thiện toàn diện so với thế hệ trước, đặc biệt là về toán học, khoa học, hiểu văn bản dài và năng lực tác nhân."
  },
  "hunyuan-code": {
    "description": "Mô hình sinh mã mới nhất của Hunyuan, được huấn luyện trên 200B dữ liệu mã chất lượng cao, trải qua nửa năm huấn luyện dữ liệu SFT chất lượng cao, độ dài cửa sổ ngữ cảnh tăng lên 8K, đứng đầu trong các chỉ số đánh giá tự động sinh mã cho năm ngôn ngữ lớn; trong đánh giá chất lượng cao của 10 tiêu chí mã tổng hợp cho năm ngôn ngữ, hiệu suất nằm trong nhóm đầu."
  },
  "hunyuan-functioncall": {
    "description": "Mô hình FunctionCall với cấu trúc MOE mới nhất của Hunyuan, được huấn luyện trên dữ liệu FunctionCall chất lượng cao, với cửa sổ ngữ cảnh đạt 32K, dẫn đầu trong nhiều chỉ số đánh giá."
  },
  "hunyuan-large": {
    "description": "Mô hình Hunyuan-large có tổng số tham số khoảng 389B, số tham số kích hoạt khoảng 52B, là mô hình MoE mã nguồn mở có quy mô tham số lớn nhất và hiệu quả nhất trong ngành hiện nay."
  },
  "hunyuan-large-longcontext": {
    "description": "Chuyên xử lý các nhiệm vụ văn bản dài như tóm tắt tài liệu và hỏi đáp tài liệu, đồng thời cũng có khả năng xử lý các nhiệm vụ tạo văn bản chung. Thể hiện xuất sắc trong phân tích và tạo nội dung văn bản dài, có thể đáp ứng hiệu quả các yêu cầu xử lý nội dung dài phức tạp và chi tiết."
  },
  "hunyuan-large-vision": {
    "description": "Mô hình này phù hợp với các kịch bản hiểu hình ảnh và văn bản, là mô hình ngôn ngữ thị giác lớn dựa trên Hunyuan Large, hỗ trợ đầu vào nhiều hình ảnh với độ phân giải tùy ý cùng văn bản, tạo ra nội dung văn bản, tập trung vào các nhiệm vụ liên quan đến hiểu hình ảnh và văn bản, có sự cải thiện đáng kể về khả năng hiểu đa ngôn ngữ hình ảnh và văn bản."
  },
  "hunyuan-lite": {
    "description": "Nâng cấp lên cấu trúc MOE, với cửa sổ ngữ cảnh 256k, dẫn đầu nhiều mô hình mã nguồn mở trong các bộ đánh giá NLP, mã, toán học, ngành nghề, v.v."
  },
  "hunyuan-lite-vision": {
    "description": "Mô hình đa phương thức mới nhất 7B của Hunyuan, cửa sổ ngữ cảnh 32K, hỗ trợ đối thoại đa phương thức trong các tình huống tiếng Trung và tiếng Anh, nhận diện đối tượng hình ảnh, hiểu biết tài liệu và bảng biểu, toán học đa phương thức, v.v., với các chỉ số đánh giá vượt trội hơn các mô hình cạnh tranh 7B ở nhiều khía cạnh."
  },
  "hunyuan-pro": {
    "description": "Mô hình văn bản dài MOE-32K với quy mô hàng triệu tham số. Đạt được mức độ dẫn đầu tuyệt đối trên nhiều benchmark, có khả năng xử lý các lệnh phức tạp và suy diễn, có khả năng toán học phức tạp, hỗ trợ functioncall, được tối ưu hóa cho các lĩnh vực dịch thuật đa ngôn ngữ, tài chính, pháp lý và y tế."
  },
  "hunyuan-role": {
    "description": "Mô hình đóng vai trò mới nhất của Hunyuan, được tinh chỉnh và huấn luyện bởi Hunyuan, dựa trên mô hình Hunyuan kết hợp với bộ dữ liệu tình huống đóng vai trò để tăng cường huấn luyện, có hiệu suất cơ bản tốt hơn trong các tình huống đóng vai trò."
  },
  "hunyuan-standard": {
    "description": "Sử dụng chiến lược định tuyến tốt hơn, đồng thời giảm thiểu vấn đề cân bằng tải và đồng nhất chuyên gia. Về mặt văn bản dài, chỉ số tìm kiếm đạt 99.9%. MOE-32K có giá trị hiệu suất tương đối cao, cân bằng giữa hiệu quả và giá cả, có thể xử lý đầu vào văn bản dài."
  },
  "hunyuan-standard-256K": {
    "description": "Sử dụng chiến lược định tuyến tốt hơn, đồng thời giảm thiểu vấn đề cân bằng tải và đồng nhất chuyên gia. Về mặt văn bản dài, chỉ số tìm kiếm đạt 99.9%. MOE-256K đã có bước đột phá về độ dài và hiệu quả, mở rộng đáng kể độ dài đầu vào có thể."
  },
  "hunyuan-standard-vision": {
    "description": "Mô hình đa phương thức mới nhất của Hunyuan, hỗ trợ trả lời đa ngôn ngữ, khả năng tiếng Trung và tiếng Anh cân bằng."
  },
  "hunyuan-t1-20250321": {
    "description": "Xây dựng toàn diện khả năng mô hình cho cả khoa học tự nhiên và khoa học xã hội, khả năng nắm bắt thông tin văn bản dài mạnh mẽ. Hỗ trợ suy luận và giải đáp các vấn đề khoa học như toán học, logic, khoa học và mã với nhiều độ khó khác nhau."
  },
  "hunyuan-t1-20250403": {
    "description": "Nâng cao khả năng tạo mã cấp dự án; cải thiện chất lượng viết văn bản; nâng cao khả năng hiểu chủ đề văn bản đa vòng, tuân thủ chỉ thị toB và hiểu từ ngữ; tối ưu hóa vấn đề đầu ra hỗn hợp phồn thể và giản thể, cũng như hỗn hợp tiếng Trung và tiếng Anh."
  },
  "hunyuan-t1-20250529": {
    "description": "Tối ưu hóa sáng tạo văn bản, viết luận, cải thiện khả năng lập trình frontend, toán học, suy luận logic và các kỹ năng khoa học tự nhiên, nâng cao khả năng tuân thủ chỉ dẫn."
  },
  "hunyuan-t1-20250711": {
    "description": "Nâng cao đáng kể khả năng toán học, logic và mã hóa khó, tối ưu độ ổn định đầu ra mô hình, cải thiện khả năng xử lý văn bản dài."
  },
  "hunyuan-t1-latest": {
    "description": "Nâng cao đáng kể năng lực của mô hình chính và mô hình suy nghĩ chậm trong các lĩnh vực toán học khó, suy luận phức tạp, mã hóa khó, tuân thủ chỉ thị và chất lượng sáng tạo văn bản."
  },
  "hunyuan-t1-vision-20250619": {
    "description": "Phiên bản mới nhất của Hunyuan t1-vision là mô hình suy nghĩ sâu đa phương thức, hỗ trợ chuỗi tư duy dài nguyên bản đa phương thức, cải thiện toàn diện so với phiên bản mặc định thế hệ trước."
  },
  "hunyuan-t1-vision-20250916": {
    "description": "Phiên bản mới nhất của mô hình tư duy thị giác Hunyuan t1-vision đã được nâng cấp toàn diện so với phiên bản trước trong các nhiệm vụ như hỏi đáp hình ảnh, định vị thị giác, OCR, biểu đồ, giải bài tập qua ảnh và sáng tạo từ hình ảnh. Khả năng xử lý tiếng Anh và các ngôn ngữ ít phổ biến cũng được cải thiện rõ rệt."
  },
  "hunyuan-turbo": {
    "description": "Phiên bản xem trước của thế hệ mới mô hình ngôn ngữ lớn Hunyuan, sử dụng cấu trúc mô hình chuyên gia hỗn hợp (MoE) hoàn toàn mới, so với hunyuan-pro, hiệu suất suy diễn nhanh hơn và hiệu quả mạnh mẽ hơn."
  },
  "hunyuan-turbo-20241223": {
    "description": "Phiên bản này tối ưu hóa: quy mô chỉ thị dữ liệu, nâng cao đáng kể khả năng tổng quát của mô hình; nâng cao đáng kể khả năng toán học, lập trình, và suy luận logic; tối ưu hóa khả năng hiểu biết văn bản và từ ngữ; tối ưu hóa chất lượng tạo nội dung văn bản."
  },
  "hunyuan-turbo-latest": {
    "description": "Tối ưu hóa trải nghiệm chung, bao gồm hiểu biết NLP, sáng tạo văn bản, trò chuyện, hỏi đáp kiến thức, dịch thuật, và các lĩnh vực khác; nâng cao tính nhân văn, tối ưu hóa trí tuệ cảm xúc của mô hình; cải thiện khả năng làm rõ khi ý định không rõ ràng; nâng cao khả năng xử lý các vấn đề phân tích từ ngữ; nâng cao chất lượng và khả năng tương tác trong sáng tạo; cải thiện trải nghiệm đa vòng."
  },
  "hunyuan-turbo-vision": {
    "description": "Mô hình ngôn ngữ hình ảnh thế hệ mới của Hunyuan, sử dụng cấu trúc mô hình chuyên gia hỗn hợp (MoE) hoàn toàn mới, nâng cao toàn diện khả năng nhận diện cơ bản, sáng tạo nội dung, hỏi đáp kiến thức, và phân tích suy luận so với mô hình thế hệ trước."
  },
  "hunyuan-turbos-20250313": {
    "description": "Thống nhất phong cách các bước giải toán, tăng cường hỏi đáp toán học đa vòng. Tối ưu hóa phong cách trả lời trong sáng tác văn bản, loại bỏ cảm giác AI, tăng thêm tính văn chương."
  },
  "hunyuan-turbos-20250416": {
    "description": "Nâng cấp nền tảng tiền huấn luyện, tăng cường khả năng hiểu và tuân thủ chỉ thị của nền tảng; tăng cường năng lực các môn khoa học tự nhiên như toán học, lập trình, logic, khoa học trong giai đoạn căn chỉnh; cải thiện chất lượng sáng tạo văn học, hiểu văn bản, độ chính xác dịch thuật, hỏi đáp kiến thức và các năng lực khoa học xã hội; tăng cường năng lực Agent trong các lĩnh vực, đặc biệt là khả năng hiểu đối thoại đa vòng."
  },
  "hunyuan-turbos-20250604": {
    "description": "Nâng cấp nền tảng tiền huấn luyện, cải thiện khả năng viết và đọc hiểu, tăng cường đáng kể năng lực lập trình và khoa học tự nhiên, tiếp tục nâng cao khả năng tuân thủ các chỉ dẫn phức tạp."
  },
  "hunyuan-turbos-20250926": {
    "description": "Nâng cấp chất lượng dữ liệu nền tiền huấn luyện. Tối ưu chiến lược huấn luyện giai đoạn posttrain, liên tục nâng cao khả năng Agent, tiếng Anh và các ngôn ngữ nhỏ, tuân thủ chỉ thị, mã hóa và năng lực khoa học tự nhiên."
  },
  "hunyuan-turbos-latest": {
    "description": "hunyuan-TurboS là phiên bản mới nhất của mô hình lớn hỗn hợp Hunyuan, có khả năng tư duy mạnh mẽ hơn và trải nghiệm tốt hơn."
  },
  "hunyuan-turbos-longtext-128k-20250325": {
    "description": "Chuyên xử lý các nhiệm vụ văn bản dài như tóm tắt tài liệu và hỏi đáp tài liệu, đồng thời cũng có khả năng xử lý các nhiệm vụ tạo văn bản chung. Nó thể hiện xuất sắc trong việc phân tích và tạo ra văn bản dài, có khả năng đáp ứng hiệu quả các yêu cầu xử lý nội dung dài phức tạp và chi tiết."
  },
  "hunyuan-turbos-role-plus": {
    "description": "Mô hình nhập vai phiên bản mới nhất của Hunyuan, được tinh chỉnh chính thức bởi Hunyuan, dựa trên mô hình Hunyuan kết hợp với bộ dữ liệu kịch bản nhập vai để tăng cường huấn luyện, mang lại hiệu quả cơ bản tốt hơn trong các kịch bản nhập vai."
  },
  "hunyuan-turbos-vision": {
    "description": "Mô hình này phù hợp với các kịch bản hiểu hình ảnh và văn bản, là mô hình ngôn ngữ thị giác hàng đầu thế hệ mới dựa trên Hunyuan turbos mới nhất, tập trung vào các nhiệm vụ liên quan đến hiểu hình ảnh và văn bản, bao gồm nhận dạng thực thể dựa trên hình ảnh, hỏi đáp kiến thức, sáng tạo nội dung, giải bài tập qua ảnh chụp, với cải tiến toàn diện so với thế hệ trước."
  },
  "hunyuan-turbos-vision-20250619": {
    "description": "Phiên bản mới nhất của Hunyuan turbos-vision là mô hình ngôn ngữ thị giác hàng đầu, cải thiện toàn diện so với phiên bản mặc định thế hệ trước trong các nhiệm vụ liên quan đến hiểu hình ảnh và văn bản, bao gồm nhận dạng thực thể dựa trên hình ảnh, hỏi đáp kiến thức, sáng tạo nội dung, giải bài tập qua ảnh chụp."
  },
  "hunyuan-vision": {
    "description": "Mô hình đa phương thức mới nhất của Hunyuan, hỗ trợ đầu vào hình ảnh + văn bản để tạo ra nội dung văn bản."
  },
  "image-01": {
    "description": "Mô hình tạo hình ảnh hoàn toàn mới, thể hiện hình ảnh tinh tế, hỗ trợ tạo hình ảnh từ văn bản và hình ảnh."
  },
  "image-01-live": {
    "description": "Mô hình tạo hình ảnh với chất lượng tinh tế, hỗ trợ tạo hình ảnh từ văn bản và thiết lập phong cách hình ảnh."
  },
  "imagen-4.0-fast-generate-001": {
    "description": "Imagen — dòng mô hình tạo ảnh từ văn bản thế hệ thứ 4, phiên bản nhanh."
  },
  "imagen-4.0-generate-001": {
    "description": "Dòng mô hình Imagen thế hệ thứ tư chuyển văn bản thành hình ảnh"
  },
  "imagen-4.0-generate-preview-06-06": {
    "description": "Dòng mô hình tạo hình ảnh từ văn bản thế hệ thứ tư của Imagen"
  },
  "imagen-4.0-ultra-generate-001": {
    "description": "Imagen thế hệ thứ 4, dòng mô hình chuyển văn bản sang hình ảnh — phiên bản Ultra"
  },
  "imagen-4.0-ultra-generate-preview-06-06": {
    "description": "Phiên bản Ultra của dòng mô hình tạo hình ảnh từ văn bản thế hệ thứ tư của Imagen"
  },
  "inception/mercury-coder-small": {
    "description": "Mercury Coder Small là lựa chọn lý tưởng cho các nhiệm vụ tạo mã, gỡ lỗi và tái cấu trúc với độ trễ tối thiểu."
  },
  "inclusionAI/Ling-1T": {
    "description": "Ling-1T là mô hình non-thinking hàng đầu đầu tiên trong dòng sản phẩm \"Linh 2.0\", sở hữu tổng cộng 1 nghìn tỷ tham số và khoảng 50 tỷ tham số hoạt động cho mỗi token. Được xây dựng trên kiến trúc Linh 2.0, Ling-1T hướng đến việc vượt qua giới hạn của suy luận hiệu quả và nhận thức có thể mở rộng. Ling-1T-base được huấn luyện trên hơn 20 nghìn tỷ token chất lượng cao, đòi hỏi suy luận chuyên sâu."
  },
  "inclusionAI/Ling-flash-2.0": {
    "description": "Ling-flash-2.0 là mô hình thứ ba trong dòng kiến trúc Ling 2.0 do đội ngũ Bailing của Ant Group phát hành. Đây là mô hình chuyên gia hỗn hợp (MoE) với tổng số tham số lên đến 100 tỷ, nhưng mỗi token chỉ kích hoạt 6.1 tỷ tham số (không bao gồm embedding là 4.8 tỷ). Là mô hình cấu hình nhẹ, Ling-flash-2.0 thể hiện hiệu năng ngang hoặc vượt trội so với các mô hình dày đặc (Dense) 40 tỷ tham số và các mô hình MoE quy mô lớn hơn trong nhiều bài đánh giá uy tín. Mô hình này nhằm khám phá con đường hiệu quả trong bối cảnh quan niệm “mô hình lớn đồng nghĩa với tham số lớn” thông qua thiết kế kiến trúc và chiến lược huấn luyện tối ưu."
  },
  "inclusionAI/Ling-mini-2.0": {
    "description": "Ling-mini-2.0 là mô hình ngôn ngữ lớn hiệu năng cao kích thước nhỏ dựa trên kiến trúc MoE. Nó có tổng số 16 tỷ tham số, nhưng mỗi token chỉ kích hoạt 1.4 tỷ tham số (không bao gồm embedding là 789 triệu), từ đó đạt tốc độ sinh nhanh vượt trội. Nhờ thiết kế MoE hiệu quả và dữ liệu huấn luyện quy mô lớn, chất lượng cao, mặc dù tham số kích hoạt chỉ 1.4 tỷ, Ling-mini-2.0 vẫn thể hiện hiệu năng hàng đầu trong các tác vụ hạ nguồn, có thể so sánh với các mô hình dense dưới 10 tỷ tham số và các mô hình MoE quy mô lớn hơn."
  },
  "inclusionAI/Ring-1T": {
    "description": "Ring-1T là mô hình tư duy mã nguồn mở quy mô nghìn tỷ tham số do nhóm Bailing phát triển. Dựa trên kiến trúc Linh 2.0 và mô hình nền tảng Ling-1T-base, mô hình này có tổng cộng 1 nghìn tỷ tham số và 50 tỷ tham số hoạt động, hỗ trợ cửa sổ ngữ cảnh lên đến 128K. Mô hình được tối ưu hóa thông qua học tăng cường với phần thưởng có thể xác minh ở quy mô lớn."
  },
  "inclusionAI/Ring-flash-2.0": {
    "description": "Ring-flash-2.0 là mô hình tư duy hiệu năng cao được tối ưu sâu dựa trên Ling-flash-2.0-base. Nó sử dụng kiến trúc chuyên gia hỗn hợp (MoE) với tổng số 100 tỷ tham số, nhưng mỗi lần suy luận chỉ kích hoạt 6.1 tỷ tham số. Mô hình này áp dụng thuật toán độc quyền icepop, giải quyết vấn đề không ổn định trong huấn luyện tăng cường (RL) của các mô hình MoE lớn, giúp năng lực suy luận phức tạp được cải thiện liên tục trong quá trình huấn luyện dài hạn. Ring-flash-2.0 đạt bước đột phá đáng kể trong các bài kiểm tra chuẩn khó như thi toán, tạo mã và suy luận logic, hiệu năng không chỉ vượt các mô hình dense hàng đầu dưới 40 tỷ tham số mà còn có thể sánh ngang các mô hình MoE mã nguồn mở quy mô lớn và các mô hình tư duy hiệu năng cao đóng nguồn. Mặc dù tập trung vào suy luận phức tạp, mô hình cũng thể hiện tốt trong các tác vụ sáng tạo viết lách. Ngoài ra, nhờ thiết kế kiến trúc hiệu quả, Ring-flash-2.0 vừa cung cấp hiệu năng mạnh mẽ vừa đạt tốc độ suy luận cao, giảm đáng kể chi phí triển khai mô hình tư duy trong các kịch bản tải cao."
  },
  "internlm/internlm2_5-7b-chat": {
    "description": "InternLM2.5 cung cấp giải pháp đối thoại thông minh cho nhiều tình huống."
  },
  "internlm2.5-latest": {
    "description": "Dòng mô hình mới nhất của chúng tôi, có hiệu suất suy luận xuất sắc, hỗ trợ độ dài ngữ cảnh 1M và khả năng theo dõi chỉ dẫn và gọi công cụ mạnh mẽ hơn."
  },
  "internlm3-latest": {
    "description": "Dòng mô hình mới nhất của chúng tôi, có hiệu suất suy luận xuất sắc, dẫn đầu trong số các mô hình mã nguồn mở cùng cấp. Mặc định chỉ đến mô hình InternLM3 mới nhất mà chúng tôi đã phát hành."
  },
  "internvl2.5-38b-mpo": {
    "description": "InternVL2.5 38B MPO, mô hình tiền huấn luyện đa phương thức, hỗ trợ các nhiệm vụ suy luận hình ảnh-văn bản phức tạp."
  },
  "internvl2.5-latest": {
    "description": "Phiên bản InternVL2.5 mà chúng tôi vẫn đang duy trì, có hiệu suất xuất sắc và ổn định. Mặc định chỉ đến mô hình InternVL2.5 mới nhất của chúng tôi, hiện tại chỉ đến internvl2.5-78b."
  },
  "internvl3-14b": {
    "description": "InternVL3 14B, mô hình đa phương thức quy mô trung bình, cân bằng giữa hiệu suất và chi phí."
  },
  "internvl3-1b": {
    "description": "InternVL3 1B, mô hình đa phương thức nhẹ, phù hợp với triển khai trong môi trường tài nguyên hạn chế."
  },
  "internvl3-38b": {
    "description": "InternVL3 38B, mô hình đa phương thức mã nguồn mở quy mô lớn, phù hợp với các nhiệm vụ hiểu hình ảnh-văn bản độ chính xác cao."
  },
  "internvl3-latest": {
    "description": "Chúng tôi vừa phát hành mô hình lớn đa phương thức mới nhất, có khả năng hiểu hình ảnh và văn bản mạnh mẽ hơn, khả năng hiểu hình ảnh theo chuỗi thời gian dài, hiệu suất tương đương với các mô hình đóng nguồn hàng đầu. Mặc định chỉ đến mô hình InternVL mới nhất của chúng tôi, hiện tại chỉ đến internvl3-78b."
  },
  "irag-1.0": {
    "description": "ERNIE iRAG, mô hình sinh tăng cường truy xuất hình ảnh, hỗ trợ tìm kiếm bằng hình ảnh, truy xuất hình ảnh-văn bản và sinh nội dung."
  },
  "jamba-large": {
    "description": "Mô hình mạnh mẽ và tiên tiến nhất của chúng tôi, được thiết kế đặc biệt để xử lý các nhiệm vụ phức tạp cấp doanh nghiệp, với hiệu suất xuất sắc."
  },
  "jamba-mini": {
    "description": "Mô hình hiệu quả nhất trong cùng phân khúc, cân bằng giữa tốc độ và chất lượng, có kích thước nhỏ hơn."
  },
  "jina-deepsearch-v1": {
    "description": "Tìm kiếm sâu kết hợp tìm kiếm trên mạng, đọc và suy luận, có thể thực hiện điều tra toàn diện. Bạn có thể coi nó như một đại lý, nhận nhiệm vụ nghiên cứu của bạn - nó sẽ thực hiện tìm kiếm rộng rãi và qua nhiều lần lặp lại trước khi đưa ra câu trả lời. Quá trình này liên quan đến nghiên cứu liên tục, suy luận và giải quyết vấn đề từ nhiều góc độ. Điều này khác biệt hoàn toàn với việc tạo ra câu trả lời trực tiếp từ dữ liệu đã được huấn luyện trước của các mô hình lớn tiêu chuẩn và các hệ thống RAG truyền thống dựa vào tìm kiếm bề mặt một lần."
  },
  "kimi-k2": {
    "description": "Kimi-K2 là mô hình nền tảng kiến trúc MoE do Moonshot AI phát hành, có khả năng mã hóa và đại lý vượt trội, tổng tham số 1T, tham số kích hoạt 32B. Trong các bài kiểm tra chuẩn về suy luận kiến thức chung, lập trình, toán học và đại lý, hiệu suất của mô hình K2 vượt trội so với các mô hình mã nguồn mở phổ biến khác."
  },
  "kimi-k2-0711-preview": {
    "description": "kimi-k2 là mô hình cơ sở kiến trúc MoE với khả năng mã hóa và Agent cực mạnh, tổng số tham số 1T, tham số kích hoạt 32B. Trong các bài kiểm tra hiệu năng chuẩn về suy luận kiến thức chung, lập trình, toán học, Agent và các lĩnh vực chính khác, mô hình K2 vượt trội hơn các mô hình mã nguồn mở phổ biến khác."
  },
  "kimi-k2-0905-preview": {
    "description": "Mô hình kimi-k2-0905-preview có độ dài ngữ cảnh 256k, sở hữu năng lực Agentic Coding mạnh mẽ hơn, mã front-end đẹp mắt và thực dụng hơn, cùng khả năng hiểu ngữ cảnh tốt hơn."
  },
  "kimi-k2-instruct": {
    "description": "Kimi K2 Instruct, mô hình suy luận chính thức của Kimi, hỗ trợ ngữ cảnh dài, mã nguồn, hỏi đáp và nhiều tình huống khác."
  },
  "kimi-k2-turbo-preview": {
    "description": "kimi-k2 là một mô hình nền tảng kiến trúc MoE với khả năng xử lý mã và Agent rất mạnh, tổng số tham số 1T, tham số kích hoạt 32B. Trong các bài kiểm tra chuẩn về hiệu năng ở các hạng mục chính như suy luận kiến thức tổng quát, lập trình, toán học và Agent, mô hình K2 cho hiệu năng vượt trội so với các mô hình mã nguồn mở phổ biến khác."
  },
  "kimi-k2:1t": {
    "description": "Kimi K2 là mô hình ngôn ngữ chuyên gia hỗn hợp quy mô lớn (MoE) do AI Mặt Trăng Tối phát triển, với tổng cộng 1 nghìn tỷ tham số và 32 tỷ tham số kích hoạt mỗi lần truyền tiến. Nó được tối ưu hóa cho khả năng đại lý, bao gồm sử dụng công cụ nâng cao, suy luận và tổng hợp mã."
  },
  "kimi-latest": {
    "description": "Sản phẩm trợ lý thông minh Kimi sử dụng mô hình lớn Kimi mới nhất, có thể chứa các tính năng chưa ổn định. Hỗ trợ hiểu hình ảnh, đồng thời tự động chọn mô hình 8k/32k/128k làm mô hình tính phí dựa trên độ dài ngữ cảnh yêu cầu."
  },
  "kimi-thinking-preview": {
    "description": "Mô hình kimi-thinking-preview do Moon's Dark Side cung cấp, có khả năng suy luận đa phương thức và suy luận tổng quát, nổi bật với khả năng suy luận sâu, giúp giải quyết nhiều vấn đề khó khăn hơn."
  },
  "learnlm-1.5-pro-experimental": {
    "description": "LearnLM là một mô hình ngôn ngữ thử nghiệm, chuyên biệt cho các nhiệm vụ, được đào tạo để tuân theo các nguyên tắc khoa học học tập, có thể tuân theo các chỉ dẫn hệ thống trong các tình huống giảng dạy và học tập, đóng vai trò như một người hướng dẫn chuyên gia."
  },
  "learnlm-2.0-flash-experimental": {
    "description": "LearnLM là một mô hình ngôn ngữ thử nghiệm, chuyên biệt cho nhiệm vụ, được đào tạo để tuân theo các nguyên tắc khoa học học tập, có thể tuân theo hướng dẫn hệ thống trong các tình huống giảng dạy và học tập, đóng vai trò như một người hướng dẫn chuyên gia."
  },
  "lite": {
    "description": "Spark Lite là một mô hình ngôn ngữ lớn nhẹ, có độ trễ cực thấp và khả năng xử lý hiệu quả, hoàn toàn miễn phí và mở, hỗ trợ chức năng tìm kiếm trực tuyến theo thời gian thực. Đặc điểm phản hồi nhanh của nó giúp nó nổi bật trong các ứng dụng suy diễn trên thiết bị có công suất thấp và tinh chỉnh mô hình, mang lại hiệu quả chi phí và trải nghiệm thông minh xuất sắc cho người dùng, đặc biệt trong các tình huống hỏi đáp kiến thức, tạo nội dung và tìm kiếm."
  },
  "llama-3.1-70b-versatile": {
    "description": "Llama 3.1 70B cung cấp khả năng suy luận AI mạnh mẽ hơn, phù hợp cho các ứng dụng phức tạp, hỗ trợ xử lý tính toán cực lớn và đảm bảo hiệu quả và độ chính xác cao."
  },
  "llama-3.1-8b-instant": {
    "description": "Llama 3.1 8B là một mô hình hiệu suất cao, cung cấp khả năng sinh văn bản nhanh chóng, rất phù hợp cho các tình huống ứng dụng cần hiệu quả quy mô lớn và tiết kiệm chi phí."
  },
  "llama-3.1-instruct": {
    "description": "Mô hình Llama 3.1 được tối ưu hóa cho các tình huống đối thoại, vượt trội hơn nhiều mô hình trò chuyện nguồn mở hiện có trong các bài kiểm tra chuẩn ngành phổ biến."
  },
  "llama-3.2-11b-vision-instruct": {
    "description": "Khả năng suy luận hình ảnh xuất sắc trên hình ảnh độ phân giải cao, phù hợp cho các ứng dụng hiểu biết hình ảnh."
  },
  "llama-3.2-11b-vision-preview": {
    "description": "Llama 3.2 được thiết kế để xử lý các nhiệm vụ kết hợp dữ liệu hình ảnh và văn bản. Nó thể hiện xuất sắc trong các nhiệm vụ mô tả hình ảnh và hỏi đáp hình ảnh, vượt qua rào cản giữa tạo ngôn ngữ và suy luận hình ảnh."
  },
  "llama-3.2-90b-vision-instruct": {
    "description": "Khả năng suy luận hình ảnh tiên tiến dành cho các ứng dụng đại lý hiểu biết hình ảnh."
  },
  "llama-3.2-90b-vision-preview": {
    "description": "Llama 3.2 được thiết kế để xử lý các nhiệm vụ kết hợp dữ liệu hình ảnh và văn bản. Nó thể hiện xuất sắc trong các nhiệm vụ mô tả hình ảnh và hỏi đáp hình ảnh, vượt qua rào cản giữa tạo ngôn ngữ và suy luận hình ảnh."
  },
  "llama-3.2-vision-instruct": {
    "description": "Mô hình Llama 3.2-Vision đã được tối ưu hóa để nhận dạng hình ảnh, suy luận hình ảnh, mô tả hình ảnh và trả lời các câu hỏi thông thường liên quan đến hình ảnh."
  },
  "llama-3.3-70b": {
    "description": "Llama 3.3 70B: Mô hình Llama cỡ trung, cân bằng giữa khả năng suy luận và hiệu suất xử lý."
  },
  "llama-3.3-70b-versatile": {
    "description": "Mô hình ngôn ngữ lớn Meta Llama 3.3 (LLM) đa ngôn ngữ là mô hình tạo ra dựa trên 70B (đầu vào/đầu ra văn bản) đã được huấn luyện và điều chỉnh theo chỉ dẫn. Mô hình thuần văn bản Llama 3.3 được tối ưu hóa cho các trường hợp hội thoại đa ngôn ngữ và vượt trội hơn nhiều mô hình trò chuyện mã nguồn mở và đóng khác trên các tiêu chuẩn ngành thông thường."
  },
  "llama-3.3-instruct": {
    "description": "Mô hình Llama 3.3 được tối ưu hóa cho các tình huống đối thoại, và đã vượt qua nhiều mô hình trò chuyện nguồn mở hiện có trong các bài kiểm tra chuẩn ngành phổ biến."
  },
  "llama-4-scout-17b-16e-instruct": {
    "description": "Llama 4 Scout: Mô hình hiệu suất cao thuộc dòng Llama, lý tưởng cho các tình huống yêu cầu thông lượng cao và độ trễ thấp."
  },
  "llama3-70b-8192": {
    "description": "Meta Llama 3 70B cung cấp khả năng xử lý phức tạp vô song, được thiết kế riêng cho các dự án yêu cầu cao."
  },
  "llama3-8b-8192": {
    "description": "Meta Llama 3 8B mang lại hiệu suất suy luận chất lượng cao, phù hợp cho nhu cầu ứng dụng đa dạng."
  },
  "llama3-groq-70b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 70B Tool Use cung cấp khả năng gọi công cụ mạnh mẽ, hỗ trợ xử lý hiệu quả cho các nhiệm vụ phức tạp."
  },
  "llama3-groq-8b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 8B Tool Use là mô hình được tối ưu hóa cho việc sử dụng công cụ hiệu quả, hỗ trợ tính toán song song nhanh chóng."
  },
  "llama3.1": {
    "description": "Llama 3.1 là mô hình tiên tiến do Meta phát hành, hỗ trợ lên đến 405B tham số, có thể áp dụng cho các cuộc đối thoại phức tạp, dịch đa ngôn ngữ và phân tích dữ liệu."
  },
  "llama3.1-8b": {
    "description": "Llama 3.1 8B: Phiên bản Llama nhỏ gọn, độ trễ thấp, phù hợp với các tình huống suy luận trực tuyến nhẹ và tương tác thời gian thực."
  },
  "llama3.1:405b": {
    "description": "Llama 3.1 là mô hình tiên tiến do Meta phát hành, hỗ trợ lên đến 405B tham số, có thể áp dụng cho các cuộc đối thoại phức tạp, dịch đa ngôn ngữ và phân tích dữ liệu."
  },
  "llama3.1:70b": {
    "description": "Llama 3.1 là mô hình tiên tiến do Meta phát hành, hỗ trợ lên đến 405B tham số, có thể áp dụng cho các cuộc đối thoại phức tạp, dịch đa ngôn ngữ và phân tích dữ liệu."
  },
  "llava": {
    "description": "LLaVA là mô hình đa phương thức kết hợp bộ mã hóa hình ảnh và Vicuna, phục vụ cho việc hiểu biết mạnh mẽ về hình ảnh và ngôn ngữ."
  },
  "llava-v1.5-7b-4096-preview": {
    "description": "LLaVA 1.5 7B cung cấp khả năng xử lý hình ảnh tích hợp, tạo ra đầu ra phức tạp thông qua đầu vào thông tin hình ảnh."
  },
  "llava:13b": {
    "description": "LLaVA là mô hình đa phương thức kết hợp bộ mã hóa hình ảnh và Vicuna, phục vụ cho việc hiểu biết mạnh mẽ về hình ảnh và ngôn ngữ."
  },
  "llava:34b": {
    "description": "LLaVA là mô hình đa phương thức kết hợp bộ mã hóa hình ảnh và Vicuna, phục vụ cho việc hiểu biết mạnh mẽ về hình ảnh và ngôn ngữ."
  },
  "magistral-medium-latest": {
    "description": "Magistral Medium 1.2 là mô hình suy luận tiên tiến do Mistral AI phát hành vào tháng 9 năm 2025, có hỗ trợ thị giác."
  },
  "magistral-small-2509": {
    "description": "Magistral Small 1.2 là mô hình suy luận nhỏ mã nguồn mở do Mistral AI phát hành vào tháng 9 năm 2025, có hỗ trợ thị giác."
  },
  "mathstral": {
    "description": "MathΣtral được thiết kế cho nghiên cứu khoa học và suy luận toán học, cung cấp khả năng tính toán hiệu quả và giải thích kết quả."
  },
  "max-32k": {
    "description": "Spark Max 32K được cấu hình với khả năng xử lý ngữ cảnh lớn, có khả năng hiểu ngữ cảnh và suy luận logic mạnh mẽ hơn, hỗ trợ đầu vào văn bản 32K tokens, phù hợp cho việc đọc tài liệu dài, hỏi đáp kiến thức riêng tư và các tình huống khác."
  },
  "megrez-3b-instruct": {
    "description": "Megrez 3B Instruct là mô hình hiệu quả với số lượng tham số nhỏ do Wuwen Xinqiong phát triển."
  },
  "meituan/longcat-flash-chat": {
    "description": "Longcat Flash Chat là mô hình nền không tư duy do Meituan mã nguồn mở, được tối ưu hóa cho tương tác hội thoại và nhiệm vụ của tác nhân, nổi bật trong việc gọi công cụ và các tình huống tương tác nhiều vòng phức tạp."
  },
  "meta-llama-3-70b-instruct": {
    "description": "Mô hình 70 tỷ tham số mạnh mẽ, xuất sắc trong lý luận, lập trình và các ứng dụng ngôn ngữ rộng lớn."
  },
  "meta-llama-3-8b-instruct": {
    "description": "Mô hình 8 tỷ tham số đa năng, tối ưu hóa cho các tác vụ đối thoại và tạo văn bản."
  },
  "meta-llama-3.1-405b-instruct": {
    "description": "Các mô hình văn bản chỉ được tinh chỉnh theo hướng dẫn Llama 3.1 được tối ưu hóa cho các trường hợp sử dụng đối thoại đa ngôn ngữ và vượt trội hơn nhiều mô hình trò chuyện mã nguồn mở và đóng có sẵn trên các tiêu chuẩn ngành phổ biến."
  },
  "meta-llama-3.1-70b-instruct": {
    "description": "Các mô hình văn bản chỉ được tinh chỉnh theo hướng dẫn Llama 3.1 được tối ưu hóa cho các trường hợp sử dụng đối thoại đa ngôn ngữ và vượt trội hơn nhiều mô hình trò chuyện mã nguồn mở và đóng có sẵn trên các tiêu chuẩn ngành phổ biến."
  },
  "meta-llama-3.1-8b-instruct": {
    "description": "Các mô hình văn bản chỉ được tinh chỉnh theo hướng dẫn Llama 3.1 được tối ưu hóa cho các trường hợp sử dụng đối thoại đa ngôn ngữ và vượt trội hơn nhiều mô hình trò chuyện mã nguồn mở và đóng có sẵn trên các tiêu chuẩn ngành phổ biến."
  },
  "meta-llama/Llama-2-13b-chat-hf": {
    "description": "LLaMA-2 Chat (13B) cung cấp khả năng xử lý ngôn ngữ xuất sắc và trải nghiệm tương tác tuyệt vời."
  },
  "meta-llama/Llama-2-70b-hf": {
    "description": "LLaMA-2 cung cấp khả năng xử lý ngôn ngữ tuyệt vời và trải nghiệm tương tác xuất sắc."
  },
  "meta-llama/Llama-3-70b-chat-hf": {
    "description": "LLaMA-3 Chat (70B) là mô hình trò chuyện mạnh mẽ, hỗ trợ các nhu cầu đối thoại phức tạp."
  },
  "meta-llama/Llama-3-8b-chat-hf": {
    "description": "LLaMA-3 Chat (8B) cung cấp hỗ trợ đa ngôn ngữ, bao gồm nhiều lĩnh vực kiến thức phong phú."
  },
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 được thiết kế để xử lý các tác vụ kết hợp dữ liệu hình ảnh và văn bản. Nó có khả năng xuất sắc trong các tác vụ mô tả hình ảnh và trả lời câu hỏi hình ảnh, vượt qua khoảng cách giữa tạo ngôn ngữ và suy luận hình ảnh."
  },
  "meta-llama/Llama-3.2-3B-Instruct-Turbo": {
    "description": "LLaMA 3.2 được thiết kế để xử lý các tác vụ kết hợp dữ liệu hình ảnh và văn bản. Nó có khả năng xuất sắc trong các tác vụ mô tả hình ảnh và trả lời câu hỏi hình ảnh, vượt qua khoảng cách giữa tạo ngôn ngữ và suy luận hình ảnh."
  },
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 được thiết kế để xử lý các tác vụ kết hợp dữ liệu hình ảnh và văn bản. Nó có khả năng xuất sắc trong các tác vụ mô tả hình ảnh và trả lời câu hỏi hình ảnh, vượt qua khoảng cách giữa tạo ngôn ngữ và suy luận hình ảnh."
  },
  "meta-llama/Llama-3.3-70B-Instruct-Turbo": {
    "description": "Mô hình ngôn ngữ lớn đa ngôn ngữ Meta Llama 3.3 (LLM) là mô hình sinh ra từ 70B (đầu vào văn bản/đầu ra văn bản) với việc điều chỉnh trước và điều chỉnh theo lệnh. Mô hình điều chỉnh theo lệnh Llama 3.3 được tối ưu hóa cho các trường hợp sử dụng đối thoại đa ngôn ngữ và vượt trội hơn nhiều mô hình trò chuyện mã nguồn mở và đóng khác trên các bài kiểm tra chuẩn ngành phổ biến."
  },
  "meta-llama/Llama-Vision-Free": {
    "description": "LLaMA 3.2 được thiết kế để xử lý các tác vụ kết hợp dữ liệu hình ảnh và văn bản. Nó có khả năng xuất sắc trong các tác vụ mô tả hình ảnh và trả lời câu hỏi hình ảnh, vượt qua khoảng cách giữa tạo ngôn ngữ và suy luận hình ảnh."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
    "description": "Llama 3 70B Instruct Lite phù hợp cho các môi trường cần hiệu suất cao và độ trễ thấp."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": {
    "description": "Llama 3 70B Instruct Turbo cung cấp khả năng hiểu và sinh ngôn ngữ xuất sắc, phù hợp cho các nhiệm vụ tính toán khắt khe nhất."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite": {
    "description": "Llama 3 8B Instruct Lite phù hợp cho các môi trường hạn chế tài nguyên, cung cấp hiệu suất cân bằng xuất sắc."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": {
    "description": "Llama 3 8B Instruct Turbo là một mô hình ngôn ngữ lớn hiệu suất cao, hỗ trợ nhiều tình huống ứng dụng."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "description": "LLaMA 3.1 405B là mô hình mạnh mẽ cho việc đào tạo trước và điều chỉnh theo hướng dẫn."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
    "description": "Mô hình Llama 3.1 Turbo 405B cung cấp hỗ trợ ngữ cảnh dung lượng lớn cho xử lý dữ liệu lớn, thể hiện xuất sắc trong các ứng dụng trí tuệ nhân tạo quy mô lớn."
  },
  "meta-llama/Meta-Llama-3.1-70B": {
    "description": "Llama 3.1 là mô hình hàng đầu do Meta phát hành, hỗ trợ lên đến 405B tham số, có thể áp dụng cho cuộc đối thoại phức tạp, dịch đa ngôn ngữ và phân tích dữ liệu."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
    "description": "Mô hình Llama 3.1 70B được tinh chỉnh để phù hợp với các ứng dụng tải cao, định lượng đến FP8 cung cấp khả năng tính toán và độ chính xác hiệu quả hơn, đảm bảo hiệu suất xuất sắc trong các tình huống phức tạp."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
    "description": "Mô hình Llama 3.1 8B sử dụng định lượng FP8, hỗ trợ lên đến 131,072 mã ngữ cảnh, là một trong những mô hình mã nguồn mở hàng đầu, phù hợp cho các nhiệm vụ phức tạp, vượt trội hơn nhiều tiêu chuẩn ngành."
  },
  "meta-llama/llama-3-70b-instruct": {
    "description": "Llama 3 70B Instruct được tối ưu hóa cho các tình huống đối thoại chất lượng cao, thể hiện xuất sắc trong nhiều đánh giá của con người."
  },
  "meta-llama/llama-3-8b-instruct": {
    "description": "Llama 3 8B Instruct tối ưu hóa cho các tình huống đối thoại chất lượng cao, hiệu suất vượt trội hơn nhiều mô hình đóng nguồn."
  },
  "meta-llama/llama-3.1-70b-instruct": {
    "description": "Llama 3.1 70B Instruct được thiết kế đặc biệt cho các cuộc đối thoại chất lượng cao, thể hiện xuất sắc trong các đánh giá của con người, đặc biệt phù hợp cho các tình huống tương tác cao."
  },
  "meta-llama/llama-3.1-8b-instruct": {
    "description": "Llama 3.1 8B Instruct là phiên bản mới nhất do Meta phát hành, tối ưu hóa cho các tình huống đối thoại chất lượng cao, vượt trội hơn nhiều mô hình đóng nguồn hàng đầu."
  },
  "meta-llama/llama-3.1-8b-instruct:free": {
    "description": "LLaMA 3.1 cung cấp hỗ trợ đa ngôn ngữ, là một trong những mô hình sinh hàng đầu trong ngành."
  },
  "meta-llama/llama-3.2-11b-vision-instruct": {
    "description": "LLaMA 3.2 được thiết kế để xử lý các nhiệm vụ kết hợp dữ liệu hình ảnh và văn bản. Nó thể hiện xuất sắc trong các nhiệm vụ mô tả hình ảnh và hỏi đáp hình ảnh, vượt qua ranh giới giữa sinh ngôn ngữ và suy diễn hình ảnh."
  },
  "meta-llama/llama-3.2-3b-instruct": {
    "description": "meta-llama/llama-3.2-3b-instruct"
  },
  "meta-llama/llama-3.2-90b-vision-instruct": {
    "description": "LLaMA 3.2 được thiết kế để xử lý các nhiệm vụ kết hợp dữ liệu hình ảnh và văn bản. Nó thể hiện xuất sắc trong các nhiệm vụ mô tả hình ảnh và hỏi đáp hình ảnh, vượt qua ranh giới giữa sinh ngôn ngữ và suy diễn hình ảnh."
  },
  "meta-llama/llama-3.3-70b-instruct": {
    "description": "Llama 3.3 là mô hình ngôn ngữ lớn mã nguồn mở đa ngôn ngữ tiên tiến nhất trong dòng Llama, mang đến trải nghiệm hiệu suất tương đương với mô hình 405B với chi phí cực thấp. Dựa trên cấu trúc Transformer, và được cải thiện tính hữu ích và an toàn thông qua tinh chỉnh giám sát (SFT) và học tăng cường từ phản hồi của con người (RLHF). Phiên bản tinh chỉnh theo chỉ dẫn của nó được tối ưu hóa cho đối thoại đa ngôn ngữ, thể hiện tốt hơn nhiều mô hình trò chuyện mã nguồn mở và đóng kín trong nhiều tiêu chuẩn ngành. Ngày cắt đứt kiến thức là tháng 12 năm 2023."
  },
  "meta-llama/llama-3.3-70b-instruct:free": {
    "description": "Llama 3.3 là mô hình ngôn ngữ lớn mã nguồn mở đa ngôn ngữ tiên tiến nhất trong dòng Llama, mang đến trải nghiệm hiệu suất tương đương với mô hình 405B với chi phí cực thấp. Dựa trên cấu trúc Transformer, và được cải thiện tính hữu ích và an toàn thông qua tinh chỉnh giám sát (SFT) và học tăng cường từ phản hồi của con người (RLHF). Phiên bản tinh chỉnh theo chỉ dẫn của nó được tối ưu hóa cho đối thoại đa ngôn ngữ, thể hiện tốt hơn nhiều mô hình trò chuyện mã nguồn mở và đóng kín trong nhiều tiêu chuẩn ngành. Ngày cắt đứt kiến thức là tháng 12 năm 2023."
  },
  "meta.llama3-1-405b-instruct-v1:0": {
    "description": "Meta Llama 3.1 405B Instruct là mô hình lớn nhất và mạnh mẽ nhất trong mô hình Llama 3.1 Instruct, là một mô hình sinh dữ liệu và suy luận đối thoại tiên tiến, cũng có thể được sử dụng làm nền tảng cho việc tiền huấn luyện hoặc tinh chỉnh chuyên sâu trong các lĩnh vực cụ thể. Các mô hình ngôn ngữ lớn đa ngôn ngữ (LLMs) mà Llama 3.1 cung cấp là một tập hợp các mô hình sinh đã được tiền huấn luyện và điều chỉnh theo chỉ dẫn, bao gồm kích thước 8B, 70B và 405B (đầu vào/đầu ra văn bản). Các mô hình văn bản điều chỉnh theo chỉ dẫn của Llama 3.1 (8B, 70B, 405B) được tối ưu hóa cho các trường hợp đối thoại đa ngôn ngữ và đã vượt qua nhiều mô hình trò chuyện mã nguồn mở có sẵn trong các bài kiểm tra chuẩn ngành phổ biến. Llama 3.1 được thiết kế để sử dụng cho nhiều mục đích thương mại và nghiên cứu bằng nhiều ngôn ngữ. Các mô hình văn bản điều chỉnh theo chỉ dẫn phù hợp cho các cuộc trò chuyện giống như trợ lý, trong khi các mô hình đã được tiền huấn luyện có thể thích ứng với nhiều nhiệm vụ sinh ngôn ngữ tự nhiên khác nhau. Mô hình Llama 3.1 cũng hỗ trợ việc cải thiện các mô hình khác bằng cách sử dụng đầu ra của nó, bao gồm sinh dữ liệu tổng hợp và tinh chỉnh. Llama 3.1 là một mô hình ngôn ngữ tự hồi quy sử dụng kiến trúc biến áp tối ưu. Phiên bản điều chỉnh sử dụng tinh chỉnh có giám sát (SFT) và học tăng cường có phản hồi từ con người (RLHF) để phù hợp với sở thích của con người về tính hữu ích và an toàn."
  },
  "meta.llama3-1-70b-instruct-v1:0": {
    "description": "Phiên bản cập nhật của Meta Llama 3.1 70B Instruct, bao gồm độ dài ngữ cảnh mở rộng 128K, tính đa ngôn ngữ và khả năng suy luận cải tiến. Các mô hình ngôn ngữ lớn (LLMs) đa ngôn ngữ do Llama 3.1 cung cấp là một tập hợp các mô hình sinh đã được huấn luyện trước và điều chỉnh theo chỉ dẫn, bao gồm kích thước 8B, 70B và 405B (đầu vào/đầu ra văn bản). Các mô hình văn bản điều chỉnh theo chỉ dẫn của Llama 3.1 (8B, 70B, 405B) được tối ưu hóa cho các trường hợp đối thoại đa ngôn ngữ và đã vượt qua nhiều mô hình trò chuyện mã nguồn mở có sẵn trong các bài kiểm tra chuẩn ngành phổ biến. Llama 3.1 được thiết kế cho các mục đích thương mại và nghiên cứu đa ngôn ngữ. Các mô hình văn bản điều chỉnh theo chỉ dẫn phù hợp cho các cuộc trò chuyện giống như trợ lý, trong khi các mô hình đã được huấn luyện trước có thể thích ứng với nhiều nhiệm vụ sinh ngôn ngữ tự nhiên khác nhau. Mô hình Llama 3.1 cũng hỗ trợ việc sử dụng đầu ra của mô hình để cải thiện các mô hình khác, bao gồm tạo dữ liệu tổng hợp và tinh chỉnh. Llama 3.1 là mô hình ngôn ngữ tự hồi quy sử dụng kiến trúc biến áp được tối ưu hóa. Phiên bản điều chỉnh sử dụng tinh chỉnh giám sát (SFT) và học tăng cường có phản hồi của con người (RLHF) để phù hợp với sở thích của con người về tính hữu ích và an toàn."
  },
  "meta.llama3-1-8b-instruct-v1:0": {
    "description": "Phiên bản cập nhật của Meta Llama 3.1 8B Instruct, bao gồm độ dài ngữ cảnh mở rộng 128K, tính đa ngôn ngữ và khả năng suy luận cải tiến. Các mô hình ngôn ngữ lớn (LLMs) đa ngôn ngữ do Llama 3.1 cung cấp là một tập hợp các mô hình sinh đã được huấn luyện trước và điều chỉnh theo chỉ dẫn, bao gồm kích thước 8B, 70B và 405B (đầu vào/đầu ra văn bản). Các mô hình văn bản điều chỉnh theo chỉ dẫn của Llama 3.1 (8B, 70B, 405B) được tối ưu hóa cho các trường hợp đối thoại đa ngôn ngữ và đã vượt qua nhiều mô hình trò chuyện mã nguồn mở có sẵn trong các bài kiểm tra chuẩn ngành phổ biến. Llama 3.1 được thiết kế cho các mục đích thương mại và nghiên cứu đa ngôn ngữ. Các mô hình văn bản điều chỉnh theo chỉ dẫn phù hợp cho các cuộc trò chuyện giống như trợ lý, trong khi các mô hình đã được huấn luyện trước có thể thích ứng với nhiều nhiệm vụ sinh ngôn ngữ tự nhiên khác nhau. Mô hình Llama 3.1 cũng hỗ trợ việc sử dụng đầu ra của mô hình để cải thiện các mô hình khác, bao gồm tạo dữ liệu tổng hợp và tinh chỉnh. Llama 3.1 là mô hình ngôn ngữ tự hồi quy sử dụng kiến trúc biến áp được tối ưu hóa. Phiên bản điều chỉnh sử dụng tinh chỉnh giám sát (SFT) và học tăng cường có phản hồi của con người (RLHF) để phù hợp với sở thích của con người về tính hữu ích và an toàn."
  },
  "meta.llama3-70b-instruct-v1:0": {
    "description": "Meta Llama 3 là một mô hình ngôn ngữ lớn (LLM) mở dành cho các nhà phát triển, nhà nghiên cứu và doanh nghiệp, nhằm giúp họ xây dựng, thử nghiệm và mở rộng ý tưởng AI sinh một cách có trách nhiệm. Là một phần của hệ thống cơ sở hạ tầng đổi mới toàn cầu, nó rất phù hợp cho việc tạo nội dung, AI đối thoại, hiểu ngôn ngữ, nghiên cứu và ứng dụng doanh nghiệp."
  },
  "meta.llama3-8b-instruct-v1:0": {
    "description": "Meta Llama 3 là một mô hình ngôn ngữ lớn (LLM) mở dành cho các nhà phát triển, nhà nghiên cứu và doanh nghiệp, nhằm giúp họ xây dựng, thử nghiệm và mở rộng ý tưởng AI sinh một cách có trách nhiệm. Là một phần của hệ thống cơ sở hạ tầng đổi mới toàn cầu, nó rất phù hợp cho các thiết bị biên và thời gian huấn luyện nhanh hơn với khả năng tính toán và tài nguyên hạn chế."
  },
  "meta/Llama-3.2-11B-Vision-Instruct": {
    "description": "Khả năng suy luận hình ảnh xuất sắc trên hình ảnh độ phân giải cao, phù hợp cho các ứng dụng hiểu biết thị giác."
  },
  "meta/Llama-3.2-90B-Vision-Instruct": {
    "description": "Khả năng suy luận hình ảnh nâng cao dành cho các ứng dụng đại lý hiểu biết thị giác."
  },
  "meta/Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 là mô hình ngôn ngữ lớn đa ngôn ngữ mã nguồn mở tiên tiến nhất trong dòng Llama, mang lại hiệu suất tương đương mô hình 405 tỷ tham số với chi phí rất thấp. Dựa trên kiến trúc Transformer, được cải thiện qua huấn luyện giám sát (SFT) và học tăng cường từ phản hồi con người (RLHF) để nâng cao tính hữu ích và an toàn. Phiên bản tinh chỉnh chỉ dẫn được tối ưu cho đối thoại đa ngôn ngữ, vượt trội trên nhiều chuẩn mực ngành so với nhiều mô hình trò chuyện mã nguồn mở và đóng. Kiến thức cập nhật đến tháng 12 năm 2023."
  },
  "meta/Meta-Llama-3-70B-Instruct": {
    "description": "Một mô hình mạnh mẽ với 70 tỷ tham số, thể hiện xuất sắc trong suy luận, mã hóa và các ứng dụng ngôn ngữ đa dạng."
  },
  "meta/Meta-Llama-3-8B-Instruct": {
    "description": "Một mô hình đa năng với 8 tỷ tham số, được tối ưu cho các nhiệm vụ đối thoại và tạo văn bản."
  },
  "meta/Meta-Llama-3.1-405B-Instruct": {
    "description": "Mô hình văn bản Llama 3.1 được tinh chỉnh chỉ dẫn, tối ưu cho các trường hợp sử dụng đối thoại đa ngôn ngữ, thể hiện xuất sắc trên nhiều chuẩn mực ngành so với nhiều mô hình trò chuyện mã nguồn mở và đóng hiện có."
  },
  "meta/Meta-Llama-3.1-70B-Instruct": {
    "description": "Mô hình văn bản Llama 3.1 được tinh chỉnh chỉ dẫn, tối ưu cho các trường hợp sử dụng đối thoại đa ngôn ngữ, thể hiện xuất sắc trên nhiều chuẩn mực ngành so với nhiều mô hình trò chuyện mã nguồn mở và đóng hiện có."
  },
  "meta/Meta-Llama-3.1-8B-Instruct": {
    "description": "Mô hình văn bản Llama 3.1 được tinh chỉnh chỉ dẫn, tối ưu cho các trường hợp sử dụng đối thoại đa ngôn ngữ, thể hiện xuất sắc trên nhiều chuẩn mực ngành so với nhiều mô hình trò chuyện mã nguồn mở và đóng hiện có."
  },
  "meta/llama-3-70b": {
    "description": "Mô hình mã nguồn mở 70 tỷ tham số được Meta tinh chỉnh kỹ lưỡng cho mục đích tuân thủ chỉ dẫn. Được Groq phục vụ bằng phần cứng đơn vị xử lý ngôn ngữ (LPU) tùy chỉnh để cung cấp suy luận nhanh và hiệu quả."
  },
  "meta/llama-3-8b": {
    "description": "Mô hình mã nguồn mở 8 tỷ tham số được Meta tinh chỉnh kỹ lưỡng cho mục đích tuân thủ chỉ dẫn. Được Groq phục vụ bằng phần cứng đơn vị xử lý ngôn ngữ (LPU) tùy chỉnh để cung cấp suy luận nhanh và hiệu quả."
  },
  "meta/llama-3.1-405b-instruct": {
    "description": "LLM cao cấp, hỗ trợ tạo dữ liệu tổng hợp, chưng cất kiến thức và suy luận, phù hợp cho chatbot, lập trình và các nhiệm vụ chuyên biệt."
  },
  "meta/llama-3.1-70b": {
    "description": "Phiên bản cập nhật của Meta Llama 3 70B Instruct, bao gồm độ dài ngữ cảnh mở rộng 128K, đa ngôn ngữ và khả năng suy luận cải tiến."
  },
  "meta/llama-3.1-70b-instruct": {
    "description": "Tăng cường cuộc đối thoại phức tạp, có khả năng hiểu ngữ cảnh xuất sắc, suy luận và sinh văn bản."
  },
  "meta/llama-3.1-8b": {
    "description": "Llama 3.1 8B hỗ trợ cửa sổ ngữ cảnh 128K, là lựa chọn lý tưởng cho giao diện đối thoại thời gian thực và phân tích dữ liệu, đồng thời tiết kiệm chi phí đáng kể so với các mô hình lớn hơn. Được Groq phục vụ bằng phần cứng đơn vị xử lý ngôn ngữ (LPU) tùy chỉnh để cung cấp suy luận nhanh và hiệu quả."
  },
  "meta/llama-3.1-8b-instruct": {
    "description": "Mô hình tiên tiến hàng đầu, có khả năng hiểu ngôn ngữ, suy luận xuất sắc và khả năng sinh văn bản."
  },
  "meta/llama-3.2-11b": {
    "description": "Mô hình tạo suy luận hình ảnh được điều chỉnh chỉ dẫn (đầu vào văn bản + hình ảnh / đầu ra văn bản), tối ưu cho nhận dạng hình ảnh, suy luận hình ảnh, tạo chú thích và trả lời các câu hỏi chung về hình ảnh."
  },
  "meta/llama-3.2-11b-vision-instruct": {
    "description": "Mô hình thị giác-ngôn ngữ tiên tiến, xuất sắc trong việc suy luận chất lượng cao từ hình ảnh."
  },
  "meta/llama-3.2-1b": {
    "description": "Mô hình chỉ văn bản, hỗ trợ các trường hợp sử dụng trên thiết bị như truy xuất kiến thức địa phương đa ngôn ngữ, tóm tắt và viết lại."
  },
  "meta/llama-3.2-1b-instruct": {
    "description": "Mô hình ngôn ngữ nhỏ tiên tiến hàng đầu, có khả năng hiểu ngôn ngữ, suy luận xuất sắc và khả năng sinh văn bản."
  },
  "meta/llama-3.2-3b": {
    "description": "Mô hình chỉ văn bản, được tinh chỉnh kỹ lưỡng để hỗ trợ các trường hợp sử dụng trên thiết bị như truy xuất kiến thức địa phương đa ngôn ngữ, tóm tắt và viết lại."
  },
  "meta/llama-3.2-3b-instruct": {
    "description": "Mô hình ngôn ngữ nhỏ tiên tiến hàng đầu, có khả năng hiểu ngôn ngữ, suy luận xuất sắc và khả năng sinh văn bản."
  },
  "meta/llama-3.2-90b": {
    "description": "Mô hình tạo suy luận hình ảnh được điều chỉnh chỉ dẫn (đầu vào văn bản + hình ảnh / đầu ra văn bản), tối ưu cho nhận dạng hình ảnh, suy luận hình ảnh, tạo chú thích và trả lời các câu hỏi chung về hình ảnh."
  },
  "meta/llama-3.2-90b-vision-instruct": {
    "description": "Mô hình thị giác-ngôn ngữ tiên tiến, xuất sắc trong việc suy luận chất lượng cao từ hình ảnh."
  },
  "meta/llama-3.3-70b": {
    "description": "Sự kết hợp hoàn hảo giữa hiệu suất và hiệu quả. Mô hình hỗ trợ AI đối thoại hiệu suất cao, được thiết kế cho tạo nội dung, ứng dụng doanh nghiệp và nghiên cứu, cung cấp khả năng hiểu ngôn ngữ tiên tiến bao gồm tóm tắt văn bản, phân loại, phân tích cảm xúc và tạo mã."
  },
  "meta/llama-3.3-70b-instruct": {
    "description": "Mô hình LLM tiên tiến, xuất sắc trong suy luận, toán học, kiến thức chung và gọi hàm."
  },
  "meta/llama-4-maverick": {
    "description": "Bộ mô hình Llama 4 là các mô hình AI đa phương thức nguyên bản, hỗ trợ trải nghiệm văn bản và đa phương thức. Các mô hình này sử dụng kiến trúc chuyên gia hỗn hợp để cung cấp hiệu suất hàng đầu ngành trong hiểu văn bản và hình ảnh. Llama 4 Maverick, mô hình 17 tỷ tham số với 128 chuyên gia. Được DeepInfra phục vụ."
  },
  "meta/llama-4-scout": {
    "description": "Bộ mô hình Llama 4 là các mô hình AI đa phương thức nguyên bản, hỗ trợ trải nghiệm văn bản và đa phương thức. Các mô hình này sử dụng kiến trúc chuyên gia hỗn hợp để cung cấp hiệu suất hàng đầu ngành trong hiểu văn bản và hình ảnh. Llama 4 Scout, mô hình 17 tỷ tham số với 16 chuyên gia. Được DeepInfra phục vụ."
  },
  "microsoft/Phi-3-medium-128k-instruct": {
    "description": "Cùng mô hình Phi-3-medium nhưng với kích thước ngữ cảnh lớn hơn, phù hợp cho RAG hoặc ít gợi ý."
  },
  "microsoft/Phi-3-medium-4k-instruct": {
    "description": "Mô hình 14 tỷ tham số, chất lượng vượt trội so với Phi-3-mini, tập trung vào dữ liệu suy luận chất lượng cao."
  },
  "microsoft/Phi-3-mini-128k-instruct": {
    "description": "Cùng mô hình Phi-3-mini nhưng với kích thước ngữ cảnh lớn hơn, phù hợp cho RAG hoặc ít gợi ý."
  },
  "microsoft/Phi-3-mini-4k-instruct": {
    "description": "Thành viên nhỏ nhất trong gia đình Phi-3, được tối ưu cho chất lượng và độ trễ thấp."
  },
  "microsoft/Phi-3-small-128k-instruct": {
    "description": "Cùng mô hình Phi-3-small nhưng với kích thước ngữ cảnh lớn hơn, phù hợp cho RAG hoặc ít gợi ý."
  },
  "microsoft/Phi-3-small-8k-instruct": {
    "description": "Mô hình 7 tỷ tham số, chất lượng vượt trội so với Phi-3-mini, tập trung vào dữ liệu suy luận chất lượng cao."
  },
  "microsoft/Phi-3.5-mini-instruct": {
    "description": "Phiên bản cập nhật của mô hình Phi-3-mini."
  },
  "microsoft/Phi-3.5-vision-instruct": {
    "description": "Phiên bản cập nhật của mô hình Phi-3-vision."
  },
  "microsoft/WizardLM-2-8x22B": {
    "description": "WizardLM 2 là mô hình ngôn ngữ do AI của Microsoft cung cấp, thể hiện xuất sắc trong các lĩnh vực đối thoại phức tạp, đa ngôn ngữ, suy luận và trợ lý thông minh."
  },
  "microsoft/wizardlm-2-8x22b": {
    "description": "WizardLM-2 8x22B là mô hình Wizard tiên tiến nhất của Microsoft AI, thể hiện hiệu suất cực kỳ cạnh tranh."
  },
  "minicpm-v": {
    "description": "MiniCPM-V là mô hình đa phương thức thế hệ mới do OpenBMB phát triển, có khả năng nhận diện OCR xuất sắc và hiểu biết đa phương thức, hỗ trợ nhiều ứng dụng khác nhau."
  },
  "minimax-m2": {
    "description": "MiniMax M2 là một mô hình ngôn ngữ lớn hiệu quả, được xây dựng dành riêng cho quy trình làm việc liên quan đến lập trình và tác vụ đại lý."
  },
  "minimax/minimax-m2": {
    "description": "Được sinh ra để phục vụ mã hóa hiệu quả và quy trình làm việc của Agent."
  },
  "minimaxai/minimax-m2": {
    "description": "MiniMax-M2 là một mô hình chuyên gia hỗn hợp (MoE) nhỏ gọn, nhanh chóng và tiết kiệm chi phí, với tổng số 230 tỷ tham số và 10 tỷ tham số kích hoạt, được thiết kế để đạt hiệu suất hàng đầu trong các tác vụ mã hóa và tác nhân, đồng thời duy trì trí tuệ nhân tạo tổng quát mạnh mẽ. Mô hình này thể hiện xuất sắc trong chỉnh sửa nhiều tệp, vòng lặp mã hóa-chạy-sửa lỗi, kiểm thử và sửa lỗi, cũng như các chuỗi công cụ liên kết dài phức tạp, là lựa chọn lý tưởng cho quy trình làm việc của nhà phát triển."
  },
  "ministral-3b-latest": {
    "description": "Ministral 3B là mô hình hàng đầu thế giới của Mistral về hiệu suất cạnh biên."
  },
  "ministral-8b-latest": {
    "description": "Ministral 8B là mô hình cạnh biên cực kỳ tiết kiệm chi phí của Mistral."
  },
  "mistral": {
    "description": "Mistral là mô hình 7B do Mistral AI phát hành, phù hợp cho các nhu cầu xử lý ngôn ngữ đa dạng."
  },
  "mistral-ai/Mistral-Large-2411": {
    "description": "Mô hình chủ lực của Mistral, phù hợp cho các nhiệm vụ phức tạp cần khả năng suy luận quy mô lớn hoặc chuyên môn cao (tổng hợp văn bản, tạo mã, RAG hoặc đại lý)."
  },
  "mistral-ai/Mistral-Nemo": {
    "description": "Mistral Nemo là một mô hình ngôn ngữ tiên tiến (LLM), sở hữu khả năng suy luận, kiến thức thế giới và mã hóa hàng đầu trong phân khúc kích thước của nó."
  },
  "mistral-ai/mistral-small-2503": {
    "description": "Mistral Small phù hợp cho bất kỳ nhiệm vụ dựa trên ngôn ngữ nào cần hiệu quả cao và độ trễ thấp."
  },
  "mistral-large": {
    "description": "Mixtral Large là mô hình hàng đầu của Mistral, kết hợp khả năng sinh mã, toán học và suy luận, hỗ trợ cửa sổ ngữ cảnh 128k."
  },
  "mistral-large-instruct": {
    "description": "Mistral-Large-Instruct-2407 là một mô hình ngôn ngữ lớn (LLM) tiên tiến, có 123 tỷ tham số, với khả năng suy luận, kiến thức và lập trình hàng đầu."
  },
  "mistral-large-latest": {
    "description": "Mistral Large là mô hình lớn hàng đầu, chuyên về các nhiệm vụ đa ngôn ngữ, suy luận phức tạp và sinh mã, là lựa chọn lý tưởng cho các ứng dụng cao cấp."
  },
  "mistral-medium-latest": {
    "description": "Mistral Medium 3 cung cấp hiệu suất tiên tiến với chi phí gấp 8 lần và đơn giản hóa việc triển khai doanh nghiệp."
  },
  "mistral-nemo": {
    "description": "Mistral Nemo được phát triển hợp tác giữa Mistral AI và NVIDIA, là mô hình 12B hiệu suất cao."
  },
  "mistral-nemo-instruct": {
    "description": "Mô hình ngôn ngữ lớn Mistral-Nemo-Instruct-2407 (LLM) là phiên bản điều chỉnh lệnh của Mistral-Nemo-Base-2407."
  },
  "mistral-small": {
    "description": "Mistral Small có thể được sử dụng cho bất kỳ nhiệm vụ nào dựa trên ngôn ngữ yêu cầu hiệu suất cao và độ trễ thấp."
  },
  "mistral-small-latest": {
    "description": "Mistral Small là lựa chọn hiệu quả về chi phí, nhanh chóng và đáng tin cậy, phù hợp cho các trường hợp như dịch thuật, tóm tắt và phân tích cảm xúc."
  },
  "mistral/codestral": {
    "description": "Mistral Codestral 25.01 là mô hình mã hóa tiên tiến, được tối ưu cho các trường hợp sử dụng độ trễ thấp và tần suất cao. Thành thạo hơn 80 ngôn ngữ lập trình, nó thể hiện xuất sắc trong các nhiệm vụ như điền giữa (FIM), sửa lỗi mã và tạo kiểm thử."
  },
  "mistral/codestral-embed": {
    "description": "Mô hình nhúng mã để tích hợp vào cơ sở dữ liệu và kho lưu trữ mã, hỗ trợ trợ lý mã hóa."
  },
  "mistral/devstral-small": {
    "description": "Devstral là mô hình ngôn ngữ lớn đại lý cho các nhiệm vụ kỹ thuật phần mềm, là lựa chọn tuyệt vời cho đại lý kỹ thuật phần mềm."
  },
  "mistral/magistral-medium": {
    "description": "Tư duy phức tạp được hỗ trợ bởi sự hiểu biết sâu sắc, với suy luận minh bạch mà bạn có thể theo dõi và xác minh. Mô hình duy trì suy luận độ trung thực cao trên nhiều ngôn ngữ ngay cả khi chuyển đổi ngôn ngữ giữa chừng trong nhiệm vụ."
  },
  "mistral/magistral-small": {
    "description": "Tư duy phức tạp được hỗ trợ bởi sự hiểu biết sâu sắc, với suy luận minh bạch mà bạn có thể theo dõi và xác minh. Mô hình duy trì suy luận độ trung thực cao trên nhiều ngôn ngữ ngay cả khi chuyển đổi ngôn ngữ giữa chừng trong nhiệm vụ."
  },
  "mistral/ministral-3b": {
    "description": "Mô hình nhỏ gọn, hiệu quả cho các nhiệm vụ trên thiết bị như trợ lý thông minh và phân tích cục bộ, cung cấp hiệu suất độ trễ thấp."
  },
  "mistral/ministral-8b": {
    "description": "Mô hình mạnh mẽ hơn với suy luận nhanh hơn và tiết kiệm bộ nhớ, là lựa chọn lý tưởng cho các quy trình làm việc phức tạp và ứng dụng biên đòi hỏi cao."
  },
  "mistral/mistral-embed": {
    "description": "Mô hình nhúng văn bản đa năng cho tìm kiếm ngữ nghĩa, tương đồng, phân cụm và quy trình làm việc RAG."
  },
  "mistral/mistral-large": {
    "description": "Mistral Large là lựa chọn lý tưởng cho các nhiệm vụ phức tạp đòi hỏi khả năng suy luận lớn hoặc chuyên môn cao — như tạo văn bản tổng hợp, tạo mã, RAG hoặc đại lý."
  },
  "mistral/mistral-small": {
    "description": "Mistral Small là lựa chọn lý tưởng cho các nhiệm vụ đơn giản có thể xử lý theo lô — như phân loại, hỗ trợ khách hàng hoặc tạo văn bản. Nó cung cấp hiệu suất xuất sắc với mức giá phải chăng."
  },
  "mistral/mixtral-8x22b-instruct": {
    "description": "Mô hình 8x22b Instruct. 8x22b là mô hình chuyên gia hỗn hợp mã nguồn mở được Mistral phục vụ."
  },
  "mistral/pixtral-12b": {
    "description": "Mô hình 12B có khả năng hiểu hình ảnh cùng với văn bản."
  },
  "mistral/pixtral-large": {
    "description": "Pixtral Large là mô hình thứ hai trong gia đình đa phương thức của chúng tôi, thể hiện khả năng hiểu hình ảnh tiên tiến. Đặc biệt, mô hình có thể hiểu tài liệu, biểu đồ và hình ảnh tự nhiên, đồng thời duy trì khả năng hiểu văn bản hàng đầu của Mistral Large 2."
  },
  "mistralai/Mistral-7B-Instruct-v0.1": {
    "description": "Mistral (7B) Instruct nổi bật với hiệu suất cao, phù hợp cho nhiều nhiệm vụ ngôn ngữ."
  },
  "mistralai/Mistral-7B-Instruct-v0.2": {
    "description": "Mistral 7B là mô hình fine-tuning theo yêu cầu, cung cấp giải pháp tối ưu cho các nhiệm vụ."
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "description": "Mistral (7B) Instruct v0.3 cung cấp khả năng tính toán hiệu quả và hiểu ngôn ngữ tự nhiên, phù hợp cho nhiều ứng dụng."
  },
  "mistralai/Mistral-7B-v0.1": {
    "description": "Mistral 7B là một mô hình nhỏ gọn nhưng hiệu suất cao, chuyên về xử lý hàng loạt và các tác vụ đơn giản như phân loại và sinh văn bản, với khả năng suy luận tốt."
  },
  "mistralai/Mixtral-8x22B-Instruct-v0.1": {
    "description": "Mixtral-8x22B Instruct (141B) là một mô hình ngôn ngữ lớn siêu cấp, hỗ trợ nhu cầu xử lý cực cao."
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "description": "Mixtral 8x7B là mô hình chuyên gia hỗn hợp thưa được tiền huấn luyện, dùng cho các nhiệm vụ văn bản tổng quát."
  },
  "mistralai/Mixtral-8x7B-v0.1": {
    "description": "Mixtral 8x7B là một mô hình chuyên gia thưa thớt, tận dụng nhiều tham số để tăng tốc độ suy luận, phù hợp để xử lý đa ngôn ngữ và tạo mã."
  },
  "mistralai/mistral-nemo": {
    "description": "Mistral Nemo là mô hình 7.3B tham số hỗ trợ đa ngôn ngữ và lập trình hiệu suất cao."
  },
  "mixtral": {
    "description": "Mixtral là mô hình chuyên gia của Mistral AI, có trọng số mã nguồn mở và cung cấp hỗ trợ cho việc sinh mã và hiểu ngôn ngữ."
  },
  "mixtral-8x7b-32768": {
    "description": "Mixtral 8x7B cung cấp khả năng tính toán song song có độ dung sai cao, phù hợp cho các nhiệm vụ phức tạp."
  },
  "mixtral:8x22b": {
    "description": "Mixtral là mô hình chuyên gia của Mistral AI, có trọng số mã nguồn mở và cung cấp hỗ trợ cho việc sinh mã và hiểu ngôn ngữ."
  },
  "moonshot-v1-128k": {
    "description": "Moonshot V1 128K là một mô hình có khả năng xử lý ngữ cảnh siêu dài, phù hợp cho việc sinh văn bản siêu dài, đáp ứng nhu cầu nhiệm vụ sinh phức tạp, có thể xử lý nội dung lên đến 128.000 tokens, rất phù hợp cho nghiên cứu, học thuật và sinh tài liệu lớn."
  },
  "moonshot-v1-128k-vision-preview": {
    "description": "Mô hình hình ảnh Kimi (bao gồm moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, v.v.) có khả năng hiểu nội dung hình ảnh, bao gồm văn bản hình ảnh, màu sắc hình ảnh và hình dạng vật thể."
  },
  "moonshot-v1-32k": {
    "description": "Moonshot V1 32K cung cấp khả năng xử lý ngữ cảnh độ dài trung bình, có thể xử lý 32.768 tokens, đặc biệt phù hợp cho việc sinh các tài liệu dài và đối thoại phức tạp, ứng dụng trong sáng tạo nội dung, sinh báo cáo và hệ thống đối thoại."
  },
  "moonshot-v1-32k-vision-preview": {
    "description": "Mô hình hình ảnh Kimi (bao gồm moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, v.v.) có khả năng hiểu nội dung hình ảnh, bao gồm văn bản hình ảnh, màu sắc hình ảnh và hình dạng vật thể."
  },
  "moonshot-v1-8k": {
    "description": "Moonshot V1 8K được thiết kế đặc biệt cho các nhiệm vụ sinh văn bản ngắn, có hiệu suất xử lý cao, có thể xử lý 8.192 tokens, rất phù hợp cho các cuộc đối thoại ngắn, ghi chú nhanh và sinh nội dung nhanh chóng."
  },
  "moonshot-v1-8k-vision-preview": {
    "description": "Mô hình hình ảnh Kimi (bao gồm moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, v.v.) có khả năng hiểu nội dung hình ảnh, bao gồm văn bản hình ảnh, màu sắc hình ảnh và hình dạng vật thể."
  },
  "moonshot-v1-auto": {
    "description": "Moonshot V1 Auto có thể chọn mô hình phù hợp dựa trên số lượng Tokens hiện tại đang chiếm dụng trong ngữ cảnh."
  },
  "moonshotai/Kimi-Dev-72B": {
    "description": "Kimi-Dev-72B là một mô hình mã nguồn mở lớn, được tối ưu hóa qua học tăng cường quy mô lớn, có khả năng tạo ra các bản vá ổn định và có thể triển khai trực tiếp. Mô hình này đã đạt điểm cao kỷ lục 60,4% trên SWE-bench Verified, phá vỡ các kỷ lục của mô hình mã nguồn mở trong các nhiệm vụ kỹ thuật phần mềm tự động như sửa lỗi và đánh giá mã."
  },
  "moonshotai/Kimi-K2-Instruct-0905": {
    "description": "Kimi K2-Instruct-0905 là phiên bản mới nhất và mạnh mẽ nhất của Kimi K2. Đây là một mô hình ngôn ngữ chuyên gia hỗn hợp (MoE) hàng đầu với tổng số tham số lên đến 1 nghìn tỷ và 32 tỷ tham số kích hoạt. Các đặc điểm chính của mô hình bao gồm: tăng cường trí tuệ mã hóa tác nhân, thể hiện sự cải thiện đáng kể trong các bài kiểm tra chuẩn công khai và các nhiệm vụ mã hóa tác nhân trong thế giới thực; cải tiến trải nghiệm mã hóa giao diện người dùng, nâng cao cả về tính thẩm mỹ và tính thực tiễn trong lập trình giao diện."
  },
  "moonshotai/kimi-k2": {
    "description": "Kimi K2 là mô hình ngôn ngữ chuyên gia hỗn hợp (MoE) quy mô lớn do Moonshot AI phát triển, với tổng số tham số lên đến 1 nghìn tỷ và 32 tỷ tham số kích hoạt mỗi lần truyền tiến. Nó được tối ưu cho khả năng đại lý, bao gồm sử dụng công cụ nâng cao, suy luận và tổng hợp mã."
  },
  "moonshotai/kimi-k2-0905": {
    "description": "Mô hình kimi-k2-0905-preview có độ dài ngữ cảnh 256k, sở hữu năng lực Agentic Coding mạnh mẽ hơn, mã front-end đẹp mắt và thực dụng hơn, cùng khả năng hiểu ngữ cảnh tốt hơn."
  },
  "moonshotai/kimi-k2-instruct-0905": {
    "description": "Mô hình kimi-k2-0905-preview có độ dài ngữ cảnh 256k, sở hữu năng lực Agentic Coding mạnh mẽ hơn, mã front-end đẹp mắt và thực dụng hơn, cùng khả năng hiểu ngữ cảnh tốt hơn."
  },
  "morph/morph-v3-fast": {
    "description": "Morph cung cấp mô hình AI chuyên biệt, áp dụng các thay đổi mã được đề xuất bởi các mô hình tiên tiến như Claude hoặc GPT-4o vào các tệp mã hiện có của bạn với tốc độ nhanh — hơn 4500 token/giây. Nó đóng vai trò là bước cuối cùng trong quy trình làm việc mã hóa AI. Hỗ trợ 16k token đầu vào và 16k token đầu ra."
  },
  "morph/morph-v3-large": {
    "description": "Morph cung cấp mô hình AI chuyên biệt, áp dụng các thay đổi mã được đề xuất bởi các mô hình tiên tiến như Claude hoặc GPT-4o vào các tệp mã hiện có của bạn với tốc độ nhanh — hơn 2500 token/giây. Nó đóng vai trò là bước cuối cùng trong quy trình làm việc mã hóa AI. Hỗ trợ 16k token đầu vào và 16k token đầu ra."
  },
  "nousresearch/hermes-2-pro-llama-3-8b": {
    "description": "Hermes 2 Pro Llama 3 8B là phiên bản nâng cấp của Nous Hermes 2, bao gồm bộ dữ liệu phát triển nội bộ mới nhất."
  },
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
    "description": "Llama 3.1 Nemotron 70B là một mô hình ngôn ngữ quy mô lớn tùy chỉnh bởi NVIDIA, nhằm nâng cao mức độ hỗ trợ của phản hồi do LLM tạo ra đối với các truy vấn của người dùng. Mô hình này đã thể hiện xuất sắc trong các bài kiểm tra chuẩn như Arena Hard, AlpacaEval 2 LC và GPT-4-Turbo MT-Bench, đứng đầu trong cả ba bài kiểm tra tự động cho đến ngày 1 tháng 10 năm 2024. Mô hình sử dụng RLHF (đặc biệt là REINFORCE), Llama-3.1-Nemotron-70B-Reward và HelpSteer2-Preference để đào tạo trên cơ sở mô hình Llama-3.1-70B-Instruct."
  },
  "nvidia/llama-3.1-nemotron-51b-instruct": {
    "description": "Mô hình ngôn ngữ độc đáo, cung cấp độ chính xác và hiệu suất không thể sánh kịp."
  },
  "nvidia/llama-3.1-nemotron-70b-instruct": {
    "description": "Llama-3.1-Nemotron-70B là mô hình ngôn ngữ lớn tùy chỉnh của NVIDIA, nhằm nâng cao tính hữu ích của các phản hồi do LLM tạo ra."
  },
  "o1": {
    "description": "Tập trung vào suy diễn nâng cao và giải quyết các vấn đề phức tạp, bao gồm các nhiệm vụ toán học và khoa học. Rất phù hợp cho các ứng dụng cần hiểu biết sâu sắc về ngữ cảnh và quy trình làm việc đại diện."
  },
  "o1-mini": {
    "description": "o1-mini là một mô hình suy diễn nhanh chóng và tiết kiệm chi phí, được thiết kế cho các ứng dụng lập trình, toán học và khoa học. Mô hình này có ngữ cảnh 128K và thời điểm cắt kiến thức vào tháng 10 năm 2023."
  },
  "o1-preview": {
    "description": "Tập trung vào suy luận nâng cao và giải quyết các vấn đề phức tạp, bao gồm các bài toán và nhiệm vụ khoa học. Rất phù hợp cho những ứng dụng cần khả năng hiểu biết ngữ cảnh sâu sắc và quy trình làm việc tự chủ."
  },
  "o1-pro": {
    "description": "Dòng mô hình o1 được huấn luyện qua học tăng cường, có khả năng suy nghĩ trước khi trả lời và thực hiện các nhiệm vụ suy luận phức tạp. Mô hình o1-pro sử dụng nhiều tài nguyên tính toán hơn để suy nghĩ sâu hơn, từ đó liên tục cung cấp câu trả lời chất lượng cao hơn."
  },
  "o3": {
    "description": "o3 là một mô hình toàn năng mạnh mẽ, thể hiện xuất sắc trong nhiều lĩnh vực. Nó thiết lập tiêu chuẩn mới cho các nhiệm vụ toán học, khoa học, lập trình và suy luận hình ảnh. Nó cũng giỏi trong việc viết kỹ thuật và tuân thủ hướng dẫn. Người dùng có thể sử dụng nó để phân tích văn bản, mã và hình ảnh, giải quyết các vấn đề phức tạp nhiều bước."
  },
  "o3-2025-04-16": {
    "description": "o3 là mô hình suy luận mới của OpenAI, hỗ trợ đầu vào hình ảnh và văn bản, xuất ra văn bản, phù hợp cho các tác vụ phức tạp cần kiến thức phổ quát rộng."
  },
  "o3-deep-research": {
    "description": "o3-deep-research là mô hình nghiên cứu sâu tiên tiến nhất của chúng tôi, được thiết kế đặc biệt để xử lý các nhiệm vụ nghiên cứu phức tạp nhiều bước. Nó có thể tìm kiếm và tổng hợp thông tin từ Internet, cũng như truy cập và tận dụng dữ liệu riêng của bạn thông qua kết nối MCP."
  },
  "o3-mini": {
    "description": "o3-mini là mô hình suy diễn nhỏ gọn mới nhất của chúng tôi, cung cấp trí thông minh cao với chi phí và độ trễ tương tự như o1-mini."
  },
  "o3-pro": {
    "description": "Mô hình o3-pro sử dụng nhiều tài nguyên tính toán hơn để suy nghĩ sâu sắc hơn và luôn cung cấp câu trả lời tốt hơn, chỉ hỗ trợ sử dụng dưới API Responses."
  },
  "o3-pro-2025-06-10": {
    "description": "o3 Pro là mô hình suy luận mới của OpenAI, hỗ trợ đầu vào hình ảnh và văn bản, xuất ra văn bản, phù hợp cho các tác vụ phức tạp cần kiến thức phổ quát rộng."
  },
  "o4-mini": {
    "description": "o4-mini là mô hình nhỏ gọn mới nhất trong dòng o của chúng tôi. Nó được tối ưu hóa cho suy luận nhanh chóng và hiệu quả, thể hiện hiệu suất và hiệu quả cao trong các nhiệm vụ mã hóa và hình ảnh."
  },
  "o4-mini-2025-04-16": {
    "description": "o4-mini là mô hình suy luận của OpenAI, hỗ trợ đầu vào hình ảnh và văn bản, xuất ra văn bản, phù hợp cho các tác vụ phức tạp cần kiến thức phổ quát rộng. Mô hình này có ngữ cảnh 200K."
  },
  "o4-mini-deep-research": {
    "description": "o4-mini-deep-research là mô hình nghiên cứu sâu nhanh hơn và tiết kiệm hơn của chúng tôi — rất phù hợp để xử lý các nhiệm vụ nghiên cứu phức tạp nhiều bước. Nó có thể tìm kiếm và tổng hợp thông tin từ Internet, cũng như truy cập và tận dụng dữ liệu riêng của bạn thông qua kết nối MCP."
  },
  "open-codestral-mamba": {
    "description": "Codestral Mamba là mô hình ngôn ngữ Mamba 2 tập trung vào sinh mã, cung cấp hỗ trợ mạnh mẽ cho các nhiệm vụ mã và suy luận tiên tiến."
  },
  "open-mistral-7b": {
    "description": "Mistral 7B là một mô hình nhỏ gọn nhưng hiệu suất cao, chuyên về xử lý hàng loạt và các nhiệm vụ đơn giản như phân loại và sinh văn bản, có khả năng suy luận tốt."
  },
  "open-mistral-nemo": {
    "description": "Mistral Nemo là một mô hình 12B được phát triển hợp tác với Nvidia, cung cấp hiệu suất suy luận và mã hóa xuất sắc, dễ dàng tích hợp và thay thế."
  },
  "open-mixtral-8x22b": {
    "description": "Mixtral 8x22B là một mô hình chuyên gia lớn hơn, tập trung vào các nhiệm vụ phức tạp, cung cấp khả năng suy luận xuất sắc và thông lượng cao hơn."
  },
  "open-mixtral-8x7b": {
    "description": "Mixtral 8x7B là một mô hình chuyên gia thưa thớt, sử dụng nhiều tham số để tăng tốc độ suy luận, phù hợp cho việc xử lý đa ngôn ngữ và sinh mã."
  },
  "openai/gpt-3.5-turbo": {
    "description": "Mô hình hiệu quả nhất và tiết kiệm chi phí nhất trong dòng GPT-3.5 của OpenAI, được tối ưu cho mục đích trò chuyện nhưng cũng hoạt động tốt trong các nhiệm vụ hoàn thành truyền thống."
  },
  "openai/gpt-3.5-turbo-instruct": {
    "description": "Khả năng tương tự các mô hình thời GPT-3. Tương thích với điểm cuối hoàn thành truyền thống thay vì điểm cuối hoàn thành trò chuyện."
  },
  "openai/gpt-4-turbo": {
    "description": "gpt-4-turbo của OpenAI có kiến thức tổng quát rộng và chuyên môn lĩnh vực, cho phép tuân theo các chỉ dẫn ngôn ngữ tự nhiên phức tạp và giải quyết chính xác các vấn đề khó. Kiến thức cập nhật đến tháng 4 năm 2023, cửa sổ ngữ cảnh 128.000 token."
  },
  "openai/gpt-4.1": {
    "description": "GPT 4.1 là mô hình hàng đầu của OpenAI, phù hợp cho các nhiệm vụ phức tạp. Nó rất thích hợp để giải quyết vấn đề đa lĩnh vực."
  },
  "openai/gpt-4.1-mini": {
    "description": "GPT 4.1 mini cân bằng giữa trí tuệ, tốc độ và chi phí, là mô hình hấp dẫn cho nhiều trường hợp sử dụng."
  },
  "openai/gpt-4.1-nano": {
    "description": "GPT-4.1 nano là mô hình GPT 4.1 nhanh nhất và tiết kiệm chi phí nhất."
  },
  "openai/gpt-4o": {
    "description": "GPT-4o của OpenAI có kiến thức tổng quát rộng và chuyên môn lĩnh vực, có khả năng tuân theo các chỉ dẫn ngôn ngữ tự nhiên phức tạp và giải quyết chính xác các vấn đề khó. Nó cung cấp hiệu suất tương đương GPT-4 Turbo với API nhanh hơn và rẻ hơn."
  },
  "openai/gpt-4o-mini": {
    "description": "GPT-4o mini của OpenAI là mô hình nhỏ tiên tiến và tiết kiệm chi phí nhất của họ. Nó đa phương thức (chấp nhận đầu vào văn bản hoặc hình ảnh và xuất ra văn bản), thông minh hơn gpt-3.5-turbo nhưng tốc độ tương đương."
  },
  "openai/gpt-5": {
    "description": "GPT-5 là mô hình ngôn ngữ hàng đầu của OpenAI, xuất sắc trong suy luận phức tạp, kiến thức thực tế rộng lớn, các nhiệm vụ mã hóa chuyên sâu và đại lý đa bước."
  },
  "openai/gpt-5-mini": {
    "description": "GPT-5 mini là mô hình tối ưu chi phí, thể hiện tốt trong các nhiệm vụ suy luận/trò chuyện. Nó cung cấp sự cân bằng tốt nhất giữa tốc độ, chi phí và khả năng."
  },
  "openai/gpt-5-nano": {
    "description": "GPT-5 nano là mô hình có thông lượng cao, thể hiện tốt trong các nhiệm vụ chỉ dẫn đơn giản hoặc phân loại."
  },
  "openai/gpt-oss-120b": {
    "description": "Mô hình ngôn ngữ lớn đa năng cực kỳ năng lực, với khả năng suy luận mạnh mẽ và có thể kiểm soát."
  },
  "openai/gpt-oss-20b": {
    "description": "Mô hình ngôn ngữ trọng số mã nguồn mở nhỏ gọn, được tối ưu cho độ trễ thấp và môi trường tài nguyên hạn chế, bao gồm triển khai cục bộ và biên."
  },
  "openai/o1": {
    "description": "o1 của OpenAI là mô hình suy luận hàng đầu, được thiết kế cho các vấn đề phức tạp đòi hỏi suy nghĩ sâu sắc. Nó cung cấp khả năng suy luận mạnh mẽ và độ chính xác cao cho các nhiệm vụ đa bước phức tạp."
  },
  "openai/o1-mini": {
    "description": "o1-mini là một mô hình suy diễn nhanh chóng và tiết kiệm chi phí, được thiết kế cho các ứng dụng lập trình, toán học và khoa học. Mô hình này có ngữ cảnh 128K và thời điểm cắt kiến thức vào tháng 10 năm 2023."
  },
  "openai/o1-preview": {
    "description": "o1 là mô hình suy diễn mới của OpenAI, phù hợp cho các nhiệm vụ phức tạp cần kiến thức tổng quát rộng rãi. Mô hình này có ngữ cảnh 128K và thời điểm cắt kiến thức vào tháng 10 năm 2023."
  },
  "openai/o3": {
    "description": "o3 của OpenAI là mô hình suy luận mạnh nhất, thiết lập các tiêu chuẩn mới trong mã hóa, toán học, khoa học và nhận thức thị giác. Nó xuất sắc trong các truy vấn phức tạp đòi hỏi phân tích đa chiều, có lợi thế đặc biệt trong phân tích hình ảnh, biểu đồ và đồ họa."
  },
  "openai/o3-mini": {
    "description": "o3-mini là mô hình suy luận nhỏ mới nhất của OpenAI, cung cấp trí tuệ cao với chi phí và độ trễ tương đương o1-mini."
  },
  "openai/o3-mini-high": {
    "description": "o3-mini phiên bản cao cấp về suy luận, cung cấp trí tuệ cao với cùng chi phí và mục tiêu độ trễ như o1-mini."
  },
  "openai/o4-mini": {
    "description": "o4-mini của OpenAI cung cấp suy luận nhanh và tiết kiệm chi phí, với hiệu suất xuất sắc trong kích thước của nó, đặc biệt trong toán học (đạt điểm cao nhất trong bài kiểm tra chuẩn AIME), mã hóa và các nhiệm vụ thị giác."
  },
  "openai/o4-mini-high": {
    "description": "o4-mini phiên bản cao cấp, được tối ưu hóa cho suy luận nhanh chóng và hiệu quả, thể hiện hiệu suất và hiệu quả cao trong các nhiệm vụ mã hóa và hình ảnh."
  },
  "openai/text-embedding-3-large": {
    "description": "Mô hình nhúng hiệu quả nhất của OpenAI, phù hợp cho các nhiệm vụ tiếng Anh và phi tiếng Anh."
  },
  "openai/text-embedding-3-small": {
    "description": "Phiên bản cải tiến và hiệu suất cao hơn của mô hình nhúng ada của OpenAI."
  },
  "openai/text-embedding-ada-002": {
    "description": "Mô hình nhúng văn bản truyền thống của OpenAI."
  },
  "openrouter/auto": {
    "description": "Dựa trên độ dài ngữ cảnh, chủ đề và độ phức tạp, yêu cầu của bạn sẽ được gửi đến Llama 3 70B Instruct, Claude 3.5 Sonnet (tự điều chỉnh) hoặc GPT-4o."
  },
  "perplexity/sonar": {
    "description": "Sản phẩm nhẹ của Perplexity với khả năng tìm kiếm có căn cứ, nhanh hơn và rẻ hơn Sonar Pro."
  },
  "perplexity/sonar-pro": {
    "description": "Sản phẩm hàng đầu của Perplexity với khả năng tìm kiếm có căn cứ, hỗ trợ truy vấn nâng cao và các thao tác tiếp theo."
  },
  "perplexity/sonar-reasoning": {
    "description": "Mô hình tập trung vào suy luận, xuất ra chuỗi suy nghĩ (CoT) trong phản hồi, cung cấp giải thích chi tiết có căn cứ tìm kiếm."
  },
  "perplexity/sonar-reasoning-pro": {
    "description": "Mô hình tập trung suy luận nâng cao, xuất ra chuỗi suy nghĩ (CoT) trong phản hồi, cung cấp giải thích toàn diện với khả năng tìm kiếm nâng cao và nhiều truy vấn tìm kiếm cho mỗi yêu cầu."
  },
  "phi3": {
    "description": "Phi-3 là mô hình mở nhẹ do Microsoft phát hành, phù hợp cho việc tích hợp hiệu quả và suy luận kiến thức quy mô lớn."
  },
  "phi3:14b": {
    "description": "Phi-3 là mô hình mở nhẹ do Microsoft phát hành, phù hợp cho việc tích hợp hiệu quả và suy luận kiến thức quy mô lớn."
  },
  "pixtral-12b-2409": {
    "description": "Mô hình Pixtral thể hiện khả năng mạnh mẽ trong các nhiệm vụ như hiểu biểu đồ và hình ảnh, hỏi đáp tài liệu, suy luận đa phương tiện và tuân thủ hướng dẫn, có khả năng tiếp nhận hình ảnh với độ phân giải và tỷ lệ khung hình tự nhiên, cũng như xử lý bất kỳ số lượng hình ảnh nào trong cửa sổ ngữ cảnh dài lên đến 128K token."
  },
  "pixtral-large-latest": {
    "description": "Pixtral Large là một mô hình đa phương thức mã nguồn mở với 1240 tỷ tham số, được xây dựng dựa trên Mistral Large 2. Đây là mô hình thứ hai trong gia đình đa phương thức của chúng tôi, thể hiện khả năng hiểu hình ảnh ở mức tiên tiến."
  },
  "pro-128k": {
    "description": "Spark Pro 128K được cấu hình với khả năng xử lý ngữ cảnh cực lớn, có thể xử lý tới 128K thông tin ngữ cảnh, đặc biệt phù hợp cho việc phân tích toàn bộ và xử lý mối liên hệ logic lâu dài trong nội dung văn bản dài, có thể cung cấp logic mạch lạc và hỗ trợ trích dẫn đa dạng trong giao tiếp văn bản phức tạp."
  },
  "pro-deepseek-r1": {
    "description": "Mô hình chuyên dụng cho dịch vụ doanh nghiệp, hỗ trợ dịch vụ đồng thời."
  },
  "pro-deepseek-v3": {
    "description": "Mô hình chuyên dụng cho dịch vụ doanh nghiệp, hỗ trợ dịch vụ đồng thời."
  },
  "qianfan-70b": {
    "description": "Qianfan 70B, mô hình tiếng Trung với tham số lớn, phù hợp cho việc tạo nội dung chất lượng cao và các nhiệm vụ suy luận phức tạp."
  },
  "qianfan-8b": {
    "description": "Qianfan 8B, mô hình đa năng cỡ trung, cân bằng giữa chi phí và hiệu quả trong tạo văn bản và hỏi đáp."
  },
  "qianfan-agent-intent-32k": {
    "description": "Qianfan Agent Intent 32K, mô hình nhận diện ý định và điều phối tác tử, hỗ trợ ngữ cảnh dài."
  },
  "qianfan-agent-lite-8k": {
    "description": "Qianfan Agent Lite 8K, mô hình tác tử nhẹ, phù hợp cho hội thoại đa lượt chi phí thấp và điều phối nghiệp vụ."
  },
  "qianfan-agent-speed-32k": {
    "description": "Qianfan Agent Speed 32K, mô hình tác tử kiểm soát lưu lượng cao, thích hợp cho ứng dụng Agent quy mô lớn và đa nhiệm."
  },
  "qianfan-agent-speed-8k": {
    "description": "Qianfan Agent Speed 8K, mô hình tác tử hiệu suất cao cho hội thoại ngắn và phản hồi nhanh với khả năng xử lý đồng thời cao."
  },
  "qianfan-check-vl": {
    "description": "Qianfan Check VL, mô hình kiểm duyệt và phát hiện nội dung đa phương tiện, hỗ trợ kiểm tra tuân thủ và nhận diện hình ảnh-văn bản."
  },
  "qianfan-composition": {
    "description": "Qianfan Composition, mô hình sáng tạo đa phương tiện, hỗ trợ hiểu và tạo nội dung kết hợp hình ảnh và văn bản."
  },
  "qianfan-engcard-vl": {
    "description": "Qianfan EngCard VL, mô hình nhận diện đa phương tiện chuyên biệt cho ngữ cảnh tiếng Anh."
  },
  "qianfan-lightning-128b-a19b": {
    "description": "Qianfan Lightning 128B A19B, mô hình tiếng Trung hiệu suất cao, phù hợp cho hỏi đáp phức tạp và suy luận quy mô lớn."
  },
  "qianfan-llama-vl-8b": {
    "description": "Qianfan Llama VL 8B, mô hình đa phương tiện dựa trên Llama, hướng đến nhiệm vụ hiểu hình ảnh-văn bản tổng quát."
  },
  "qianfan-multipicocr": {
    "description": "Qianfan MultiPicOCR, mô hình OCR cho nhiều hình ảnh, hỗ trợ phát hiện và nhận diện văn bản từ nhiều ảnh."
  },
  "qianfan-qi-vl": {
    "description": "Qianfan QI VL, mô hình hỏi đáp đa phương tiện, hỗ trợ truy xuất và trả lời chính xác trong các ngữ cảnh hình ảnh-văn bản phức tạp."
  },
  "qianfan-singlepicocr": {
    "description": "Qianfan SinglePicOCR, mô hình OCR cho một hình ảnh, hỗ trợ nhận diện ký tự với độ chính xác cao."
  },
  "qianfan-vl-70b": {
    "description": "Qianfan VL 70B, mô hình ngôn ngữ thị giác với tham số lớn, phù hợp cho các tình huống hiểu hình ảnh-văn bản phức tạp."
  },
  "qianfan-vl-8b": {
    "description": "Qianfan VL 8B, mô hình ngôn ngữ thị giác nhẹ, thích hợp cho hỏi đáp và phân tích hình ảnh-văn bản hàng ngày."
  },
  "qvq-72b-preview": {
    "description": "Mô hình QVQ là mô hình nghiên cứu thử nghiệm do đội ngũ Qwen phát triển, tập trung vào việc nâng cao khả năng suy luận hình ảnh, đặc biệt trong lĩnh vực suy luận toán học."
  },
  "qvq-max": {
    "description": "Mô hình suy luận thị giác QVQ của Tongyi Qianwen, hỗ trợ đầu vào thị giác và xuất ra chuỗi suy nghĩ, thể hiện năng lực mạnh mẽ trong toán học, lập trình, phân tích thị giác, sáng tạo và các nhiệm vụ chung."
  },
  "qvq-plus": {
    "description": "Mô hình suy luận thị giác. Hỗ trợ đầu vào hình ảnh và đầu ra chuỗi suy nghĩ, phiên bản plus ra mắt sau mô hình qvq-max, với tốc độ suy luận nhanh hơn, hiệu quả và chi phí cân bằng hơn so với qvq-max."
  },
  "qwen-3-32b": {
    "description": "Qwen 3 32B: Mô hình dòng Qwen có hiệu suất tốt trong các nhiệm vụ đa ngôn ngữ và lập trình, thích hợp cho các ứng dụng sản xuất quy mô trung bình."
  },
  "qwen-3-coder-480b": {
    "description": "Qwen 3 Coder 480B: Mô hình ngữ cảnh dài dành cho sinh mã và các nhiệm vụ lập trình phức tạp."
  },
  "qwen-coder-plus": {
    "description": "Mô hình mã hóa Tongyi Qianwen."
  },
  "qwen-coder-turbo": {
    "description": "Mô hình mã hóa Tongyi Qianwen."
  },
  "qwen-coder-turbo-latest": {
    "description": "Mô hình mã Qwen."
  },
  "qwen-flash": {
    "description": "Các mô hình thuộc dòng 通义千问 có tốc độ nhanh nhất và chi phí rất thấp, phù hợp cho các nhiệm vụ đơn giản."
  },
  "qwen-image": {
    "description": "Qwen-Image là một mô hình sinh hình ảnh đa dụng, hỗ trợ nhiều phong cách nghệ thuật và đặc biệt giỏi trong việc tái hiện văn bản phức tạp, nhất là văn bản tiếng Trung và tiếng Anh. Mô hình hỗ trợ bố cục nhiều dòng, sinh văn bản ở cấp đoạn và khắc họa các chi tiết tinh tế, cho phép thực hiện các thiết kế bố cục kết hợp hình ảnh và văn bản phức tạp."
  },
  "qwen-image-edit": {
    "description": "Qwen Image Edit là một mô hình tạo hình ảnh từ hình ảnh, hỗ trợ chỉnh sửa và thay đổi hình ảnh dựa trên hình ảnh đầu vào và các gợi ý văn bản, có khả năng điều chỉnh chính xác và sáng tạo hình ảnh gốc theo yêu cầu của người dùng."
  },
  "qwen-long": {
    "description": "Mô hình ngôn ngữ quy mô lớn Qwen, hỗ trợ ngữ cảnh văn bản dài và chức năng đối thoại dựa trên tài liệu dài, nhiều tài liệu."
  },
  "qwen-math-plus": {
    "description": "Mô hình toán học Tongyi Qianwen được thiết kế chuyên biệt cho việc giải toán."
  },
  "qwen-math-plus-latest": {
    "description": "Mô hình toán học Qwen được thiết kế đặc biệt để giải quyết các bài toán toán học."
  },
  "qwen-math-turbo": {
    "description": "Mô hình toán học Tongyi Qianwen được thiết kế chuyên biệt cho việc giải toán."
  },
  "qwen-math-turbo-latest": {
    "description": "Mô hình toán học Qwen được thiết kế đặc biệt để giải quyết các bài toán toán học."
  },
  "qwen-max": {
    "description": "Mô hình ngôn ngữ quy mô lớn Qwen cấp tỷ, hỗ trợ đầu vào bằng tiếng Trung, tiếng Anh và nhiều ngôn ngữ khác, là mô hình API đằng sau phiên bản sản phẩm Qwen 2.5 hiện tại."
  },
  "qwen-omni-turbo": {
    "description": "Dòng mô hình Qwen-Omni hỗ trợ đầu vào đa dạng các loại dữ liệu đa phương thức, bao gồm video, âm thanh, hình ảnh, văn bản, và xuất ra âm thanh cùng văn bản."
  },
  "qwen-plus": {
    "description": "Mô hình ngôn ngữ quy mô lớn Qwen phiên bản nâng cao, hỗ trợ đầu vào bằng tiếng Trung, tiếng Anh và nhiều ngôn ngữ khác."
  },
  "qwen-turbo": {
    "description": "通义千问 Turbo 将不再更新，建议替换为通义千问 Flash。通义千问是一款超大规模的语言模型，支持中文、英文及其他语言的输入。"
  },
  "qwen-vl-chat-v1": {
    "description": "Mô hình Qwen VL hỗ trợ các phương thức tương tác linh hoạt, bao gồm nhiều hình ảnh, nhiều vòng hỏi đáp, sáng tạo, v.v."
  },
  "qwen-vl-max": {
    "description": "Mô hình ngôn ngữ thị giác quy mô siêu lớn Tongyi Qianwen. So với phiên bản nâng cao, tiếp tục cải thiện khả năng suy luận thị giác và tuân thủ chỉ thị, cung cấp mức độ nhận thức và cảm nhận thị giác cao hơn."
  },
  "qwen-vl-max-latest": {
    "description": "Mô hình ngôn ngữ hình ảnh quy mô siêu lớn của Tongyi Qianwen. So với phiên bản nâng cao, nó lại nâng cao khả năng suy luận hình ảnh và khả năng tuân thủ chỉ dẫn, cung cấp mức độ nhận thức và cảm nhận hình ảnh cao hơn."
  },
  "qwen-vl-ocr": {
    "description": "Tongyi Qianwen OCR là mô hình chuyên biệt cho trích xuất văn bản, tập trung vào khả năng trích xuất chữ viết từ các loại hình ảnh như tài liệu, bảng biểu, đề thi, chữ viết tay. Mô hình có thể nhận diện nhiều ngôn ngữ, hiện hỗ trợ: tiếng Trung, tiếng Anh, tiếng Pháp, tiếng Nhật, tiếng Hàn, tiếng Đức, tiếng Nga, tiếng Ý, tiếng Việt, tiếng Ả Rập."
  },
  "qwen-vl-plus": {
    "description": "Phiên bản nâng cao của mô hình ngôn ngữ thị giác quy mô lớn Tongyi Qianwen. Nâng cao đáng kể khả năng nhận diện chi tiết và nhận dạng văn bản, hỗ trợ hình ảnh có độ phân giải trên một triệu điểm ảnh và tỷ lệ khung hình tùy ý."
  },
  "qwen-vl-plus-latest": {
    "description": "Mô hình ngôn ngữ hình ảnh quy mô lớn phiên bản nâng cao của Tongyi Qianwen. Nâng cao khả năng nhận diện chi tiết và nhận diện văn bản, hỗ trợ độ phân giải trên một triệu pixel và các tỷ lệ chiều dài và chiều rộng tùy ý."
  },
  "qwen-vl-v1": {
    "description": "Mô hình được khởi tạo bằng mô hình ngôn ngữ Qwen-7B, thêm mô hình hình ảnh, mô hình được huấn luyện trước với độ phân giải đầu vào hình ảnh là 448."
  },
  "qwen/qwen-2-7b-instruct": {
    "description": "Qwen2 là dòng mô hình ngôn ngữ lớn hoàn toàn mới. Qwen2 7B là một mô hình dựa trên transformer, thể hiện xuất sắc trong việc hiểu ngôn ngữ, khả năng đa ngôn ngữ, lập trình, toán học và suy luận."
  },
  "qwen/qwen-2-7b-instruct:free": {
    "description": "Qwen2 là một loạt mô hình ngôn ngữ lớn hoàn toàn mới, có khả năng hiểu và sinh mạnh mẽ hơn."
  },
  "qwen/qwen-2-vl-72b-instruct": {
    "description": "Qwen2-VL là phiên bản cải tiến mới nhất của mô hình Qwen-VL, đã đạt được hiệu suất tiên tiến trong các bài kiểm tra hiểu biết thị giác, bao gồm MathVista, DocVQA, RealWorldQA và MTVQA. Qwen2-VL có khả năng hiểu video dài hơn 20 phút, phục vụ cho các câu hỏi, đối thoại và sáng tạo nội dung dựa trên video chất lượng cao. Nó cũng có khả năng suy luận và ra quyết định phức tạp, có thể tích hợp với các thiết bị di động, robot, v.v., để thực hiện các thao tác tự động dựa trên môi trường thị giác và hướng dẫn văn bản. Ngoài tiếng Anh và tiếng Trung, Qwen2-VL hiện cũng hỗ trợ hiểu văn bản trong hình ảnh bằng nhiều ngôn ngữ khác nhau, bao gồm hầu hết các ngôn ngữ châu Âu, tiếng Nhật, tiếng Hàn, tiếng Ả Rập và tiếng Việt."
  },
  "qwen/qwen-2.5-72b-instruct": {
    "description": "Qwen2.5-72B-Instruct là một trong những mô hình ngôn ngữ lớn mới nhất được phát hành bởi Alibaba Cloud. Mô hình 72B này có khả năng cải thiện đáng kể trong các lĩnh vực như mã hóa và toán học. Mô hình cũng cung cấp hỗ trợ đa ngôn ngữ, bao gồm hơn 29 ngôn ngữ, bao gồm tiếng Trung, tiếng Anh, v.v. Mô hình đã có sự cải thiện đáng kể trong việc theo dõi hướng dẫn, hiểu dữ liệu có cấu trúc và tạo ra đầu ra có cấu trúc (đặc biệt là JSON)."
  },
  "qwen/qwen2.5-32b-instruct": {
    "description": "Qwen2.5-32B-Instruct là một trong những mô hình ngôn ngữ lớn mới nhất được phát hành bởi Alibaba Cloud. Mô hình 32B này có khả năng cải thiện đáng kể trong các lĩnh vực như mã hóa và toán học. Mô hình cung cấp hỗ trợ đa ngôn ngữ, bao gồm hơn 29 ngôn ngữ, bao gồm tiếng Trung, tiếng Anh, v.v. Mô hình đã có sự cải thiện đáng kể trong việc theo dõi hướng dẫn, hiểu dữ liệu có cấu trúc và tạo ra đầu ra có cấu trúc (đặc biệt là JSON)."
  },
  "qwen/qwen2.5-7b-instruct": {
    "description": "LLM hướng đến tiếng Trung và tiếng Anh, tập trung vào ngôn ngữ, lập trình, toán học, suy luận và các lĩnh vực khác."
  },
  "qwen/qwen2.5-coder-32b-instruct": {
    "description": "LLM cao cấp, hỗ trợ tạo mã, suy luận và sửa chữa, bao gồm các ngôn ngữ lập trình phổ biến."
  },
  "qwen/qwen2.5-coder-7b-instruct": {
    "description": "Mô hình mã mạnh mẽ cỡ trung, hỗ trợ độ dài ngữ cảnh 32K, xuất sắc trong lập trình đa ngôn ngữ."
  },
  "qwen/qwen3-14b": {
    "description": "Qwen3-14B là một mô hình ngôn ngữ nguyên nhân dày đặc với 14,8 tỷ tham số trong dòng Qwen3, được thiết kế cho suy luận phức tạp và đối thoại hiệu quả. Nó hỗ trợ chuyển đổi liền mạch giữa chế độ \"suy nghĩ\" cho các nhiệm vụ như toán học, lập trình và suy luận logic với chế độ \"không suy nghĩ\" cho đối thoại thông thường. Mô hình này đã được tinh chỉnh để sử dụng cho việc tuân theo chỉ dẫn, sử dụng công cụ đại lý, viết sáng tạo và các nhiệm vụ đa ngôn ngữ trên hơn 100 ngôn ngữ và phương ngữ. Nó xử lý ngữ cảnh 32K token một cách tự nhiên và có thể mở rộng lên 131K token bằng cách sử dụng mở rộng dựa trên YaRN."
  },
  "qwen/qwen3-14b:free": {
    "description": "Qwen3-14B là một mô hình ngôn ngữ nguyên nhân dày đặc với 14,8 tỷ tham số trong dòng Qwen3, được thiết kế cho suy luận phức tạp và đối thoại hiệu quả. Nó hỗ trợ chuyển đổi liền mạch giữa chế độ \"suy nghĩ\" cho các nhiệm vụ như toán học, lập trình và suy luận logic với chế độ \"không suy nghĩ\" cho đối thoại thông thường. Mô hình này đã được tinh chỉnh để sử dụng cho việc tuân theo chỉ dẫn, sử dụng công cụ đại lý, viết sáng tạo và các nhiệm vụ đa ngôn ngữ trên hơn 100 ngôn ngữ và phương ngữ. Nó xử lý ngữ cảnh 32K token một cách tự nhiên và có thể mở rộng lên 131K token bằng cách sử dụng mở rộng dựa trên YaRN."
  },
  "qwen/qwen3-235b-a22b": {
    "description": "Qwen3-235B-A22B là mô hình hỗn hợp chuyên gia (MoE) với 235B tham số được phát triển bởi Qwen, kích hoạt 22B tham số mỗi lần truyền tiến. Nó hỗ trợ chuyển đổi liền mạch giữa chế độ \"suy nghĩ\" cho suy luận phức tạp, toán học và các nhiệm vụ mã với chế độ \"không suy nghĩ\" cho hiệu suất đối thoại thông thường. Mô hình này thể hiện khả năng suy luận mạnh mẽ, hỗ trợ đa ngôn ngữ (hơn 100 ngôn ngữ và phương ngữ), tuân theo chỉ dẫn nâng cao và khả năng gọi công cụ đại lý. Nó xử lý cửa sổ ngữ cảnh 32K token một cách tự nhiên và có thể mở rộng lên 131K token bằng cách sử dụng mở rộng dựa trên YaRN."
  },
  "qwen/qwen3-235b-a22b:free": {
    "description": "Qwen3-235B-A22B là mô hình hỗn hợp chuyên gia (MoE) với 235B tham số được phát triển bởi Qwen, kích hoạt 22B tham số mỗi lần truyền tiến. Nó hỗ trợ chuyển đổi liền mạch giữa chế độ \"suy nghĩ\" cho suy luận phức tạp, toán học và các nhiệm vụ mã với chế độ \"không suy nghĩ\" cho hiệu suất đối thoại thông thường. Mô hình này thể hiện khả năng suy luận mạnh mẽ, hỗ trợ đa ngôn ngữ (hơn 100 ngôn ngữ và phương ngữ), tuân theo chỉ dẫn nâng cao và khả năng gọi công cụ đại lý. Nó xử lý cửa sổ ngữ cảnh 32K token một cách tự nhiên và có thể mở rộng lên 131K token bằng cách sử dụng mở rộng dựa trên YaRN."
  },
  "qwen/qwen3-30b-a3b": {
    "description": "Qwen3 là thế hệ mới nhất trong dòng mô hình ngôn ngữ lớn Qwen, với kiến trúc hỗn hợp chuyên gia (MoE) dày đặc, thể hiện xuất sắc trong suy luận, hỗ trợ đa ngôn ngữ và các nhiệm vụ đại lý nâng cao. Khả năng chuyển đổi liền mạch giữa chế độ suy nghĩ cho suy luận phức tạp và chế độ không suy nghĩ cho đối thoại hiệu quả đảm bảo hiệu suất đa chức năng và chất lượng cao.\n\nQwen3 vượt trội hơn hẳn các mô hình trước như QwQ và Qwen2.5, cung cấp khả năng toán học, lập trình, suy luận thông thường, viết sáng tạo và đối thoại tương tác xuất sắc. Biến thể Qwen3-30B-A3B chứa 30,5 tỷ tham số (3,3 tỷ tham số kích hoạt), 48 lớp, 128 chuyên gia (mỗi nhiệm vụ kích hoạt 8), và hỗ trợ ngữ cảnh lên đến 131K token (sử dụng YaRN), thiết lập tiêu chuẩn mới cho các mô hình mã nguồn mở."
  },
  "qwen/qwen3-30b-a3b:free": {
    "description": "Qwen3 là thế hệ mới nhất trong dòng mô hình ngôn ngữ lớn Qwen, với kiến trúc hỗn hợp chuyên gia (MoE) dày đặc, thể hiện xuất sắc trong suy luận, hỗ trợ đa ngôn ngữ và các nhiệm vụ đại lý nâng cao. Khả năng chuyển đổi liền mạch giữa chế độ suy nghĩ cho suy luận phức tạp và chế độ không suy nghĩ cho đối thoại hiệu quả đảm bảo hiệu suất đa chức năng và chất lượng cao.\n\nQwen3 vượt trội hơn hẳn các mô hình trước như QwQ và Qwen2.5, cung cấp khả năng toán học, lập trình, suy luận thông thường, viết sáng tạo và đối thoại tương tác xuất sắc. Biến thể Qwen3-30B-A3B chứa 30,5 tỷ tham số (3,3 tỷ tham số kích hoạt), 48 lớp, 128 chuyên gia (mỗi nhiệm vụ kích hoạt 8), và hỗ trợ ngữ cảnh lên đến 131K token (sử dụng YaRN), thiết lập tiêu chuẩn mới cho các mô hình mã nguồn mở."
  },
  "qwen/qwen3-32b": {
    "description": "Qwen3-32B là một mô hình ngôn ngữ nguyên nhân dày đặc với 32,8 tỷ tham số trong dòng Qwen3, được tối ưu hóa cho suy luận phức tạp và đối thoại hiệu quả. Nó hỗ trợ chuyển đổi liền mạch giữa chế độ \"suy nghĩ\" cho các nhiệm vụ như toán học, lập trình và suy luận logic với chế độ \"không suy nghĩ\" cho đối thoại nhanh hơn và thông thường. Mô hình này thể hiện hiệu suất mạnh mẽ trong việc tuân theo chỉ dẫn, sử dụng công cụ đại lý, viết sáng tạo và các nhiệm vụ đa ngôn ngữ trên hơn 100 ngôn ngữ và phương ngữ. Nó xử lý ngữ cảnh 32K token một cách tự nhiên và có thể mở rộng lên 131K token bằng cách sử dụng mở rộng dựa trên YaRN."
  },
  "qwen/qwen3-32b:free": {
    "description": "Qwen3-32B là một mô hình ngôn ngữ nguyên nhân dày đặc với 32,8 tỷ tham số trong dòng Qwen3, được tối ưu hóa cho suy luận phức tạp và đối thoại hiệu quả. Nó hỗ trợ chuyển đổi liền mạch giữa chế độ \"suy nghĩ\" cho các nhiệm vụ như toán học, lập trình và suy luận logic với chế độ \"không suy nghĩ\" cho đối thoại nhanh hơn và thông thường. Mô hình này thể hiện hiệu suất mạnh mẽ trong việc tuân theo chỉ dẫn, sử dụng công cụ đại lý, viết sáng tạo và các nhiệm vụ đa ngôn ngữ trên hơn 100 ngôn ngữ và phương ngữ. Nó xử lý ngữ cảnh 32K token một cách tự nhiên và có thể mở rộng lên 131K token bằng cách sử dụng mở rộng dựa trên YaRN."
  },
  "qwen/qwen3-8b:free": {
    "description": "Qwen3-8B là một mô hình ngôn ngữ nguyên nhân dày đặc với 8,2 tỷ tham số trong dòng Qwen3, được thiết kế cho các nhiệm vụ yêu cầu suy luận dày đặc và đối thoại hiệu quả. Nó hỗ trợ chuyển đổi liền mạch giữa chế độ \"suy nghĩ\" cho toán học, lập trình và suy luận logic với chế độ \"không suy nghĩ\" cho đối thoại thông thường. Mô hình này đã được tinh chỉnh để sử dụng cho việc tuân theo chỉ dẫn, tích hợp đại lý, viết sáng tạo và sử dụng đa ngôn ngữ trên hơn 100 ngôn ngữ và phương ngữ. Nó hỗ trợ cửa sổ ngữ cảnh 32K token và có thể mở rộng lên 131K token thông qua YaRN."
  },
  "qwen2": {
    "description": "Qwen2 là mô hình ngôn ngữ quy mô lớn thế hệ mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen2.5": {
    "description": "Qwen2.5 là thế hệ mô hình ngôn ngữ quy mô lớn mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen2.5-14b-instruct": {
    "description": "Mô hình 14B quy mô mở nguồn của Qwen 2.5."
  },
  "qwen2.5-14b-instruct-1m": {
    "description": "Mô hình quy mô 72B được mở nguồn từ Qianwen 2.5."
  },
  "qwen2.5-32b-instruct": {
    "description": "Mô hình 32B quy mô mở nguồn của Qwen 2.5."
  },
  "qwen2.5-72b-instruct": {
    "description": "Mô hình 72B quy mô mở nguồn của Qwen 2.5."
  },
  "qwen2.5-7b-instruct": {
    "description": "Qwen2.5 7B Instruct, mô hình chỉ dẫn mã nguồn mở trưởng thành, phù hợp cho hội thoại và tạo nội dung trong nhiều ngữ cảnh."
  },
  "qwen2.5-coder-1.5b-instruct": {
    "description": "Phiên bản mã nguồn mở của mô hình mã Qwen."
  },
  "qwen2.5-coder-14b-instruct": {
    "description": "Phiên bản mã nguồn mở của mô hình mã hóa Tongyi Qianwen."
  },
  "qwen2.5-coder-32b-instruct": {
    "description": "Phiên bản mã nguồn mở của mô hình mã Qwen."
  },
  "qwen2.5-coder-7b-instruct": {
    "description": "Phiên bản mã nguồn mở của mô hình mã Qwen."
  },
  "qwen2.5-coder-instruct": {
    "description": "Qwen2.5-Coder là mô hình ngôn ngữ lớn mới nhất trong series Qwen, chuyên dụng cho lập trình (trước đây được gọi là CodeQwen)."
  },
  "qwen2.5-instruct": {
    "description": "Qwen2.5 là phiên bản mới nhất của mô hình ngôn ngữ lớn Qwen. Đối với Qwen2.5, chúng tôi đã phát hành nhiều mô hình ngôn ngữ cơ sở và mô hình ngôn ngữ điều chỉnh theo lệnh, với phạm vi tham số từ 500 triệu đến 7,2 tỷ."
  },
  "qwen2.5-math-1.5b-instruct": {
    "description": "Mô hình Qwen-Math có khả năng giải toán mạnh mẽ."
  },
  "qwen2.5-math-72b-instruct": {
    "description": "Mô hình Qwen-Math có khả năng giải quyết bài toán toán học mạnh mẽ."
  },
  "qwen2.5-math-7b-instruct": {
    "description": "Mô hình Qwen-Math có khả năng giải quyết bài toán toán học mạnh mẽ."
  },
  "qwen2.5-omni-7b": {
    "description": "Mô hình Qwen-Omni hỗ trợ đầu vào từ nhiều loại dữ liệu khác nhau, bao gồm video, âm thanh, hình ảnh và văn bản, và xuất ra âm thanh và văn bản."
  },
  "qwen2.5-vl-32b-instruct": {
    "description": "Qwen2.5 VL 32B Instruct, mô hình đa phương tiện mã nguồn mở, thích hợp cho triển khai riêng tư và ứng dụng đa ngữ cảnh."
  },
  "qwen2.5-vl-72b-instruct": {
    "description": "Nâng cao khả năng theo dõi lệnh, toán học, giải quyết vấn đề, mã hóa, nâng cao khả năng nhận diện mọi thứ, hỗ trợ định vị chính xác các yếu tố thị giác từ nhiều định dạng khác nhau, hỗ trợ hiểu và định vị thời gian sự kiện trong các tệp video dài (tối đa 10 phút), có khả năng hiểu thứ tự thời gian và tốc độ, hỗ trợ điều khiển Agent trên OS hoặc Mobile dựa trên khả năng phân tích và định vị, khả năng trích xuất thông tin quan trọng và xuất định dạng Json mạnh mẽ, phiên bản này là phiên bản 72B, phiên bản mạnh nhất trong dòng sản phẩm này."
  },
  "qwen2.5-vl-7b-instruct": {
    "description": "Qwen2.5 VL 7B Instruct, mô hình đa phương tiện nhẹ, cân bằng giữa chi phí triển khai và khả năng nhận diện."
  },
  "qwen2.5-vl-instruct": {
    "description": "Qwen2.5-VL là phiên bản mới nhất của mô hình ngôn ngữ và hình ảnh trong gia đình mô hình Qwen."
  },
  "qwen2.5:0.5b": {
    "description": "Qwen2.5 là thế hệ mô hình ngôn ngữ quy mô lớn mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen2.5:1.5b": {
    "description": "Qwen2.5 là thế hệ mô hình ngôn ngữ quy mô lớn mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen2.5:72b": {
    "description": "Qwen2.5 là thế hệ mô hình ngôn ngữ quy mô lớn mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen2:0.5b": {
    "description": "Qwen2 là mô hình ngôn ngữ quy mô lớn thế hệ mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen2:1.5b": {
    "description": "Qwen2 là mô hình ngôn ngữ quy mô lớn thế hệ mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen2:72b": {
    "description": "Qwen2 là mô hình ngôn ngữ quy mô lớn thế hệ mới của Alibaba, hỗ trợ các nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen3": {
    "description": "Qwen3 là mô hình ngôn ngữ quy mô lớn thế hệ mới của Alibaba, hỗ trợ nhu cầu ứng dụng đa dạng với hiệu suất xuất sắc."
  },
  "qwen3-0.6b": {
    "description": "Qwen3 0.6B, mô hình cấp nhập môn, phù hợp cho suy luận đơn giản và môi trường tài nguyên cực kỳ hạn chế."
  },
  "qwen3-1.7b": {
    "description": "Qwen3 1.7B, mô hình siêu nhẹ, dễ dàng triển khai tại biên và thiết bị đầu cuối."
  },
  "qwen3-14b": {
    "description": "Qwen3 14B, mô hình cỡ trung, phù hợp cho hỏi đáp đa ngôn ngữ và tạo văn bản."
  },
  "qwen3-235b-a22b": {
    "description": "Qwen3 235B A22B, mô hình lớn đa năng, hướng đến nhiều nhiệm vụ phức tạp."
  },
  "qwen3-235b-a22b-instruct-2507": {
    "description": "Qwen3 235B A22B Instruct 2507, mô hình chỉ dẫn hàng đầu đa năng, phù hợp cho nhiều nhiệm vụ tạo nội dung và suy luận."
  },
  "qwen3-235b-a22b-thinking-2507": {
    "description": "Qwen3 235B A22B Thinking 2507, mô hình tư duy quy mô siêu lớn, thích hợp cho suy luận độ khó cao."
  },
  "qwen3-30b-a3b": {
    "description": "Qwen3 30B A3B, mô hình đa năng cỡ trung-lớn, cân bằng giữa chi phí và hiệu quả."
  },
  "qwen3-30b-a3b-instruct-2507": {
    "description": "Qwen3 30B A3B Instruct 2507, mô hình chỉ dẫn cỡ trung-lớn, phù hợp cho tạo nội dung chất lượng cao và hỏi đáp."
  },
  "qwen3-30b-a3b-thinking-2507": {
    "description": "Qwen3 30B A3B Thinking 2507, mô hình tư duy cỡ trung-lớn, cân bằng giữa độ chính xác và chi phí."
  },
  "qwen3-32b": {
    "description": "Qwen3 32B, phù hợp cho các nhiệm vụ đa năng yêu cầu khả năng hiểu sâu hơn."
  },
  "qwen3-4b": {
    "description": "Qwen3 4B, phù hợp cho ứng dụng vừa và nhỏ và các tình huống suy luận cục bộ."
  },
  "qwen3-8b": {
    "description": "Qwen3 8B, mô hình nhẹ, triển khai linh hoạt, phù hợp cho các nghiệp vụ có tần suất cao."
  },
  "qwen3-coder-30b-a3b-instruct": {
    "description": "Phiên bản mã nguồn mở của mô hình mã hóa Tongyi Qianwen. Mô hình qwen3-coder-30b-a3b-instruct mới nhất được phát triển dựa trên Qwen3, có khả năng hoạt động như một Tác nhân Lập trình mạnh mẽ, thành thạo trong việc gọi công cụ và tương tác môi trường, hỗ trợ lập trình tự động với năng lực mã hóa xuất sắc và khả năng tổng quát cao."
  },
  "qwen3-coder-480b-a35b-instruct": {
    "description": "Qwen3 Coder 480B A35B Instruct, mô hình mã hóa hàng đầu, hỗ trợ lập trình đa ngôn ngữ và hiểu mã phức tạp."
  },
  "qwen3-coder-flash": {
    "description": "Mô hình mã nguồn của Thông Nghĩa Thiên Vấn. Bộ mô hình Qwen3-Coder mới nhất dựa trên Qwen3 là mô hình tạo mã, có khả năng Coding Agent mạnh mẽ, thành thạo gọi công cụ và tương tác môi trường, có thể tự lập trình, vừa xuất sắc về năng lực mã hóa vừa có khả năng tổng quát."
  },
  "qwen3-coder-plus": {
    "description": "Mô hình mã nguồn của Thông Nghĩa Thiên Vấn. Bộ mô hình Qwen3-Coder mới nhất dựa trên Qwen3 là mô hình tạo mã, có khả năng Coding Agent mạnh mẽ, thành thạo gọi công cụ và tương tác môi trường, có thể tự lập trình, vừa xuất sắc về năng lực mã hóa vừa có khả năng tổng quát."
  },
  "qwen3-coder:480b": {
    "description": "Mô hình ngữ cảnh dài hiệu năng cao của Alibaba dành cho các tác vụ đại diện và mã hóa."
  },
  "qwen3-max": {
    "description": "Dòng mô hình Max của Tongyi Qianwen 3, so với dòng 2.5 có sự cải thiện lớn về khả năng chung, bao gồm hiểu văn bản song ngữ Trung-Anh, tuân thủ chỉ dẫn phức tạp, khả năng thực hiện các tác vụ mở chủ quan, đa ngôn ngữ và gọi công cụ; giảm thiểu ảo tưởng kiến thức của mô hình. Phiên bản qwen3-max mới nhất đã nâng cấp chuyên biệt về lập trình tác nhân và gọi công cụ so với phiên bản qwen3-max-preview. Mô hình chính thức phát hành đạt mức SOTA trong lĩnh vực, phù hợp với các nhu cầu tác nhân phức tạp hơn."
  },
  "qwen3-max-preview": {
    "description": "Mô hình mạnh nhất trong dòng Tongyi Qianwen, phù hợp với các tác vụ phức tạp và nhiều bước. Phiên bản xem trước đã hỗ trợ khả năng suy luận."
  },
  "qwen3-next-80b-a3b-instruct": {
    "description": "Mô hình mã nguồn mở thế hệ mới không có chế độ suy nghĩ dựa trên Qwen3, so với phiên bản trước (Thông Nghĩa Thiên Vấn 3-235B-A22B-Instruct-2507) có khả năng hiểu văn bản tiếng Trung tốt hơn, năng lực suy luận logic được cải thiện, và hiệu suất trong các nhiệm vụ tạo văn bản cũng tốt hơn."
  },
  "qwen3-next-80b-a3b-thinking": {
    "description": "Qwen3 Next 80B A3B Thinking, phiên bản mô hình tư duy hàng đầu cho các nhiệm vụ phức tạp."
  },
  "qwen3-omni-flash": {
    "description": "Mô hình Qwen-Omni có thể tiếp nhận đầu vào kết hợp từ nhiều phương thức như văn bản, hình ảnh, âm thanh và video, và tạo phản hồi dưới dạng văn bản hoặc giọng nói. Mô hình cung cấp nhiều giọng nói nhân hóa, hỗ trợ đầu ra bằng nhiều ngôn ngữ và phương ngữ, phù hợp với các ứng dụng như sáng tác văn bản, nhận diện hình ảnh và trợ lý giọng nói."
  },
  "qwen3-vl-235b-a22b-instruct": {
    "description": "Qwen3 VL 235B A22B Instruct, mô hình đa phương tiện hàng đầu, hướng đến các tình huống yêu cầu cao về hiểu và sáng tạo."
  },
  "qwen3-vl-235b-a22b-thinking": {
    "description": "Qwen3 VL 235B A22B Thinking, phiên bản tư duy hàng đầu, dùng cho suy luận và lập kế hoạch đa phương tiện phức tạp."
  },
  "qwen3-vl-30b-a3b-instruct": {
    "description": "Qwen3 VL 30B A3B Instruct, mô hình đa phương tiện lớn, cân bằng giữa độ chính xác và hiệu suất suy luận."
  },
  "qwen3-vl-30b-a3b-thinking": {
    "description": "Qwen3 VL 30B A3B Thinking, phiên bản tư duy sâu cho các nhiệm vụ đa phương tiện phức tạp."
  },
  "qwen3-vl-32b-instruct": {
    "description": "Qwen3 VL 32B Instruct, mô hình chỉ dẫn đa phương tiện, phù hợp cho hỏi đáp và sáng tạo hình ảnh-văn bản chất lượng cao."
  },
  "qwen3-vl-32b-thinking": {
    "description": "Qwen3 VL 32B Thinking, phiên bản tư duy sâu đa phương tiện, tăng cường suy luận phức tạp và phân tích chuỗi dài."
  },
  "qwen3-vl-8b-instruct": {
    "description": "Qwen3 VL 8B Instruct, mô hình đa phương tiện nhẹ, phù hợp cho hỏi đáp thị giác hàng ngày và tích hợp ứng dụng."
  },
  "qwen3-vl-8b-thinking": {
    "description": "Qwen3 VL 8B Thinking, mô hình chuỗi tư duy đa phương tiện, thích hợp cho suy luận chi tiết về thông tin hình ảnh."
  },
  "qwen3-vl-flash": {
    "description": "Qwen3 VL Flash: phiên bản suy luận tốc độ cao và nhẹ, phù hợp với các tình huống yêu cầu độ trễ thấp hoặc xử lý số lượng lớn yêu cầu."
  },
  "qwen3-vl-plus": {
    "description": "Tongyi Qianwen VL là mô hình sinh văn bản có khả năng hiểu thị giác (hình ảnh), không chỉ thực hiện OCR (nhận dạng chữ trong ảnh) mà còn có thể tóm tắt và suy luận thêm, ví dụ như trích xuất thuộc tính từ ảnh sản phẩm, giải bài tập dựa trên hình ảnh minh họa."
  },
  "qwq": {
    "description": "QwQ là một mô hình nghiên cứu thử nghiệm, tập trung vào việc nâng cao khả năng suy luận của AI."
  },
  "qwq-32b": {
    "description": "Mô hình suy diễn QwQ được đào tạo dựa trên mô hình Qwen2.5-32B, đã được cải thiện đáng kể khả năng suy diễn của mô hình thông qua học tăng cường. Các chỉ số cốt lõi của mô hình như mã toán (AIME 24/25, LiveCodeBench) và một số chỉ số chung (IFEval, LiveBench, v.v.) đạt đến mức độ của phiên bản đầy đủ DeepSeek-R1, tất cả các chỉ số đều vượt trội so với DeepSeek-R1-Distill-Qwen-32B cũng dựa trên Qwen2.5-32B."
  },
  "qwq-32b-preview": {
    "description": "Mô hình QwQ là một mô hình nghiên cứu thử nghiệm được phát triển bởi đội ngũ Qwen, tập trung vào việc nâng cao khả năng suy luận của AI."
  },
  "qwq-plus": {
    "description": "Mô hình suy luận QwQ dựa trên mô hình Qwen2.5, đã nâng cao đáng kể khả năng suy luận thông qua học tăng cường. Các chỉ số cốt lõi về toán học, mã hóa (AIME 24/25, LiveCodeBench) và một số chỉ số chung (IFEval, LiveBench, v.v.) đạt mức tương đương phiên bản đầy đủ của DeepSeek-R1."
  },
  "qwq_32b": {
    "description": "Mô hình suy luận có quy mô trung bình trong dòng Qwen. So với các mô hình tinh chỉnh hướng dẫn truyền thống, QwQ có khả năng suy nghĩ và suy luận, có thể nâng cao hiệu suất đáng kể trong các nhiệm vụ hạ nguồn, đặc biệt là trong việc giải quyết các vấn đề khó khăn."
  },
  "r1-1776": {
    "description": "R1-1776 là một phiên bản của mô hình DeepSeek R1, đã được huấn luyện lại, cung cấp thông tin sự thật chưa được kiểm duyệt và không thiên lệch."
  },
  "solar-mini": {
    "description": "Solar Mini là một LLM dạng nhỏ gọn, hiệu suất vượt trội hơn GPT-3.5, có khả năng đa ngôn ngữ mạnh mẽ, hỗ trợ tiếng Anh và tiếng Hàn, cung cấp giải pháp hiệu quả và nhỏ gọn."
  },
  "solar-mini-ja": {
    "description": "Solar Mini (Ja) mở rộng khả năng của Solar Mini, tập trung vào tiếng Nhật, đồng thời duy trì hiệu quả và hiệu suất xuất sắc trong việc sử dụng tiếng Anh và tiếng Hàn."
  },
  "solar-pro": {
    "description": "Solar Pro là một LLM thông minh cao do Upstage phát hành, tập trung vào khả năng tuân theo hướng dẫn trên một GPU, đạt điểm IFEval trên 80. Hiện tại hỗ trợ tiếng Anh, phiên bản chính thức dự kiến ra mắt vào tháng 11 năm 2024, sẽ mở rộng hỗ trợ ngôn ngữ và độ dài ngữ cảnh."
  },
  "sonar": {
    "description": "Sản phẩm tìm kiếm nhẹ dựa trên ngữ cảnh tìm kiếm, nhanh hơn và rẻ hơn so với Sonar Pro."
  },
  "sonar-deep-research": {
    "description": "Nghiên cứu sâu tiến hành nghiên cứu chuyên gia toàn diện và tổng hợp thành các báo cáo có thể truy cập và có thể hành động."
  },
  "sonar-pro": {
    "description": "Sản phẩm tìm kiếm nâng cao hỗ trợ ngữ cảnh tìm kiếm, cho phép truy vấn và theo dõi nâng cao."
  },
  "sonar-reasoning": {
    "description": "Sản phẩm API mới được hỗ trợ bởi mô hình suy luận của DeepSeek."
  },
  "sonar-reasoning-pro": {
    "description": "Sản phẩm API mới được hỗ trợ bởi mô hình suy diễn DeepSeek."
  },
  "stable-diffusion-3-medium": {
    "description": "Mô hình tạo hình ảnh từ văn bản mới nhất do Stability AI phát hành. Phiên bản này kế thừa ưu điểm của thế hệ trước, cải tiến đáng kể về chất lượng hình ảnh, hiểu văn bản và đa dạng phong cách, có thể giải thích chính xác các gợi ý ngôn ngữ tự nhiên phức tạp và tạo ra hình ảnh chính xác, đa dạng hơn."
  },
  "stable-diffusion-3.5-large": {
    "description": "stable-diffusion-3.5-large là mô hình tạo hình ảnh từ văn bản đa phương thức khuếch tán biến áp (MMDiT) với 800 triệu tham số, có chất lượng hình ảnh xuất sắc và độ khớp gợi ý cao, hỗ trợ tạo hình ảnh độ phân giải cao 1 triệu pixel, đồng thời vận hành hiệu quả trên phần cứng tiêu dùng phổ thông."
  },
  "stable-diffusion-3.5-large-turbo": {
    "description": "stable-diffusion-3.5-large-turbo là mô hình dựa trên stable-diffusion-3.5-large, sử dụng kỹ thuật chưng cất khuếch tán đối kháng (ADD), có tốc độ nhanh hơn."
  },
  "stable-diffusion-v1.5": {
    "description": "stable-diffusion-v1.5 được khởi tạo từ trọng số checkpoint stable-diffusion-v1.2, được tinh chỉnh 595k bước ở độ phân giải 512x512 trên \"laion-aesthetics v2 5+\", giảm 10% điều kiện hóa văn bản để cải thiện lấy mẫu hướng dẫn không bộ phân loại."
  },
  "stable-diffusion-xl": {
    "description": "stable-diffusion-xl có cải tiến lớn so với v1.5 và đạt hiệu quả tương đương mô hình SOTA mã nguồn mở hiện tại như midjourney. Cải tiến cụ thể bao gồm: unet backbone lớn hơn gấp 3 lần; thêm module tinh chỉnh để cải thiện chất lượng hình ảnh tạo ra; kỹ thuật huấn luyện hiệu quả hơn."
  },
  "stable-diffusion-xl-base-1.0": {
    "description": "Mô hình tạo hình ảnh từ văn bản quy mô lớn do Stability AI phát triển và mã nguồn mở, có khả năng tạo hình ảnh sáng tạo đứng đầu ngành. Có khả năng hiểu chỉ dẫn xuất sắc, hỗ trợ định nghĩa prompt ngược để tạo nội dung chính xác."
  },
  "step-1-128k": {
    "description": "Cân bằng hiệu suất và chi phí, phù hợp cho các tình huống chung."
  },
  "step-1-256k": {
    "description": "Có khả năng xử lý ngữ cảnh siêu dài, đặc biệt phù hợp cho phân tích tài liệu dài."
  },
  "step-1-32k": {
    "description": "Hỗ trợ đối thoại có độ dài trung bình, phù hợp cho nhiều tình huống ứng dụng."
  },
  "step-1-8k": {
    "description": "Mô hình nhỏ, phù hợp cho các nhiệm vụ nhẹ."
  },
  "step-1-flash": {
    "description": "Mô hình tốc độ cao, phù hợp cho đối thoại thời gian thực."
  },
  "step-1.5v-mini": {
    "description": "Mô hình này có khả năng hiểu video mạnh mẽ."
  },
  "step-1o-turbo-vision": {
    "description": "Mô hình này có khả năng hiểu hình ảnh mạnh mẽ, vượt trội hơn 1o trong lĩnh vực toán học và mã. Mô hình nhỏ hơn 1o và có tốc độ xuất ra nhanh hơn."
  },
  "step-1o-vision-32k": {
    "description": "Mô hình này có khả năng hiểu hình ảnh mạnh mẽ. So với các mô hình trong series step-1v, nó có hiệu suất thị giác vượt trội hơn."
  },
  "step-1v-32k": {
    "description": "Hỗ trợ đầu vào hình ảnh, tăng cường trải nghiệm tương tác đa mô hình."
  },
  "step-1v-8k": {
    "description": "Mô hình thị giác nhỏ, phù hợp cho các nhiệm vụ cơ bản về văn bản và hình ảnh."
  },
  "step-1x-edit": {
    "description": "Mô hình tập trung vào tác vụ chỉnh sửa hình ảnh, có thể sửa đổi và nâng cao hình ảnh dựa trên hình ảnh và mô tả văn bản do người dùng cung cấp. Hỗ trợ nhiều định dạng đầu vào, bao gồm mô tả văn bản và hình ảnh mẫu. Mô hình hiểu ý định người dùng và tạo ra kết quả chỉnh sửa hình ảnh phù hợp."
  },
  "step-1x-medium": {
    "description": "Mô hình có khả năng tạo hình ảnh mạnh mẽ, hỗ trợ đầu vào mô tả văn bản. Hỗ trợ tiếng Trung bản địa, có thể hiểu và xử lý mô tả văn bản tiếng Trung tốt hơn, nắm bắt chính xác thông tin ngữ nghĩa trong mô tả và chuyển đổi thành đặc trưng hình ảnh, từ đó tạo hình ảnh chính xác hơn. Mô hình có thể tạo hình ảnh độ phân giải cao, chất lượng tốt và có khả năng chuyển đổi phong cách nhất định."
  },
  "step-2-16k": {
    "description": "Hỗ trợ tương tác ngữ cảnh quy mô lớn, phù hợp cho các tình huống đối thoại phức tạp."
  },
  "step-2-16k-exp": {
    "description": "Phiên bản thử nghiệm của mô hình step-2, bao gồm các tính năng mới nhất, đang được cập nhật liên tục. Không khuyến nghị sử dụng trong môi trường sản xuất chính thức."
  },
  "step-2-mini": {
    "description": "Mô hình lớn siêu tốc dựa trên kiến trúc Attention tự nghiên cứu thế hệ mới MFA, đạt được hiệu quả tương tự như step1 với chi phí rất thấp, đồng thời duy trì thông lượng cao hơn và độ trễ phản hồi nhanh hơn. Có khả năng xử lý các nhiệm vụ chung, đặc biệt có năng lực trong lập trình."
  },
  "step-2x-large": {
    "description": "Mô hình tạo hình ảnh thế hệ mới của Step Star, tập trung vào tác vụ tạo hình ảnh, có thể tạo ra hình ảnh chất lượng cao dựa trên mô tả văn bản do người dùng cung cấp. Mô hình mới tạo ra hình ảnh có cảm giác thực hơn, khả năng tạo chữ tiếng Trung và tiếng Anh mạnh hơn."
  },
  "step-3": {
    "description": "Mô hình này có khả năng nhận thức thị giác mạnh mẽ và suy luận phức tạp. Có thể chính xác hoàn thành việc hiểu các kiến thức phức tạp liên ngành, phân tích chéo giữa thông tin toán học và thông tin thị giác, cũng như xử lý các vấn đề phân tích hình ảnh trong đời sống hàng ngày."
  },
  "step-r1-v-mini": {
    "description": "Mô hình này là một mô hình suy luận lớn với khả năng hiểu hình ảnh mạnh mẽ, có thể xử lý thông tin hình ảnh và văn bản, và xuất ra nội dung văn bản sau khi suy nghĩ sâu. Mô hình này thể hiện xuất sắc trong lĩnh vực suy luận hình ảnh, đồng thời có khả năng toán học, mã và suy luận văn bản hàng đầu. Độ dài ngữ cảnh là 100k."
  },
  "step3": {
    "description": "Step3 là mô hình đa phương thức do Jiexue Xingchen phát triển, có khả năng hiểu hình ảnh mạnh mẽ."
  },
  "stepfun-ai/step3": {
    "description": "Step3 là mô hình suy luận đa mô thức tiên tiến được phát hành bởi 阶跃星辰 (StepFun). Mô hình này được xây dựng trên kiến trúc Mixture-of-Experts (MoE) với 321B tham số tổng và 38B tham số kích hoạt. Thiết kế đầu-cuối (end-to-end) nhằm tối thiểu hóa chi phí giải mã, đồng thời cung cấp hiệu năng hàng đầu trong suy luận thị giác-ngôn ngữ. Thông qua thiết kế phối hợp giữa Multi-Matrix Factorized Attention (MFA) và Attention-FFN Decoupling (AFD), Step3 duy trì hiệu suất vượt trội trên cả bộ tăng tốc cao cấp và các thiết bị tăng tốc cấp thấp. Trong giai đoạn tiền huấn luyện, Step3 đã xử lý hơn 20T token văn bản và 4T token hỗn hợp ảnh-văn bản, bao phủ hơn mười ngôn ngữ. Mô hình này đã đạt vị thế dẫn đầu trong các benchmark mã nguồn mở ở nhiều lĩnh vực, bao gồm toán học, mã (code) và các nhiệm vụ đa mô thức."
  },
  "taichu_llm": {
    "description": "Mô hình ngôn ngữ lớn Taichu có khả năng hiểu ngôn ngữ mạnh mẽ và các khả năng như sáng tạo văn bản, trả lời câu hỏi kiến thức, lập trình mã, tính toán toán học, suy luận logic, phân tích cảm xúc, tóm tắt văn bản. Đổi mới kết hợp giữa đào tạo trước với dữ liệu phong phú từ nhiều nguồn, thông qua việc liên tục cải tiến công nghệ thuật toán và hấp thụ kiến thức mới từ dữ liệu văn bản khổng lồ, giúp mô hình ngày càng hoàn thiện. Cung cấp thông tin và dịch vụ tiện lợi hơn cho người dùng cùng trải nghiệm thông minh hơn."
  },
  "taichu_o1": {
    "description": "taichu_o1 là mô hình suy luận lớn thế hệ mới, đạt được chuỗi suy nghĩ giống con người thông qua tương tác đa phương tiện và học tăng cường, hỗ trợ suy diễn quyết định phức tạp, đồng thời thể hiện con đường suy luận có thể mô hình hóa trong khi duy trì đầu ra chính xác cao, phù hợp cho phân tích chiến lược và suy nghĩ sâu."
  },
  "taichu_vl": {
    "description": "Kết hợp khả năng hiểu hình ảnh, chuyển giao kiến thức, suy luận logic, thể hiện xuất sắc trong lĩnh vực hỏi đáp hình ảnh và văn bản."
  },
  "tencent/Hunyuan-A13B-Instruct": {
    "description": "Hunyuan-A13B-Instruct có 80 tỷ tham số, kích hoạt 13 tỷ tham số để đạt hiệu năng tương đương các mô hình lớn hơn, hỗ trợ suy luận kết hợp “tư duy nhanh/tư duy chậm”; khả năng hiểu văn bản dài ổn định; được xác nhận qua BFCL-v3 và τ-Bench, năng lực Agent dẫn đầu; kết hợp GQA và nhiều định dạng lượng tử hóa, đạt hiệu quả suy luận cao."
  },
  "tencent/Hunyuan-MT-7B": {
    "description": "Mô hình dịch Hunyuan (Hunyuan Translation Model) bao gồm một mô hình dịch Hunyuan-MT-7B và một mô hình tích hợp Hunyuan-MT-Chimera. Hunyuan-MT-7B là một mô hình dịch nhẹ với 7 tỷ tham số, dùng để dịch văn bản nguồn sang ngôn ngữ đích. Mô hình hỗ trợ dịch qua lại giữa 33 ngôn ngữ và 5 ngôn ngữ dân tộc thiểu số Trung Quốc. Trong cuộc thi dịch máy quốc tế WMT25, Hunyuan-MT-7B đã giành được 30 giải nhất trong số 31 hạng mục ngôn ngữ tham gia, thể hiện năng lực dịch thuật xuất sắc. Đối với các tình huống dịch thuật, Tencent Hunyuan đã đề xuất một quy trình huấn luyện hoàn chỉnh từ tiền huấn luyện đến tinh chỉnh có giám sát, sau đó là tăng cường dịch thuật và tăng cường tích hợp, giúp mô hình đạt hiệu suất hàng đầu trong các mô hình cùng quy mô. Mô hình có hiệu suất tính toán cao, dễ triển khai và phù hợp với nhiều tình huống ứng dụng."
  },
  "text-embedding-3-large": {
    "description": "Mô hình vector hóa mạnh mẽ nhất, phù hợp cho các nhiệm vụ tiếng Anh và không phải tiếng Anh."
  },
  "text-embedding-3-small": {
    "description": "Mô hình Embedding thế hệ mới hiệu quả và tiết kiệm, phù hợp cho tìm kiếm kiến thức, ứng dụng RAG và các tình huống khác."
  },
  "thudm/glm-4-32b": {
    "description": "GLM-4-32B-0414 là một mô hình ngôn ngữ mở với trọng số 32B song ngữ (Trung-Anh), được tối ưu hóa cho việc tạo mã, gọi hàm và các nhiệm vụ theo kiểu đại lý. Nó đã được huấn luyện trước trên 15T dữ liệu chất lượng cao và dữ liệu suy luận lại, và được hoàn thiện thêm bằng cách sử dụng sự phù hợp với sở thích của con người, lấy mẫu từ chối và học tăng cường. Mô hình này thể hiện xuất sắc trong suy luận phức tạp, tạo ra sản phẩm và các nhiệm vụ đầu ra có cấu trúc, đạt được hiệu suất tương đương với GPT-4o và DeepSeek-V3-0324 trong nhiều bài kiểm tra chuẩn."
  },
  "thudm/glm-4-32b:free": {
    "description": "GLM-4-32B-0414 là một mô hình ngôn ngữ mở với trọng số 32B song ngữ (Trung-Anh), được tối ưu hóa cho việc tạo mã, gọi hàm và các nhiệm vụ theo kiểu đại lý. Nó đã được huấn luyện trước trên 15T dữ liệu chất lượng cao và dữ liệu suy luận lại, và được hoàn thiện thêm bằng cách sử dụng sự phù hợp với sở thích của con người, lấy mẫu từ chối và học tăng cường. Mô hình này thể hiện xuất sắc trong suy luận phức tạp, tạo ra sản phẩm và các nhiệm vụ đầu ra có cấu trúc, đạt được hiệu suất tương đương với GPT-4o và DeepSeek-V3-0324 trong nhiều bài kiểm tra chuẩn."
  },
  "thudm/glm-4-9b-chat": {
    "description": "Phiên bản mã nguồn mở của thế hệ mô hình tiền huấn luyện GLM-4 mới nhất được phát hành bởi Zhiyu AI."
  },
  "thudm/glm-z1-32b": {
    "description": "GLM-Z1-32B-0414 là biến thể suy luận nâng cao của GLM-4-32B, được xây dựng cho việc giải quyết các vấn đề sâu về toán học, logic và lập trình. Nó áp dụng học tăng cường mở rộng (cụ thể cho nhiệm vụ và dựa trên sở thích cặp chung) để cải thiện hiệu suất cho các nhiệm vụ phức tạp nhiều bước. So với mô hình GLM-4-32B cơ bản, Z1 đã nâng cao đáng kể khả năng suy luận có cấu trúc và trong các lĩnh vực chính thức.\n\nMô hình này hỗ trợ thực hiện các bước 'suy nghĩ' thông qua kỹ thuật nhắc nhở và cung cấp tính liên kết cải thiện cho đầu ra định dạng dài. Nó được tối ưu hóa cho quy trình làm việc của đại lý và hỗ trợ ngữ cảnh dài (thông qua YaRN), gọi công cụ JSON và cấu hình lấy mẫu chi tiết cho suy luận ổn định. Rất phù hợp cho các trường hợp cần suy nghĩ sâu sắc, suy luận nhiều bước hoặc suy diễn chính thức."
  },
  "thudm/glm-z1-rumination-32b": {
    "description": "THUDM: GLM Z1 Rumination 32B là mô hình suy luận sâu với 32B tham số trong dòng GLM-4-Z1, được tối ưu hóa cho các nhiệm vụ phức tạp, mở cần suy nghĩ lâu dài. Nó được xây dựng trên nền tảng glm-4-32b-0414, tăng cường thêm giai đoạn học tăng cường và chiến lược căn chỉnh đa giai đoạn, giới thiệu khả năng \"phản tư\" nhằm mô phỏng quá trình xử lý nhận thức mở rộng. Điều này bao gồm suy luận lặp đi lặp lại, phân tích đa bước và quy trình làm việc tăng cường công cụ như tìm kiếm, truy xuất và tổng hợp nhận thức trích dẫn.\n\nMô hình này thể hiện xuất sắc trong viết nghiên cứu, phân tích so sánh và câu hỏi phức tạp. Nó hỗ trợ gọi hàm cho các nguyên ngữ tìm kiếm và điều hướng (`search`, `click`, `open`, `finish`), cho phép sử dụng trong quy trình đại lý. Hành vi phản tư được hình thành bởi các phần thưởng dựa trên quy tắc và cơ chế quyết định trì hoãn trong kiểm soát vòng lặp đa vòng, và được chuẩn hóa theo các khung nghiên cứu sâu như ngăn xếp căn chỉnh nội bộ của OpenAI. Biến thể này phù hợp cho các tình huống cần độ sâu hơn là tốc độ."
  },
  "tngtech/deepseek-r1t-chimera:free": {
    "description": "DeepSeek-R1T-Chimera được tạo ra bằng cách kết hợp DeepSeek-R1 và DeepSeek-V3 (0324), kết hợp khả năng suy luận của R1 và cải tiến hiệu quả token của V3. Nó dựa trên kiến trúc DeepSeek-MoE Transformer và được tối ưu hóa cho các nhiệm vụ tạo văn bản tổng quát.\n\nMô hình này kết hợp trọng số tiền huấn luyện của hai mô hình nguồn để cân bằng hiệu suất trong suy luận, hiệu quả và các nhiệm vụ tuân theo chỉ dẫn. Nó được phát hành theo giấy phép MIT, nhằm mục đích sử dụng cho nghiên cứu và thương mại."
  },
  "togethercomputer/StripedHyena-Nous-7B": {
    "description": "StripedHyena Nous (7B) cung cấp khả năng tính toán nâng cao thông qua chiến lược và kiến trúc mô hình hiệu quả."
  },
  "tts-1": {
    "description": "Mô hình chuyển văn bản thành giọng nói mới nhất, tối ưu hóa tốc độ cho các tình huống thời gian thực."
  },
  "tts-1-hd": {
    "description": "Mô hình chuyển văn bản thành giọng nói mới nhất, tối ưu hóa cho chất lượng."
  },
  "upstage/SOLAR-10.7B-Instruct-v1.0": {
    "description": "Upstage SOLAR Instruct v1 (11B) phù hợp cho các nhiệm vụ chỉ dẫn tinh vi, cung cấp khả năng xử lý ngôn ngữ xuất sắc."
  },
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet nâng cao tiêu chuẩn ngành, hiệu suất vượt trội so với các mô hình cạnh tranh và Claude 3 Opus, thể hiện xuất sắc trong nhiều đánh giá, đồng thời có tốc độ và chi phí tương đương với các mô hình tầm trung của chúng tôi."
  },
  "us.anthropic.claude-3-7-sonnet-20250219-v1:0": {
    "description": "Claude 3.7 sonnet là mô hình thế hệ tiếp theo nhanh nhất của Anthropic. So với Claude 3 Haiku, Claude 3.7 Sonnet đã cải thiện ở nhiều kỹ năng và vượt qua mô hình lớn nhất thế hệ trước là Claude 3 Opus trong nhiều bài kiểm tra trí tuệ."
  },
  "us.anthropic.claude-haiku-4-5-20251001-v1:0": {
    "description": "Claude Haiku 4.5 là mô hình Haiku nhanh nhất và thông minh nhất của Anthropic, với tốc độ như chớp và khả năng tư duy mở rộng."
  },
  "us.anthropic.claude-sonnet-4-5-20250929-v1:0": {
    "description": "Claude Sonnet 4.5 là mô hình thông minh nhất của Anthropic cho đến nay."
  },
  "v0-1.0-md": {
    "description": "Mô hình v0-1.0-md là phiên bản cũ được cung cấp dịch vụ qua API v0"
  },
  "v0-1.5-lg": {
    "description": "Mô hình v0-1.5-lg phù hợp cho các nhiệm vụ suy nghĩ hoặc lý luận nâng cao"
  },
  "v0-1.5-md": {
    "description": "Mô hình v0-1.5-md phù hợp cho các nhiệm vụ hàng ngày và tạo giao diện người dùng (UI)"
  },
  "vercel/v0-1.0-md": {
    "description": "Truy cập mô hình phía sau v0 để tạo, sửa lỗi và tối ưu hóa ứng dụng Web hiện đại, với suy luận theo khung cụ thể và kiến thức cập nhật."
  },
  "vercel/v0-1.5-md": {
    "description": "Truy cập mô hình phía sau v0 để tạo, sửa lỗi và tối ưu hóa ứng dụng Web hiện đại, với suy luận theo khung cụ thể và kiến thức cập nhật."
  },
  "wan2.2-t2i-flash": {
    "description": "Phiên bản tốc độ cao Wanxiang 2.2, là mô hình mới nhất hiện nay. Nâng cấp toàn diện về sáng tạo, ổn định và cảm giác thực, tốc độ tạo nhanh, hiệu quả chi phí cao."
  },
  "wan2.2-t2i-plus": {
    "description": "Phiên bản chuyên nghiệp Wanxiang 2.2, là mô hình mới nhất hiện nay. Nâng cấp toàn diện về sáng tạo, ổn định và cảm giác thực, tạo chi tiết phong phú."
  },
  "wanx-v1": {
    "description": "Mô hình tạo hình ảnh từ văn bản cơ bản, tương ứng với mô hình chung 1.0 trên trang chính thức Tongyi Wanxiang."
  },
  "wanx2.0-t2i-turbo": {
    "description": "Chuyên về chân dung có cảm giác thực, tốc độ trung bình, chi phí thấp. Tương ứng với mô hình tốc độ cao 2.0 trên trang chính thức Tongyi Wanxiang."
  },
  "wanx2.1-t2i-plus": {
    "description": "Phiên bản nâng cấp toàn diện, tạo hình ảnh chi tiết phong phú hơn, tốc độ hơi chậm. Tương ứng với mô hình chuyên nghiệp 2.1 trên trang chính thức Tongyi Wanxiang."
  },
  "wanx2.1-t2i-turbo": {
    "description": "Phiên bản nâng cấp toàn diện, tốc độ tạo nhanh, hiệu quả toàn diện, chi phí tổng hợp cao. Tương ứng với mô hình tốc độ cao 2.1 trên trang chính thức Tongyi Wanxiang."
  },
  "whisper-1": {
    "description": "Mô hình nhận dạng giọng nói đa năng, hỗ trợ nhận dạng giọng nói đa ngôn ngữ, dịch giọng nói và nhận diện ngôn ngữ."
  },
  "wizardlm2": {
    "description": "WizardLM 2 là mô hình ngôn ngữ do Microsoft AI cung cấp, đặc biệt xuất sắc trong các lĩnh vực đối thoại phức tạp, đa ngôn ngữ, suy luận và trợ lý thông minh."
  },
  "wizardlm2:8x22b": {
    "description": "WizardLM 2 là mô hình ngôn ngữ do Microsoft AI cung cấp, đặc biệt xuất sắc trong các lĩnh vực đối thoại phức tạp, đa ngôn ngữ, suy luận và trợ lý thông minh."
  },
  "x-ai/grok-4-fast": {
    "description": "Chúng tôi rất vui mừng ra mắt Grok 4 Fast, bước tiến mới nhất của chúng tôi trong các mô hình suy luận hiệu quả về chi phí."
  },
  "x-ai/grok-code-fast-1": {
    "description": "Chúng tôi rất vui mừng giới thiệu grok-code-fast-1, một mô hình suy luận nhanh và tiết kiệm chi phí, nổi bật trong mã hóa cho tác nhân."
  },
  "x1": {
    "description": "Mô hình Spark X1 sẽ được nâng cấp thêm, trên nền tảng dẫn đầu trong các nhiệm vụ toán học trong nước, đạt được hiệu quả trong các nhiệm vụ chung như suy luận, tạo văn bản, hiểu ngôn ngữ tương đương với OpenAI o1 và DeepSeek R1."
  },
  "xai/grok-2": {
    "description": "Grok 2 là mô hình ngôn ngữ tiên tiến với khả năng suy luận hàng đầu. Nó có năng lực vượt trội trong trò chuyện, mã hóa và suy luận, đứng trên Claude 3.5 Sonnet và GPT-4-Turbo trên bảng xếp hạng LMSYS."
  },
  "xai/grok-2-vision": {
    "description": "Mô hình thị giác Grok 2 thể hiện xuất sắc trong các nhiệm vụ dựa trên hình ảnh, cung cấp hiệu suất tiên tiến trong suy luận toán học dựa trên hình ảnh (MathVista) và hỏi đáp dựa trên tài liệu (DocVQA). Nó có khả năng xử lý đa dạng thông tin hình ảnh, bao gồm tài liệu, biểu đồ, đồ thị, ảnh chụp màn hình và ảnh chụp."
  },
  "xai/grok-3": {
    "description": "Mô hình hàng đầu của xAI, xuất sắc trong các trường hợp sử dụng doanh nghiệp như trích xuất dữ liệu, mã hóa và tóm tắt văn bản. Có kiến thức chuyên sâu trong các lĩnh vực tài chính, chăm sóc sức khỏe, pháp lý và khoa học."
  },
  "xai/grok-3-fast": {
    "description": "Mô hình hàng đầu của xAI, xuất sắc trong các trường hợp sử dụng doanh nghiệp như trích xuất dữ liệu, mã hóa và tóm tắt văn bản. Biến thể mô hình nhanh phục vụ trên cơ sở hạ tầng nhanh hơn, cung cấp thời gian phản hồi nhanh hơn nhiều so với tiêu chuẩn. Tốc độ tăng đi kèm chi phí token đầu ra cao hơn."
  },
  "xai/grok-3-mini": {
    "description": "Mô hình nhẹ của xAI, suy nghĩ trước khi phản hồi. Rất phù hợp cho các nhiệm vụ đơn giản hoặc dựa trên logic không đòi hỏi kiến thức chuyên sâu. Có thể truy cập đường đi suy nghĩ thô."
  },
  "xai/grok-3-mini-fast": {
    "description": "Mô hình nhẹ của xAI, suy nghĩ trước khi phản hồi. Rất phù hợp cho các nhiệm vụ đơn giản hoặc dựa trên logic không đòi hỏi kiến thức chuyên sâu. Có thể truy cập đường đi suy nghĩ thô. Biến thể mô hình nhanh phục vụ trên cơ sở hạ tầng nhanh hơn, cung cấp thời gian phản hồi nhanh hơn nhiều so với tiêu chuẩn. Tốc độ tăng đi kèm chi phí token đầu ra cao hơn."
  },
  "xai/grok-4": {
    "description": "Mô hình hàng đầu mới nhất và tuyệt vời nhất của xAI, cung cấp hiệu suất vô song trong ngôn ngữ tự nhiên, toán học và suy luận — lựa chọn toàn năng hoàn hảo."
  },
  "yi-large": {
    "description": "Mô hình với hàng trăm tỷ tham số mới, cung cấp khả năng hỏi đáp và sinh văn bản mạnh mẽ."
  },
  "yi-large-fc": {
    "description": "Hỗ trợ và tăng cường khả năng gọi công cụ trên cơ sở mô hình yi-large, phù hợp cho nhiều tình huống kinh doanh cần xây dựng agent hoặc workflow."
  },
  "yi-large-preview": {
    "description": "Phiên bản ban đầu, khuyến nghị sử dụng yi-large (phiên bản mới)."
  },
  "yi-large-rag": {
    "description": "Dịch vụ cao cấp dựa trên mô hình yi-large mạnh mẽ, kết hợp công nghệ tìm kiếm và sinh để cung cấp câu trả lời chính xác, dịch vụ tìm kiếm thông tin toàn mạng theo thời gian thực."
  },
  "yi-large-turbo": {
    "description": "Hiệu suất vượt trội với chi phí hợp lý. Tối ưu hóa độ chính xác cao dựa trên hiệu suất, tốc độ suy luận và chi phí."
  },
  "yi-lightning": {
    "description": "Mô hình hiệu suất cao mới nhất, đảm bảo đầu ra chất lượng cao trong khi tốc độ suy luận được cải thiện đáng kể."
  },
  "yi-lightning-lite": {
    "description": "Phiên bản nhẹ, được khuyến nghị sử dụng yi-lightning."
  },
  "yi-medium": {
    "description": "Mô hình kích thước trung bình được nâng cấp và tinh chỉnh, khả năng cân bằng, chi phí hiệu quả cao. Tối ưu hóa sâu khả năng tuân theo chỉ dẫn."
  },
  "yi-medium-200k": {
    "description": "Cửa sổ ngữ cảnh siêu dài 200K, cung cấp khả năng hiểu và sinh văn bản sâu cho các văn bản dài."
  },
  "yi-spark": {
    "description": "Mô hình nhỏ gọn và nhanh chóng. Cung cấp khả năng tính toán toán học và viết mã được tăng cường."
  },
  "yi-vision": {
    "description": "Mô hình cho các nhiệm vụ hình ảnh phức tạp, cung cấp khả năng hiểu và phân tích hình ảnh hiệu suất cao."
  },
  "yi-vision-v2": {
    "description": "Mô hình nhiệm vụ thị giác phức tạp, cung cấp khả năng hiểu và phân tích hiệu suất cao dựa trên nhiều hình ảnh."
  },
  "z-ai/glm-4.6": {
    "description": "GLM-4.6, mô hình hàng đầu mới nhất của Zhipu AI, vượt trội hoàn toàn so với thế hệ trước về mã hóa nâng cao, xử lý văn bản dài, suy luận và năng lực tác nhân."
  },
  "zai-org/GLM-4.5": {
    "description": "GLM-4.5 là mô hình nền tảng dành cho ứng dụng tác nhân thông minh, sử dụng kiến trúc chuyên gia hỗn hợp (Mixture-of-Experts). Được tối ưu sâu trong các lĩnh vực gọi công cụ, duyệt web, kỹ thuật phần mềm và lập trình front-end, hỗ trợ tích hợp liền mạch vào các tác nhân mã như Claude Code, Roo Code. GLM-4.5 sử dụng chế độ suy luận hỗn hợp, thích ứng với nhiều kịch bản ứng dụng như suy luận phức tạp và sử dụng hàng ngày."
  },
  "zai-org/GLM-4.5-Air": {
    "description": "GLM-4.5-Air là mô hình nền tảng dành cho ứng dụng tác nhân thông minh, sử dụng kiến trúc chuyên gia hỗn hợp (Mixture-of-Experts). Được tối ưu sâu trong các lĩnh vực gọi công cụ, duyệt web, kỹ thuật phần mềm và lập trình front-end, hỗ trợ tích hợp liền mạch vào các tác nhân mã như Claude Code, Roo Code. GLM-4.5 sử dụng chế độ suy luận hỗn hợp, thích ứng với nhiều kịch bản ứng dụng như suy luận phức tạp và sử dụng hàng ngày."
  },
  "zai-org/GLM-4.5V": {
    "description": "GLM-4.5V là thế hệ mô hình ngôn ngữ thị giác (VLM) mới nhất được phát hành bởi Zhipu AI. Mô hình này được xây dựng trên cơ sở mô hình văn bản chủ lực GLM-4.5-Air với tổng 106 tỷ tham số và 12 tỷ tham số kích hoạt, sử dụng kiến trúc chuyên gia hỗn hợp (Mixture of Experts - MoE), nhằm đạt hiệu năng xuất sắc với chi phí suy luận thấp hơn. Về mặt kỹ thuật, GLM-4.5V tiếp nối hướng phát triển của GLM-4.1V-Thinking và giới thiệu các đổi mới như mã hóa vị trí xoay ba chiều (3D-RoPE), đáng kể nâng cao khả năng nhận thức và suy luận về các mối quan hệ trong không gian 3D. Thông qua tối ưu hóa ở các giai đoạn tiền huấn luyện, tinh chỉnh có giám sát và học tăng cường, mô hình có khả năng xử lý nhiều dạng nội dung thị giác như hình ảnh, video và tài liệu dài, và đã đạt vị trí hàng đầu trong số các mô hình mã nguồn mở cùng cấp trên 41 bộ đánh giá đa phương thức công khai. Ngoài ra, mô hình còn bổ sung công tắc “chế độ tư duy”, cho phép người dùng linh hoạt lựa chọn giữa phản hồi nhanh và suy luận sâu để cân bằng hiệu quả và chất lượng."
  },
  "zai-org/GLM-4.6": {
    "description": "So với GLM-4.5, GLM-4.6 mang đến nhiều cải tiến quan trọng. Cửa sổ ngữ cảnh được mở rộng từ 128K lên 200K token, giúp mô hình xử lý các nhiệm vụ Agent phức tạp hơn. Mô hình đạt điểm cao hơn trong các bài kiểm tra chuẩn về mã và thể hiện hiệu suất thực tế mạnh mẽ hơn trong các ứng dụng như Claude Code, Cline, Roo Code và Kilo Code, bao gồm cải tiến trong việc tạo giao diện frontend với hiệu ứng hình ảnh tinh tế. GLM-4.6 cải thiện rõ rệt hiệu suất suy luận và hỗ trợ sử dụng công cụ trong quá trình suy luận, mang lại năng lực tổng hợp mạnh mẽ hơn. Mô hình thể hiện sức mạnh hơn trong việc sử dụng công cụ và Agent dựa trên tìm kiếm, đồng thời tích hợp hiệu quả hơn vào khung Agent. Về mặt viết lách, mô hình phù hợp hơn với sở thích về phong cách và khả năng đọc hiểu của con người, và thể hiện tự nhiên hơn trong các tình huống nhập vai."
  },
  "zai/glm-4.5": {
    "description": "Dòng mô hình GLM-4.5 được thiết kế đặc biệt cho các tác nhân thông minh. Mô hình hàng đầu GLM-4.5 tích hợp 355 tỷ tham số tổng (32 tỷ tham số kích hoạt), hợp nhất khả năng suy luận, mã hóa và đại lý để giải quyết các yêu cầu ứng dụng phức tạp. Là hệ thống suy luận hỗn hợp, nó cung cấp hai chế độ hoạt động."
  },
  "zai/glm-4.5-air": {
    "description": "GLM-4.5 và GLM-4.5-Air là các mô hình hàng đầu mới nhất của chúng tôi, được thiết kế đặc biệt làm mô hình nền tảng cho các ứng dụng đại lý. Cả hai đều sử dụng kiến trúc chuyên gia hỗn hợp (MoE). GLM-4.5 có tổng số tham số 355 tỷ với 32 tỷ tham số kích hoạt mỗi lần truyền tiến, trong khi GLM-4.5-Air có thiết kế đơn giản hơn với tổng số tham số 106 tỷ và 12 tỷ tham số kích hoạt."
  },
  "zai/glm-4.5v": {
    "description": "GLM-4.5V được xây dựng trên mô hình nền tảng GLM-4.5-Air, kế thừa công nghệ đã được xác minh của GLM-4.1V-Thinking, đồng thời mở rộng hiệu quả với kiến trúc MoE 106 tỷ tham số mạnh mẽ."
  }
}
