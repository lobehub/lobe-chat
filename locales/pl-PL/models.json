{
  "01-ai/yi-1.5-34b-chat": {
    "description": "Zero One, najnowszy model open source z dostrojeniem, zawierający 34 miliardy parametrów, dostosowany do różnych scenariuszy dialogowych, z wysokiej jakości danymi treningowymi, dostosowany do preferencji ludzkich."
  },
  "01-ai/yi-1.5-9b-chat": {
    "description": "Zero One, najnowszy model open source z dostrojeniem, zawierający 9 miliardów parametrów, dostosowany do różnych scenariuszy dialogowych, z wysokiej jakości danymi treningowymi, dostosowany do preferencji ludzkich."
  },
  "360/deepseek-r1": {
    "description": "[Wersja 360] DeepSeek-R1 wykorzystuje techniki uczenia przez wzmocnienie na dużą skalę w fazie po treningu, znacznie poprawiając zdolności wnioskowania modelu przy minimalnej ilości oznaczonych danych. W zadaniach matematycznych, kodowania i wnioskowania w języku naturalnym osiąga wyniki porównywalne z oficjalną wersją OpenAI o1."
  },
  "360gpt-pro": {
    "description": "360GPT Pro, jako ważny członek serii modeli AI 360, zaspokaja różnorodne potrzeby aplikacji przetwarzania języka naturalnego dzięki wydajnym zdolnościom przetwarzania tekstu, obsługując zrozumienie długich tekstów i wielokrotne dialogi."
  },
  "360gpt-pro-trans": {
    "description": "Model dedykowany do tłumaczeń, głęboko dostrojony i zoptymalizowany, oferujący wiodące efekty tłumaczeniowe."
  },
  "360gpt-turbo": {
    "description": "360GPT Turbo oferuje potężne zdolności obliczeniowe i dialogowe, charakteryzując się doskonałym rozumieniem semantycznym i wydajnością generacyjną, stanowiąc idealne rozwiązanie dla firm i deweloperów jako inteligentny asystent."
  },
  "360gpt-turbo-responsibility-8k": {
    "description": "360GPT Turbo Responsibility 8K kładzie nacisk na bezpieczeństwo semantyczne i odpowiedzialność, zaprojektowany specjalnie dla aplikacji o wysokich wymaganiach dotyczących bezpieczeństwa treści, zapewniając dokładność i stabilność doświadczeń użytkowników."
  },
  "360gpt2-o1": {
    "description": "360gpt2-o1 wykorzystuje wyszukiwanie drzew do budowy łańcucha myślenia i wprowadza mechanizm refleksji, wykorzystując uczenie przez wzmocnienie, model posiada zdolność do samorefleksji i korekty błędów."
  },
  "360gpt2-pro": {
    "description": "360GPT2 Pro to zaawansowany model przetwarzania języka naturalnego wydany przez firmę 360, charakteryzujący się doskonałymi zdolnościami generowania i rozumienia tekstu, szczególnie w obszarze generowania i tworzenia treści, zdolny do obsługi skomplikowanych zadań związanych z konwersją językową i odgrywaniem ról."
  },
  "360zhinao2-o1": {
    "description": "Model 360zhinao2-o1 wykorzystuje wyszukiwanie drzewne do budowy łańcucha myślowego i wprowadza mechanizm refleksji, wykorzystując uczenie przez wzmocnienie do treningu, co pozwala modelowi na samorefleksję i korekcję błędów."
  },
  "4.0Ultra": {
    "description": "Spark4.0 Ultra to najsilniejsza wersja w serii modeli Spark, która, oprócz ulepszonego łącza wyszukiwania w sieci, zwiększa zdolność rozumienia i podsumowywania treści tekstowych. Jest to kompleksowe rozwiązanie mające na celu zwiększenie wydajności biurowej i dokładne odpowiadanie na potrzeby, stanowiące inteligentny produkt wiodący w branży."
  },
  "AnimeSharp": {
    "description": "AnimeSharp (znany również jako „4x‑AnimeSharp”) to otwarty model superrozdzielczości opracowany przez Kim2091 na bazie architektury ESRGAN, skoncentrowany na powiększaniu i wyostrzaniu obrazów w stylu anime. W lutym 2022 roku zmieniono jego nazwę z „4x-TextSharpV1”. Początkowo model był również stosowany do obrazów tekstowych, ale jego wydajność została znacznie zoptymalizowana pod kątem treści anime."
  },
  "Baichuan2-Turbo": {
    "description": "Wykorzystuje technologię wzmacniania wyszukiwania, aby połączyć duży model z wiedzą branżową i wiedzą z całej sieci. Obsługuje przesyłanie różnych dokumentów, takich jak PDF, Word, oraz wprowadzanie adresów URL, zapewniając szybki i kompleksowy dostęp do informacji oraz dokładne i profesjonalne wyniki."
  },
  "Baichuan3-Turbo": {
    "description": "Optymalizowany pod kątem częstych scenariuszy biznesowych, znacznie poprawiający efektywność i oferujący korzystny stosunek jakości do ceny. W porównaniu do modelu Baichuan2, generowanie treści wzrosło o 20%, pytania o wiedzę o 17%, a zdolności odgrywania ról o 40%. Ogólna wydajność jest lepsza niż GPT3.5."
  },
  "Baichuan3-Turbo-128k": {
    "description": "Oferuje 128K ultra długi kontekst, zoptymalizowany pod kątem częstych scenariuszy biznesowych, znacznie poprawiający efektywność i oferujący korzystny stosunek jakości do ceny. W porównaniu do modelu Baichuan2, generowanie treści wzrosło o 20%, pytania o wiedzę o 17%, a zdolności odgrywania ról o 40%. Ogólna wydajność jest lepsza niż GPT3.5."
  },
  "Baichuan4": {
    "description": "Model o najwyższej wydajności w kraju, przewyższający zagraniczne modele w zadaniach związanych z encyklopedią, długimi tekstami i generowaniem treści w języku chińskim. Posiada również wiodące w branży zdolności multimodalne, osiągając doskonałe wyniki w wielu autorytatywnych testach."
  },
  "Baichuan4-Air": {
    "description": "Model o najlepszych możliwościach w kraju, przewyższający zagraniczne modele w zadaniach związanych z wiedzą encyklopedyczną, długimi tekstami i twórczością w języku chińskim. Posiada również wiodące w branży możliwości multimodalne, osiągając doskonałe wyniki w wielu autorytatywnych testach."
  },
  "Baichuan4-Turbo": {
    "description": "Model o najlepszych możliwościach w kraju, przewyższający zagraniczne modele w zadaniach związanych z wiedzą encyklopedyczną, długimi tekstami i twórczością w języku chińskim. Posiada również wiodące w branży możliwości multimodalne, osiągając doskonałe wyniki w wielu autorytatywnych testach."
  },
  "ByteDance-Seed/Seed-OSS-36B-Instruct": {
    "description": "Seed-OSS to seria otwartych modeli językowych dużej skali opracowanych przez zespół Seed ByteDance, zaprojektowanych specjalnie do zaawansowanego przetwarzania długich kontekstów, wnioskowania, agentów i zdolności ogólnych. Model Seed-OSS-36B-Instruct z tej serii to model dostrojony instrukcyjnie z 36 miliardami parametrów, natywnie obsługujący bardzo długie konteksty, co pozwala na jednorazowe przetwarzanie ogromnych dokumentów lub złożonych baz kodu. Model jest szczególnie zoptymalizowany pod kątem wnioskowania, generowania kodu i zadań agentów (np. użycia narzędzi), zachowując jednocześnie zrównoważone i doskonałe zdolności ogólne. Jedną z kluczowych cech tego modelu jest funkcja „budżetu myślenia” (Thinking Budget), która pozwala użytkownikom elastycznie dostosowywać długość wnioskowania, skutecznie zwiększając efektywność w praktycznych zastosowaniach."
  },
  "DeepSeek-R1": {
    "description": "Najnowocześniejszy, wydajny LLM, specjalizujący się w wnioskowaniu, matematyce i programowaniu."
  },
  "DeepSeek-R1-Distill-Llama-70B": {
    "description": "DeepSeek R1 — większy i inteligentniejszy model w zestawie DeepSeek — został skondensowany do architektury Llama 70B. Na podstawie testów porównawczych i ocen ludzkich, model ten jest bardziej inteligentny niż oryginalny Llama 70B, zwłaszcza w zadaniach wymagających precyzji matematycznej i faktograficznej."
  },
  "DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Model destylacyjny DeepSeek-R1 oparty na Qwen2.5-Math-1.5B, optymalizujący wydajność wnioskowania dzięki uczeniu przez wzmocnienie i danym z zimnego startu, otwarty model ustanawiający nowe standardy w wielu zadaniach."
  },
  "DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Model destylacyjny DeepSeek-R1 oparty na Qwen2.5-14B, optymalizujący wydajność wnioskowania dzięki uczeniu przez wzmocnienie i danym z zimnego startu, otwarty model ustanawiający nowe standardy w wielu zadaniach."
  },
  "DeepSeek-R1-Distill-Qwen-32B": {
    "description": "Seria DeepSeek-R1 optymalizuje wydajność wnioskowania dzięki uczeniu przez wzmocnienie i danym z zimnego startu, otwarty model ustanawiający nowe standardy w wielu zadaniach, przewyższający poziom OpenAI-o1-mini."
  },
  "DeepSeek-R1-Distill-Qwen-7B": {
    "description": "Model destylacyjny DeepSeek-R1 oparty na Qwen2.5-Math-7B, optymalizujący wydajność wnioskowania dzięki uczeniu przez wzmocnienie i danym z zimnego startu, otwarty model ustanawiający nowe standardy w wielu zadaniach."
  },
  "DeepSeek-V3": {
    "description": "DeepSeek-V3 to model MoE opracowany przez firmę DeepSeek. Wyniki DeepSeek-V3 w wielu testach przewyższają inne modele open source, takie jak Qwen2.5-72B i Llama-3.1-405B, a jego wydajność jest porównywalna z najlepszymi zamkniętymi modelami na świecie, takimi jak GPT-4o i Claude-3.5-Sonnet."
  },
  "DeepSeek-V3-1": {
    "description": "DeepSeek V3.1: model nowej generacji do wnioskowania, poprawiający zdolności do złożonych rozumowań i myślenia łańcuchowego, idealny do zadań wymagających dogłębnej analizy."
  },
  "DeepSeek-V3-Fast": {
    "description": "Dostawca modelu: platforma sophnet. DeepSeek V3 Fast to szybka wersja o wysokim TPS modelu DeepSeek V3 0324, w pełni nienkwantyzowana, z ulepszonym kodem i zdolnościami matematycznymi, zapewniająca szybszą reakcję!"
  },
  "DeepSeek-V3.1": {
    "description": "DeepSeek-V3.1 tryb bez myślenia; DeepSeek-V3.1 to nowy hybrydowy model wnioskowania od DeepSeek, obsługujący dwa tryby: myślenia i bez myślenia, z wyższą efektywnością myślenia niż DeepSeek-R1-0528. Po optymalizacji post-treningowej znacznie poprawiono użycie narzędzi agenta oraz wydajność zadań agentów."
  },
  "DeepSeek-V3.1-Fast": {
    "description": "DeepSeek V3.1 Fast to szybka wersja DeepSeek V3.1 o wysokim TPS. Hybrydowy tryb myślenia: poprzez zmianę szablonu rozmowy jeden model może obsługiwać jednocześnie tryb myślenia i bez myślenia. Inteligentniejsze wywoływanie narzędzi: dzięki optymalizacji po treningu model znacząco poprawił wydajność w użyciu narzędzi i zadaniach agentów."
  },
  "DeepSeek-V3.1-Think": {
    "description": "DeepSeek-V3.1 tryb myślenia; DeepSeek-V3.1 to nowy hybrydowy model wnioskowania od DeepSeek, obsługujący dwa tryby: myślenia i bez myślenia, z wyższą efektywnością myślenia niż DeepSeek-R1-0528. Po optymalizacji post-treningowej znacznie poprawiono użycie narzędzi agenta oraz wydajność zadań agentów."
  },
  "DeepSeek-V3.2-Exp": {
    "description": "DeepSeek V3.2 to najnowszy uniwersalny model dużej skali DeepSeek, wspierający hybrydową architekturę inferencyjną i oferujący znacznie ulepszone możliwości agenta."
  },
  "DeepSeek-V3.2-Exp-Think": {
    "description": "Tryb myślenia DeepSeek V3.2. Przed wygenerowaniem ostatecznej odpowiedzi model najpierw przedstawia łańcuch rozumowania, co zwiększa dokładność końcowej odpowiedzi."
  },
  "Doubao-lite-128k": {
    "description": "Doubao-lite oferuje niezwykle szybkie reakcje i lepszy stosunek jakości do ceny, zapewniając klientom elastyczne opcje dla różnych scenariuszy. Obsługuje wnioskowanie i dostrajanie z kontekstem do 128k."
  },
  "Doubao-lite-32k": {
    "description": "Doubao-lite oferuje niezwykle szybkie reakcje i lepszy stosunek jakości do ceny, zapewniając klientom elastyczne opcje dla różnych scenariuszy. Obsługuje wnioskowanie i dostrajanie z kontekstem do 32k."
  },
  "Doubao-lite-4k": {
    "description": "Doubao-lite oferuje niezwykle szybkie reakcje i lepszy stosunek jakości do ceny, zapewniając klientom elastyczne opcje dla różnych scenariuszy. Obsługuje wnioskowanie i dostrajanie z kontekstem do 4k."
  },
  "Doubao-pro-128k": {
    "description": "Najlepszy model główny, odpowiedni do złożonych zadań, osiągający doskonałe wyniki w scenariuszach takich jak pytania i odpowiedzi, streszczenia, twórczość, klasyfikacja tekstu i odgrywanie ról. Obsługuje wnioskowanie i dostrajanie z kontekstem do 128k."
  },
  "Doubao-pro-32k": {
    "description": "Najlepszy model główny, odpowiedni do złożonych zadań, osiągający doskonałe wyniki w scenariuszach takich jak pytania i odpowiedzi, streszczenia, twórczość, klasyfikacja tekstu i odgrywanie ról. Obsługuje wnioskowanie i dostrajanie z kontekstem do 32k."
  },
  "Doubao-pro-4k": {
    "description": "Najlepszy model główny, odpowiedni do złożonych zadań, osiągający doskonałe wyniki w scenariuszach takich jak pytania i odpowiedzi, streszczenia, twórczość, klasyfikacja tekstu i odgrywanie ról. Obsługuje wnioskowanie i dostrajanie z kontekstem do 4k."
  },
  "DreamO": {
    "description": "DreamO to otwarty model generowania obrazów opracowany wspólnie przez ByteDance i Uniwersytet Pekiński, mający na celu wsparcie wielozadaniowej generacji obrazów w ramach jednolitej architektury. Wykorzystuje efektywną metodę modelowania kombinacyjnego, umożliwiając generowanie spójnych i dostosowanych obrazów na podstawie wielu warunków, takich jak tożsamość, temat, styl czy tło wskazane przez użytkownika."
  },
  "ERNIE-3.5-128K": {
    "description": "Flagowy model dużego języka opracowany przez Baidu, obejmujący ogromne zbiory danych w języku chińskim i angielskim, charakteryzujący się silnymi zdolnościami ogólnymi, zdolny do spełnienia wymagań w większości scenariuszy związanych z pytaniami i odpowiedziami, generowaniem treści oraz aplikacjami wtyczek; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji w odpowiedziach."
  },
  "ERNIE-3.5-8K": {
    "description": "Flagowy model dużego języka opracowany przez Baidu, obejmujący ogromne zbiory danych w języku chińskim i angielskim, charakteryzujący się silnymi zdolnościami ogólnymi, zdolny do spełnienia wymagań w większości scenariuszy związanych z pytaniami i odpowiedziami, generowaniem treści oraz aplikacjami wtyczek; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji w odpowiedziach."
  },
  "ERNIE-3.5-8K-Preview": {
    "description": "Flagowy model dużego języka opracowany przez Baidu, obejmujący ogromne zbiory danych w języku chińskim i angielskim, charakteryzujący się silnymi zdolnościami ogólnymi, zdolny do spełnienia wymagań w większości scenariuszy związanych z pytaniami i odpowiedziami, generowaniem treści oraz aplikacjami wtyczek; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji w odpowiedziach."
  },
  "ERNIE-4.0-8K-Latest": {
    "description": "Flagowy model ultra dużego języka opracowany przez Baidu, w porównaniu do ERNIE 3.5, oferujący kompleksową aktualizację możliwości modelu, szeroko stosowany w złożonych scenariuszach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji."
  },
  "ERNIE-4.0-8K-Preview": {
    "description": "Flagowy model ultra dużego języka opracowany przez Baidu, w porównaniu do ERNIE 3.5, oferujący kompleksową aktualizację możliwości modelu, szeroko stosowany w złożonych scenariuszach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji."
  },
  "ERNIE-4.0-Turbo-8K-Latest": {
    "description": "Opracowany przez Baidu flagowy, ultra-duży model językowy, który wykazuje doskonałe ogólne rezultaty i jest szeroko stosowany w złożonych zadaniach w różnych dziedzinach; obsługuje automatyczne łączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji odpowiadających na pytania. W porównaniu do ERNIE 4.0 wykazuje lepszą wydajność."
  },
  "ERNIE-4.0-Turbo-8K-Preview": {
    "description": "Flagowy model ultra dużego języka opracowany przez Baidu, charakteryzujący się doskonałymi wynikami ogólnymi, szeroko stosowany w złożonych scenariuszach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji. W porównaniu do ERNIE 4.0, oferuje lepsze wyniki wydajności."
  },
  "ERNIE-Character-8K": {
    "description": "Model dużego języka opracowany przez Baidu, skoncentrowany na specyficznych scenariuszach, odpowiedni do zastosowań takich jak NPC w grach, rozmowy z obsługą klienta, odgrywanie ról w dialogach, charakteryzujący się wyraźnym i spójnym stylem postaci, silniejszą zdolnością do przestrzegania poleceń oraz lepszą wydajnością wnioskowania."
  },
  "ERNIE-Lite-Pro-128K": {
    "description": "Lekki model dużego języka opracowany przez Baidu, łączący doskonałe wyniki modelu z wydajnością wnioskowania, oferujący lepsze wyniki niż ERNIE Lite, odpowiedni do użycia w niskomocowych kartach przyspieszających AI."
  },
  "ERNIE-Speed-128K": {
    "description": "Najnowocześniejszy model dużego języka opracowany przez Baidu w 2024 roku, charakteryzujący się doskonałymi zdolnościami ogólnymi, odpowiedni jako model bazowy do dalszego dostosowywania, lepiej radzący sobie z problemami w specyficznych scenariuszach, a także zapewniający doskonałą wydajność wnioskowania."
  },
  "ERNIE-Speed-Pro-128K": {
    "description": "Najnowocześniejszy model dużego języka opracowany przez Baidu w 2024 roku, charakteryzujący się doskonałymi zdolnościami ogólnymi, oferujący lepsze wyniki niż ERNIE Speed, odpowiedni jako model bazowy do dalszego dostosowywania, lepiej radzący sobie z problemami w specyficznych scenariuszach, a także zapewniający doskonałą wydajność wnioskowania."
  },
  "FLUX-1.1-pro": {
    "description": "FLUX.1.1 Pro"
  },
  "FLUX.1-Kontext-dev": {
    "description": "FLUX.1-Kontext-dev to multimodalny model generowania i edycji obrazów opracowany przez Black Forest Labs, oparty na architekturze Rectified Flow Transformer, posiadający 12 miliardów parametrów. Skupia się na generowaniu, rekonstrukcji, wzmacnianiu i edycji obrazów w oparciu o podane warunki kontekstowe. Model łączy zalety kontrolowanej generacji modeli dyfuzyjnych z możliwościami modelowania kontekstu transformera, oferując wysoką jakość obrazów i szerokie zastosowanie w zadaniach takich jak naprawa, uzupełnianie i rekonstrukcja scen wizualnych."
  },
  "FLUX.1-Kontext-pro": {
    "description": "FLUX.1 Kontext [pro]"
  },
  "FLUX.1-dev": {
    "description": "FLUX.1-dev to otwarty multimodalny model językowy (MLLM) opracowany przez Black Forest Labs, zoptymalizowany pod kątem zadań tekstowo-obrazowych, łączący zdolności rozumienia i generowania obrazów oraz tekstu. Bazuje na zaawansowanych dużych modelach językowych (np. Mistral-7B) i dzięki starannie zaprojektowanemu enkoderowi wizualnemu oraz wieloetapowemu dostrajaniu instrukcji umożliwia współpracę tekstu i obrazu oraz złożone wnioskowanie."
  },
  "Gryphe/MythoMax-L2-13b": {
    "description": "MythoMax-L2 (13B) to innowacyjny model, idealny do zastosowań w wielu dziedzinach i złożonych zadań."
  },
  "HelloMeme": {
    "description": "HelloMeme to narzędzie AI, które automatycznie generuje memy, animacje lub krótkie filmy na podstawie dostarczonych przez Ciebie obrazów lub ruchów. Nie wymaga żadnych umiejętności rysunkowych ani programistycznych — wystarczy przygotować obraz referencyjny, a narzędzie stworzy atrakcyjne, zabawne i spójne stylistycznie treści."
  },
  "HiDream-I1-Full": {
    "description": "HiDream-E1-Full to otwarty, multimodalny model do edycji obrazów opracowany przez HiDream.ai, oparty na zaawansowanej architekturze Diffusion Transformer i wyposażony w potężne zdolności rozumienia języka (wbudowany LLaMA 3.1-8B-Instruct). Umożliwia generowanie obrazów, transfer stylu, lokalną edycję i przerysowywanie treści za pomocą naturalnych poleceń językowych, oferując doskonałe rozumienie i realizację zadań tekstowo-obrazowych."
  },
  "HunyuanDiT-v1.2-Diffusers-Distilled": {
    "description": "hunyuandit-v1.2-distilled to lekki model generowania obrazów na podstawie tekstu, zoptymalizowany przez destylację, umożliwiający szybkie tworzenie wysokiej jakości obrazów, szczególnie odpowiedni do środowisk o ograniczonych zasobach i zadań generacji w czasie rzeczywistym."
  },
  "InstantCharacter": {
    "description": "InstantCharacter to model generowania spersonalizowanych postaci bez potrzeby dostrajania, wydany przez zespół AI Tencent w 2025 roku. Model umożliwia wierne i spójne generowanie postaci w różnych scenariuszach na podstawie pojedynczego obrazu referencyjnego oraz elastyczne przenoszenie tej postaci do różnych stylów, ruchów i tła."
  },
  "InternVL2-8B": {
    "description": "InternVL2-8B to potężny model językowy wizualny, wspierający przetwarzanie multimodalne obrazów i tekstu, zdolny do precyzyjnego rozpoznawania treści obrazów i generowania odpowiednich opisów lub odpowiedzi."
  },
  "InternVL2.5-26B": {
    "description": "InternVL2.5-26B to potężny model językowy wizualny, wspierający przetwarzanie multimodalne obrazów i tekstu, zdolny do precyzyjnego rozpoznawania treści obrazów i generowania odpowiednich opisów lub odpowiedzi."
  },
  "Kolors": {
    "description": "Kolors to model generowania obrazów na podstawie tekstu opracowany przez zespół Kolors z Kuaishou. Trenowany na miliardach parametrów, wyróżnia się wysoką jakością wizualną, doskonałym rozumieniem semantyki języka chińskiego oraz precyzyjnym renderowaniem tekstu."
  },
  "Kwai-Kolors/Kolors": {
    "description": "Kolors to duży model generowania obrazów na podstawie tekstu oparty na latentnej dyfuzji, opracowany przez zespół Kolors z Kuaishou. Trenowany na miliardach par tekst-obraz, wykazuje znakomitą jakość wizualną, precyzję w rozumieniu złożonych semantyk oraz doskonałe renderowanie znaków chińskich i angielskich. Obsługuje wejścia w języku chińskim i angielskim, a także wyróżnia się w generowaniu specyficznych treści w języku chińskim."
  },
  "Llama-3.2-11B-Vision-Instruct": {
    "description": "Wyróżniające się zdolnościami wnioskowania obrazów na wysokiej rozdzielczości, odpowiednie do zastosowań w rozumieniu wizualnym."
  },
  "Llama-3.2-90B-Vision-Instruct\t": {
    "description": "Zaawansowane zdolności wnioskowania obrazów, odpowiednie do zastosowań w agentach rozumienia wizualnego."
  },
  "Meta-Llama-3-3-70B-Instruct": {
    "description": "Llama 3.3 70B: uniwersalny model Transformer, odpowiedni do zadań dialogowych i generowania tekstu."
  },
  "Meta-Llama-3.1-405B-Instruct": {
    "description": "Model tekstowy Llama 3.1 dostosowany do instrukcji, zoptymalizowany do wielojęzycznych przypadków użycia dialogów, osiągający doskonałe wyniki w wielu dostępnych modelach czatu, zarówno otwartych, jak i zamkniętych, w powszechnych benchmarkach branżowych."
  },
  "Meta-Llama-3.1-70B-Instruct": {
    "description": "Model tekstowy Llama 3.1 dostosowany do instrukcji, zoptymalizowany do wielojęzycznych przypadków użycia dialogów, osiągający doskonałe wyniki w wielu dostępnych modelach czatu, zarówno otwartych, jak i zamkniętych, w powszechnych benchmarkach branżowych."
  },
  "Meta-Llama-3.1-8B-Instruct": {
    "description": "Model tekstowy Llama 3.1 dostosowany do instrukcji, zoptymalizowany do wielojęzycznych przypadków użycia dialogów, osiągający doskonałe wyniki w wielu dostępnych modelach czatu, zarówno otwartych, jak i zamkniętych, w powszechnych benchmarkach branżowych."
  },
  "Meta-Llama-3.2-1B-Instruct": {
    "description": "Zaawansowany, nowoczesny mały model językowy, posiadający zdolności rozumienia języka, doskonałe umiejętności wnioskowania oraz generowania tekstu."
  },
  "Meta-Llama-3.2-3B-Instruct": {
    "description": "Zaawansowany, nowoczesny mały model językowy, posiadający zdolności rozumienia języka, doskonałe umiejętności wnioskowania oraz generowania tekstu."
  },
  "Meta-Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 to najnowocześniejszy wielojęzyczny otwarty model językowy z serii Llama, oferujący wydajność porównywalną z modelem 405B przy bardzo niskich kosztach. Oparty na strukturze Transformer, poprawiony dzięki nadzorowanemu dostrajaniu (SFT) oraz uczeniu ze wzmocnieniem opartym na ludzkiej opinii (RLHF), co zwiększa jego użyteczność i bezpieczeństwo. Jego wersja dostosowana do instrukcji została zoptymalizowana do wielojęzycznych dialogów, osiągając lepsze wyniki niż wiele dostępnych modeli czatu, zarówno otwartych, jak i zamkniętych, w wielu branżowych benchmarkach. Data graniczna wiedzy to grudzień 2023."
  },
  "Meta-Llama-4-Maverick-17B-128E-Instruct-FP8": {
    "description": "Llama 4 Maverick: duży model oparty na architekturze Mixture-of-Experts, oferujący efektywną strategię aktywacji ekspertów dla doskonałej wydajności podczas wnioskowania."
  },
  "MiniMax-M1": {
    "description": "Nowy, samodzielnie opracowany model inferencyjny. Globalny lider: 80K łańcuchów myślowych x 1M wejść, efektywność porównywalna z najlepszymi modelami zagranicznymi."
  },
  "MiniMax-Text-01": {
    "description": "W serii modeli MiniMax-01 wprowadziliśmy odważne innowacje: po raz pierwszy na dużą skalę zrealizowano mechanizm liniowej uwagi, tradycyjna architektura Transformera nie jest już jedynym wyborem. Liczba parametrów tego modelu wynosi aż 456 miliardów, z aktywacją wynoszącą 45,9 miliarda. Ogólna wydajność modelu dorównuje najlepszym modelom zagranicznym, jednocześnie efektywnie przetwarzając kontekst o długości do 4 milionów tokenów, co stanowi 32 razy więcej niż GPT-4o i 20 razy więcej niż Claude-3.5-Sonnet."
  },
  "MiniMaxAI/MiniMax-M1-80k": {
    "description": "MiniMax-M1 to otwartoźródłowy model inferencyjny o dużej skali z mieszanym mechanizmem uwagi, posiadający 456 miliardów parametrów, z których około 45,9 miliarda jest aktywowanych na każdy token. Model natywnie obsługuje ultra-długi kontekst do 1 miliona tokenów i dzięki mechanizmowi błyskawicznej uwagi oszczędza 75% operacji zmiennoprzecinkowych w zadaniach generowania na 100 tysiącach tokenów w porównaniu do DeepSeek R1. Ponadto MiniMax-M1 wykorzystuje architekturę MoE (mieszani eksperci), łącząc algorytm CISPO z efektywnym treningiem wzmacniającym opartym na mieszanej uwadze, osiągając wiodącą w branży wydajność w inferencji długich wejść i rzeczywistych scenariuszach inżynierii oprogramowania."
  },
  "Moonshot-Kimi-K2-Instruct": {
    "description": "Model o łącznej liczbie parametrów 1 biliona i aktywowanych 32 miliardach parametrów. Wśród modeli nie myślących osiąga czołowe wyniki w wiedzy specjalistycznej, matematyce i kodowaniu, lepiej radząc sobie z zadaniami ogólnymi agenta. Model jest starannie zoptymalizowany pod kątem zadań agenta, potrafi nie tylko odpowiadać na pytania, ale także podejmować działania. Idealny do improwizacji, ogólnej rozmowy i doświadczeń agenta, działający na poziomie refleksu bez potrzeby długiego przetwarzania."
  },
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) to model poleceń o wysokiej precyzji, idealny do złożonych obliczeń."
  },
  "OmniConsistency": {
    "description": "OmniConsistency poprawia spójność stylu i zdolność generalizacji w zadaniach obraz-do-obrazu (Image-to-Image) poprzez wprowadzenie dużych modeli Diffusion Transformers (DiTs) oraz parowanych danych stylizowanych, zapobiegając degradacji stylu."
  },
  "Phi-3-medium-128k-instruct": {
    "description": "Ten sam model Phi-3-medium, ale z większym rozmiarem kontekstu do RAG lub kilku strzałowego wywoływania."
  },
  "Phi-3-medium-4k-instruct": {
    "description": "Model z 14 miliardami parametrów, oferujący lepszą jakość niż Phi-3-mini, z naciskiem na dane o wysokiej jakości i gęstości rozumowania."
  },
  "Phi-3-mini-128k-instruct": {
    "description": "Ten sam model Phi-3-mini, ale z większym rozmiarem kontekstu do RAG lub kilku strzałowego wywoływania."
  },
  "Phi-3-mini-4k-instruct": {
    "description": "Najmniejszy członek rodziny Phi-3. Zoptymalizowany zarówno pod kątem jakości, jak i niskiej latencji."
  },
  "Phi-3-small-128k-instruct": {
    "description": "Ten sam model Phi-3-small, ale z większym rozmiarem kontekstu do RAG lub kilku strzałowego wywoływania."
  },
  "Phi-3-small-8k-instruct": {
    "description": "Model z 7 miliardami parametrów, oferujący lepszą jakość niż Phi-3-mini, z naciskiem na dane o wysokiej jakości i gęstości rozumowania."
  },
  "Phi-3.5-mini-instruct": {
    "description": "Zaktualizowana wersja modelu Phi-3-mini."
  },
  "Phi-3.5-vision-instrust": {
    "description": "Zaktualizowana wersja modelu Phi-3-vision."
  },
  "Pro/Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-7B-Instruct to model dużego języka z serii Qwen2, dostosowany do instrukcji, o rozmiarze parametrów wynoszącym 7B. Model ten oparty jest na architekturze Transformer, wykorzystując funkcję aktywacji SwiGLU, przesunięcia QKV w uwadze oraz grupowe zapytania uwagi. Może obsługiwać duże wejścia. Model ten wykazuje doskonałe wyniki w wielu testach benchmarkowych dotyczących rozumienia języka, generowania, zdolności wielojęzycznych, kodowania, matematyki i wnioskowania, przewyższając większość modeli open-source i wykazując konkurencyjność z modelami własnościowymi w niektórych zadaniach. Qwen2-7B-Instruct wykazuje znaczną poprawę wydajności w wielu ocenach w porównaniu do Qwen1.5-7B-Chat."
  },
  "Pro/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct to jeden z najnowszych modeli dużych języków wydanych przez Alibaba Cloud. Model 7B ma znacząco poprawione zdolności w zakresie kodowania i matematyki. Oferuje również wsparcie dla wielu języków, obejmując ponad 29 języków, w tym chiński i angielski. Model ten wykazuje znaczną poprawę w zakresie przestrzegania instrukcji, rozumienia danych strukturalnych oraz generowania strukturalnych wyników (szczególnie JSON)."
  },
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct to najnowsza wersja serii dużych modeli językowych specyficznych dla kodu wydana przez Alibaba Cloud. Model ten, oparty na Qwen2.5, został przeszkolony na 55 bilionach tokenów, znacznie poprawiając zdolności generowania kodu, wnioskowania i naprawy. Wzmacnia on nie tylko zdolności kodowania, ale także utrzymuje przewagę w zakresie matematyki i ogólnych umiejętności. Model ten stanowi bardziej kompleksową podstawę dla rzeczywistych zastosowań, takich jak inteligentne agenty kodowe."
  },
  "Pro/Qwen/Qwen2.5-VL-7B-Instruct": {
    "description": "Qwen2.5-VL to nowa wersja serii Qwen, posiadająca zaawansowane zdolności zrozumienia wizualnego. Potrafi analizować tekst, wykresy i układ w obrazach, a także zrozumieć długie filmy i wykrywać zdarzenia. Jest zdolny do przeprowadzania wnioskowania, operowania narzędziami, obsługuje lokalizację obiektów w różnych formatach i generowanie wyjścia strukturalnego. Optymalizuje trening rozdzielczości i klatki wideo, a także zwiększa efektywność kodera wizualnego."
  },
  "Pro/THUDM/GLM-4.1V-9B-Thinking": {
    "description": "GLM-4.1V-9B-Thinking to otwarty model wizualno-językowy (VLM) opracowany wspólnie przez Zhipu AI i Laboratorium KEG Uniwersytetu Tsinghua, zaprojektowany do obsługi złożonych zadań poznawczych wielomodalnych. Model opiera się na bazowym modelu GLM-4-9B-0414 i znacząco poprawia zdolności wnioskowania międzymodalnego oraz stabilność dzięki wprowadzeniu mechanizmu rozumowania „łańcucha myślowego” (Chain-of-Thought) oraz zastosowaniu strategii uczenia ze wzmocnieniem."
  },
  "Pro/THUDM/glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat to otwarta wersja modelu pretrenowanego z serii GLM-4, wydana przez Zhipu AI. Model ten wykazuje doskonałe wyniki w zakresie semantyki, matematyki, wnioskowania, kodu i wiedzy. Oprócz wsparcia dla wieloetapowych rozmów, GLM-4-9B-Chat oferuje również zaawansowane funkcje, takie jak przeglądanie stron internetowych, wykonywanie kodu, wywoływanie niestandardowych narzędzi (Function Call) oraz wnioskowanie z długich tekstów. Model obsługuje 26 języków, w tym chiński, angielski, japoński, koreański i niemiecki. W wielu testach benchmarkowych, takich jak AlignBench-v2, MT-Bench, MMLU i C-Eval, GLM-4-9B-Chat wykazuje doskonałą wydajność. Model obsługuje maksymalną długość kontekstu 128K, co czyni go odpowiednim do badań akademickich i zastosowań komercyjnych."
  },
  "Pro/deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 to model wnioskowania napędzany uczeniem ze wzmocnieniem (RL), który rozwiązuje problemy z powtarzalnością i czytelnością modeli. Przed RL, DeepSeek-R1 wprowadził dane do zimnego startu, co dodatkowo zoptymalizowało wydajność wnioskowania. W zadaniach matematycznych, kodowych i wnioskowania, osiąga wyniki porównywalne z OpenAI-o1, a dzięki starannie zaprojektowanym metodom treningowym poprawia ogólne wyniki."
  },
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B to model stworzony na podstawie Qwen2.5-Math-7B poprzez proces wiedzy distylacji. Model ten został wytrenowany na 800 000 wybrukowanych próbkach wygenerowanych przez DeepSeek-R1, co pozwoliło mu wykazać się doskonałymi zdolnościami wnioskowania. W wielu testach referencyjnych osiągnął znakomite wyniki, w tym 92,8% dokładności na MATH-500, 55,5% sukcesów na AIME 2024 oraz 1189 punktów na CodeForces, co potwierdza jego silne umiejętności matematyczne i programistyczne jako modelu o rozmiarze 7B."
  },
  "Pro/deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 to model językowy z 6710 miliardami parametrów, oparty na architekturze mieszanych ekspertów (MoE), wykorzystujący wielogłowicową potencjalną uwagę (MLA) oraz strategię równoważenia obciążenia bez dodatkowych strat, co optymalizuje wydajność wnioskowania i treningu. Dzięki wstępnemu treningowi na 14,8 bilionach wysokiej jakości tokenów oraz nadzorowanemu dostrajaniu i uczeniu ze wzmocnieniem, DeepSeek-V3 przewyższa inne modele open source, zbliżając się do wiodących modeli zamkniętych."
  },
  "Pro/deepseek-ai/DeepSeek-V3.1": {
    "description": "DeepSeek-V3.1 to hybrydowy duży model językowy wydany przez DeepSeek AI, który wprowadza wiele istotnych ulepszeń w stosunku do poprzednich wersji. Jedną z innowacji jest integracja trybu myślenia (Thinking Mode) i trybu bezmyślnego (Non-thinking Mode), które użytkownik może elastycznie przełączać, dostosowując szablony rozmów do różnych zadań. Dzięki specjalnej optymalizacji po treningu, wersja V3.1 znacznie poprawiła wydajność w wywoływaniu narzędzi i zadaniach agenta, lepiej wspierając zewnętrzne narzędzia wyszukiwania oraz realizację wieloetapowych, złożonych zadań. Model bazuje na DeepSeek-V3.1-Base i został poddany dalszemu treningowi z zastosowaniem dwufazowej metody rozszerzania długich tekstów, co znacznie zwiększyło ilość danych treningowych i poprawiło działanie na długich dokumentach oraz rozbudowanym kodzie. Jako model open source, DeepSeek-V3.1 wykazuje zdolności porównywalne z najlepszymi zamkniętymi modelami w benchmarkach kodowania, matematyki i wnioskowania, a dzięki architekturze hybrydowych ekspertów (MoE) utrzymuje ogromną pojemność modelu przy jednoczesnym efektywnym obniżeniu kosztów wnioskowania."
  },
  "Pro/deepseek-ai/DeepSeek-V3.1-Terminus": {
    "description": "DeepSeek-V3.1-Terminus to zaktualizowana wersja modelu V3.1 wydanego przez DeepSeek, zaprojektowana jako hybrydowy model językowy z agentami. Aktualizacja skupia się na naprawie zgłoszonych przez użytkowników problemów i poprawie stabilności, zachowując jednocześnie dotychczasowe możliwości modelu. Znacząco poprawiono spójność językową, zmniejszając mieszanie języka chińskiego i angielskiego oraz eliminując nieprawidłowe znaki. Model integruje tryb myślenia (Thinking Mode) oraz tryb bez myślenia (Non-thinking Mode), które użytkownicy mogą elastycznie przełączać za pomocą szablonów czatu, dostosowując się do różnych zadań. Ważną optymalizacją jest wzmocnienie wydajności agenta kodu (Code Agent) i agenta wyszukiwania (Search Agent), co czyni je bardziej niezawodnymi w wywoływaniu narzędzi i realizacji wieloetapowych, złożonych zadań."
  },
  "Pro/moonshotai/Kimi-K2-Instruct-0905": {
    "description": "Kimi K2-Instruct-0905 to najnowsza i najpotężniejsza wersja Kimi K2. Jest to zaawansowany model językowy typu Mixture of Experts (MoE) z 1 bilionem parametrów ogółem i 32 miliardami aktywowanych parametrów. Główne cechy modelu to: wzmocniona inteligencja kodowania agentów, która wykazuje znaczącą poprawę wydajności w publicznych testach porównawczych oraz w rzeczywistych zadaniach kodowania agentów; ulepszone doświadczenie kodowania front-end, z postępami zarówno w estetyce, jak i funkcjonalności programowania front-endowego."
  },
  "QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview to innowacyjny model przetwarzania języka naturalnego, który efektywnie radzi sobie z złożonymi zadaniami generowania dialogów i rozumienia kontekstu."
  },
  "Qwen/QVQ-72B-Preview": {
    "description": "QVQ-72B-Preview to model badawczy opracowany przez zespół Qwen, skoncentrowany na zdolnościach wnioskowania wizualnego, który ma unikalne zalety w zrozumieniu złożonych scenariuszy i rozwiązywaniu wizualnie związanych problemów matematycznych."
  },
  "Qwen/QwQ-32B": {
    "description": "QwQ jest modelem inferencyjnym z serii Qwen. W porównaniu do tradycyjnych modeli dostosowanych do instrukcji, QwQ posiada zdolności myślenia i wnioskowania, co pozwala na znaczące zwiększenie wydajności w zadaniach końcowych, szczególnie w rozwiązywaniu trudnych problemów. QwQ-32B to średniej wielkości model inferencyjny, który osiąga konkurencyjną wydajność w porównaniu z najnowocześniejszymi modelami inferencyjnymi, takimi jak DeepSeek-R1 i o1-mini. Model ten wykorzystuje technologie takie jak RoPE, SwiGLU, RMSNorm oraz Attention QKV bias, posiada 64-warstwową strukturę sieci i 40 głowic uwagi Q (w architekturze GQA KV wynosi 8)."
  },
  "Qwen/QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview to najnowszy eksperymentalny model badawczy Qwen, skoncentrowany na zwiększeniu zdolności wnioskowania AI. Poprzez eksplorację złożonych mechanizmów, takich jak mieszanie języków i wnioskowanie rekurencyjne, główne zalety obejmują silne zdolności analizy wnioskowania, matematyki i programowania. Jednocześnie występują problemy z przełączaniem języków, cyklami wnioskowania, kwestiami bezpieczeństwa oraz różnicami w innych zdolnościach."
  },
  "Qwen/Qwen2-72B-Instruct": {
    "description": "Qwen2 to zaawansowany uniwersalny model językowy, wspierający różne typy poleceń."
  },
  "Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-72B-Instruct to model dużego języka z serii Qwen2, dostosowany do instrukcji, o rozmiarze parametrów wynoszącym 72B. Model ten oparty jest na architekturze Transformer, wykorzystując funkcję aktywacji SwiGLU, przesunięcia QKV w uwadze oraz grupowe zapytania uwagi. Może obsługiwać duże wejścia. Model ten wykazuje doskonałe wyniki w wielu testach benchmarkowych dotyczących rozumienia języka, generowania, zdolności wielojęzycznych, kodowania, matematyki i wnioskowania, przewyższając większość modeli open-source i wykazując konkurencyjność z modelami własnościowymi w niektórych zadaniach."
  },
  "Qwen/Qwen2-VL-72B-Instruct": {
    "description": "Qwen2-VL to najnowsza iteracja modelu Qwen-VL, osiągająca najnowocześniejsze wyniki w benchmarkach zrozumienia wizualnego."
  },
  "Qwen/Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5 to nowa seria dużych modeli językowych, zaprojektowana w celu optymalizacji przetwarzania zadań instrukcyjnych."
  },
  "Qwen/Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5 to nowa seria dużych modeli językowych, zaprojektowana w celu optymalizacji przetwarzania zadań instrukcyjnych."
  },
  "Qwen/Qwen2.5-72B-Instruct": {
    "description": "Duży model językowy opracowany przez zespół Alibaba Cloud Tongyi Qianwen"
  },
  "Qwen/Qwen2.5-72B-Instruct-128K": {
    "description": "Qwen2.5 to nowa seria dużych modeli językowych, charakteryzująca się mocniejszymi zdolnościami rozumienia i generowania."
  },
  "Qwen/Qwen2.5-72B-Instruct-Turbo": {
    "description": "Qwen2.5 to nowa seria dużych modeli językowych, mająca na celu optymalizację przetwarzania zadań instruktażowych."
  },
  "Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5 to nowa seria dużych modeli językowych, zaprojektowana w celu optymalizacji przetwarzania zadań instrukcyjnych."
  },
  "Qwen/Qwen2.5-7B-Instruct-Turbo": {
    "description": "Qwen2.5 to nowa seria dużych modeli językowych, mająca na celu optymalizację przetwarzania zadań instruktażowych."
  },
  "Qwen/Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder koncentruje się na pisaniu kodu."
  },
  "Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct to najnowsza wersja serii dużych modeli językowych specyficznych dla kodu wydana przez Alibaba Cloud. Model ten, oparty na Qwen2.5, został przeszkolony na 55 bilionach tokenów, znacznie poprawiając zdolności generowania kodu, wnioskowania i naprawy. Wzmacnia on nie tylko zdolności kodowania, ale także utrzymuje przewagę w zakresie matematyki i ogólnych umiejętności. Model ten stanowi bardziej kompleksową podstawę dla rzeczywistych zastosowań, takich jak inteligentne agenty kodowe."
  },
  "Qwen/Qwen2.5-VL-32B-Instruct": {
    "description": "Qwen2.5-VL-32B-Instruct to wielomodalny model stworzony przez zespół Qwen2.5-VL, który jest częścią serii Qwen2.5-VL. Ten model nie tylko doskonale rozpoznaje obiekty, ale także analizuje tekst, wykresy, ikony, rysunki i układ w obrazach. Może działać jako inteligentny agent wizualny, który potrafi rozumować i dynamicznie sterować narzędziami, posiadając umiejętności korzystania z komputerów i telefonów. Ponadto, ten model może precyzyjnie lokalizować obiekty w obrazach i generować strukturalne wyjścia dla faktur, tabel i innych dokumentów. W porównaniu do poprzedniego modelu Qwen2-VL, ta wersja została dalej rozwinięta w zakresie umiejętności matematycznych i rozwiązywania problemów poprzez uczenie wzmacnianie, a styl odpowiedzi jest bardziej zgodny z preferencjami ludzkimi."
  },
  "Qwen/Qwen2.5-VL-72B-Instruct": {
    "description": "Qwen2.5-VL to model językowo-wizualny z serii Qwen2.5. Ten model przynosi znaczące poprawy w wielu aspektach: posiada lepsze zdolności zrozumienia wizualnego, umożliwiając rozpoznawanie powszechnych obiektów, analizowanie tekstu, wykresów i układu; jako wizualny agent może wnioskować i dynamicznie kierować użyciem narzędzi; obsługuje zrozumienie filmów o długości przekraczającej 1 godzinę i łapanie kluczowych zdarzeń; może precyzyjnie lokalizować obiekty na obrazach poprzez generowanie ramki granicznej lub punktów; obsługuje generowanie danych strukturalnych, szczególnie przydatnych dla skanowanych danych, takich jak faktury i tabele."
  },
  "Qwen/Qwen3-14B": {
    "description": "Qwen3 to nowa generacja modelu Qwen, która znacznie zwiększa zdolności w zakresie wnioskowania, ogólnych zadań, agentów i wielojęzyczności, osiągając wiodące w branży wyniki oraz wspierając przełączanie trybu myślenia."
  },
  "Qwen/Qwen3-235B-A22B": {
    "description": "Qwen3 to nowa generacja modelu Qwen, która znacznie zwiększa zdolności w zakresie wnioskowania, ogólnych zadań, agentów i wielojęzyczności, osiągając wiodące w branży wyniki oraz wspierając przełączanie trybu myślenia."
  },
  "Qwen/Qwen3-235B-A22B-Instruct-2507": {
    "description": "Qwen3-235B-A22B-Instruct-2507 to flagowy model dużego języka hybrydowego ekspertów (MoE) z serii Qwen3, opracowany przez zespół Alibaba Cloud Tongyi Qianwen. Model posiada 235 miliardów parametrów ogółem, z 22 miliardami aktywowanymi podczas inferencji. Jest to zaktualizowana wersja trybu nie myślącego Qwen3-235B-A22B, skupiająca się na znaczącej poprawie w zakresie przestrzegania instrukcji, wnioskowania logicznego, rozumienia tekstu, matematyki, nauki, programowania i użycia narzędzi. Model rozszerza pokrycie wiedzy wielojęzycznej i lepiej dostosowuje się do preferencji użytkowników w zadaniach subiektywnych i otwartych, generując bardziej pomocne i wysokiej jakości teksty."
  },
  "Qwen/Qwen3-235B-A22B-Thinking-2507": {
    "description": "Qwen3-235B-A22B-Thinking-2507 to model z serii Qwen3 opracowany przez zespół Alibaba Tongyi Qianwen, skoncentrowany na złożonych zadaniach wymagających zaawansowanego wnioskowania. Model oparty na architekturze hybrydowych ekspertów (MoE) posiada 235 miliardów parametrów, z aktywacją około 22 miliardów parametrów na token, co pozwala na wysoką wydajność przy efektywności obliczeniowej. Jako model „myślący” osiąga czołowe wyniki w zadaniach wymagających wiedzy specjalistycznej, takich jak logika, matematyka, nauka, programowanie i testy akademickie. Ponadto wzmacnia zdolności ogólne, takie jak przestrzeganie instrukcji, użycie narzędzi i generowanie tekstu, oraz natywnie obsługuje kontekst o długości do 256K tokenów, co czyni go idealnym do głębokiego wnioskowania i pracy z długimi dokumentami."
  },
  "Qwen/Qwen3-30B-A3B": {
    "description": "Qwen3 to nowa generacja modelu Qwen, która znacznie zwiększa zdolności w zakresie wnioskowania, ogólnych zadań, agentów i wielojęzyczności, osiągając wiodące w branży wyniki oraz wspierając przełączanie trybu myślenia."
  },
  "Qwen/Qwen3-30B-A3B-Instruct-2507": {
    "description": "Qwen3-30B-A3B-Instruct-2507 to zaktualizowana wersja modelu Qwen3-30B-A3B w trybie bez myślenia. Jest to model ekspertowy mieszany (MoE) z 30,5 miliardami parametrów ogółem i 3,3 miliardami parametrów aktywacyjnych. Model został znacząco ulepszony pod wieloma względami, w tym w zakresie przestrzegania instrukcji, rozumowania logicznego, rozumienia tekstu, matematyki, nauki, kodowania oraz korzystania z narzędzi. Ponadto osiągnął istotny postęp w pokryciu wiedzy wielojęzycznej oraz lepsze dopasowanie do preferencji użytkowników w zadaniach subiektywnych i otwartych, co pozwala generować bardziej pomocne odpowiedzi i teksty wyższej jakości. Dodatkowo zdolność rozumienia długich tekstów została zwiększona do 256K. Model ten obsługuje wyłącznie tryb bez myślenia i nie generuje tagów `<think></think>` w swoich odpowiedziach."
  },
  "Qwen/Qwen3-30B-A3B-Thinking-2507": {
    "description": "Qwen3-30B-A3B-Thinking-2507 jest najnowszym modelem „Thinking” z serii Qwen3, wydanym przez zespół Tongyi Qianwen firmy Alibaba. Jako hybrydowy model ekspertów (MoE) z 30,5 mld parametrów łącznie i 3,3 mld parametrów aktywacji koncentruje się na zwiększaniu zdolności do obsługi złożonych zadań. Model wykazuje znaczące usprawnienia wydajności w benchmarkach akademickich obejmujących rozumowanie logiczne, matematykę, nauki ścisłe, programowanie oraz zadania wymagające wiedzy eksperckiej. Ponadto jego ogólne możliwości — takie jak zgodność z instrukcjami, korzystanie z narzędzi, generowanie tekstu i dostosowanie do preferencji użytkowników — zostały istotnie wzmocnione. Model natywnie obsługuje długi kontekst o długości 256K tokenów i można go skalować do 1 miliona tokenów. Ta wersja została zaprojektowana do 'trybu myślenia' i ma na celu rozwiązywanie wysoce złożonych zadań poprzez szczegółowe, krok po kroku rozumowanie; jego zdolności jako agenta także wypadają znakomicie."
  },
  "Qwen/Qwen3-32B": {
    "description": "Qwen3 to nowa generacja modelu Qwen, która znacznie zwiększa zdolności w zakresie wnioskowania, ogólnych zadań, agentów i wielojęzyczności, osiągając wiodące w branży wyniki oraz wspierając przełączanie trybu myślenia."
  },
  "Qwen/Qwen3-8B": {
    "description": "Qwen3 to nowa generacja modelu Qwen, która znacznie zwiększa zdolności w zakresie wnioskowania, ogólnych zadań, agentów i wielojęzyczności, osiągając wiodące w branży wyniki oraz wspierając przełączanie trybu myślenia."
  },
  "Qwen/Qwen3-Coder-30B-A3B-Instruct": {
    "description": "Qwen3-Coder-30B-A3B-Instruct jest modelem kodowania z serii Qwen3 opracowanym przez zespół Tongyi Qianwen firmy Alibaba. Jako model poddany uproszczeniu i optymalizacji, przy zachowaniu wysokiej wydajności i efektywności, skupia się na udoskonaleniu zdolności przetwarzania kodu. Model wykazuje wyraźną przewagę wydajnościową wśród modeli open-source w złożonych zadaniach, takich jak programowanie agentowe (Agentic Coding), automatyzacja działań w przeglądarce oraz wywoływanie narzędzi. Natywnie obsługuje długi kontekst o długości 256K tokenów i można go rozszerzyć do 1M tokenów, co pozwala na lepsze rozumienie i przetwarzanie na poziomie repozytorium kodu. Ponadto model zapewnia silne wsparcie dla agentowego kodowania na platformach takich jak Qwen Code i CLINE oraz został zaprojektowany z dedykowanym formatem wywoływania funkcji."
  },
  "Qwen/Qwen3-Coder-480B-A35B-Instruct": {
    "description": "Qwen3-Coder-480B-A35B-Instruct został wydany przez Alibaba i jest jak dotąd modelem kodowania o największych zdolnościach agentskich (agentic). Jest to model typu Mixture-of-Experts (MoE) z 480 miliardami parametrów ogółem i 35 miliardami parametrów aktywacyjnych, osiągający równowagę między wydajnością a efektywnością. Model natywnie obsługuje kontekst o długości 256K (około 260 tys.) tokenów i może być rozszerzony do 1 miliona tokenów za pomocą metod ekstrapolacji, takich jak YaRN, co pozwala mu przetwarzać duże repozytoria kodu i złożone zadania programistyczne. Qwen3-Coder został zaprojektowany pod kątem agentowego przepływu pracy kodowania — nie tylko generuje kod, ale również potrafi autonomicznie współdziałać z narzędziami i środowiskami deweloperskimi, aby rozwiązywać złożone problemy programistyczne. W wielu benchmarkach dotyczących zadań kodowania i agentowych model osiągnął czołowe wyniki wśród modeli open-source, a jego wydajność dorównuje wiodącym modelom, takim jak Claude Sonnet 4."
  },
  "Qwen/Qwen3-Next-80B-A3B-Instruct": {
    "description": "Qwen3-Next-80B-A3B-Instruct to kolejna generacja modelu bazowego wydanego przez zespół Tongyi Qianwen z Alibaba. Opiera się na nowej architekturze Qwen3-Next, zaprojektowanej w celu osiągnięcia maksymalnej efektywności treningu i inferencji. Model wykorzystuje innowacyjny hybrydowy mechanizm uwagi (Gated DeltaNet i Gated Attention), wysoko rzadką strukturę ekspertów mieszanych (MoE) oraz liczne optymalizacje stabilności treningu. Jako model rzadki z 80 miliardami parametrów, podczas inferencji aktywuje jedynie około 3 miliardów parametrów, co znacznie obniża koszty obliczeniowe. Przy zadaniach z bardzo długim kontekstem przekraczającym 32 tysiące tokenów, przepustowość inferencji jest ponad 10 razy wyższa niż w modelu Qwen3-32B. Ten model jest wersją dostrojoną pod kątem instrukcji, zaprojektowaną do zadań ogólnego przeznaczenia i nie obsługuje trybu łańcucha myślenia (Thinking). Pod względem wydajności dorównuje flagowemu modelowi Tongyi Qianwen Qwen3-235B w niektórych benchmarkach, szczególnie wykazując wyraźną przewagę w zadaniach z bardzo długim kontekstem."
  },
  "Qwen/Qwen3-Next-80B-A3B-Thinking": {
    "description": "Qwen3-Next-80B-A3B-Thinking to kolejna generacja modelu bazowego wydanego przez zespół Tongyi Qianwen z Alibaba, specjalnie zaprojektowana do złożonych zadań wnioskowania. Opiera się na innowacyjnej architekturze Qwen3-Next, która łączy hybrydowy mechanizm uwagi (Gated DeltaNet i Gated Attention) oraz wysoko rzadką strukturę ekspertów mieszanych (MoE), dążąc do maksymalnej efektywności treningu i inferencji. Jako model rzadki z 80 miliardami parametrów, podczas inferencji aktywuje jedynie około 3 miliardów parametrów, co znacznie obniża koszty obliczeniowe. Przy zadaniach z bardzo długim kontekstem przekraczającym 32 tysiące tokenów, przepustowość jest ponad 10 razy wyższa niż w modelu Qwen3-32B. Wersja „Thinking” jest zoptymalizowana do wykonywania złożonych, wieloetapowych zadań takich jak dowody matematyczne, synteza kodu, analiza logiczna i planowanie, domyślnie generując proces wnioskowania w ustrukturyzowanej formie łańcucha myślenia. Pod względem wydajności przewyższa modele o wyższych kosztach, takie jak Qwen3-32B-Thinking, a także w wielu benchmarkach jest lepszy od Gemini-2.5-Flash-Thinking."
  },
  "Qwen2-72B-Instruct": {
    "description": "Qwen2 to najnowsza seria modeli Qwen, obsługująca kontekst 128k. W porównaniu do obecnie najlepszych modeli open source, Qwen2-72B znacznie przewyższa w zakresie rozumienia języka naturalnego, wiedzy, kodowania, matematyki i wielu języków."
  },
  "Qwen2-7B-Instruct": {
    "description": "Qwen2 to najnowsza seria modeli Qwen, która przewyższa najlepsze modele open source o podobnej skali, a nawet większe. Qwen2 7B osiągnęła znaczną przewagę w wielu testach, szczególnie w zakresie kodowania i rozumienia języka chińskiego."
  },
  "Qwen2-VL-72B": {
    "description": "Qwen2-VL-72B to potężny model językowo-wizualny, wspierający przetwarzanie multimodalne obrazów i tekstu, zdolny do precyzyjnego rozpoznawania treści obrazów i generowania odpowiednich opisów lub odpowiedzi."
  },
  "Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5-14B-Instruct to model językowy z 14 miliardami parametrów, o doskonałej wydajności, optymalizujący scenariusze w języku chińskim i wielojęzyczne, wspierający inteligentne odpowiedzi, generowanie treści i inne zastosowania."
  },
  "Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5-32B-Instruct to model językowy z 32 miliardami parametrów, o zrównoważonej wydajności, optymalizujący scenariusze w języku chińskim i wielojęzyczne, wspierający inteligentne odpowiedzi, generowanie treści i inne zastosowania."
  },
  "Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct obsługuje kontekst 16k, generując długie teksty przekraczające 8K. Wspiera wywołania funkcji i bezproblemową interakcję z systemami zewnętrznymi, znacznie zwiększając elastyczność i skalowalność. Wiedza modelu znacznie wzrosła, a jego zdolności w zakresie kodowania i matematyki uległy znacznemu poprawieniu, z obsługą ponad 29 języków."
  },
  "Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct to model językowy z 7 miliardami parametrów, wspierający wywołania funkcji i bezproblemową interakcję z systemami zewnętrznymi, znacznie zwiększając elastyczność i skalowalność. Optymalizuje scenariusze w języku chińskim i wielojęzyczne, wspierając inteligentne odpowiedzi, generowanie treści i inne zastosowania."
  },
  "Qwen2.5-Coder-14B-Instruct": {
    "description": "Qwen2.5-Coder-14B-Instruct to model instrukcji programowania oparty na dużych wstępnych treningach, posiadający silne zdolności rozumienia i generowania kodu, zdolny do efektywnego przetwarzania różnych zadań programistycznych, szczególnie odpowiedni do inteligentnego pisania kodu, generowania skryptów automatycznych i rozwiązywania problemów programistycznych."
  },
  "Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder-32B-Instruct to duży model językowy zaprojektowany specjalnie do generowania kodu, rozumienia kodu i efektywnych scenariuszy rozwoju, wykorzystujący wiodącą w branży skalę 32B parametrów, zdolny do zaspokojenia różnorodnych potrzeb programistycznych."
  },
  "Qwen3-235B": {
    "description": "Qwen3-235B-A22B to model MoE (ekspert mieszany), który wprowadza „hybrydowy tryb rozumowania”, umożliwiający użytkownikom płynne przełączanie się między trybem myślenia a trybem bez myślenia. Obsługuje rozumienie i rozumowanie w 119 językach i dialektach oraz posiada zaawansowane możliwości wywoływania narzędzi. W testach porównawczych obejmujących zdolności ogólne, kodowanie, matematykę, wielojęzyczność, wiedzę i rozumowanie konkuruje z czołowymi modelami rynkowymi, takimi jak DeepSeek R1, OpenAI o1, o3-mini, Grok 3 oraz Google Gemini 2.5 Pro."
  },
  "Qwen3-235B-A22B-Instruct-2507-FP8": {
    "description": "Qwen3 235B A22B Instruct 2507: model zoptymalizowany pod kątem zaawansowanego wnioskowania i instrukcji dialogowych, z hybrydową architekturą ekspertów zapewniającą efektywność wnioskowania przy dużej liczbie parametrów."
  },
  "Qwen3-32B": {
    "description": "Qwen3-32B to model gęsty (Dense Model), który wprowadza „hybrydowy tryb rozumowania”, umożliwiający użytkownikom płynne przełączanie się między trybem myślenia a trybem bez myślenia. Dzięki ulepszonej architekturze modelu, zwiększonej ilości danych treningowych oraz bardziej efektywnym metodom treningu, jego ogólna wydajność jest porównywalna z Qwen2.5-72B."
  },
  "SenseChat": {
    "description": "Podstawowa wersja modelu (V4), długość kontekstu 4K, silne zdolności ogólne."
  },
  "SenseChat-128K": {
    "description": "Podstawowa wersja modelu (V4), długość kontekstu 128K, doskonałe wyniki w zadaniach związanych z rozumieniem i generowaniem długich tekstów."
  },
  "SenseChat-32K": {
    "description": "Podstawowa wersja modelu (V4), długość kontekstu 32K, elastycznie stosowana w różnych scenariuszach."
  },
  "SenseChat-5": {
    "description": "Najnowsza wersja modelu (V5.5), długość kontekstu 128K, znacznie poprawione zdolności w zakresie rozumowania matematycznego, rozmów w języku angielskim, podążania za instrukcjami oraz rozumienia długich tekstów, dorównująca GPT-4o."
  },
  "SenseChat-5-1202": {
    "description": "Oparty na najnowszej wersji V5.5, z wyraźnymi ulepszeniami w podstawowych zdolnościach w języku chińskim i angielskim, czacie, wiedzy ścisłej i humanistycznej, pisaniu, logice matematycznej oraz kontroli liczby słów."
  },
  "SenseChat-5-Cantonese": {
    "description": "Długość kontekstu 32K, w rozumieniu rozmów w języku kantońskim przewyższa GPT-4, w wielu dziedzinach, takich jak wiedza, rozumowanie, matematyka i programowanie, dorównuje GPT-4 Turbo."
  },
  "SenseChat-5-beta": {
    "description": "Częściowo lepsza wydajność niż SenseCat-5-1202"
  },
  "SenseChat-Character": {
    "description": "Standardowa wersja modelu, długość kontekstu 8K, wysoka szybkość reakcji."
  },
  "SenseChat-Character-Pro": {
    "description": "Zaawansowana wersja modelu, długość kontekstu 32K, znacznie poprawione zdolności, obsługuje rozmowy w języku chińskim i angielskim."
  },
  "SenseChat-Turbo": {
    "description": "Idealny do szybkich odpowiedzi i scenariuszy dostosowywania modelu."
  },
  "SenseChat-Turbo-1202": {
    "description": "Jest to najnowsza wersja modelu o niskiej wadze, osiągająca ponad 90% możliwości pełnego modelu, znacznie obniżając koszty wnioskowania."
  },
  "SenseChat-Vision": {
    "description": "Najnowsza wersja modelu (V5.5), obsługująca wiele obrazów jako wejście, w pełni optymalizuje podstawowe możliwości modelu, osiągając znaczną poprawę w rozpoznawaniu atrybutów obiektów, relacji przestrzennych, rozpoznawaniu zdarzeń, zrozumieniu scen, rozpoznawaniu emocji, wnioskowaniu logicznym oraz generowaniu i rozumieniu tekstu."
  },
  "SenseNova-V6-5-Pro": {
    "description": "Dzięki kompleksowej aktualizacji danych multimodalnych, językowych i rozumowania oraz optymalizacji strategii treningowej, nowy model osiągnął znaczące ulepszenia w zakresie rozumowania multimodalnego i uniwersalnego przestrzegania instrukcji. Obsługuje kontekst o długości do 128k i wykazuje doskonałe wyniki w specjalistycznych zadaniach, takich jak OCR oraz rozpoznawanie IP w turystyce i kulturze."
  },
  "SenseNova-V6-5-Turbo": {
    "description": "Dzięki kompleksowej aktualizacji danych multimodalnych, językowych i rozumowania oraz optymalizacji strategii treningowej, nowy model osiągnął znaczące ulepszenia w zakresie rozumowania multimodalnego i uniwersalnego przestrzegania instrukcji. Obsługuje kontekst o długości do 128k i wykazuje doskonałe wyniki w specjalistycznych zadaniach, takich jak OCR oraz rozpoznawanie IP w turystyce i kulturze."
  },
  "SenseNova-V6-Pro": {
    "description": "Osiąga natywną jedność zdolności do przetwarzania obrazów, tekstów i wideo, przełamując tradycyjne ograniczenia rozdzielnych modalności, zdobywając podwójne mistrzostwo w ocenach OpenCompass i SuperCLUE."
  },
  "SenseNova-V6-Reasoner": {
    "description": "Łączy głębokie rozumienie wizualne i językowe, umożliwiając powolne myślenie i głęboką analizę, prezentując pełny proces myślowy."
  },
  "SenseNova-V6-Turbo": {
    "description": "Osiąga natywną jedność zdolności do przetwarzania obrazów, tekstów i wideo, przełamując tradycyjne ograniczenia rozdzielnych modalności, przewyższając w kluczowych wymiarach, takich jak podstawowe umiejętności multimodalne i językowe, oraz osiągając wysokie wyniki w wielu testach, wielokrotnie plasując się w czołówce krajowej i międzynarodowej."
  },
  "Skylark2-lite-8k": {
    "description": "Model drugiej generacji Skylark (Skylark2) o wysokiej szybkości reakcji, odpowiedni do scenariuszy wymagających wysokiej reaktywności, wrażliwych na koszty, z mniejszymi wymaganiami co do precyzji modelu, z długością okna kontekstowego 8k."
  },
  "Skylark2-pro-32k": {
    "description": "Model drugiej generacji Skylark (Skylark2) o wysokiej precyzji, odpowiedni do bardziej złożonych scenariuszy generowania tekstu, takich jak generowanie treści w profesjonalnych dziedzinach, tworzenie powieści oraz tłumaczenia wysokiej jakości, z długością okna kontekstowego 32k."
  },
  "Skylark2-pro-4k": {
    "description": "Model drugiej generacji Skylark (Skylark2) o wysokiej precyzji, odpowiedni do bardziej złożonych scenariuszy generowania tekstu, takich jak generowanie treści w profesjonalnych dziedzinach, tworzenie powieści oraz tłumaczenia wysokiej jakości, z długością okna kontekstowego 4k."
  },
  "Skylark2-pro-character-4k": {
    "description": "Model drugiej generacji Skylark (Skylark2) z doskonałymi umiejętnościami w odgrywaniu ról i czatowaniu. Doskonale reaguje na prompty użytkowników, odgrywając różne role w naturalny sposób, idealny do budowy chatbotów, wirtualnych asystentów i obsługi klienta online, cechujący się wysoką szybkością reakcji."
  },
  "Skylark2-pro-turbo-8k": {
    "description": "Model drugiej generacji Skylark (Skylark2) z szybszym wnioskowaniem i niższymi kosztami, z długością okna kontekstowego 8k."
  },
  "THUDM/GLM-4-32B-0414": {
    "description": "GLM-4-32B-0414 to nowa generacja otwartego modelu z serii GLM, posiadająca 32 miliardy parametrów. Model ten osiąga wyniki porównywalne z serią GPT OpenAI i serią V3/R1 DeepSeek."
  },
  "THUDM/GLM-4-9B-0414": {
    "description": "GLM-4-9B-0414 to mały model z serii GLM, mający 9 miliardów parametrów. Model ten dziedziczy cechy technologiczne serii GLM-4-32B, ale oferuje lżejsze opcje wdrożeniowe. Mimo mniejszych rozmiarów, GLM-4-9B-0414 nadal wykazuje doskonałe zdolności w generowaniu kodu, projektowaniu stron internetowych, generowaniu grafiki SVG i pisaniu opartym na wyszukiwaniu."
  },
  "THUDM/GLM-4.1V-9B-Thinking": {
    "description": "GLM-4.1V-9B-Thinking to otwarty model wizualno-językowy (VLM) opracowany wspólnie przez Zhipu AI i Laboratorium KEG Uniwersytetu Tsinghua, zaprojektowany do obsługi złożonych zadań poznawczych wielomodalnych. Model opiera się na bazowym modelu GLM-4-9B-0414 i znacząco poprawia zdolności wnioskowania międzymodalnego oraz stabilność dzięki wprowadzeniu mechanizmu rozumowania „łańcucha myślowego” (Chain-of-Thought) oraz zastosowaniu strategii uczenia ze wzmocnieniem."
  },
  "THUDM/GLM-Z1-32B-0414": {
    "description": "GLM-Z1-32B-0414 to model wnioskowania z głęboką zdolnością myślenia. Model ten oparty jest na GLM-4-32B-0414, rozwinięty poprzez zimny start i rozszerzone uczenie przez wzmocnienie, a także przeszedł dalsze szkolenie w zadaniach matematycznych, kodowania i logiki. W porównaniu do modelu bazowego, GLM-Z1-32B-0414 znacznie poprawił zdolności matematyczne i umiejętność rozwiązywania złożonych zadań."
  },
  "THUDM/GLM-Z1-9B-0414": {
    "description": "GLM-Z1-9B-0414 to mały model z serii GLM, mający tylko 9 miliardów parametrów, ale zachowujący tradycję otwartego źródła, jednocześnie wykazując zdumiewające zdolności. Mimo mniejszych rozmiarów, model ten nadal osiąga doskonałe wyniki w wnioskowaniu matematycznym i ogólnych zadaniach, a jego ogólna wydajność jest na czołowej pozycji wśród modeli o podobnej wielkości."
  },
  "THUDM/GLM-Z1-Rumination-32B-0414": {
    "description": "GLM-Z1-Rumination-32B-0414 to model głębokiego wnioskowania z zdolnością do refleksji (konkurujący z Deep Research OpenAI). W przeciwieństwie do typowych modeli głębokiego myślenia, model refleksyjny stosuje dłuższy czas głębokiego myślenia do rozwiązywania bardziej otwartych i złożonych problemów."
  },
  "THUDM/glm-4-9b-chat": {
    "description": "GLM-4 9B to otwarta wersja, oferująca zoptymalizowane doświadczenie dialogowe dla aplikacji konwersacyjnych."
  },
  "Tongyi-Zhiwen/QwenLong-L1-32B": {
    "description": "QwenLong-L1-32B to pierwszy duży model wnioskowania z długim kontekstem (LRM) wytrenowany z użyciem uczenia ze wzmocnieniem, zoptymalizowany pod kątem zadań wnioskowania na długich tekstach. Model osiąga stabilne przejście od krótkiego do długiego kontekstu dzięki progresywnemu rozszerzaniu kontekstu w ramach uczenia ze wzmocnieniem. W siedmiu benchmarkach dotyczących pytań i odpowiedzi na długich dokumentach QwenLong-L1-32B przewyższa flagowe modele takie jak OpenAI-o3-mini i Qwen3-235B-A22B, osiągając wydajność porównywalną z Claude-3.7-Sonnet-Thinking. Model jest szczególnie silny w złożonych zadaniach matematycznego, logicznego i wieloetapowego wnioskowania."
  },
  "Yi-34B-Chat": {
    "description": "Yi-1.5-34B, zachowując doskonałe ogólne zdolności językowe oryginalnej serii modeli, znacznie poprawił zdolności logiczne i kodowania dzięki dodatkowym treningom na 500 miliardach wysokiej jakości tokenów."
  },
  "abab5.5-chat": {
    "description": "Skierowany do scenariuszy produkcyjnych, wspierający przetwarzanie złożonych zadań i efektywne generowanie tekstu, odpowiedni do zastosowań w profesjonalnych dziedzinach."
  },
  "abab5.5s-chat": {
    "description": "Zaprojektowany specjalnie do scenariuszy dialogowych w języku chińskim, oferujący wysokiej jakości generowanie dialogów w języku chińskim, odpowiedni do różnych zastosowań."
  },
  "abab6.5g-chat": {
    "description": "Zaprojektowany specjalnie do dialogów z wielojęzycznymi postaciami, wspierający wysokiej jakości generowanie dialogów w języku angielskim i innych językach."
  },
  "abab6.5s-chat": {
    "description": "Odpowiedni do szerokiego zakresu zadań przetwarzania języka naturalnego, w tym generowania tekstu, systemów dialogowych itp."
  },
  "abab6.5t-chat": {
    "description": "Optymalizowany do scenariuszy dialogowych w języku chińskim, oferujący płynne i zgodne z chińskimi zwyczajami generowanie dialogów."
  },
  "accounts/fireworks/models/deepseek-r1": {
    "description": "DeepSeek-R1 to zaawansowany model językowy, który został zoptymalizowany dzięki uczeniu przez wzmocnienie i danym z zimnego startu, oferując doskonałe możliwości wnioskowania, matematyki i programowania."
  },
  "accounts/fireworks/models/deepseek-v3": {
    "description": "Potężny model językowy Mixture-of-Experts (MoE) oferowany przez Deepseek, z całkowitą liczbą parametrów wynoszącą 671 miliardów, aktywującym 37 miliardów parametrów na każdy token."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct": {
    "description": "Model Llama 3 70B Instruct, zaprojektowany do wielojęzycznych dialogów i rozumienia języka naturalnego, przewyższa większość konkurencyjnych modeli."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct": {
    "description": "Model Llama 3 8B Instruct, zoptymalizowany do dialogów i zadań wielojęzycznych, oferuje doskonałe i efektywne osiągi."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct-hf": {
    "description": "Model Llama 3 8B Instruct (wersja HF), zgodny z wynikami oficjalnej implementacji, zapewnia wysoką spójność i kompatybilność międzyplatformową."
  },
  "accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "description": "Model Llama 3.1 405B Instruct, z ogromną liczbą parametrów, idealny do złożonych zadań i śledzenia poleceń w scenariuszach o dużym obciążeniu."
  },
  "accounts/fireworks/models/llama-v3p1-70b-instruct": {
    "description": "Model Llama 3.1 70B Instruct oferuje doskonałe możliwości rozumienia i generowania języka, idealny do zadań dialogowych i analitycznych."
  },
  "accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "description": "Model Llama 3.1 8B Instruct, zoptymalizowany do wielojęzycznych dialogów, potrafi przewyższyć większość modeli open source i closed source w powszechnych standardach branżowych."
  },
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct": {
    "description": "Model wnioskowania wizualnego z 11B parametrów od Meta. Model zoptymalizowany do rozpoznawania wizualnego, wnioskowania obrazów, opisywania obrazów oraz odpowiadania na ogólne pytania dotyczące obrazów. Model potrafi rozumieć dane wizualne, takie jak wykresy i grafiki, a dzięki generowaniu tekstowych opisów szczegółów obrazów, łączy wizję z językiem."
  },
  "accounts/fireworks/models/llama-v3p2-3b-instruct": {
    "description": "Model instruktażowy Llama 3.2 3B to lekki model wielojęzyczny zaprezentowany przez Meta. Zaprojektowany, aby poprawić wydajność, oferując znaczące usprawnienia w opóźnieniu i kosztach w porównaniu do większych modeli. Przykładowe przypadki użycia tego modelu obejmują zapytania i przepisanie sugestii oraz pomoc w pisaniu."
  },
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct": {
    "description": "Model wnioskowania wizualnego z 90B parametrów od Meta. Model zoptymalizowany do rozpoznawania wizualnego, wnioskowania obrazów, opisywania obrazów oraz odpowiadania na ogólne pytania dotyczące obrazów. Model potrafi rozumieć dane wizualne, takie jak wykresy i grafiki, a dzięki generowaniu tekstowych opisów szczegółów obrazów, łączy wizję z językiem."
  },
  "accounts/fireworks/models/llama-v3p3-70b-instruct": {
    "description": "Llama 3.3 70B Instruct to zaktualizowana wersja Llama 3.1 70B z grudnia. Model ten został ulepszony w oparciu o Llama 3.1 70B (wydany w lipcu 2024), wzmacniając możliwości wywoływania narzędzi, wsparcie dla tekstów w wielu językach, a także umiejętności matematyczne i programistyczne. Model osiągnął wiodący w branży poziom w zakresie wnioskowania, matematyki i przestrzegania instrukcji, oferując wydajność porównywalną z 3.1 405B, jednocześnie zapewniając znaczące korzyści w zakresie szybkości i kosztów."
  },
  "accounts/fireworks/models/mistral-small-24b-instruct-2501": {
    "description": "Model z 24 miliardami parametrów, oferujący zaawansowane możliwości porównywalne z większymi modelami."
  },
  "accounts/fireworks/models/mixtral-8x22b-instruct": {
    "description": "Model Mixtral MoE 8x22B Instruct, z dużą liczbą parametrów i architekturą wielu ekspertów, kompleksowo wspierający efektywne przetwarzanie złożonych zadań."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct": {
    "description": "Model Mixtral MoE 8x7B Instruct, architektura wielu ekspertów, oferująca efektywne śledzenie i wykonanie poleceń."
  },
  "accounts/fireworks/models/mythomax-l2-13b": {
    "description": "Model MythoMax L2 13B, łączący nowatorskie techniki łączenia, doskonały w narracji i odgrywaniu ról."
  },
  "accounts/fireworks/models/phi-3-vision-128k-instruct": {
    "description": "Model Phi 3 Vision Instruct, lekki model multimodalny, zdolny do przetwarzania złożonych informacji wizualnych i tekstowych, z silnymi zdolnościami wnioskowania."
  },
  "accounts/fireworks/models/qwen-qwq-32b-preview": {
    "description": "Model QwQ to eksperymentalny model badawczy opracowany przez zespół Qwen, skoncentrowany na zwiększeniu zdolności wnioskowania AI."
  },
  "accounts/fireworks/models/qwen2-vl-72b-instruct": {
    "description": "Wersja 72B modelu Qwen-VL to najnowszy owoc iteracji Alibaba, reprezentujący innowacje z ostatniego roku."
  },
  "accounts/fireworks/models/qwen2p5-72b-instruct": {
    "description": "Qwen2.5 to seria modeli językowych opracowana przez zespół Qwen na chmurze Alibaba, która zawiera jedynie dekodery. Modele te występują w różnych rozmiarach, w tym 0.5B, 1.5B, 3B, 7B, 14B, 32B i 72B, i oferują dwie wersje: bazową (base) i instruktażową (instruct)."
  },
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "description": "Qwen2.5 Coder 32B Instruct to najnowsza wersja serii dużych modeli językowych specyficznych dla kodu wydana przez Alibaba Cloud. Model ten, oparty na Qwen2.5, został przeszkolony na 55 bilionach tokenów, znacznie poprawiając zdolności generowania kodu, wnioskowania i naprawy. Wzmacnia on nie tylko zdolności kodowania, ale także utrzymuje przewagę w zakresie matematyki i ogólnych umiejętności. Model ten stanowi bardziej kompleksową podstawę dla rzeczywistych zastosowań, takich jak inteligentne agenty kodowe."
  },
  "accounts/yi-01-ai/models/yi-large": {
    "description": "Model Yi-Large, oferujący doskonałe możliwości przetwarzania wielojęzycznego, nadający się do różnych zadań generowania i rozumienia języka."
  },
  "ai21-jamba-1.5-large": {
    "description": "Model wielojęzyczny z 398 miliardami parametrów (94 miliardy aktywnych), oferujący okno kontekstowe o długości 256K, wywoływanie funkcji, strukturalne wyjście i generację opartą na kontekście."
  },
  "ai21-jamba-1.5-mini": {
    "description": "Model wielojęzyczny z 52 miliardami parametrów (12 miliardów aktywnych), oferujący okno kontekstowe o długości 256K, wywoływanie funkcji, strukturalne wyjście i generację opartą na kontekście."
  },
  "ai21-labs/AI21-Jamba-1.5-Large": {
    "description": "Model wielojęzyczny o 398 miliardach parametrów (94 miliardy aktywnych), oferujący okno kontekstowe o długości 256K, wywoływanie funkcji, strukturalne wyjście oraz generowanie oparte na faktach."
  },
  "ai21-labs/AI21-Jamba-1.5-Mini": {
    "description": "Model wielojęzyczny o 52 miliardach parametrów (12 miliardów aktywnych), oferujący okno kontekstowe o długości 256K, wywoływanie funkcji, strukturalne wyjście oraz generowanie oparte na faktach."
  },
  "alibaba/qwen-3-14b": {
    "description": "Qwen3 to najnowsza generacja dużych modeli językowych z serii Qwen, oferująca kompleksowy zestaw modeli gęstych i hybrydowych ekspertów (MoE). Dzięki szerokiemu treningowi Qwen3 zapewnia przełomowe postępy w zakresie wnioskowania, przestrzegania instrukcji, zdolności agentów oraz wsparcia wielojęzycznego."
  },
  "alibaba/qwen-3-235b": {
    "description": "Qwen3 to najnowsza generacja dużych modeli językowych z serii Qwen, oferująca kompleksowy zestaw modeli gęstych i hybrydowych ekspertów (MoE). Dzięki szerokiemu treningowi Qwen3 zapewnia przełomowe postępy w zakresie wnioskowania, przestrzegania instrukcji, zdolności agentów oraz wsparcia wielojęzycznego."
  },
  "alibaba/qwen-3-30b": {
    "description": "Qwen3 to najnowsza generacja dużych modeli językowych z serii Qwen, oferująca kompleksowy zestaw modeli gęstych i hybrydowych ekspertów (MoE). Dzięki szerokiemu treningowi Qwen3 zapewnia przełomowe postępy w zakresie wnioskowania, przestrzegania instrukcji, zdolności agentów oraz wsparcia wielojęzycznego."
  },
  "alibaba/qwen-3-32b": {
    "description": "Qwen3 to najnowsza generacja dużych modeli językowych z serii Qwen, oferująca kompleksowy zestaw modeli gęstych i hybrydowych ekspertów (MoE). Dzięki szerokiemu treningowi Qwen3 zapewnia przełomowe postępy w zakresie wnioskowania, przestrzegania instrukcji, zdolności agentów oraz wsparcia wielojęzycznego."
  },
  "alibaba/qwen3-coder": {
    "description": "Qwen3-Coder-480B-A35B-Instruct to najbardziej agentowy model kodowania z serii Qwen, wyróżniający się znakomitą wydajnością w kodowaniu agentowym, korzystaniu z przeglądarki przez agenta oraz innych podstawowych zadaniach kodowania, osiągając wyniki porównywalne z Claude Sonnet."
  },
  "amazon/nova-lite": {
    "description": "Bardzo niskokosztowy model multimodalny, który przetwarza obrazy, wideo i tekst z niezwykłą szybkością."
  },
  "amazon/nova-micro": {
    "description": "Model tekstowy oferujący najniższe opóźnienia przy bardzo niskich kosztach."
  },
  "amazon/nova-pro": {
    "description": "Wysoce kompetentny model multimodalny, oferujący optymalne połączenie dokładności, szybkości i kosztów, odpowiedni do szerokiego zakresu zadań."
  },
  "amazon/titan-embed-text-v2": {
    "description": "Amazon Titan Text Embeddings V2 to lekki, wydajny model wielojęzycznych osadzeń, obsługujący wymiary 1024, 512 i 256."
  },
  "anthropic.claude-3-5-sonnet-20240620-v1:0": {
    "description": "Claude 3.5 Sonnet podnosi standardy branżowe, przewyższając modele konkurencji oraz Claude 3 Opus, osiągając doskonałe wyniki w szerokim zakresie ocen, jednocześnie oferując szybkość i koszty na poziomie naszych modeli średniej klasy."
  },
  "anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet podnosi standardy branżowe, przewyższając modele konkurencji oraz Claude 3 Opus, wykazując doskonałe wyniki w szerokich ocenach, jednocześnie oferując prędkość i koszty naszych modeli średniego poziomu."
  },
  "anthropic.claude-3-haiku-20240307-v1:0": {
    "description": "Claude 3 Haiku to najszybszy i najbardziej kompaktowy model od Anthropic, oferujący niemal natychmiastową szybkość odpowiedzi. Może szybko odpowiadać na proste zapytania i prośby. Klienci będą mogli budować płynne doświadczenia AI, które naśladują interakcje międzyludzkie. Claude 3 Haiku może przetwarzać obrazy i zwracać wyjścia tekstowe, z oknem kontekstowym wynoszącym 200K."
  },
  "anthropic.claude-3-opus-20240229-v1:0": {
    "description": "Claude 3 Opus to najpotężniejszy model AI od Anthropic, z najnowocześniejszymi osiągami w wysoko złożonych zadaniach. Może obsługiwać otwarte podpowiedzi i nieznane scenariusze, oferując doskonałą płynność i ludzkie zdolności rozumienia. Claude 3 Opus pokazuje granice możliwości generatywnej AI. Claude 3 Opus może przetwarzać obrazy i zwracać wyjścia tekstowe, z oknem kontekstowym wynoszącym 200K."
  },
  "anthropic.claude-3-sonnet-20240229-v1:0": {
    "description": "Claude 3 Sonnet od Anthropic osiąga idealną równowagę między inteligencją a szybkością — szczególnie odpowiedni do obciążeń roboczych w przedsiębiorstwach. Oferuje maksymalną użyteczność po niższej cenie niż konkurencja i został zaprojektowany jako niezawodny, wytrzymały model główny, odpowiedni do skalowalnych wdrożeń AI. Claude 3 Sonnet może przetwarzać obrazy i zwracać wyjścia tekstowe, z oknem kontekstowym wynoszącym 200K."
  },
  "anthropic.claude-instant-v1": {
    "description": "Szybki, ekonomiczny model, który wciąż jest bardzo zdolny, może obsługiwać szereg zadań, w tym codzienne rozmowy, analizę tekstu, podsumowania i pytania dotyczące dokumentów."
  },
  "anthropic.claude-v2": {
    "description": "Model Anthropic wykazuje wysokie zdolności w szerokim zakresie zadań, od złożonych rozmów i generowania treści kreatywnych po szczegółowe przestrzeganie instrukcji."
  },
  "anthropic.claude-v2:1": {
    "description": "Zaktualizowana wersja Claude 2, z podwójnym oknem kontekstowym oraz poprawioną niezawodnością, wskaźnikiem halucynacji i dokładnością opartą na dowodach w kontekście długich dokumentów i RAG."
  },
  "anthropic/claude-3-haiku": {
    "description": "Claude 3 Haiku to najszybszy model Anthropic, zaprojektowany do obciążeń korporacyjnych zwykle obejmujących długie podpowiedzi. Haiku potrafi szybko analizować duże ilości dokumentów, takich jak raporty kwartalne, umowy czy sprawy prawne, przy kosztach stanowiących połowę innych modeli o podobnej klasie wydajności."
  },
  "anthropic/claude-3-opus": {
    "description": "Claude 3 Opus to najbardziej inteligentny model Anthropic, oferujący wiodącą na rynku wydajność w bardzo złożonych zadaniach. Potrafi płynnie i z ludzkim zrozumieniem radzić sobie z otwartymi podpowiedziami i nieznanymi wcześniej scenariuszami."
  },
  "anthropic/claude-3.5-haiku": {
    "description": "Claude 3.5 Haiku to następna generacja naszego najszybszego modelu. Oferuje podobną szybkość jak Claude 3 Haiku, ale z ulepszeniami we wszystkich zestawach umiejętności i przewyższa w wielu testach inteligencji nasz poprzedni największy model Claude 3 Opus."
  },
  "anthropic/claude-3.5-sonnet": {
    "description": "Claude 3.5 Sonnet osiąga idealną równowagę między inteligencją a szybkością — szczególnie dla obciążeń korporacyjnych. W porównaniu z konkurencją oferuje potężną wydajność przy niższych kosztach i jest zaprojektowany z myślą o wysokiej trwałości w dużych wdrożeniach AI."
  },
  "anthropic/claude-3.7-sonnet": {
    "description": "Claude 3.7 Sonnet to pierwszy model hybrydowego wnioskowania i najbardziej inteligentny model Anthropic do tej pory. Oferuje zaawansowaną wydajność w kodowaniu, generowaniu treści, analizie danych i planowaniu, budując na fundamentach inżynierii oprogramowania i umiejętności komputerowych poprzednika Claude 3.5 Sonnet."
  },
  "anthropic/claude-opus-4": {
    "description": "Claude Opus 4 to najsilniejszy model Anthropic i najlepszy na świecie model kodowania, prowadzący w benchmarkach SWE-bench (72,5%) i Terminal-bench (43,2%). Zapewnia ciągłą wydajność dla długotrwałych zadań wymagających skupienia i tysięcy kroków, mogąc pracować nieprzerwanie przez wiele godzin — znacznie rozszerzając możliwości agentów AI."
  },
  "anthropic/claude-opus-4.1": {
    "description": "Claude Opus 4.1 to plug-and-play alternatywa dla Opus 4, oferująca doskonałą wydajność i precyzję w praktycznych zadaniach kodowania i agentów. Podnosi najnowocześniejszą wydajność kodowania do 74,5% w SWE-bench Verified i radzi sobie złożonymi, wieloetapowymi problemami z większą rygorystycznością i dbałością o szczegóły."
  },
  "anthropic/claude-sonnet-4": {
    "description": "Claude Sonnet 4 to znacząca poprawa w stosunku do Sonnet 3.7, oferująca doskonałą wydajność w kodowaniu z rekordowym wynikiem 72,7% w SWE-bench. Model osiąga równowagę między wydajnością a efektywnością, nadaje się do zastosowań wewnętrznych i zewnętrznych oraz zapewnia większą kontrolę dzięki ulepszonej sterowalności."
  },
  "anthropic/claude-sonnet-4.5": {
    "description": "Claude Sonnet 4.5 to jak dotąd najbardziej inteligentny model Anthropic."
  },
  "ascend-tribe/pangu-pro-moe": {
    "description": "Pangu-Pro-MoE 72B-A16B to rzadki, duży model językowy o 72 miliardach parametrów i 16 miliardach aktywowanych parametrów, oparty na architekturze grupowanych ekspertów (MoGE). W fazie wyboru ekspertów model grupuje ekspertów i ogranicza aktywację tokenów do równej liczby ekspertów w każdej grupie, co zapewnia równomierne obciążenie ekspertów i znacznie poprawia efektywność wdrożenia modelu na platformie Ascend."
  },
  "aya": {
    "description": "Aya 23 to model wielojęzyczny wydany przez Cohere, wspierający 23 języki, ułatwiający różnorodne zastosowania językowe."
  },
  "aya:35b": {
    "description": "Aya 23 to model wielojęzyczny wydany przez Cohere, wspierający 23 języki, ułatwiający różnorodne zastosowania językowe."
  },
  "azure-DeepSeek-R1-0528": {
    "description": "Dostarczony i wdrożony przez Microsoft; model DeepSeek R1 przeszedł drobną aktualizację wersji, obecna wersja to DeepSeek-R1-0528. W najnowszej aktualizacji DeepSeek R1 znacznie poprawił głębokość wnioskowania i zdolności inferencyjne poprzez zwiększenie zasobów obliczeniowych oraz wprowadzenie optymalizacji algorytmów w fazie post-treningowej. Model ten osiąga doskonałe wyniki w testach bazowych z matematyki, programowania i logiki ogólnej, a jego ogólna wydajność zbliża się do czołowych modeli, takich jak O3 i Gemini 2.5 Pro."
  },
  "baichuan/baichuan2-13b-chat": {
    "description": "Baichuan-13B to otwarty model językowy stworzony przez Baichuan Intelligence, zawierający 13 miliardów parametrów, który osiągnął najlepsze wyniki w swojej klasie w autorytatywnych benchmarkach w języku chińskim i angielskim."
  },
  "baidu/ERNIE-4.5-300B-A47B": {
    "description": "ERNIE-4.5-300B-A47B to duży model językowy opracowany przez firmę Baidu, oparty na hybrydowej architekturze ekspertów (MoE). Model ma 300 miliardów parametrów, ale podczas inferencji aktywuje tylko 47 miliardów parametrów na token, co zapewnia doskonałą wydajność przy efektywności obliczeniowej. Jako jeden z kluczowych modeli serii ERNIE 4.5, wykazuje znakomite zdolności w rozumieniu tekstu, generowaniu, wnioskowaniu i programowaniu. Model wykorzystuje innowacyjną metodę pretrenowania multimodalnego heterogenicznego MoE, łącząc trening tekstu i wizji, co skutecznie zwiększa jego zdolności, zwłaszcza w zakresie przestrzegania instrukcji i pamięci wiedzy o świecie."
  },
  "c4ai-aya-expanse-32b": {
    "description": "Aya Expanse to model wielojęzyczny o wysokiej wydajności 32B, zaprojektowany w celu wyzwania wydajności modeli jednolanguage poprzez innowacje w zakresie dostosowywania instrukcji, arbitrażu danych, treningu preferencji i łączenia modeli. Obsługuje 23 języki."
  },
  "c4ai-aya-expanse-8b": {
    "description": "Aya Expanse to model wielojęzyczny o wysokiej wydajności 8B, zaprojektowany w celu wyzwania wydajności modeli jednolanguage poprzez innowacje w zakresie dostosowywania instrukcji, arbitrażu danych, treningu preferencji i łączenia modeli. Obsługuje 23 języki."
  },
  "c4ai-aya-vision-32b": {
    "description": "Aya Vision to zaawansowany model wielomodalny, który osiąga doskonałe wyniki w wielu kluczowych benchmarkach dotyczących zdolności językowych, tekstowych i obrazowych. Obsługuje 23 języki. Ta wersja z 32 miliardami parametrów koncentruje się na najnowocześniejszej wydajności wielojęzycznej."
  },
  "c4ai-aya-vision-8b": {
    "description": "Aya Vision to zaawansowany model wielomodalny, który osiąga doskonałe wyniki w wielu kluczowych benchmarkach dotyczących zdolności językowych, tekstowych i obrazowych. Ta wersja z 8 miliardami parametrów koncentruje się na niskiej latencji i najlepszej wydajności."
  },
  "charglm-3": {
    "description": "CharGLM-3 zaprojektowany z myślą o odgrywaniu ról i emocjonalnym towarzyszeniu, obsługujący ultra-długą pamięć wielokrotną i spersonalizowane dialogi, z szerokim zakresem zastosowań."
  },
  "charglm-4": {
    "description": "CharGLM-4 zaprojektowany z myślą o odgrywaniu ról i emocjonalnym towarzyszeniu, wspierający długotrwałą pamięć i spersonalizowane rozmowy, z szerokim zakresem zastosowań."
  },
  "chatglm3": {
    "description": "ChatGLM3 to zamknięty model opracowany przez AI ZhiPu i KEG Laboratorium z Politechniki Tsinghua, który przeszedł wstępne treningi na ogromnej liczbie identyfikatorów chińskich i angielskich oraz trening zgodności z preferencjami ludzkimi. W porównaniu do pierwszej generacji modelu, ChatGLM3 osiągnął poprawę o 16%, 36% i 280% w testach MMLU, C-Eval i GSM8K, oraz zajął pierwsze miejsce na liście chińskich zadań C-Eval. Jest odpowiedni do zastosowań, które wymagają wysokiej wiedzy, zdolności wnioskowania i kreatywności, takich jak tworzenie tekstów reklamowych, pisarstwo powieści, pisarstwo naukowe i generowanie kodu."
  },
  "chatglm3-6b-base": {
    "description": "ChatGLM3-6b-base to najnowszy model z serii ChatGLM opracowany przez ZhiPu, o skali 6 miliardów parametrów, dostępny jako oprogramowanie open source."
  },
  "chatgpt-4o-latest": {
    "description": "ChatGPT-4o to dynamiczny model, który jest na bieżąco aktualizowany, aby utrzymać najnowszą wersję. Łączy potężne zdolności rozumienia i generowania języka, co czyni go odpowiednim do zastosowań na dużą skalę, w tym obsługi klienta, edukacji i wsparcia technicznego."
  },
  "claude-2.0": {
    "description": "Claude 2 oferuje postępy w kluczowych możliwościach dla przedsiębiorstw, w tym wiodącą w branży kontekst 200K tokenów, znacznie zmniejszającą częstość występowania halucynacji modelu, systemowe podpowiedzi oraz nową funkcję testową: wywołania narzędzi."
  },
  "claude-2.1": {
    "description": "Claude 2 oferuje postępy w kluczowych możliwościach dla przedsiębiorstw, w tym wiodącą w branży kontekst 200K tokenów, znacznie zmniejszającą częstość występowania halucynacji modelu, systemowe podpowiedzi oraz nową funkcję testową: wywołania narzędzi."
  },
  "claude-3-5-haiku-20241022": {
    "description": "Claude 3.5 Haiku to najszybszy model następnej generacji od Anthropic. W porównaniu do Claude 3 Haiku, Claude 3.5 Haiku wykazuje poprawę w różnych umiejętnościach i przewyższa największy model poprzedniej generacji, Claude 3 Opus, w wielu testach inteligencji."
  },
  "claude-3-5-haiku-latest": {
    "description": "Claude 3.5 Haiku zapewnia szybkie odpowiedzi, idealne do lekkich zadań."
  },
  "claude-3-5-sonnet-20240620": {
    "description": "Claude 3.5 Sonnet oferuje możliwości przewyższające Opus oraz szybsze tempo niż Sonnet, przy zachowaniu tej samej ceny. Sonnet szczególnie dobrze radzi sobie z programowaniem, nauką danych, przetwarzaniem wizualnym i zadaniami agenta."
  },
  "claude-3-5-sonnet-20241022": {
    "description": "Claude 3.5 Sonnet oferuje możliwości wykraczające poza Opus oraz szybsze działanie niż Sonnet, zachowując jednocześnie tę samą cenę. Sonnet jest szczególnie uzdolniony w programowaniu, naukach danych, przetwarzaniu wizualnym oraz zadaniach związanych z pośrednictwem."
  },
  "claude-3-7-sonnet-20250219": {
    "description": "Claude 3.7 Sonnet to najnowszy model od Anthropic, który oferuje doskonałe wyniki w szerokim zakresie zadań, w tym generowanie treści, rozumienie języka naturalnego i przestrzeganie instrukcji. Claude 3.7 Sonnet jest szybki, niezawodny i ekonomiczny, co sprawia, że jest idealny do zastosowań produkcyjnych."
  },
  "claude-3-7-sonnet-latest": {
    "description": "Claude 3.7 Sonnet to najnowszy, najpotężniejszy model Anthropic do obsługi wysoce złożonych zadań. Wyróżnia się doskonałą wydajnością, inteligencją, płynnością i zdolnością rozumienia."
  },
  "claude-3-haiku-20240307": {
    "description": "Claude 3 Haiku to najszybszy i najbardziej kompaktowy model Anthropic, zaprojektowany do osiągania niemal natychmiastowych odpowiedzi. Oferuje szybkie i dokładne wyniki w ukierunkowanych zadaniach."
  },
  "claude-3-opus-20240229": {
    "description": "Claude 3 Opus to najpotężniejszy model Anthropic do przetwarzania wysoce złożonych zadań. Wykazuje doskonałe osiągi w zakresie wydajności, inteligencji, płynności i zrozumienia."
  },
  "claude-3-sonnet-20240229": {
    "description": "Claude 3 Sonnet zapewnia idealną równowagę między inteligencją a szybkością dla obciążeń roboczych w przedsiębiorstwach. Oferuje maksymalną użyteczność przy niższej cenie, jest niezawodny i odpowiedni do dużych wdrożeń."
  },
  "claude-opus-4-1-20250805": {
    "description": "Claude Opus 4.1 to najnowszy i najpotężniejszy model Anthropic do obsługi wysoce złożonych zadań. Wyróżnia się doskonałą wydajnością, inteligencją, płynnością i zdolnością rozumienia."
  },
  "claude-opus-4-1-20250805-thinking": {
    "description": "Model myślenia Claude Opus 4.1, zaawansowana wersja pokazująca procesy rozumowania."
  },
  "claude-opus-4-20250514": {
    "description": "Claude Opus 4 to najpotężniejszy model Anthropic, zaprojektowany do obsługi wysoce złożonych zadań. Wyróżnia się doskonałymi osiągami, inteligencją, płynnością i zdolnością rozumienia."
  },
  "claude-sonnet-4-20250514": {
    "description": "Claude Sonnet 4 może generować niemal natychmiastowe odpowiedzi lub wydłużone, stopniowe rozważania, które użytkownik może wyraźnie obserwować."
  },
  "claude-sonnet-4-20250514-thinking": {
    "description": "Model myślenia Claude Sonnet 4 może generować niemal natychmiastowe odpowiedzi lub wydłużone, stopniowe rozważania, które użytkownik może wyraźnie obserwować."
  },
  "claude-sonnet-4-5-20250929": {
    "description": "Claude Sonnet 4.5 to jak dotąd najbardziej inteligentny model Anthropic."
  },
  "codegeex-4": {
    "description": "CodeGeeX-4 to potężny asystent programowania AI, obsługujący inteligentne pytania i odpowiedzi oraz uzupełnianie kodu w różnych językach programowania, zwiększając wydajność programistów."
  },
  "codegeex4-all-9b": {
    "description": "CodeGeeX4-ALL-9B to model generowania kodu w wielu językach, który obsługuje kompleksowe funkcje, w tym uzupełnianie i generowanie kodu, interpreter kodu, wyszukiwanie w sieci, wywołania funkcji oraz pytania i odpowiedzi na poziomie repozytoriów, obejmując różne scenariusze rozwoju oprogramowania. Jest to wiodący model generowania kodu z mniej niż 10B parametrów."
  },
  "codegemma": {
    "description": "CodeGemma to lekki model językowy, specjalizujący się w różnych zadaniach programistycznych, wspierający szybkie iteracje i integrację."
  },
  "codegemma:2b": {
    "description": "CodeGemma to lekki model językowy, specjalizujący się w różnych zadaniach programistycznych, wspierający szybkie iteracje i integrację."
  },
  "codellama": {
    "description": "Code Llama to model LLM skoncentrowany na generowaniu i dyskusji kodu, łączący wsparcie dla szerokiego zakresu języków programowania, odpowiedni do środowisk deweloperskich."
  },
  "codellama/CodeLlama-34b-Instruct-hf": {
    "description": "Code Llama to LLM skoncentrowany na generowaniu i omawianiu kodu, z szerokim wsparciem dla różnych języków programowania, odpowiedni dla środowisk deweloperskich."
  },
  "codellama:13b": {
    "description": "Code Llama to model LLM skoncentrowany na generowaniu i dyskusji kodu, łączący wsparcie dla szerokiego zakresu języków programowania, odpowiedni do środowisk deweloperskich."
  },
  "codellama:34b": {
    "description": "Code Llama to model LLM skoncentrowany na generowaniu i dyskusji kodu, łączący wsparcie dla szerokiego zakresu języków programowania, odpowiedni do środowisk deweloperskich."
  },
  "codellama:70b": {
    "description": "Code Llama to model LLM skoncentrowany na generowaniu i dyskusji kodu, łączący wsparcie dla szerokiego zakresu języków programowania, odpowiedni do środowisk deweloperskich."
  },
  "codeqwen": {
    "description": "CodeQwen1.5 to duży model językowy wytrenowany na dużej ilości danych kodowych, zaprojektowany do rozwiązywania złożonych zadań programistycznych."
  },
  "codestral": {
    "description": "Codestral to pierwszy model kodowy Mistral AI, oferujący doskonałe wsparcie dla zadań generowania kodu."
  },
  "codestral-latest": {
    "description": "Codestral to nowoczesny model generacyjny skoncentrowany na generowaniu kodu, zoptymalizowany do zadań wypełniania i uzupełniania kodu."
  },
  "codex-mini-latest": {
    "description": "codex-mini-latest to wersja dostrojona o4-mini, specjalnie zaprojektowana do Codex CLI. Do bezpośredniego użycia przez API zalecamy rozpoczęcie od gpt-4.1."
  },
  "cogview-4": {
    "description": "CogView-4 to pierwszy otwartoźródłowy model generowania obrazów tekstowych firmy Zhipu, który obsługuje generowanie znaków chińskich. Model oferuje kompleksowe ulepszenia w zakresie rozumienia semantycznego, jakości generowanych obrazów oraz zdolności generowania tekstu w języku chińskim i angielskim. Obsługuje dwujęzyczne wejście w dowolnej długości i potrafi generować obrazy o dowolnej rozdzielczości w określonym zakresie."
  },
  "cohere-command-r": {
    "description": "Command R to skalowalny model generatywny, który koncentruje się na RAG i użyciu narzędzi, aby umożliwić AI na skalę produkcyjną dla przedsiębiorstw."
  },
  "cohere-command-r-plus": {
    "description": "Command R+ to model zoptymalizowany pod kątem RAG, zaprojektowany do obsługi obciążeń roboczych na poziomie przedsiębiorstwa."
  },
  "cohere/Cohere-command-r": {
    "description": "Command R to skalowalny model generatywny zaprojektowany do zastosowań RAG i narzędziowych, umożliwiający firmom wdrożenie AI na poziomie produkcyjnym."
  },
  "cohere/Cohere-command-r-plus": {
    "description": "Command R+ to zaawansowany model zoptymalizowany pod kątem RAG, stworzony do obsługi obciążeń na poziomie przedsiębiorstwa."
  },
  "cohere/command-a": {
    "description": "Command A to najsilniejszy model Cohere, wyróżniający się w użyciu narzędzi, agentach, generowaniu wspomaganym wyszukiwaniem (RAG) i zastosowaniach wielojęzycznych. Posiada długość kontekstu 256K i działa na zaledwie dwóch GPU, oferując 150% wyższą przepustowość w porównaniu do Command R+ 08-2024."
  },
  "cohere/command-r": {
    "description": "Command R to duży model językowy zoptymalizowany pod kątem interakcji konwersacyjnych i zadań z długim kontekstem. Należy do kategorii \"skalowalnych\" modeli, łącząc wysoką wydajność z dużą dokładnością, umożliwiając firmom przejście od proof-of-concept do produkcji."
  },
  "cohere/command-r-plus": {
    "description": "Command R+ to najnowszy duży model językowy Cohere, zoptymalizowany pod kątem interakcji konwersacyjnych i zadań z długim kontekstem. Jego celem jest osiągnięcie wyjątkowej wydajności, umożliwiając firmom przejście od proof-of-concept do produkcji."
  },
  "cohere/embed-v4.0": {
    "description": "Model umożliwiający klasyfikację tekstu, obrazów lub treści mieszanych oraz konwersję na osadzenia."
  },
  "command": {
    "description": "Model konwersacyjny, który przestrzega instrukcji, oferujący wysoką jakość i niezawodność w zadaniach językowych, a także dłuższą długość kontekstu w porównaniu do naszych podstawowych modeli generacyjnych."
  },
  "command-a-03-2025": {
    "description": "Command A to nasz najsilniejszy model, który do tej pory osiągnął najlepsze wyniki, doskonale sprawdzający się w zastosowaniach narzędziowych, agentowych, generacji wzbogaconej o wyszukiwanie (RAG) oraz w kontekście wielojęzycznym. Command A ma długość kontekstu 256K, działa na zaledwie dwóch GPU i w porównaniu do Command R+ 08-2024 zwiększa wydajność o 150%."
  },
  "command-light": {
    "description": "Mniejsza i szybsza wersja Command, niemal równie potężna, ale szybsza."
  },
  "command-light-nightly": {
    "description": "Aby skrócić czas między wydaniami głównych wersji, wprowadziliśmy nocną wersję modelu Command. Dla serii command-light ta wersja nazywa się command-light-nightly. Proszę pamiętać, że command-light-nightly to najnowsza, najbardziej eksperymentalna i (możliwie) niestabilna wersja. Wersje nocne są regularnie aktualizowane i nie są zapowiadane z wyprzedzeniem, dlatego nie zaleca się ich używania w środowisku produkcyjnym."
  },
  "command-nightly": {
    "description": "Aby skrócić czas między wydaniami głównych wersji, wprowadziliśmy nocną wersję modelu Command. Dla serii Command ta wersja nazywa się command-cightly. Proszę pamiętać, że command-nightly to najnowsza, najbardziej eksperymentalna i (możliwie) niestabilna wersja. Wersje nocne są regularnie aktualizowane i nie są zapowiadane z wyprzedzeniem, dlatego nie zaleca się ich używania w środowisku produkcyjnym."
  },
  "command-r": {
    "description": "Command R to LLM zoptymalizowany do dialogów i zadań z długim kontekstem, szczególnie odpowiedni do dynamicznej interakcji i zarządzania wiedzą."
  },
  "command-r-03-2024": {
    "description": "Command R to model konwersacyjny, który przestrzega instrukcji, oferujący wyższą jakość i niezawodność w zadaniach językowych, a także dłuższą długość kontekstu w porównaniu do wcześniejszych modeli. Może być używany w złożonych przepływach pracy, takich jak generacja kodu, generacja wzbogacona o wyszukiwanie (RAG), korzystanie z narzędzi i agentów."
  },
  "command-r-08-2024": {
    "description": "command-r-08-2024 to zaktualizowana wersja modelu Command R, wydana w sierpniu 2024 roku."
  },
  "command-r-plus": {
    "description": "Command R+ to model językowy o wysokiej wydajności, zaprojektowany z myślą o rzeczywistych scenariuszach biznesowych i złożonych zastosowaniach."
  },
  "command-r-plus-04-2024": {
    "description": "Command R+ to model konwersacyjny, który przestrzega instrukcji, oferujący wyższą jakość i niezawodność w zadaniach językowych, a także dłuższą długość kontekstu w porównaniu do wcześniejszych modeli. Jest najlepiej dostosowany do złożonych przepływów pracy RAG i wieloetapowego korzystania z narzędzi."
  },
  "command-r-plus-08-2024": {
    "description": "Command R+ to model konwersacyjny przestrzegający instrukcji, oferujący wyższą jakość i niezawodność w zadaniach językowych, a także dłuższy kontekst w porównaniu do wcześniejszych modeli. Najlepiej sprawdza się w złożonych przepływach pracy RAG i wieloetapowym korzystaniu z narzędzi."
  },
  "command-r7b-12-2024": {
    "description": "command-r7b-12-2024 to mała i wydajna zaktualizowana wersja, wydana w grudniu 2024 roku. Doskonale sprawdza się w zadaniach wymagających złożonego rozumowania i wieloetapowego przetwarzania, takich jak RAG, korzystanie z narzędzi i agenci."
  },
  "computer-use-preview": {
    "description": "Model computer-use-preview to dedykowany model zaprojektowany specjalnie do „narzędzi użycia komputera”, wytrenowany do rozumienia i wykonywania zadań związanych z komputerem."
  },
  "dall-e-2": {
    "description": "Druga generacja modelu DALL·E, obsługująca bardziej realistyczne i dokładne generowanie obrazów, o rozdzielczości czterokrotnie większej niż pierwsza generacja."
  },
  "dall-e-3": {
    "description": "Najnowocześniejszy model DALL·E, wydany w listopadzie 2023 roku. Obsługuje bardziej realistyczne i dokładne generowanie obrazów, z lepszą zdolnością do oddawania szczegółów."
  },
  "databricks/dbrx-instruct": {
    "description": "DBRX Instruct oferuje wysoką niezawodność w przetwarzaniu poleceń, wspierając różne branże."
  },
  "deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 to model wnioskowania napędzany uczeniem przez wzmacnianie (RL), który rozwiązuje problemy z powtarzalnością i czytelnością modelu. Przed RL, DeepSeek-R1 wprowadził dane z zimnego startu, co dodatkowo zoptymalizowało wydajność wnioskowania. W zadaniach matematycznych, kodowania i wnioskowania osiąga wyniki porównywalne z OpenAI-o1, a dzięki starannie zaprojektowanym metodom treningowym poprawia ogólne efekty."
  },
  "deepseek-ai/DeepSeek-R1-0528": {
    "description": "DeepSeek R1 znacząco zwiększa głębokość zdolności wnioskowania i dedukcji dzięki zwiększonym zasobom obliczeniowym oraz wprowadzeniu mechanizmów optymalizacji algorytmów w trakcie dalszego treningu. Model osiąga doskonałe wyniki w różnych benchmarkach, w tym w matematyce, programowaniu i logice ogólnej. Jego ogólna wydajność jest obecnie zbliżona do czołowych modeli, takich jak O3 i Gemini 2.5 Pro."
  },
  "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": {
    "description": "DeepSeek-R1-0528-Qwen3-8B to model uzyskany przez destylację łańcuchów myślowych z modelu DeepSeek-R1-0528 do Qwen3 8B Base. Model osiąga najnowocześniejszą (SOTA) wydajność wśród modeli open source, przewyższając Qwen3 8B o 10% w teście AIME 2024 i osiągając poziom wydajności Qwen3-235B-thinking. Wykazuje doskonałe wyniki w matematycznym wnioskowaniu, programowaniu i logice ogólnej, posiadając architekturę identyczną z Qwen3-8B, ale korzystając z tokenizera DeepSeek-R1-0528."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
    "description": "Model destylacyjny DeepSeek-R1, optymalizujący wydajność wnioskowania dzięki uczeniu przez wzmocnienie i danym z zimnego startu, otwarty model ustanawiający nowe standardy w wielu zadaniach."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Model destylacyjny DeepSeek-R1, optymalizujący wydajność wnioskowania dzięki uczeniu przez wzmocnienie i danym z zimnego startu, otwarty model ustanawiający nowe standardy w wielu zadaniach."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Model destylacyjny DeepSeek-R1, optymalizujący wydajność wnioskowania dzięki uczeniu przez wzmocnienie i danym z zimnego startu, otwarty model ustanawiający nowe standardy w wielu zadaniach."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
    "description": "DeepSeek-R1-Distill-Qwen-32B to model uzyskany przez destylację Qwen2.5-32B. Model ten został dostosowany przy użyciu 800 000 starannie wybranych próbek wygenerowanych przez DeepSeek-R1, wykazując doskonałe osiągi w wielu dziedzinach, takich jak matematyka, programowanie i wnioskowanie. Osiągnął znakomite wyniki w wielu testach referencyjnych, w tym 94,3% dokładności w MATH-500, co pokazuje jego silne zdolności wnioskowania matematycznego."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B to model uzyskany przez destylację Qwen2.5-Math-7B. Model ten został dostosowany przy użyciu 800 000 starannie wybranych próbek wygenerowanych przez DeepSeek-R1, wykazując doskonałe zdolności wnioskowania. Osiągnął znakomite wyniki w wielu testach referencyjnych, w tym 92,8% dokładności w MATH-500, 55,5% wskaźnika zdawalności w AIME 2024 oraz 1189 punktów w CodeForces, demonstrując silne zdolności matematyczne i programistyczne jako model o skali 7B."
  },
  "deepseek-ai/DeepSeek-V2.5": {
    "description": "DeepSeek V2.5 łączy doskonałe cechy wcześniejszych wersji, wzmacniając zdolności ogólne i kodowania."
  },
  "deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 to model językowy z 6710 miliardami parametrów, oparty na mieszanych ekspertach (MoE), wykorzystujący wielogłowicową potencjalną uwagę (MLA) oraz architekturę DeepSeekMoE, łączącą strategię równoważenia obciążenia bez dodatkowych strat, co optymalizuje wydajność wnioskowania i treningu. Dzięki wstępnemu treningowi na 14,8 bilionach wysokiej jakości tokenów oraz nadzorowanemu dostrajaniu i uczeniu przez wzmacnianie, DeepSeek-V3 przewyższa inne modele open source, zbliżając się do wiodących modeli zamkniętych."
  },
  "deepseek-ai/DeepSeek-V3.1": {
    "description": "DeepSeek-V3.1 to hybrydowy duży model językowy wydany przez DeepSeek AI, który wprowadza wiele istotnych ulepszeń w stosunku do poprzednich wersji. Jedną z innowacji jest integracja trybu myślenia (Thinking Mode) i trybu bezmyślnego (Non-thinking Mode), które użytkownik może elastycznie przełączać, dostosowując szablony rozmów do różnych zadań. Dzięki specjalnej optymalizacji po treningu, wersja V3.1 znacznie poprawiła wydajność w wywoływaniu narzędzi i zadaniach agenta, lepiej wspierając zewnętrzne narzędzia wyszukiwania oraz realizację wieloetapowych, złożonych zadań. Model bazuje na DeepSeek-V3.1-Base i został poddany dalszemu treningowi z zastosowaniem dwufazowej metody rozszerzania długich tekstów, co znacznie zwiększyło ilość danych treningowych i poprawiło działanie na długich dokumentach oraz rozbudowanym kodzie. Jako model open source, DeepSeek-V3.1 wykazuje zdolności porównywalne z najlepszymi zamkniętymi modelami w benchmarkach kodowania, matematyki i wnioskowania, a dzięki architekturze hybrydowych ekspertów (MoE) utrzymuje ogromną pojemność modelu przy jednoczesnym efektywnym obniżeniu kosztów wnioskowania."
  },
  "deepseek-ai/DeepSeek-V3.1-Terminus": {
    "description": "DeepSeek-V3.1-Terminus to zaktualizowana wersja modelu V3.1 wydanego przez DeepSeek, zaprojektowana jako hybrydowy model językowy z agentami. Aktualizacja skupia się na naprawie zgłoszonych przez użytkowników problemów i poprawie stabilności, zachowując jednocześnie dotychczasowe możliwości modelu. Znacząco poprawiono spójność językową, zmniejszając mieszanie języka chińskiego i angielskiego oraz eliminując nieprawidłowe znaki. Model integruje tryb myślenia (Thinking Mode) oraz tryb bez myślenia (Non-thinking Mode), które użytkownicy mogą elastycznie przełączać za pomocą szablonów czatu, dostosowując się do różnych zadań. Ważną optymalizacją jest wzmocnienie wydajności agenta kodu (Code Agent) i agenta wyszukiwania (Search Agent), co czyni je bardziej niezawodnymi w wywoływaniu narzędzi i realizacji wieloetapowych, złożonych zadań."
  },
  "deepseek-ai/deepseek-llm-67b-chat": {
    "description": "DeepSeek 67B to zaawansowany model przeszkolony do złożonych dialogów."
  },
  "deepseek-ai/deepseek-r1": {
    "description": "Najnowocześniejszy, wydajny LLM, specjalizujący się w wnioskowaniu, matematyce i programowaniu."
  },
  "deepseek-ai/deepseek-v3.1": {
    "description": "DeepSeek V3.1: kolejna generacja modelu inferencyjnego, poprawiająca zdolności do złożonego wnioskowania i łańcuchowego myślenia, odpowiednia do zadań wymagających dogłębnej analizy."
  },
  "deepseek-ai/deepseek-vl2": {
    "description": "DeepSeek-VL2 to model wizualno-językowy oparty na DeepSeekMoE-27B, wykorzystujący architekturę MoE z rzadką aktywacją, osiągający doskonałe wyniki przy aktywacji jedynie 4,5 miliarda parametrów. Model ten wyróżnia się w wielu zadaniach, takich jak wizualne pytania i odpowiedzi, optyczne rozpoznawanie znaków, zrozumienie dokumentów/tabel/wykresów oraz lokalizacja wizualna."
  },
  "deepseek-chat": {
    "description": "Nowy otwarty model łączący zdolności ogólne i kodowe, który nie tylko zachowuje ogólne zdolności dialogowe oryginalnego modelu czatu i potężne zdolności przetwarzania kodu modelu Coder, ale także lepiej dostosowuje się do ludzkich preferencji. Ponadto, DeepSeek-V2.5 osiągnął znaczne poprawy w zadaniach pisarskich, przestrzeganiu instrukcji i innych obszarach."
  },
  "deepseek-coder-33B-instruct": {
    "description": "DeepSeek Coder 33B to model języka kodu, wytrenowany na 20 bilionach danych, z czego 87% to kod, a 13% to języki chiński i angielski. Model wprowadza okno o rozmiarze 16K oraz zadania uzupełniania, oferując funkcje uzupełniania kodu na poziomie projektu i wypełniania fragmentów."
  },
  "deepseek-coder-v2": {
    "description": "DeepSeek Coder V2 to otwarty model kodowy Mixture-of-Experts, który doskonale radzi sobie z zadaniami kodowymi, porównywalny z GPT4-Turbo."
  },
  "deepseek-coder-v2:236b": {
    "description": "DeepSeek Coder V2 to otwarty model kodowy Mixture-of-Experts, który doskonale radzi sobie z zadaniami kodowymi, porównywalny z GPT4-Turbo."
  },
  "deepseek-r1": {
    "description": "DeepSeek-R1 to model wnioskowania napędzany uczeniem przez wzmacnianie (RL), który rozwiązuje problemy z powtarzalnością i czytelnością modelu. Przed RL, DeepSeek-R1 wprowadził dane z zimnego startu, co dodatkowo zoptymalizowało wydajność wnioskowania. W zadaniach matematycznych, kodowania i wnioskowania osiąga wyniki porównywalne z OpenAI-o1, a dzięki starannie zaprojektowanym metodom treningowym poprawia ogólne efekty."
  },
  "deepseek-r1-0528": {
    "description": "Model w pełnej wersji 685B, wydany 28 maja 2025 roku. DeepSeek-R1 wykorzystuje techniki uczenia ze wzmocnieniem na dużą skalę w fazie post-treningowej, co znacznie poprawia zdolności wnioskowania modelu przy minimalnej ilości oznaczonych danych. Wysoka wydajność i zdolności w zadaniach matematycznych, kodowaniu oraz rozumowaniu języka naturalnego."
  },
  "deepseek-r1-70b-fast-online": {
    "description": "DeepSeek R1 70B szybka wersja, wspierająca wyszukiwanie w czasie rzeczywistym, oferująca szybszy czas reakcji przy zachowaniu wydajności modelu."
  },
  "deepseek-r1-70b-online": {
    "description": "DeepSeek R1 70B standardowa wersja, wspierająca wyszukiwanie w czasie rzeczywistym, odpowiednia do zadań konwersacyjnych i przetwarzania tekstu wymagających najnowszych informacji."
  },
  "deepseek-r1-distill-llama": {
    "description": "deepseek-r1-distill-llama to model stworzony na podstawie Llamy, uzyskany przez destylację z DeepSeek-R1."
  },
  "deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 — większy i inteligentniejszy model w zestawie DeepSeek — został destylowany do architektury Llama 70B. Na podstawie testów referencyjnych i ocen ręcznych, model ten jest bardziej inteligentny niż oryginalna Llama 70B, szczególnie w zadaniach wymagających precyzji matematycznej i faktograficznej."
  },
  "deepseek-r1-distill-llama-8b": {
    "description": "Modele z serii DeepSeek-R1-Distill są dostosowywane do modeli open source, takich jak Qwen i Llama, poprzez technologię destylacji wiedzy, na podstawie próbek generowanych przez DeepSeek-R1."
  },
  "deepseek-r1-distill-qianfan-llama-70b": {
    "description": "Pierwsze wydanie 14 lutego 2025 roku, wyodrębnione przez zespół badawczy Qianfan z modelu bazowego Llama3_70B (zbudowanego z Meta Llama), w którym dodano również korpus Qianfan."
  },
  "deepseek-r1-distill-qianfan-llama-8b": {
    "description": "Pierwsze wydanie 14 lutego 2025 roku, wyodrębnione przez zespół badawczy Qianfan z modelu bazowego Llama3_8B (zbudowanego z Meta Llama), w którym dodano również korpus Qianfan."
  },
  "deepseek-r1-distill-qwen": {
    "description": "deepseek-r1-distill-qwen to model stworzony na podstawie Qwen poprzez destylację z DeepSeek-R1."
  },
  "deepseek-r1-distill-qwen-1.5b": {
    "description": "Modele z serii DeepSeek-R1-Distill są dostosowywane do modeli open source, takich jak Qwen i Llama, poprzez technologię destylacji wiedzy, na podstawie próbek generowanych przez DeepSeek-R1."
  },
  "deepseek-r1-distill-qwen-14b": {
    "description": "Modele z serii DeepSeek-R1-Distill są dostosowywane do modeli open source, takich jak Qwen i Llama, poprzez technologię destylacji wiedzy, na podstawie próbek generowanych przez DeepSeek-R1."
  },
  "deepseek-r1-distill-qwen-32b": {
    "description": "Modele z serii DeepSeek-R1-Distill są dostosowywane do modeli open source, takich jak Qwen i Llama, poprzez technologię destylacji wiedzy, na podstawie próbek generowanych przez DeepSeek-R1."
  },
  "deepseek-r1-distill-qwen-7b": {
    "description": "Modele z serii DeepSeek-R1-Distill są dostosowywane do modeli open source, takich jak Qwen i Llama, poprzez technologię destylacji wiedzy, na podstawie próbek generowanych przez DeepSeek-R1."
  },
  "deepseek-r1-fast-online": {
    "description": "DeepSeek R1 pełna szybka wersja, wspierająca wyszukiwanie w czasie rzeczywistym, łącząca potężne możliwości 671 miliardów parametrów z szybszym czasem reakcji."
  },
  "deepseek-r1-online": {
    "description": "DeepSeek R1 pełna wersja, z 671 miliardami parametrów, wspierająca wyszukiwanie w czasie rzeczywistym, z potężniejszymi zdolnościami rozumienia i generowania."
  },
  "deepseek-reasoner": {
    "description": "Tryb myślenia DeepSeek V3.2. Przed wygenerowaniem ostatecznej odpowiedzi model najpierw przedstawia łańcuch rozumowania, co zwiększa dokładność końcowej odpowiedzi."
  },
  "deepseek-v2": {
    "description": "DeepSeek V2 to wydajny model językowy Mixture-of-Experts, odpowiedni do ekonomicznych potrzeb przetwarzania."
  },
  "deepseek-v2:236b": {
    "description": "DeepSeek V2 236B to model kodowy zaprojektowany przez DeepSeek, oferujący potężne możliwości generowania kodu."
  },
  "deepseek-v3": {
    "description": "DeepSeek-V3 to model MoE opracowany przez Hangzhou DeepSeek AI Technology Research Co., Ltd., który osiągnął znakomite wyniki w wielu testach, zajmując pierwsze miejsce wśród modeli open-source na głównych listach. W porównaniu do modelu V2.5, prędkość generowania wzrosła trzykrotnie, co zapewnia użytkownikom szybsze i płynniejsze doświadczenia."
  },
  "deepseek-v3-0324": {
    "description": "DeepSeek-V3-0324 to model MoE z 671 miliardami parametrów, który wyróżnia się w zakresie programowania i umiejętności technicznych, rozumienia kontekstu oraz przetwarzania długich tekstów."
  },
  "deepseek-v3.1": {
    "description": "DeepSeek-V3.1 to nowy hybrydowy model wnioskowania opracowany przez DeepSeek, obsługujący dwa tryby wnioskowania: myślenia i bezmyślny, z wyższą efektywnością myślenia niż DeepSeek-R1-0528. Dzięki optymalizacji po treningu, wykorzystanie narzędzi agenta i wydajność zadań inteligentnych agentów zostały znacznie poprawione. Obsługuje okno kontekstowe do 128k oraz maksymalną długość wyjścia do 64k tokenów."
  },
  "deepseek-v3.1:671b": {
    "description": "DeepSeek V3.1: kolejna generacja modelu inferencyjnego, poprawiająca zdolności do złożonego wnioskowania i łańcuchowego myślenia, odpowiednia do zadań wymagających dogłębnej analizy."
  },
  "deepseek-v3.2-exp": {
    "description": "deepseek-v3.2-exp wprowadza mechanizm rzadkiej uwagi, mający na celu poprawę efektywności treningu i wnioskowania podczas przetwarzania długich tekstów, przy cenie niższej niż deepseek-v3.1."
  },
  "deepseek/deepseek-chat-v3-0324": {
    "description": "DeepSeek V3 to model mieszany z 685B parametrami, będący najnowszą iteracją flagowej serii modeli czatu zespołu DeepSeek.\n\nDziedziczy po modelu [DeepSeek V3](/deepseek/deepseek-chat-v3) i wykazuje doskonałe wyniki w różnych zadaniach."
  },
  "deepseek/deepseek-chat-v3-0324:free": {
    "description": "DeepSeek V3 to model mieszany z 685B parametrami, będący najnowszą iteracją flagowej serii modeli czatu zespołu DeepSeek.\n\nDziedziczy po modelu [DeepSeek V3](/deepseek/deepseek-chat-v3) i wykazuje doskonałe wyniki w różnych zadaniach."
  },
  "deepseek/deepseek-chat-v3.1": {
    "description": "DeepSeek-V3.1 to duży hybrydowy model wnioskowania obsługujący długi kontekst 128K i efektywne przełączanie trybów, osiągający doskonałą wydajność i szybkość w wywoływaniu narzędzi, generowaniu kodu oraz złożonych zadaniach wnioskowania."
  },
  "deepseek/deepseek-r1": {
    "description": "Model DeepSeek R1 przeszedł drobną aktualizację do wersji DeepSeek-R1-0528. W najnowszej aktualizacji DeepSeek R1 znacznie poprawił głębokość i zdolności wnioskowania dzięki zwiększonym zasobom obliczeniowym i wprowadzeniu optymalizacji algorytmicznych po treningu. Model osiąga znakomite wyniki w benchmarkach matematycznych, programistycznych i ogólnej logiki, zbliżając się do czołowych modeli, takich jak O3 i Gemini 2.5 Pro."
  },
  "deepseek/deepseek-r1-0528": {
    "description": "DeepSeek-R1 znacząco poprawia zdolność wnioskowania modelu nawet przy minimalnej ilości oznaczonych danych. Przed wygenerowaniem ostatecznej odpowiedzi model najpierw generuje łańcuch myślowy, co zwiększa dokładność końcowej odpowiedzi."
  },
  "deepseek/deepseek-r1-0528:free": {
    "description": "DeepSeek-R1 znacząco poprawia zdolność wnioskowania modelu nawet przy minimalnej ilości oznaczonych danych. Przed wygenerowaniem ostatecznej odpowiedzi model najpierw generuje łańcuch myślowy, co zwiększa dokładność końcowej odpowiedzi."
  },
  "deepseek/deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek-R1-Distill-Llama-70B to zdystylowana, bardziej wydajna wersja modelu Llama 70B. Utrzymuje silną wydajność w zadaniach generowania tekstu, zmniejszając koszty obliczeniowe dla łatwiejszego wdrożenia i badań. Obsługiwany przez Groq na ich niestandardowym sprzęcie LPU, zapewnia szybkie i efektywne wnioskowanie."
  },
  "deepseek/deepseek-r1-distill-llama-8b": {
    "description": "DeepSeek R1 Distill Llama 8B to destylowany duży model językowy oparty na Llama-3.1-8B-Instruct, wytrenowany przy użyciu wyjścia DeepSeek R1."
  },
  "deepseek/deepseek-r1-distill-qwen-14b": {
    "description": "DeepSeek R1 Distill Qwen 14B to destylowany duży model językowy oparty na Qwen 2.5 14B, wytrenowany przy użyciu wyjścia DeepSeek R1. Model ten przewyższył OpenAI o1-mini w wielu testach benchmarkowych, osiągając najnowsze osiągnięcia technologiczne w dziedzinie modeli gęstych (dense models). Oto niektóre wyniki testów benchmarkowych:\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nModel ten, dostrojony na podstawie wyjścia DeepSeek R1, wykazuje konkurencyjną wydajność porównywalną z większymi modelami na czołowej pozycji."
  },
  "deepseek/deepseek-r1-distill-qwen-32b": {
    "description": "DeepSeek R1 Distill Qwen 32B to destylowany duży model językowy oparty na Qwen 2.5 32B, wytrenowany przy użyciu wyjścia DeepSeek R1. Model ten przewyższył OpenAI o1-mini w wielu testach benchmarkowych, osiągając najnowsze osiągnięcia technologiczne w dziedzinie modeli gęstych (dense models). Oto niektóre wyniki testów benchmarkowych:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nModel ten, dostrojony na podstawie wyjścia DeepSeek R1, wykazuje konkurencyjną wydajność porównywalną z większymi modelami na czołowej pozycji."
  },
  "deepseek/deepseek-r1/community": {
    "description": "DeepSeek R1 to najnowszy model open source wydany przez zespół DeepSeek, który charakteryzuje się bardzo silnymi możliwościami wnioskowania, szczególnie w zadaniach matematycznych, programistycznych i logicznych, osiągając poziom porównywalny z modelem o1 OpenAI."
  },
  "deepseek/deepseek-r1:free": {
    "description": "DeepSeek-R1 znacznie poprawił zdolności wnioskowania modelu przy minimalnej ilości oznaczonych danych. Przed wygenerowaniem ostatecznej odpowiedzi, model najpierw wygeneruje fragment myślenia, aby zwiększyć dokładność końcowej odpowiedzi."
  },
  "deepseek/deepseek-v3": {
    "description": "Szybki, uniwersalny duży model językowy z ulepszonymi zdolnościami wnioskowania."
  },
  "deepseek/deepseek-v3.1-base": {
    "description": "DeepSeek V3.1 Base to ulepszona wersja modelu DeepSeek V3."
  },
  "deepseek/deepseek-v3/community": {
    "description": "DeepSeek-V3 osiągnął znaczący przełom w szybkości wnioskowania w porównaniu do wcześniejszych modeli. Zajmuje pierwsze miejsce wśród modeli open source i może konkurować z najnowocześniejszymi modelami zamkniętymi na świecie. DeepSeek-V3 wykorzystuje architekturę wielogłowicowej uwagi (MLA) oraz DeepSeekMoE, które zostały w pełni zweryfikowane w DeepSeek-V2. Ponadto, DeepSeek-V3 wprowadza pomocniczą strategię bezstratną do równoważenia obciążenia oraz ustala cele treningowe dla wieloetykietowego przewidywania, aby uzyskać lepszą wydajność."
  },
  "deepseek_r1": {
    "description": "DeepSeek-R1 to model wnioskowania napędzany uczeniem przez wzmocnienie (RL), który rozwiązuje problemy z powtarzalnością i czytelnością modelu. Przed RL, DeepSeek-R1 wprowadził dane z zimnego startu, co dodatkowo zoptymalizowało wydajność wnioskowania. W zadaniach matematycznych, kodowania i wnioskowania osiąga wyniki porównywalne z OpenAI-o1, a dzięki starannie zaprojektowanym metodom szkoleniowym poprawia ogólne efekty."
  },
  "deepseek_r1_distill_llama_70b": {
    "description": "DeepSeek-R1-Distill-Llama-70B to model uzyskany poprzez destylację treningową z Llama-3.3-70B-Instruct. Model ten jest częścią serii DeepSeek-R1, a dzięki użyciu próbek wygenerowanych przez DeepSeek-R1, wykazuje doskonałe wyniki w matematyce, programowaniu i wnioskowaniu."
  },
  "deepseek_r1_distill_qwen_14b": {
    "description": "DeepSeek-R1-Distill-Qwen-14B to model uzyskany poprzez destylację wiedzy z Qwen2.5-14B. Model ten został dostrojony przy użyciu 800 000 starannie wybranych próbek wygenerowanych przez DeepSeek-R1, wykazując doskonałe zdolności wnioskowania."
  },
  "deepseek_r1_distill_qwen_32b": {
    "description": "DeepSeek-R1-Distill-Qwen-32B to model uzyskany poprzez destylację wiedzy z Qwen2.5-32B. Model ten został dostrojony przy użyciu 800 000 starannie wybranych próbek wygenerowanych przez DeepSeek-R1, wykazując doskonałe wyniki w wielu dziedzinach, w tym matematyce, programowaniu i wnioskowaniu."
  },
  "doubao-1.5-lite-32k": {
    "description": "Doubao-1.5-lite to nowa generacja modelu o lekkiej konstrukcji, charakteryzująca się ekstremalną szybkością reakcji, osiągając światowy poziom zarówno w zakresie wydajności, jak i opóźnienia."
  },
  "doubao-1.5-pro-256k": {
    "description": "Doubao-1.5-pro-256k to kompleksowa wersja ulepszona na bazie Doubao-1.5-Pro, która oferuje znaczny wzrost wydajności o 10%. Obsługuje wnioskowanie w kontekście 256k, a maksymalna długość wyjścia wynosi 12k tokenów. Wyższa wydajność, większe okno, doskonały stosunek jakości do ceny, odpowiedni do szerszego zakresu zastosowań."
  },
  "doubao-1.5-pro-32k": {
    "description": "Doubao-1.5-pro to nowa generacja głównego modelu, który oferuje kompleksowe ulepszenia wydajności, wykazując doskonałe wyniki w zakresie wiedzy, kodowania, wnioskowania i innych obszarów."
  },
  "doubao-1.5-thinking-pro": {
    "description": "Model głębokiego myślenia Doubao-1.5, nowa generacja, wyróżnia się w dziedzinach takich jak matematyka, programowanie, rozumowanie naukowe oraz w zadaniach ogólnych, takich jak twórcze pisanie. Osiąga lub zbliża się do poziomu czołowych graczy w branży w wielu uznawanych benchmarkach, takich jak AIME 2024, Codeforces, GPQA. Obsługuje okno kontekstowe o wielkości 128k oraz 16k wyjścia."
  },
  "doubao-1.5-thinking-pro-m": {
    "description": "Doubao-1.5 to nowy model głębokiego myślenia (wersja m z natywną wielomodalną zdolnością głębokiego wnioskowania), wyróżniający się w dziedzinach takich jak matematyka, programowanie, rozumowanie naukowe oraz twórcze pisanie. Osiąga lub zbliża się do czołówki branży na wielu prestiżowych benchmarkach, takich jak AIME 2024, Codeforces, GPQA. Obsługuje kontekst do 128k i wyjście do 16k."
  },
  "doubao-1.5-thinking-vision-pro": {
    "description": "Nowy model głębokiego myślenia wizualnego, oferujący zaawansowane zdolności uniwersalnego wielomodalnego rozumienia i wnioskowania, osiągając SOTA w 37 z 59 publicznych benchmarków."
  },
  "doubao-1.5-ui-tars": {
    "description": "Doubao-1.5-UI-TARS to natywny model agenta zaprojektowany do interakcji z graficznym interfejsem użytkownika (GUI). Dzięki zdolnościom percepcji, wnioskowania i działania na poziomie ludzkim umożliwia płynną interakcję z GUI."
  },
  "doubao-1.5-vision-lite": {
    "description": "Doubao-1.5-vision-lite to nowo zaktualizowany model multimodalny, który obsługuje rozpoznawanie obrazów o dowolnej rozdzielczości i ekstremalnych proporcjach, wzmacniając zdolności wnioskowania wizualnego, rozpoznawania dokumentów, rozumienia szczegółowych informacji i przestrzegania instrukcji. Obsługuje okno kontekstowe 128k, maksymalna długość wyjścia to 16k tokenów."
  },
  "doubao-1.5-vision-pro": {
    "description": "Doubao-1.5-vision-pro to nowo ulepszony wielomodalny model dużej skali, obsługujący rozpoznawanie obrazów o dowolnej rozdzielczości i ekstremalnych proporcjach, wzmacniający zdolności wizualnego wnioskowania, rozpoznawania dokumentów, rozumienia szczegółów i przestrzegania instrukcji."
  },
  "doubao-1.5-vision-pro-32k": {
    "description": "Doubao-1.5-vision-pro to nowo ulepszony wielomodalny model dużej skali, obsługujący rozpoznawanie obrazów o dowolnej rozdzielczości i ekstremalnych proporcjach, wzmacniający zdolności wizualnego wnioskowania, rozpoznawania dokumentów, rozumienia szczegółów i przestrzegania instrukcji."
  },
  "doubao-lite-128k": {
    "description": "Oferuje niezwykle szybkie reakcje i lepszy stosunek jakości do ceny, zapewniając klientom elastyczne opcje dla różnych scenariuszy. Obsługuje wnioskowanie i dostrajanie z kontekstem do 128k."
  },
  "doubao-lite-32k": {
    "description": "Oferuje niezwykle szybkie reakcje i lepszy stosunek jakości do ceny, zapewniając klientom elastyczne opcje dla różnych scenariuszy. Obsługuje wnioskowanie i dostrajanie z kontekstem do 32k."
  },
  "doubao-lite-4k": {
    "description": "Oferuje niezwykle szybkie reakcje i lepszy stosunek jakości do ceny, zapewniając klientom elastyczne opcje dla różnych scenariuszy. Obsługuje wnioskowanie i dostrajanie z kontekstem do 4k."
  },
  "doubao-pro-256k": {
    "description": "Najlepszy model główny, odpowiedni do złożonych zadań, osiągający doskonałe wyniki w scenariuszach takich jak pytania i odpowiedzi, streszczenia, twórczość, klasyfikacja tekstu i odgrywanie ról. Obsługuje wnioskowanie i dostrajanie z kontekstem do 256k."
  },
  "doubao-pro-32k": {
    "description": "Najlepszy model główny, odpowiedni do złożonych zadań, osiągający doskonałe wyniki w scenariuszach takich jak pytania i odpowiedzi, streszczenia, twórczość, klasyfikacja tekstu i odgrywanie ról. Obsługuje wnioskowanie i dostrajanie z kontekstem do 32k."
  },
  "doubao-seed-1.6": {
    "description": "Doubao-Seed-1.6 to nowy, wielomodalny model głębokiego myślenia, obsługujący trzy tryby myślenia: auto, thinking i non-thinking. W trybie non-thinking model osiąga znacznie lepsze wyniki w porównaniu do Doubao-1.5-pro/250115. Obsługuje kontekst do 256k oraz maksymalną długość wyjścia do 16k tokenów."
  },
  "doubao-seed-1.6-flash": {
    "description": "Doubao-Seed-1.6-flash to ultraszybki model wielomodalnego głębokiego myślenia, z czasem TPOT zaledwie 10 ms; obsługuje zarówno rozumienie tekstu, jak i obrazu, z lepszymi zdolnościami tekstowymi niż poprzednia generacja lite oraz wizualnymi porównywalnymi do modeli pro konkurencji. Obsługuje kontekst do 256k oraz maksymalną długość wyjścia do 16k tokenów."
  },
  "doubao-seed-1.6-thinking": {
    "description": "Model Doubao-Seed-1.6-thinking ma znacznie wzmocnione zdolności myślenia, w porównaniu do Doubao-1.5-thinking-pro osiąga dalsze ulepszenia w podstawowych umiejętnościach takich jak kodowanie, matematyka i rozumowanie logiczne, wspiera również rozumienie wizualne. Obsługuje kontekst do 256k oraz maksymalną długość wyjścia do 16k tokenów."
  },
  "doubao-seed-1.6-vision": {
    "description": "Doubao-Seed-1.6-vision to wizualny model głębokiego myślenia, który wykazuje silniejsze zdolności ogólnego rozumienia multimodalnego i wnioskowania w scenariuszach edukacyjnych, przeglądu obrazów, inspekcji i bezpieczeństwa oraz AI w wyszukiwaniu i odpowiadaniu na pytania. Obsługuje okno kontekstowe do 256k oraz maksymalną długość wyjścia do 64k tokenów."
  },
  "doubao-seededit-3-0-i2i-250628": {
    "description": "Model generowania obrazów Doubao opracowany przez zespół Seed ByteDance, obsługuje wejścia tekstowe i obrazowe, oferując wysoką kontrolę i jakość generowanych obrazów. Umożliwia edycję obrazów za pomocą poleceń tekstowych, generując obrazy o rozmiarach od 512 do 1536 pikseli."
  },
  "doubao-seedream-3-0-t2i-250415": {
    "description": "Model generowania obrazów Seedream 3.0 opracowany przez zespół Seed ByteDance, obsługuje wejścia tekstowe i obrazowe, oferując wysoką kontrolę i jakość generowanych obrazów. Generuje obrazy na podstawie tekstowych wskazówek."
  },
  "doubao-seedream-4-0-250828": {
    "description": "Model generowania obrazów Seedream 4.0 opracowany przez zespół Seed ByteDance, obsługuje wejścia tekstowe i obrazowe, oferując wysoką kontrolę i jakość generowanych obrazów. Generuje obrazy na podstawie tekstowych wskazówek."
  },
  "doubao-vision-lite-32k": {
    "description": "Model Doubao-vision to wielomodalny model dużej skali opracowany przez Doubao, oferujący potężne zdolności rozumienia i wnioskowania obrazów oraz precyzyjne rozumienie poleceń. Model wykazuje silne wyniki w ekstrakcji informacji z obrazów i tekstu oraz w zadaniach wnioskowania opartych na obrazach, umożliwiając zastosowanie w bardziej złożonych i szerokich zadaniach wizualnych pytań i odpowiedzi."
  },
  "doubao-vision-pro-32k": {
    "description": "Model Doubao-vision to wielomodalny model dużej skali opracowany przez Doubao, oferujący potężne zdolności rozumienia i wnioskowania obrazów oraz precyzyjne rozumienie poleceń. Model wykazuje silne wyniki w ekstrakcji informacji z obrazów i tekstu oraz w zadaniach wnioskowania opartych na obrazach, umożliwiając zastosowanie w bardziej złożonych i szerokich zadaniach wizualnych pytań i odpowiedzi."
  },
  "emohaa": {
    "description": "Emohaa to model psychologiczny, posiadający profesjonalne umiejętności doradcze, pomagający użytkownikom zrozumieć problemy emocjonalne."
  },
  "ernie-3.5-128k": {
    "description": "Flagowy model językowy opracowany przez Baidu, obejmujący ogromne zbiory danych w języku chińskim i angielskim, charakteryzujący się silnymi zdolnościami ogólnymi, spełniającym wymagania większości zastosowań w dialogach, generowaniu treści i aplikacjach wtyczek; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji."
  },
  "ernie-3.5-8k": {
    "description": "Flagowy model językowy opracowany przez Baidu, obejmujący ogromne zbiory danych w języku chińskim i angielskim, charakteryzujący się silnymi zdolnościami ogólnymi, spełniającym wymagania większości zastosowań w dialogach, generowaniu treści i aplikacjach wtyczek; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji."
  },
  "ernie-3.5-8k-preview": {
    "description": "Flagowy model językowy opracowany przez Baidu, obejmujący ogromne zbiory danych w języku chińskim i angielskim, charakteryzujący się silnymi zdolnościami ogólnymi, spełniającym wymagania większości zastosowań w dialogach, generowaniu treści i aplikacjach wtyczek; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji."
  },
  "ernie-4.0-8k-latest": {
    "description": "Flagowy model językowy Baidu o ultra dużej skali, w porównaniu do ERNIE 3.5, oferujący kompleksową aktualizację zdolności modelu, szeroko stosowany w złożonych zadaniach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji."
  },
  "ernie-4.0-8k-preview": {
    "description": "Flagowy model językowy Baidu o ultra dużej skali, w porównaniu do ERNIE 3.5, oferujący kompleksową aktualizację zdolności modelu, szeroko stosowany w złożonych zadaniach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji."
  },
  "ernie-4.0-turbo-128k": {
    "description": "Flagowy model językowy Baidu o ultra dużej skali, charakteryzujący się doskonałymi wynikami ogólnymi, szeroko stosowany w złożonych zadaniach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji. W porównaniu do ERNIE 4.0, oferuje lepsze wyniki wydajności."
  },
  "ernie-4.0-turbo-8k-latest": {
    "description": "Flagowy model językowy Baidu o ultra dużej skali, charakteryzujący się doskonałymi wynikami ogólnymi, szeroko stosowany w złożonych zadaniach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji. W porównaniu do ERNIE 4.0, oferuje lepsze wyniki wydajności."
  },
  "ernie-4.0-turbo-8k-preview": {
    "description": "Flagowy model językowy Baidu o ultra dużej skali, charakteryzujący się doskonałymi wynikami ogólnymi, szeroko stosowany w złożonych zadaniach w różnych dziedzinach; wspiera automatyczne połączenie z wtyczką wyszukiwania Baidu, zapewniając aktualność informacji. W porównaniu do ERNIE 4.0, oferuje lepsze wyniki wydajności."
  },
  "ernie-4.5-8k-preview": {
    "description": "Model ERNIE 4.5 to nowa generacja natywnego modelu wielomodalnego opracowanego przez Baidu, który osiąga doskonałe wyniki w zakresie zrozumienia wielomodalnego dzięki wspólnemu modelowaniu wielu modalności; posiada bardziej zaawansowane zdolności językowe, a także znacznie poprawione zdolności rozumienia, generowania, logicznego myślenia i pamięci, eliminując halucynacje oraz poprawiając zdolności wnioskowania logicznego i kodowania."
  },
  "ernie-4.5-turbo-128k": {
    "description": "Wenxin 4.5 Turbo ma wyraźnie poprawione zdolności w zakresie eliminacji halucynacji, rozumowania logicznego i programowania. W porównaniu do Wenxin 4.5, jest szybszy i tańszy. Zdolności modelu zostały znacznie zwiększone, lepiej spełniając wymagania dotyczące przetwarzania długich rozmów z wieloma rundami oraz zadań związanych z rozumieniem długich dokumentów."
  },
  "ernie-4.5-turbo-32k": {
    "description": "Wenxin 4.5 Turbo ma wyraźnie poprawione zdolności w zakresie eliminacji halucynacji, rozumowania logicznego i programowania. W porównaniu do Wenxin 4.5, jest szybszy i tańszy. Zdolności w zakresie twórczości tekstowej i pytań i odpowiedzi znacznie się poprawiły. Długość wyjścia i opóźnienie całych zdań w porównaniu do ERNIE 4.5 wzrosły."
  },
  "ernie-4.5-turbo-vl-32k": {
    "description": "Nowa wersja dużego modelu Wenxin Yiyan, z wyraźnie poprawionymi zdolnościami w zakresie rozumienia obrazów, twórczości, tłumaczenia i programowania, po raz pierwszy obsługująca długość kontekstu 32K, z wyraźnie zmniejszonym opóźnieniem pierwszego tokena."
  },
  "ernie-char-8k": {
    "description": "Model językowy opracowany przez Baidu, skoncentrowany na specyficznych scenariuszach, odpowiedni do zastosowań w grach NPC, dialogach obsługi klienta, odgrywaniu ról w dialogach, charakteryzujący się wyraźnym i spójnym stylem postaci, silniejszą zdolnością do podążania za instrukcjami oraz lepszą wydajnością wnioskowania."
  },
  "ernie-char-fiction-8k": {
    "description": "Model językowy opracowany przez Baidu, skoncentrowany na specyficznych scenariuszach, odpowiedni do zastosowań w grach NPC, dialogach obsługi klienta, odgrywaniu ról w dialogach, charakteryzujący się wyraźnym i spójnym stylem postaci, silniejszą zdolnością do podążania za instrukcjami oraz lepszą wydajnością wnioskowania."
  },
  "ernie-irag-edit": {
    "description": "Model edycji obrazów ERNIE iRAG opracowany przez Baidu, wspierający operacje takie jak usuwanie obiektów (erase), przemalowywanie (repaint) oraz generowanie wariantów (variation) na podstawie obrazów."
  },
  "ernie-lite-8k": {
    "description": "ERNIE Lite to lekki model językowy opracowany przez Baidu, łączący doskonałe wyniki modelu z wydajnością wnioskowania, odpowiedni do użycia na kartach przyspieszających AI o niskiej mocy obliczeniowej."
  },
  "ernie-lite-pro-128k": {
    "description": "Lekki model językowy opracowany przez Baidu, łączący doskonałe wyniki modelu z wydajnością wnioskowania, oferujący lepsze wyniki niż ERNIE Lite, odpowiedni do użycia na kartach przyspieszających AI o niskiej mocy obliczeniowej."
  },
  "ernie-novel-8k": {
    "description": "Ogólny model językowy opracowany przez Baidu, który wykazuje wyraźne przewagi w zakresie kontynuacji powieści, może być również stosowany w scenariuszach krótkich dramatów i filmów."
  },
  "ernie-speed-128k": {
    "description": "Najnowszy model językowy o wysokiej wydajności opracowany przez Baidu w 2024 roku, charakteryzujący się doskonałymi zdolnościami ogólnymi, odpowiedni jako model bazowy do dalszego dostosowania, lepiej radzący sobie z problemami w specyficznych scenariuszach, a także oferujący doskonałą wydajność wnioskowania."
  },
  "ernie-speed-pro-128k": {
    "description": "Najnowszy model językowy o wysokiej wydajności opracowany przez Baidu w 2024 roku, charakteryzujący się doskonałymi zdolnościami ogólnymi, oferujący lepsze wyniki niż ERNIE Speed, odpowiedni jako model bazowy do dalszego dostosowania, lepiej radzący sobie z problemami w specyficznych scenariuszach, a także oferujący doskonałą wydajność wnioskowania."
  },
  "ernie-tiny-8k": {
    "description": "ERNIE Tiny to model językowy o ultra wysokiej wydajności opracowany przez Baidu, charakteryzujący się najniższymi kosztami wdrożenia i dostosowania w serii modeli Wenxin."
  },
  "ernie-x1-32k": {
    "description": "Posiada silniejsze zdolności w zakresie rozumienia, planowania, refleksji i ewolucji. Jako model głębokiego myślenia o wszechstronnych umiejętnościach, Wenxin X1 łączy w sobie dokładność, kreatywność i styl, doskonale sprawdzając się w chińskich pytaniach i odpowiedziach, twórczości literackiej, pisaniu dokumentów, codziennych rozmowach, rozumowaniu logicznym, skomplikowanych obliczeniach oraz wywoływaniu narzędzi."
  },
  "ernie-x1-32k-preview": {
    "description": "Model ERNIE X1 charakteryzuje się silniejszymi zdolnościami w zakresie rozumienia, planowania, refleksji i ewolucji. Jako model głębokiego myślenia o szerszych możliwościach, ERNIE X1 łączy w sobie precyzję, kreatywność i styl, osiągając szczególne wyniki w chińskich pytaniach i odpowiedziach, twórczości literackiej, pisaniu tekstów, codziennych rozmowach, wnioskowaniu logicznym, złożonych obliczeniach oraz wywoływaniu narzędzi."
  },
  "ernie-x1-turbo-32k": {
    "description": "Model ma lepsze wyniki i wydajność w porównaniu do ERNIE-X1-32K."
  },
  "fal-ai/bytedance/seedream/v4": {
    "description": "Model generowania obrazów Seedream 4.0 opracowany przez zespół Seed ByteDance, obsługuje wejścia tekstowe i obrazowe, oferując wysoką kontrolę i jakość generowanych obrazów. Generuje obrazy na podstawie tekstowych wskazówek."
  },
  "fal-ai/flux-kontext/dev": {
    "description": "Model FLUX.1 skoncentrowany na zadaniach edycji obrazów, obsługuje wejścia tekstowe i obrazowe."
  },
  "fal-ai/flux-pro/kontext": {
    "description": "FLUX.1 Kontext [pro] potrafi przetwarzać tekst i obrazy referencyjne jako wejście, umożliwiając płynną, celową edycję lokalną oraz złożone transformacje całych scen."
  },
  "fal-ai/flux/krea": {
    "description": "Flux Krea [dev] to model generowania obrazów z estetycznymi preferencjami, mający na celu tworzenie bardziej realistycznych i naturalnych obrazów."
  },
  "fal-ai/flux/schnell": {
    "description": "FLUX.1 [schnell] to model generowania obrazów z 12 miliardami parametrów, skoncentrowany na szybkim tworzeniu wysokiej jakości obrazów."
  },
  "fal-ai/imagen4/preview": {
    "description": "Wysokiej jakości model generowania obrazów udostępniony przez Google."
  },
  "fal-ai/nano-banana": {
    "description": "Nano Banana to najnowszy, najszybszy i najbardziej efektywny natywny model multimodalny Google, pozwalający na generowanie i edycję obrazów poprzez dialog."
  },
  "fal-ai/qwen-image": {
    "description": "Potężny model generowania obrazów zespołu Qwen, z imponującą zdolnością generowania chińskiego tekstu i różnorodnymi stylami wizualnymi obrazów."
  },
  "fal-ai/qwen-image-edit": {
    "description": "Profesjonalny model edycji obrazów zespołu Qwen, wspierający edycję semantyczną i wyglądu, umożliwiający precyzyjną edycję tekstu w języku chińskim i angielskim, a także transformacje stylów, obrót obiektów i inne wysokiej jakości edycje obrazów."
  },
  "flux-1-schnell": {
    "description": "Model generowania obrazów na podstawie tekstu o 12 miliardach parametrów opracowany przez Black Forest Labs, wykorzystujący technikę destylacji latentnej dyfuzji przeciwstawnej, zdolny do generowania wysokiej jakości obrazów w 1 do 4 kroków. Model osiąga wydajność porównywalną z zamkniętymi alternatywami i jest udostępniony na licencji Apache-2.0, odpowiedni do użytku osobistego, badawczego i komercyjnego."
  },
  "flux-dev": {
    "description": "FLUX.1 [dev] to otwarty, dopracowany model o otwartych wagach przeznaczony do zastosowań niekomercyjnych. Zachowuje jakość obrazu i zdolność do przestrzegania instrukcji zbliżoną do wersji profesjonalnej FLUX, oferując jednocześnie wyższą efektywność działania. W porównaniu do standardowych modeli o podobnej wielkości jest bardziej efektywny w wykorzystaniu zasobów."
  },
  "flux-kontext-max": {
    "description": "Najnowocześniejsze generowanie i edycja obrazów kontekstowych — łączenie tekstu i obrazów, aby uzyskać precyzyjne i spójne rezultaty."
  },
  "flux-kontext-pro": {
    "description": "Najnowocześniejsze generowanie i edycja obrazów w kontekście — łączenie tekstu i obrazów w celu uzyskania precyzyjnych i spójnych rezultatów."
  },
  "flux-merged": {
    "description": "Model FLUX.1-merged łączy głębokie cechy eksplorowane podczas fazy rozwojowej „DEV” z zaletami szybkiego wykonania reprezentowanymi przez „Schnell”. Dzięki temu FLUX.1-merged nie tylko przesuwa granice wydajności modelu, ale także rozszerza zakres jego zastosowań."
  },
  "flux-pro": {
    "description": "Wiodący komercyjny model AI do generowania obrazów — niezrównana jakość obrazów i różnorodność generowanych rezultatów."
  },
  "flux-pro-1.1": {
    "description": "Ulepszona, profesjonalna wersja modelu AI do generowania obrazów — zapewnia doskonałą jakość obrazów i precyzyjne realizowanie poleceń."
  },
  "flux-pro-1.1-ultra": {
    "description": "Generowanie obrazów AI o ultrawysokiej rozdzielczości — obsługa wyjścia 4 megapikseli, tworzy niezwykle wyraźne obrazy w ciągu 10 sekund."
  },
  "flux-schnell": {
    "description": "FLUX.1 [schnell] to obecnie najbardziej zaawansowany otwarty model o małej liczbie kroków, przewyższający konkurencję, a nawet potężne modele nie destylowane, takie jak Midjourney v6.0 i DALL·E 3 (HD). Model został specjalnie dostrojony, aby zachować pełną różnorodność wyjść z fazy wstępnego treningu. W porównaniu z najlepszymi modelami na rynku FLUX.1 [schnell] znacząco poprawia jakość wizualną, zgodność z instrukcjami, obsługę zmian rozmiaru/proporcji, przetwarzanie czcionek oraz różnorodność generowanych obrazów, oferując użytkownikom bogatsze i bardziej zróżnicowane doświadczenia twórcze."
  },
  "flux.1-schnell": {
    "description": "Transformator przepływu skorygowanego o 12 miliardach parametrów, zdolny do generowania obrazów na podstawie opisów tekstowych."
  },
  "gemini-1.0-pro-001": {
    "description": "Gemini 1.0 Pro 001 (Tuning) oferuje stabilną i dostosowywalną wydajność, co czyni go idealnym wyborem dla rozwiązań złożonych zadań."
  },
  "gemini-1.0-pro-002": {
    "description": "Gemini 1.0 Pro 002 (Tuning) oferuje doskonałe wsparcie multimodalne, koncentrując się na efektywnym rozwiązywaniu złożonych zadań."
  },
  "gemini-1.0-pro-latest": {
    "description": "Gemini 1.0 Pro to model AI o wysokiej wydajności od Google, zaprojektowany do szerokiego rozszerzania zadań."
  },
  "gemini-1.5-flash-001": {
    "description": "Gemini 1.5 Flash 001 to wydajny model multimodalny, wspierający szerokie zastosowania."
  },
  "gemini-1.5-flash-002": {
    "description": "Gemini 1.5 Flash 002 to wydajny model multimodalny, który wspiera szeroką gamę zastosowań."
  },
  "gemini-1.5-flash-8b": {
    "description": "Gemini 1.5 Flash 8B to wydajny model multimodalny, który wspiera szeroki zakres zastosowań."
  },
  "gemini-1.5-flash-8b-exp-0924": {
    "description": "Gemini 1.5 Flash 8B 0924 to najnowszy eksperymentalny model, który wykazuje znaczące poprawy wydajności w zastosowaniach tekstowych i multimodalnych."
  },
  "gemini-1.5-flash-8b-latest": {
    "description": "Gemini 1.5 Flash 8B to wydajny model wielomodalny, który obsługuje szeroki zakres zastosowań."
  },
  "gemini-1.5-flash-exp-0827": {
    "description": "Gemini 1.5 Flash 0827 oferuje zoptymalizowane możliwości przetwarzania multimodalnego, odpowiednie dla wielu złożonych scenariuszy."
  },
  "gemini-1.5-flash-latest": {
    "description": "Gemini 1.5 Flash to najnowszy model AI Google o wielu modalnościach, który charakteryzuje się szybkim przetwarzaniem i obsługuje wejścia tekstowe, obrazowe i wideo, co czyni go odpowiednim do efektywnego rozszerzania w różnych zadaniach."
  },
  "gemini-1.5-pro-001": {
    "description": "Gemini 1.5 Pro 001 to skalowalne rozwiązanie AI multimodalnego, wspierające szeroki zakres złożonych zadań."
  },
  "gemini-1.5-pro-002": {
    "description": "Gemini 1.5 Pro 002 to najnowszy model gotowy do produkcji, oferujący wyższą jakość wyników, ze szczególnym uwzględnieniem zadań matematycznych, długich kontekstów i zadań wizualnych."
  },
  "gemini-1.5-pro-exp-0801": {
    "description": "Gemini 1.5 Pro 0801 oferuje doskonałe możliwości przetwarzania multimodalnego, zapewniając większą elastyczność w rozwoju aplikacji."
  },
  "gemini-1.5-pro-exp-0827": {
    "description": "Gemini 1.5 Pro 0827 łączy najnowsze technologie optymalizacji, oferując bardziej efektywne możliwości przetwarzania danych multimodalnych."
  },
  "gemini-1.5-pro-latest": {
    "description": "Gemini 1.5 Pro obsługuje do 2 milionów tokenów, co czyni go idealnym wyborem dla średniej wielkości modeli multimodalnych, odpowiednim do wszechstronnej obsługi złożonych zadań."
  },
  "gemini-2.0-flash": {
    "description": "Gemini 2.0 Flash oferuje funkcje i ulepszenia nowej generacji, w tym doskonałą prędkość, natywne korzystanie z narzędzi, generowanie multimodalne oraz okno kontekstowe o długości 1M tokenów."
  },
  "gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash oferuje funkcje i ulepszenia nowej generacji, w tym doskonałą prędkość, natywne korzystanie z narzędzi, generowanie multimodalne oraz okno kontekstowe o długości 1M tokenów."
  },
  "gemini-2.0-flash-exp": {
    "description": "Gemini 2.0 Flash to wariant modelu, zoptymalizowany pod kątem efektywności kosztowej i niskiego opóźnienia."
  },
  "gemini-2.0-flash-exp-image-generation": {
    "description": "Model eksperymentalny Gemini 2.0 Flash, wspierający generowanie obrazów"
  },
  "gemini-2.0-flash-lite": {
    "description": "Gemini 2.0 Flash to wariant modelu, zoptymalizowany pod kątem efektywności kosztowej i niskiego opóźnienia."
  },
  "gemini-2.0-flash-lite-001": {
    "description": "Gemini 2.0 Flash to wariant modelu, zoptymalizowany pod kątem efektywności kosztowej i niskiego opóźnienia."
  },
  "gemini-2.0-flash-preview-image-generation": {
    "description": "Model Gemini 2.0 Flash do generowania obrazów, wspierający generację obrazów"
  },
  "gemini-2.5-flash": {
    "description": "Gemini 2.5 Flash to najbardziej opłacalny model Google, oferujący wszechstronne funkcje."
  },
  "gemini-2.5-flash-image": {
    "description": "Nano Banana to najnowszy, najszybszy i najbardziej efektywny natywny model multimodalny Google, który umożliwia generowanie i edycję obrazów za pomocą rozmowy."
  },
  "gemini-2.5-flash-image-preview": {
    "description": "Nano Banana to najnowszy, najszybszy i najbardziej wydajny natywny model multimodalny Google, który pozwala generować i edytować obrazy za pomocą rozmowy."
  },
  "gemini-2.5-flash-image-preview:image": {
    "description": "Nano Banana to najnowszy, najszybszy i najbardziej wydajny natywny model multimodalny Google, który pozwala generować i edytować obrazy za pomocą rozmowy."
  },
  "gemini-2.5-flash-image:image": {
    "description": "Nano Banana to najnowszy, najszybszy i najbardziej efektywny natywny model multimodalny Google, który umożliwia generowanie i edycję obrazów za pomocą rozmowy."
  },
  "gemini-2.5-flash-lite": {
    "description": "Gemini 2.5 Flash-Lite to najmniejszy i najbardziej opłacalny model Google, zaprojektowany z myślą o szerokim zastosowaniu."
  },
  "gemini-2.5-flash-lite-preview-06-17": {
    "description": "Gemini 2.5 Flash-Lite Preview to najmniejszy i najbardziej opłacalny model Google, zaprojektowany z myślą o masowym zastosowaniu."
  },
  "gemini-2.5-flash-lite-preview-09-2025": {
    "description": "Wersja podglądowa (25 września 2025) Gemini 2.5 Flash-Lite"
  },
  "gemini-2.5-flash-preview-04-17": {
    "description": "Gemini 2.5 Flash Preview to najbardziej opłacalny model Google, oferujący wszechstronne funkcje."
  },
  "gemini-2.5-flash-preview-05-20": {
    "description": "Gemini 2.5 Flash Preview to najbardziej opłacalny model Google, oferujący wszechstronne funkcje."
  },
  "gemini-2.5-flash-preview-09-2025": {
    "description": "Wersja podglądowa (25 września 2025) Gemini 2.5 Flash"
  },
  "gemini-2.5-pro": {
    "description": "Gemini 2.5 Pro to najnowocześniejszy model myślowy Google, zdolny do rozumowania nad złożonymi problemami w dziedzinach kodowania, matematyki i STEM oraz analizowania dużych zbiorów danych, repozytoriów kodu i dokumentacji przy użyciu długiego kontekstu."
  },
  "gemini-2.5-pro-preview-03-25": {
    "description": "Gemini 2.5 Pro Preview to najnowocześniejszy model myślenia Google, zdolny do wnioskowania w zakresie kodu, matematyki i złożonych problemów w dziedzinie STEM, a także do analizy dużych zbiorów danych, repozytoriów kodu i dokumentów przy użyciu długiego kontekstu."
  },
  "gemini-2.5-pro-preview-05-06": {
    "description": "Gemini 2.5 Pro Preview to najnowocześniejszy model myślenia Google, zdolny do wnioskowania w złożonych problemach związanych z kodem, matematyką i dziedzinami STEM, a także do analizy dużych zbiorów danych, repozytoriów kodu i dokumentów przy użyciu długiego kontekstu."
  },
  "gemini-2.5-pro-preview-06-05": {
    "description": "Gemini 2.5 Pro Preview to najnowocześniejszy model myślowy Google, zdolny do rozumowania nad złożonymi problemami w dziedzinach kodowania, matematyki i STEM oraz do analizy dużych zbiorów danych, repozytoriów kodu i dokumentów z wykorzystaniem długich kontekstów."
  },
  "gemini-flash-latest": {
    "description": "Najnowsze wydanie Gemini Flash"
  },
  "gemini-flash-lite-latest": {
    "description": "Najnowsze wydanie Gemini Flash-Lite"
  },
  "gemini-pro-latest": {
    "description": "Najnowsze wydanie Gemini Pro"
  },
  "gemma-7b-it": {
    "description": "Gemma 7B nadaje się do przetwarzania zadań średniej i małej skali, łącząc efektywność kosztową."
  },
  "gemma2": {
    "description": "Gemma 2 to wydajny model wydany przez Google, obejmujący różnorodne zastosowania, od małych aplikacji po złożone przetwarzanie danych."
  },
  "gemma2-9b-it": {
    "description": "Gemma 2 9B to model zoptymalizowany do specyficznych zadań i integracji narzędzi."
  },
  "gemma2:27b": {
    "description": "Gemma 2 to wydajny model wydany przez Google, obejmujący różnorodne zastosowania, od małych aplikacji po złożone przetwarzanie danych."
  },
  "gemma2:2b": {
    "description": "Gemma 2 to wydajny model wydany przez Google, obejmujący różnorodne zastosowania, od małych aplikacji po złożone przetwarzanie danych."
  },
  "generalv3": {
    "description": "Spark Pro to model dużego języka o wysokiej wydajności, zoptymalizowany do profesjonalnych dziedzin, takich jak matematyka, programowanie, medycyna i edukacja, wspierający wyszukiwanie w sieci oraz wbudowane wtyczki, takie jak pogoda i daty. Jego zoptymalizowany model wykazuje doskonałe wyniki i wysoką wydajność w skomplikowanych pytaniach o wiedzę, rozumieniu języka oraz tworzeniu zaawansowanych tekstów, co czyni go idealnym wyborem do profesjonalnych zastosowań."
  },
  "generalv3.5": {
    "description": "Spark3.5 Max to najbardziej wszechstronna wersja, wspierająca wyszukiwanie w sieci oraz wiele wbudowanych wtyczek. Jego kompleksowo zoptymalizowane zdolności rdzeniowe oraz funkcje ustawiania ról systemowych i wywoływania funkcji sprawiają, że wykazuje się wyjątkową wydajnością w różnych skomplikowanych zastosowaniach."
  },
  "glm-4": {
    "description": "GLM-4 to stary flagowy model wydany w styczniu 2024 roku, obecnie zastąpiony przez silniejszy model GLM-4-0520."
  },
  "glm-4-0520": {
    "description": "GLM-4-0520 to najnowsza wersja modelu, zaprojektowana do wysoko złożonych i zróżnicowanych zadań, z doskonałymi wynikami."
  },
  "glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat wykazuje wysoką wydajność w wielu aspektach, takich jak semantyka, matematyka, wnioskowanie, kodowanie i wiedza. Posiada również funkcje przeglądania stron internetowych, wykonywania kodu, wywoływania niestandardowych narzędzi oraz wnioskowania z długich tekstów. Obsługuje 26 języków, w tym japoński, koreański i niemiecki."
  },
  "glm-4-air": {
    "description": "GLM-4-Air to opłacalna wersja, której wydajność jest zbliżona do GLM-4, oferująca szybkie działanie i przystępną cenę."
  },
  "glm-4-air-250414": {
    "description": "GLM-4-Air to wersja o wysokim stosunku jakości do ceny, o wydajności zbliżonej do GLM-4, oferująca szybkie tempo i przystępną cenę."
  },
  "glm-4-airx": {
    "description": "GLM-4-AirX oferuje wydajną wersję GLM-4-Air, z szybkością wnioskowania do 2,6 razy szybszą."
  },
  "glm-4-alltools": {
    "description": "GLM-4-AllTools to model inteligentny o wielu funkcjach, zoptymalizowany do wsparcia złożonego planowania instrukcji i wywołań narzędzi, takich jak przeglądanie sieci, interpretacja kodu i generowanie tekstu, odpowiedni do wykonywania wielu zadań."
  },
  "glm-4-flash": {
    "description": "GLM-4-Flash to idealny wybór do przetwarzania prostych zadań, najszybszy i najtańszy."
  },
  "glm-4-flash-250414": {
    "description": "GLM-4-Flash to idealny wybór do prostych zadań, najszybszy i darmowy."
  },
  "glm-4-flashx": {
    "description": "GLM-4-FlashX to ulepszona wersja Flash, charakteryzująca się niezwykle szybkim czasem wnioskowania."
  },
  "glm-4-long": {
    "description": "GLM-4-Long obsługuje ultra-długie wejścia tekstowe, odpowiednie do zadań pamięciowych i przetwarzania dużych dokumentów."
  },
  "glm-4-plus": {
    "description": "GLM-4-Plus jako flagowy model o wysokiej inteligencji, posiada potężne zdolności przetwarzania długich tekstów i złożonych zadań, z ogólnym wzrostem wydajności."
  },
  "glm-4.1v-thinking-flash": {
    "description": "Seria modeli GLM-4.1V-Thinking to najsilniejsze znane modele wizualno-językowe (VLM) na poziomie 10 miliardów parametrów, integrujące najnowocześniejsze zadania wizualno-językowe na tym poziomie, w tym rozumienie wideo, pytania i odpowiedzi na obrazach, rozwiązywanie problemów naukowych, rozpoznawanie tekstu OCR, interpretację dokumentów i wykresów, agenta GUI, kodowanie front-endowe stron internetowych, grounding i inne. Wiele z tych zadań przewyższa możliwości modelu Qwen2.5-VL-72B, który ma ponad 8 razy więcej parametrów. Dzięki zaawansowanym technikom uczenia ze wzmocnieniem model opanował rozumowanie łańcuchowe, co znacząco poprawia dokładność i bogactwo odpowiedzi, przewyższając tradycyjne modele bez mechanizmu thinking pod względem końcowych rezultatów i interpretowalności."
  },
  "glm-4.1v-thinking-flashx": {
    "description": "Seria modeli GLM-4.1V-Thinking to najsilniejsze znane modele wizualno-językowe (VLM) na poziomie 10 miliardów parametrów, integrujące najnowocześniejsze zadania wizualno-językowe na tym poziomie, w tym rozumienie wideo, pytania i odpowiedzi na obrazach, rozwiązywanie problemów naukowych, rozpoznawanie tekstu OCR, interpretację dokumentów i wykresów, agenta GUI, kodowanie front-endowe stron internetowych, grounding i inne. Wiele z tych zadań przewyższa możliwości modelu Qwen2.5-VL-72B, który ma ponad 8 razy więcej parametrów. Dzięki zaawansowanym technikom uczenia ze wzmocnieniem model opanował rozumowanie łańcuchowe, co znacząco poprawia dokładność i bogactwo odpowiedzi, przewyższając tradycyjne modele bez mechanizmu thinking pod względem końcowych rezultatów i interpretowalności."
  },
  "glm-4.5": {
    "description": "Flagowy model Zhipu, wspierający tryb myślenia, osiągający poziom SOTA wśród modeli open source pod względem zdolności ogólnych, z długością kontekstu do 128K."
  },
  "glm-4.5-air": {
    "description": "Lżejsza wersja GLM-4.5, łącząca wydajność i opłacalność, z możliwością elastycznego przełączania hybrydowego trybu myślenia."
  },
  "glm-4.5-airx": {
    "description": "Ekspresowa wersja GLM-4.5-Air, oferująca szybszy czas reakcji, zaprojektowana do zastosowań wymagających dużej skali i wysokiej prędkości."
  },
  "glm-4.5-flash": {
    "description": "Bezpłatna wersja GLM-4.5, wyróżniająca się doskonałą wydajnością w zadaniach inferencyjnych, kodowania i agentów."
  },
  "glm-4.5-x": {
    "description": "Ekspresowa wersja GLM-4.5, łącząca wysoką wydajność z prędkością generowania do 100 tokenów na sekundę."
  },
  "glm-4.5v": {
    "description": "Nowa generacja modelu do wnioskowania wizualnego firmy Zhipu oparta na architekturze MOE. Przy łącznej liczbie parametrów 106B i 12B parametrów aktywacji osiąga wyniki SOTA wśród otwartoźródłowych modeli multimodalnych o porównywalnej skali na różnych benchmarkach, obejmując typowe zadania związane z analizą obrazów, wideo, rozumieniem dokumentów oraz zadaniami GUI."
  },
  "glm-4.6": {
    "description": "Najnowszy flagowy model Zhipu GLM-4.6 (355B) przewyższa poprzednie generacje pod względem zaawansowanego kodowania, przetwarzania długich tekstów, wnioskowania i zdolności agentów, szczególnie wyrównując się z Claude Sonnet 4 w zakresie programowania, stając się czołowym modelem kodowania w kraju."
  },
  "glm-4v": {
    "description": "GLM-4V oferuje potężne zdolności rozumienia i wnioskowania obrazów, obsługując różne zadania wizualne."
  },
  "glm-4v-flash": {
    "description": "GLM-4V-Flash koncentruje się na efektywnym zrozumieniu pojedynczego obrazu, idealny do scenariuszy szybkiej analizy obrazu, takich jak analiza obrazów w czasie rzeczywistym lub przetwarzanie partii obrazów."
  },
  "glm-4v-plus": {
    "description": "GLM-4V-Plus ma zdolność rozumienia treści wideo oraz wielu obrazów, odpowiedni do zadań multimodalnych."
  },
  "glm-4v-plus-0111": {
    "description": "GLM-4V-Plus posiada zdolność rozumienia treści wideo oraz wielu obrazów, odpowiedni do zadań multimodalnych."
  },
  "glm-z1-air": {
    "description": "Model wnioskowania: posiadający silne zdolności wnioskowania, odpowiedni do zadań wymagających głębokiego wnioskowania."
  },
  "glm-z1-airx": {
    "description": "Ekstremalne wnioskowanie: charakteryzujące się ultra szybkim tempem wnioskowania i silnymi efektami wnioskowania."
  },
  "glm-z1-flash": {
    "description": "Seria GLM-Z1 charakteryzuje się silnymi zdolnościami do złożonego wnioskowania, osiągając doskonałe wyniki w logice, matematyce i programowaniu."
  },
  "glm-z1-flashx": {
    "description": "Wysoka prędkość i niska cena: wersja wzbogacona Flash, ultra szybkie tempo inferencji i lepsza obsługa współbieżności."
  },
  "glm-zero-preview": {
    "description": "GLM-Zero-Preview posiada silne zdolności do złożonego wnioskowania, wyróżniając się w dziedzinach takich jak wnioskowanie logiczne, matematyka i programowanie."
  },
  "google/gemini-2.0-flash": {
    "description": "Gemini 2.0 Flash oferuje funkcje nowej generacji i ulepszenia, w tym doskonałą szybkość, wbudowane użycie narzędzi, generowanie multimodalne oraz okno kontekstu o rozmiarze 1 miliona tokenów."
  },
  "google/gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash oferuje funkcje i ulepszenia nowej generacji, w tym doskonałą prędkość, natywne korzystanie z narzędzi, generowanie multimodalne oraz okno kontekstowe o długości 1M tokenów."
  },
  "google/gemini-2.0-flash-exp:free": {
    "description": "Gemini 2.0 Flash Experimental to najnowszy eksperymentalny model AI Google, który w porównaniu do wcześniejszych wersji wykazuje pewne poprawy jakości, szczególnie w zakresie wiedzy o świecie, kodu i długiego kontekstu."
  },
  "google/gemini-2.0-flash-lite": {
    "description": "Gemini 2.0 Flash Lite oferuje funkcje nowej generacji i ulepszenia, w tym doskonałą szybkość, wbudowane użycie narzędzi, generowanie multimodalne oraz okno kontekstu o rozmiarze 1 miliona tokenów."
  },
  "google/gemini-2.5-flash": {
    "description": "Gemini 2.5 Flash to model myślący, oferujący doskonałe, wszechstronne możliwości. Zaprojektowany, by znaleźć równowagę między ceną a wydajnością, obsługuje multimodalność i okno kontekstu o rozmiarze 1 miliona tokenów."
  },
  "google/gemini-2.5-flash-image-preview": {
    "description": "Eksperymentalny model Gemini 2.5 Flash, wspierający generowanie obrazów."
  },
  "google/gemini-2.5-flash-lite": {
    "description": "Gemini 2.5 Flash-Lite to zrównoważony, niskoopóźnieniowy model z konfigurowalnym budżetem myślenia i łącznością narzędzi (np. Google Search grounding i wykonywanie kodu). Obsługuje multimodalne wejścia i oferuje okno kontekstu o rozmiarze 1 miliona tokenów."
  },
  "google/gemini-2.5-flash-preview": {
    "description": "Gemini 2.5 Flash to najnowocześniejszy model główny Google, zaprojektowany z myślą o zaawansowanym wnioskowaniu, kodowaniu, matematyce i zadaniach naukowych. Zawiera wbudowaną zdolność 'myślenia', co pozwala mu na dostarczanie odpowiedzi z wyższą dokładnością i szczegółowym przetwarzaniem kontekstu.\n\nUwaga: ten model ma dwa warianty: myślenie i niemyslenie. Ceny wyjściowe różnią się znacznie w zależności od tego, czy zdolność myślenia jest aktywowana. Jeśli wybierzesz standardowy wariant (bez sufiksu ':thinking'), model wyraźnie unika generowania tokenów myślenia.\n\nAby skorzystać z zdolności myślenia i otrzymać tokeny myślenia, musisz wybrać wariant ':thinking', co spowoduje wyższe ceny wyjściowe za myślenie.\n\nPonadto Gemini 2.5 Flash można konfigurować za pomocą parametru 'maksymalna liczba tokenów do wnioskowania', jak opisano w dokumentacji (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."
  },
  "google/gemini-2.5-flash-preview:thinking": {
    "description": "Gemini 2.5 Flash to najnowocześniejszy model główny Google, zaprojektowany z myślą o zaawansowanym wnioskowaniu, kodowaniu, matematyce i zadaniach naukowych. Zawiera wbudowaną zdolność 'myślenia', co pozwala mu na dostarczanie odpowiedzi z wyższą dokładnością i szczegółowym przetwarzaniem kontekstu.\n\nUwaga: ten model ma dwa warianty: myślenie i niemyslenie. Ceny wyjściowe różnią się znacznie w zależności od tego, czy zdolność myślenia jest aktywowana. Jeśli wybierzesz standardowy wariant (bez sufiksu ':thinking'), model wyraźnie unika generowania tokenów myślenia.\n\nAby skorzystać z zdolności myślenia i otrzymać tokeny myślenia, musisz wybrać wariant ':thinking', co spowoduje wyższe ceny wyjściowe za myślenie.\n\nPonadto Gemini 2.5 Flash można konfigurować za pomocą parametru 'maksymalna liczba tokenów do wnioskowania', jak opisano w dokumentacji (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."
  },
  "google/gemini-2.5-pro": {
    "description": "Gemini 2.5 Pro to nasz najbardziej zaawansowany model wnioskowania Gemini, zdolny do rozwiązywania złożonych problemów. Posiada okno kontekstu o rozmiarze 2 milionów tokenów i obsługuje multimodalne wejścia, w tym tekst, obrazy, dźwięk, wideo i dokumenty PDF."
  },
  "google/gemini-2.5-pro-preview": {
    "description": "Gemini 2.5 Pro Preview to najnowocześniejszy model myślowy Google, zdolny do rozumowania nad złożonymi problemami w dziedzinie kodowania, matematyki i STEM oraz do analizy dużych zbiorów danych, repozytoriów kodu i dokumentów przy użyciu długiego kontekstu."
  },
  "google/gemini-embedding-001": {
    "description": "Najnowocześniejszy model osadzeń, oferujący doskonałą wydajność w zadaniach anglojęzycznych, wielojęzycznych i kodowych."
  },
  "google/gemini-flash-1.5": {
    "description": "Gemini 1.5 Flash oferuje zoptymalizowane możliwości przetwarzania multimodalnego, odpowiednie do różnych złożonych scenariuszy zadań."
  },
  "google/gemini-pro-1.5": {
    "description": "Gemini 1.5 Pro łączy najnowsze technologie optymalizacji, oferując bardziej efektywne przetwarzanie danych multimodalnych."
  },
  "google/gemma-2-27b": {
    "description": "Gemma 2 to wydajny model wydany przez Google, obejmujący różnorodne scenariusze zastosowań, od małych aplikacji po złożone przetwarzanie danych."
  },
  "google/gemma-2-27b-it": {
    "description": "Gemma 2 kontynuuje ideę lekkiego i wydajnego projektowania."
  },
  "google/gemma-2-2b-it": {
    "description": "Lekki model dostosowywania instrukcji od Google."
  },
  "google/gemma-2-9b": {
    "description": "Gemma 2 to wydajny model wydany przez Google, obejmujący różnorodne scenariusze zastosowań, od małych aplikacji po złożone przetwarzanie danych."
  },
  "google/gemma-2-9b-it": {
    "description": "Gemma 2 to lekka seria modeli tekstowych open source od Google."
  },
  "google/gemma-2-9b-it:free": {
    "description": "Gemma 2 to odchudzona seria otwartych modeli tekstowych Google."
  },
  "google/gemma-2b-it": {
    "description": "Gemma Instruct (2B) oferuje podstawowe możliwości przetwarzania poleceń, idealne do lekkich aplikacji."
  },
  "google/gemma-3-12b-it": {
    "description": "Gemma 3 12B to otwarty model językowy Google, ustanawiający nowe standardy w zakresie efektywności i wydajności."
  },
  "google/gemma-3-1b-it": {
    "description": "Gemma 3 1B to otwarty model językowy Google, ustanawiający nowe standardy w zakresie efektywności i wydajności."
  },
  "google/gemma-3-27b-it": {
    "description": "Gemma 3 27B to otwarty model językowy stworzony przez Google, który ustanowił nowe standardy w zakresie wydajności i efektywności."
  },
  "google/text-embedding-005": {
    "description": "Model osadzeń tekstowych skoncentrowany na języku angielskim, zoptymalizowany pod kątem zadań kodowania i języka angielskiego."
  },
  "google/text-multilingual-embedding-002": {
    "description": "Model osadzeń tekstowych wielojęzycznych, zoptymalizowany pod kątem zadań międzyjęzykowych, obsługujący wiele języków."
  },
  "gpt-3.5-turbo": {
    "description": "GPT 3.5 Turbo, odpowiedni do różnych zadań generowania i rozumienia tekstu, obecnie wskazuje na gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-0125": {
    "description": "GPT 3.5 Turbo, odpowiedni do różnych zadań generowania i rozumienia tekstu, obecnie wskazuje na gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-1106": {
    "description": "GPT 3.5 Turbo, odpowiedni do różnych zadań generowania i rozumienia tekstu, obecnie wskazuje na gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-instruct": {
    "description": "GPT 3.5 Turbo, odpowiedni do różnych zadań generowania i rozumienia tekstu, obecnie wskazuje na gpt-3.5-turbo-0125."
  },
  "gpt-35-turbo": {
    "description": "GPT 3.5 Turbo to wydajny model dostarczany przez OpenAI, idealny do obsługi zadań związanych z czatowaniem i generowaniem tekstu, wspierający równoległe wywołania funkcji."
  },
  "gpt-35-turbo-16k": {
    "description": "GPT 3.5 Turbo 16k, model do generowania tekstu o dużej pojemności, odpowiedni do bardziej złożonych zadań."
  },
  "gpt-4": {
    "description": "GPT-4 oferuje większe okno kontekstowe, zdolne do przetwarzania dłuższych wejść tekstowych, co czyni go odpowiednim do scenariuszy wymagających szerokiej integracji informacji i analizy danych."
  },
  "gpt-4-0125-preview": {
    "description": "Najnowszy model GPT-4 Turbo posiada funkcje wizualne. Teraz zapytania wizualne mogą być obsługiwane za pomocą formatu JSON i wywołań funkcji. GPT-4 Turbo to ulepszona wersja, która oferuje opłacalne wsparcie dla zadań multimodalnych. Znajduje równowagę między dokładnością a wydajnością, co czyni go odpowiednim do aplikacji wymagających interakcji w czasie rzeczywistym."
  },
  "gpt-4-0613": {
    "description": "GPT-4 oferuje większe okno kontekstowe, zdolne do przetwarzania dłuższych wejść tekstowych, co czyni go odpowiednim do scenariuszy wymagających szerokiej integracji informacji i analizy danych."
  },
  "gpt-4-1106-preview": {
    "description": "Najnowszy model GPT-4 Turbo posiada funkcje wizualne. Teraz zapytania wizualne mogą być obsługiwane za pomocą formatu JSON i wywołań funkcji. GPT-4 Turbo to ulepszona wersja, która oferuje opłacalne wsparcie dla zadań multimodalnych. Znajduje równowagę między dokładnością a wydajnością, co czyni go odpowiednim do aplikacji wymagających interakcji w czasie rzeczywistym."
  },
  "gpt-4-32k": {
    "description": "GPT-4 oferuje większe okno kontekstowe, zdolne do przetwarzania dłuższych wejść tekstowych, co czyni go odpowiednim do scenariuszy wymagających szerokiej integracji informacji i analizy danych."
  },
  "gpt-4-32k-0613": {
    "description": "GPT-4 oferuje większe okno kontekstowe, zdolne do przetwarzania dłuższych wejść tekstowych, co czyni go odpowiednim do scenariuszy wymagających szerokiej integracji informacji i analizy danych."
  },
  "gpt-4-turbo": {
    "description": "Najnowszy model GPT-4 Turbo posiada funkcje wizualne. Teraz zapytania wizualne mogą być obsługiwane za pomocą formatu JSON i wywołań funkcji. GPT-4 Turbo to ulepszona wersja, która oferuje opłacalne wsparcie dla zadań multimodalnych. Znajduje równowagę między dokładnością a wydajnością, co czyni go odpowiednim do aplikacji wymagających interakcji w czasie rzeczywistym."
  },
  "gpt-4-turbo-2024-04-09": {
    "description": "Najnowszy model GPT-4 Turbo posiada funkcje wizualne. Teraz zapytania wizualne mogą być obsługiwane za pomocą formatu JSON i wywołań funkcji. GPT-4 Turbo to ulepszona wersja, która oferuje opłacalne wsparcie dla zadań multimodalnych. Znajduje równowagę między dokładnością a wydajnością, co czyni go odpowiednim do aplikacji wymagających interakcji w czasie rzeczywistym."
  },
  "gpt-4-turbo-preview": {
    "description": "Najnowszy model GPT-4 Turbo posiada funkcje wizualne. Teraz zapytania wizualne mogą być obsługiwane za pomocą formatu JSON i wywołań funkcji. GPT-4 Turbo to ulepszona wersja, która oferuje opłacalne wsparcie dla zadań multimodalnych. Znajduje równowagę między dokładnością a wydajnością, co czyni go odpowiednim do aplikacji wymagających interakcji w czasie rzeczywistym."
  },
  "gpt-4-vision-preview": {
    "description": "Najnowszy model GPT-4 Turbo posiada funkcje wizualne. Teraz zapytania wizualne mogą być obsługiwane za pomocą formatu JSON i wywołań funkcji. GPT-4 Turbo to ulepszona wersja, która oferuje opłacalne wsparcie dla zadań multimodalnych. Znajduje równowagę między dokładnością a wydajnością, co czyni go odpowiednim do aplikacji wymagających interakcji w czasie rzeczywistym."
  },
  "gpt-4.1": {
    "description": "GPT-4.1 to nasz flagowy model do złożonych zadań. Doskonale nadaje się do rozwiązywania problemów w różnych dziedzinach."
  },
  "gpt-4.1-mini": {
    "description": "GPT-4.1 mini oferuje równowagę między inteligencją, szybkością a kosztami, co czyni go atrakcyjnym modelem w wielu zastosowaniach."
  },
  "gpt-4.1-nano": {
    "description": "GPT-4.1 mini oferuje równowagę między inteligencją, szybkością a kosztami, co czyni go atrakcyjnym modelem w wielu zastosowaniach."
  },
  "gpt-4.5-preview": {
    "description": "GPT-4.5-preview to najnowszy model ogólnego przeznaczenia, dysponujący rozległą wiedzą o świecie i lepszym rozumieniem intencji użytkownika. Sprawdza się w zadaniach twórczych i planowaniu działań. Wiedza modelu jest aktualna na październik 2023 r."
  },
  "gpt-4o": {
    "description": "ChatGPT-4o to dynamiczny model, który jest na bieżąco aktualizowany, aby utrzymać najnowszą wersję. Łączy potężne zdolności rozumienia i generowania języka, co czyni go odpowiednim do zastosowań na dużą skalę, w tym obsługi klienta, edukacji i wsparcia technicznego."
  },
  "gpt-4o-2024-05-13": {
    "description": "ChatGPT-4o to dynamiczny model, który jest na bieżąco aktualizowany, aby utrzymać najnowszą wersję. Łączy potężne zdolności rozumienia i generowania języka, co czyni go odpowiednim do zastosowań na dużą skalę, w tym obsługi klienta, edukacji i wsparcia technicznego."
  },
  "gpt-4o-2024-08-06": {
    "description": "ChatGPT-4o to dynamiczny model, który jest na bieżąco aktualizowany, aby utrzymać najnowszą wersję. Łączy potężne zdolności rozumienia i generowania języka, co czyni go odpowiednim do zastosowań na dużą skalę, w tym obsługi klienta, edukacji i wsparcia technicznego."
  },
  "gpt-4o-2024-11-20": {
    "description": "ChatGPT-4o to dynamiczny model, aktualizowany w czasie rzeczywistym, aby być zawsze na bieżąco z najnowszą wersją. Łączy potężne zdolności rozumienia i generowania języka, idealny do zastosowań w dużej skali, w tym obsłudze klienta, edukacji i wsparciu technicznym."
  },
  "gpt-4o-audio-preview": {
    "description": "Model GPT-4o Audio Preview, obsługujący wejście i wyjście audio."
  },
  "gpt-4o-mini": {
    "description": "GPT-4o mini to najnowszy model OpenAI, wprowadzony po GPT-4 Omni, obsługujący wejścia tekstowe i wizualne oraz generujący tekst. Jako ich najnowocześniejszy model w małej skali, jest znacznie tańszy niż inne niedawno wprowadzone modele, a jego cena jest o ponad 60% niższa niż GPT-3.5 Turbo. Utrzymuje najnowocześniejszą inteligencję, jednocześnie oferując znaczną wartość za pieniądze. GPT-4o mini uzyskał wynik 82% w teście MMLU i obecnie zajmuje wyższą pozycję w preferencjach czatu niż GPT-4."
  },
  "gpt-4o-mini-audio-preview": {
    "description": "Model GPT-4o mini Audio, obsługujący wejście i wyjście audio."
  },
  "gpt-4o-mini-realtime-preview": {
    "description": "Wersja na żywo GPT-4o-mini, obsługująca wejście i wyjście audio oraz tekstowe w czasie rzeczywistym."
  },
  "gpt-4o-mini-search-preview": {
    "description": "GPT-4o mini wersja podglądowa do wyszukiwania to model specjalnie wytrenowany do rozumienia i realizacji zapytań wyszukiwania internetowego, korzystający z API Chat Completions. Poza opłatami za tokeny, zapytania wyszukiwania internetowego są dodatkowo obciążane opłatą za każde wywołanie narzędzia."
  },
  "gpt-4o-mini-transcribe": {
    "description": "GPT-4o Mini Transcribe to model konwersji mowy na tekst wykorzystujący GPT-4o do transkrypcji audio. W porównaniu z oryginalnym modelem Whisper poprawia wskaźnik błędów słów oraz rozpoznawanie i dokładność językową. Użyj go, aby uzyskać dokładniejsze transkrypcje."
  },
  "gpt-4o-mini-tts": {
    "description": "GPT-4o mini TTS to model tekstu na mowę oparty na GPT-4o mini, oferujący wysokiej jakości generowanie mowy przy niższych kosztach."
  },
  "gpt-4o-realtime-preview": {
    "description": "Wersja na żywo GPT-4o, obsługująca wejście i wyjście audio oraz tekstowe w czasie rzeczywistym."
  },
  "gpt-4o-realtime-preview-2024-10-01": {
    "description": "Wersja na żywo GPT-4o, obsługująca wejście i wyjście audio oraz tekstowe w czasie rzeczywistym."
  },
  "gpt-4o-realtime-preview-2025-06-03": {
    "description": "Wersja GPT-4o w czasie rzeczywistym, obsługująca jednoczesne wejście i wyjście audio oraz tekstu."
  },
  "gpt-4o-search-preview": {
    "description": "GPT-4o wersja podglądowa do wyszukiwania to model specjalnie wytrenowany do rozumienia i realizacji zapytań wyszukiwania internetowego, korzystający z API Chat Completions. Poza opłatami za tokeny, zapytania wyszukiwania internetowego są dodatkowo obciążane opłatą za każde wywołanie narzędzia."
  },
  "gpt-4o-transcribe": {
    "description": "GPT-4o Transcribe to model konwersji mowy na tekst wykorzystujący GPT-4o do transkrypcji audio. W porównaniu z oryginalnym modelem Whisper poprawia wskaźnik błędów słów oraz rozpoznawanie i dokładność językową. Użyj go, aby uzyskać dokładniejsze transkrypcje."
  },
  "gpt-5": {
    "description": "Najlepszy model do zadań międzydziedzinowego kodowania i pośrednictwa. GPT-5 osiąga przełom w dokładności, szybkości, rozumowaniu, rozpoznawaniu kontekstu, myśleniu strukturalnym i rozwiązywaniu problemów."
  },
  "gpt-5-chat-latest": {
    "description": "Model GPT-5 używany w ChatGPT. Łączy potężne zdolności rozumienia i generowania języka, idealny do interakcji konwersacyjnych."
  },
  "gpt-5-codex": {
    "description": "GPT-5 Codex to wersja GPT-5 zoptymalizowana pod kątem zadań kodowania w środowiskach Codex lub podobnych."
  },
  "gpt-5-mini": {
    "description": "Szybsza i bardziej ekonomiczna wersja GPT-5, przeznaczona do jasno określonych zadań. Zapewnia szybszą reakcję przy zachowaniu wysokiej jakości wyników."
  },
  "gpt-5-nano": {
    "description": "Najszybsza i najbardziej ekonomiczna wersja GPT-5. Doskonała do zastosowań wymagających szybkiej odpowiedzi i wrażliwych na koszty."
  },
  "gpt-audio": {
    "description": "GPT Audio to uniwersalny model konwersacyjny obsługujący wejście i wyjście audio, dostępny w API Chat Completions z obsługą audio I/O."
  },
  "gpt-image-1": {
    "description": "Natywny multimodalny model generowania obrazów ChatGPT."
  },
  "gpt-oss-120b": {
    "description": "GPT-OSS-120B MXFP4: skwantowany model Transformer, który zachowuje wysoką wydajność nawet przy ograniczonych zasobach."
  },
  "gpt-oss:120b": {
    "description": "GPT-OSS 120B to duży otwarty model językowy wydany przez OpenAI, wykorzystujący technologię kwantyzacji MXFP4, przeznaczony jako model flagowy. Wymaga środowiska wielo-GPU lub wysokowydajnej stacji roboczej, oferując znakomitą wydajność w złożonym wnioskowaniu, generowaniu kodu oraz przetwarzaniu wielojęzycznym, wspierając zaawansowane wywołania funkcji i integrację narzędzi."
  },
  "gpt-oss:20b": {
    "description": "GPT-OSS 20B to otwartoźródłowy duży model językowy wydany przez OpenAI, wykorzystujący technikę kwantyzacji MXFP4, przeznaczony do uruchamiania na wysokiej klasy konsumenckich GPU lub Apple Silicon Mac. Model wykazuje doskonałe wyniki w generowaniu dialogów, pisaniu kodu i zadaniach wnioskowania, obsługuje wywołania funkcji i korzystanie z narzędzi."
  },
  "gpt-realtime": {
    "description": "Uniwersalny model czasu rzeczywistego, obsługujący tekstowe i audio wejścia i wyjścia oraz wejścia obrazów."
  },
  "grok-2-1212": {
    "description": "Model ten poprawił dokładność, przestrzeganie instrukcji oraz zdolności wielojęzyczne."
  },
  "grok-2-image-1212": {
    "description": "Nasz najnowszy model generowania obrazów potrafi tworzyć żywe i realistyczne obrazy na podstawie tekstowych wskazówek. Sprawdza się doskonale w marketingu, mediach społecznościowych i rozrywce."
  },
  "grok-2-vision-1212": {
    "description": "Model ten poprawił dokładność, przestrzeganie instrukcji oraz zdolności wielojęzyczne."
  },
  "grok-3": {
    "description": "Flagowy model, specjalizujący się w ekstrakcji danych, programowaniu i streszczaniu tekstów na poziomie korporacyjnym, z głęboką wiedzą w dziedzinach finansów, medycyny, prawa i nauki."
  },
  "grok-3-fast": {
    "description": "Flagowy model, specjalizujący się w ekstrakcji danych, programowaniu i streszczaniu tekstów na poziomie korporacyjnym, z głęboką wiedzą w dziedzinach finansów, medycyny, prawa i nauki."
  },
  "grok-3-mini": {
    "description": "Lekki model, który najpierw analizuje przed rozmową. Działa szybko i inteligentnie, odpowiedni do zadań logicznych nie wymagających głębokiej wiedzy dziedzinowej, z możliwością śledzenia pierwotnego toku myślenia."
  },
  "grok-3-mini-fast": {
    "description": "Lekki model, który najpierw analizuje przed rozmową. Działa szybko i inteligentnie, odpowiedni do zadań logicznych nie wymagających głębokiej wiedzy dziedzinowej, z możliwością śledzenia pierwotnego toku myślenia."
  },
  "grok-4": {
    "description": "Nasz najnowszy i najpotężniejszy model flagowy, który wyróżnia się doskonałymi wynikami w przetwarzaniu języka naturalnego, obliczeniach matematycznych i rozumowaniu — to idealny wszechstronny zawodnik."
  },
  "grok-4-0709": {
    "description": "Grok 4 od xAI, wyposażony w potężne zdolności rozumowania."
  },
  "grok-4-fast-non-reasoning": {
    "description": "Z radością prezentujemy Grok 4 Fast, nasz najnowszy postęp w modelach inferencyjnych o wysokiej efektywności kosztowej."
  },
  "grok-4-fast-reasoning": {
    "description": "Z radością prezentujemy Grok 4 Fast, nasz najnowszy postęp w modelach inferencyjnych o wysokiej efektywności kosztowej."
  },
  "grok-code-fast-1": {
    "description": "Z radością przedstawiamy grok-code-fast-1, szybki i ekonomiczny model inferencyjny, który doskonale sprawdza się w kodowaniu agentów."
  },
  "groq/compound": {
    "description": "Compound to złożony system AI wspierany przez wiele dostępnych publicznie modeli w GroqCloud, który inteligentnie i selektywnie wykorzystuje narzędzia do odpowiadania na zapytania użytkowników."
  },
  "groq/compound-mini": {
    "description": "Compound-mini to złożony system AI wspierany przez dostępne publicznie modele w GroqCloud, który inteligentnie i selektywnie wykorzystuje narzędzia do odpowiadania na zapytania użytkowników."
  },
  "gryphe/mythomax-l2-13b": {
    "description": "MythoMax l2 13B to model językowy łączący kreatywność i inteligencję, zintegrowany z wieloma wiodącymi modelami."
  },
  "hunyuan-a13b": {
    "description": "Hunyuan to pierwszy hybrydowy model rozumowania, będący ulepszoną wersją hunyuan-standard-256K, z 80 miliardami parametrów i 13 miliardami aktywowanych. Domyślnie działa w trybie wolnego myślenia, ale obsługuje przełączanie między trybami szybkiego i wolnego myślenia za pomocą parametrów lub instrukcji; przełączanie odbywa się przez dodanie / no_think przed zapytaniem. Ogólne zdolności modelu znacznie przewyższają poprzednią generację, zwłaszcza w matematyce, naukach ścisłych, rozumieniu długich tekstów i zdolnościach agenta."
  },
  "hunyuan-code": {
    "description": "Najnowocześniejszy model generowania kodu Hunyuan, przeszkolony na bazie 200B wysokiej jakości danych kodu, z półrocznym treningiem na wysokiej jakości danych SFT, z wydłużonym oknem kontekstowym do 8K, zajmującym czołowe miejsca w automatycznych wskaźnikach oceny generowania kodu w pięciu językach; w ocenie jakościowej zadań kodowych w pięciu językach, osiąga wyniki w pierwszej lidze."
  },
  "hunyuan-functioncall": {
    "description": "Najnowocześniejszy model FunctionCall w architekturze MOE Hunyuan, przeszkolony na wysokiej jakości danych FunctionCall, z oknem kontekstowym o długości 32K, osiągający wiodące wyniki w wielu wymiarach oceny."
  },
  "hunyuan-large": {
    "description": "Model Hunyuan-large ma całkowitą liczbę parametrów wynoszącą około 389B, z aktywowanymi parametrami wynoszącymi około 52B, co czyni go obecnie największym i najlepiej działającym modelem MoE w architekturze Transformer w branży."
  },
  "hunyuan-large-longcontext": {
    "description": "Specjalizuje się w zadaniach związanych z długimi tekstami, takich jak streszczenia dokumentów i pytania i odpowiedzi dotyczące dokumentów, a także ma zdolność do obsługi ogólnych zadań generowania tekstu. Wykazuje doskonałe wyniki w analizie i generowaniu długich tekstów, skutecznie radząc sobie z złożonymi i szczegółowymi wymaganiami dotyczącymi przetwarzania długich treści."
  },
  "hunyuan-large-vision": {
    "description": "Model przeznaczony do scenariuszy rozumienia obrazów i tekstu, oparty na modelu Hunyuan Large, obsługujący dowolną rozdzielczość i wiele obrazów wraz z tekstem, generujący treści tekstowe, skupiający się na zadaniach związanych z rozumieniem obrazowo-tekstowym, z wyraźną poprawą zdolności wielojęzycznego rozumienia obrazów i tekstu."
  },
  "hunyuan-lite": {
    "description": "Zaktualizowana do struktury MOE, z oknem kontekstowym o długości 256k, prowadzi w wielu zestawach testowych w NLP, kodowaniu, matematyce i innych dziedzinach w porównaniu do wielu modeli open source."
  },
  "hunyuan-lite-vision": {
    "description": "Najnowocześniejszy model multimodalny 7B Hunyuan, z oknem kontekstowym 32K, wspierający multimodalne dialogi w języku chińskim i angielskim, rozpoznawanie obiektów w obrazach, zrozumienie dokumentów i tabel, multimodalną matematykę itp., z wynikami w wielu wymiarach lepszymi niż modele konkurencyjne 7B."
  },
  "hunyuan-pro": {
    "description": "Model długiego tekstu MOE-32K o skali bilionów parametrów. Osiąga absolutnie wiodący poziom w różnych benchmarkach, obsługując złożone instrukcje i wnioskowanie, posiadając zaawansowane umiejętności matematyczne, wspierając wywołania funkcji, z optymalizacjami w obszarach takich jak tłumaczenia wielojęzyczne, prawo finansowe i medyczne."
  },
  "hunyuan-role": {
    "description": "Najnowocześniejszy model odgrywania ról Hunyuan, stworzony przez oficjalne dostosowanie i trening Hunyuan, oparty na modelu Hunyuan i zestawie danych scenariuszy odgrywania ról, oferujący lepsze podstawowe wyniki w scenariuszach odgrywania ról."
  },
  "hunyuan-standard": {
    "description": "Zastosowano lepszą strategię routingu, jednocześnie łagodząc problemy z równoważeniem obciążenia i zbieżnością ekspertów. W przypadku długich tekstów wskaźnik 'znalezienia igły w stogu siana' osiąga 99,9%. MOE-32K oferuje lepszy stosunek jakości do ceny, równoważąc efektywność i cenę, umożliwiając przetwarzanie długich tekstów."
  },
  "hunyuan-standard-256K": {
    "description": "Zastosowano lepszą strategię routingu, jednocześnie łagodząc problemy z równoważeniem obciążenia i zbieżnością ekspertów. W przypadku długich tekstów wskaźnik 'znalezienia igły w stogu siana' osiąga 99,9%. MOE-256K dokonuje dalszych przełomów w długości i efektywności, znacznie rozszerzając możliwą długość wejścia."
  },
  "hunyuan-standard-vision": {
    "description": "Najnowocześniejszy model multimodalny Hunyuan, wspierający odpowiedzi w wielu językach, z równoważnymi zdolnościami w języku chińskim i angielskim."
  },
  "hunyuan-t1-20250321": {
    "description": "Kompleksowy model zdolności w naukach ścisłych i humanistycznych, z silną zdolnością do uchwycenia długich informacji tekstowych. Wspiera wnioskowanie w odpowiedzi na różnorodne trudności w matematyce, logice, naukach ścisłych i kodowaniu."
  },
  "hunyuan-t1-20250403": {
    "description": "Zwiększenie zdolności generowania kodu na poziomie projektu; poprawa jakości pisania generowanego tekstu; ulepszenie wieloetapowego rozumienia tematów, przestrzegania instrukcji typu tob oraz rozumienia słów; optymalizacja problemów z mieszanym użyciem uproszczonych i tradycyjnych znaków oraz mieszanym językiem chińsko-angielskim."
  },
  "hunyuan-t1-20250529": {
    "description": "Optymalizacja tworzenia tekstów, pisania esejów, ulepszenie umiejętności w kodowaniu frontendowym, matematyce, rozumowaniu logicznym oraz zwiększenie zdolności do przestrzegania instrukcji."
  },
  "hunyuan-t1-20250711": {
    "description": "Znacząca poprawa zdolności w zakresie zaawansowanej matematyki, logiki i kodowania, optymalizacja stabilności wyjścia modelu oraz zwiększenie zdolności do pracy z długimi tekstami."
  },
  "hunyuan-t1-latest": {
    "description": "Znacząco poprawia zdolności głównego modelu wolnego myślenia w zakresie zaawansowanej matematyki, złożonego wnioskowania, trudnego kodowania, przestrzegania instrukcji oraz jakości tworzenia tekstów."
  },
  "hunyuan-t1-vision-20250619": {
    "description": "Najnowszy model wielomodalny t1-vision Hunyuan z głębokim rozumowaniem, obsługujący natywne łańcuchy myślowe wielomodalne, z kompleksową poprawą w stosunku do poprzedniej domyślnej wersji modelu."
  },
  "hunyuan-t1-vision-20250916": {
    "description": "Model głębokiego rozumienia multimodalnego Hunyuan, wspierający natywne długie łańcuchy myślowe multimodalne, doskonały w rozwiązywaniu różnorodnych zadań obrazowych, z wyraźną poprawą w porównaniu do szybkiego trybu myślenia w dziedzinie nauk ścisłych."
  },
  "hunyuan-turbo": {
    "description": "Hunyuan to nowa generacja dużego modelu językowego w wersji próbnej, wykorzystująca nową strukturę modelu mieszanych ekspertów (MoE), która w porównaniu do hunyuan-pro charakteryzuje się szybszą efektywnością wnioskowania i lepszymi wynikami."
  },
  "hunyuan-turbo-20241223": {
    "description": "Optymalizacja tej wersji: skalowanie danych instrukcji, znaczne zwiększenie ogólnej zdolności generalizacji modelu; znaczne zwiększenie zdolności w zakresie matematyki, kodowania i rozumowania logicznego; optymalizacja zdolności związanych z rozumieniem tekstu i słów; optymalizacja jakości generowania treści w tworzeniu tekstów."
  },
  "hunyuan-turbo-latest": {
    "description": "Ogólna optymalizacja doświadczeń, w tym zrozumienie NLP, tworzenie tekstów, rozmowy, pytania i odpowiedzi, tłumaczenia, obszary tematyczne itp.; zwiększenie humanizacji, optymalizacja inteligencji emocjonalnej modelu; poprawa zdolności modelu do aktywnego wyjaśniania w przypadku niejasnych intencji; poprawa zdolności do rozwiązywania problemów związanych z analizą słów; poprawa jakości i interaktywności twórczości; poprawa doświadczeń w wielokrotnych interakcjach."
  },
  "hunyuan-turbo-vision": {
    "description": "Nowa generacja flagowego modelu językowo-wizualnego Hunyuan, wykorzystująca nową strukturę modelu mieszanych ekspertów (MoE), z pełnym zwiększeniem zdolności w zakresie podstawowego rozpoznawania, tworzenia treści, pytań i odpowiedzi oraz analizy i rozumowania w porównaniu do poprzedniej generacji modeli."
  },
  "hunyuan-turbos-20250313": {
    "description": "Ujednolicenie stylu kroków rozwiązywania zadań matematycznych, wzmocnienie wieloetapowego zadawania pytań matematycznych. Optymalizacja stylu odpowiedzi w tworzeniu tekstów, eliminacja sztuczności AI, wzbogacenie języka."
  },
  "hunyuan-turbos-20250416": {
    "description": "Aktualizacja bazy pretrenowania, wzmacniająca zdolność rozumienia i przestrzegania instrukcji; w fazie dostrajania poprawa umiejętności matematycznych, programistycznych, logicznych i nauk ścisłych; podniesienie jakości twórczości literackiej, rozumienia tekstu, dokładności tłumaczeń oraz wiedzy ogólnej; wzmocnienie zdolności agentów w różnych dziedzinach, ze szczególnym naciskiem na rozumienie wieloetapowych dialogów."
  },
  "hunyuan-turbos-20250604": {
    "description": "Ulepszona baza pretrenowania, poprawa umiejętności pisania i rozumienia tekstu, znaczne zwiększenie zdolności w kodowaniu i naukach ścisłych, ciągłe doskonalenie w realizacji złożonych poleceń."
  },
  "hunyuan-turbos-20250926": {
    "description": "Ulepszona jakość danych bazy pretrenowania. Optymalizacja strategii treningowej fazy post-treningowej, ciągłe podnoszenie zdolności agenta, obsługi języków angielskiego i mniejszych języków, zgodności z instrukcjami, kodowania oraz nauk ścisłych."
  },
  "hunyuan-turbos-latest": {
    "description": "hunyuan-TurboS to najnowsza wersja flagowego modelu Hunyuan, oferująca silniejsze zdolności myślenia i lepsze efekty doświadczenia."
  },
  "hunyuan-turbos-longtext-128k-20250325": {
    "description": "Specjalizuje się w zadaniach związanych z długimi tekstami, takimi jak streszczenia dokumentów i pytania do dokumentów, a także ma zdolność do generowania ogólnych tekstów. W analizie i generowaniu długich tekstów wykazuje doskonałe wyniki, skutecznie radząc sobie z złożonymi i szczegółowymi wymaganiami przetwarzania długich treści."
  },
  "hunyuan-turbos-role-plus": {
    "description": "Najnowsza wersja modelu do odgrywania ról Hunyuan, oficjalnie dostrojona przez Hunyuan, oparta na modelu Hunyuan i wzbogacona o dane scenariuszy odgrywania ról, zapewniająca lepsze podstawowe efekty w tych scenariuszach."
  },
  "hunyuan-turbos-vision": {
    "description": "Model przeznaczony do zadań rozumienia obrazów i tekstu, oparty na najnowszym modelu turbos Hunyuan, będący nową generacją flagowego modelu wizualno-językowego. Skupia się na zadaniach związanych z rozpoznawaniem obiektów na obrazach, pytaniami i odpowiedziami opartymi na wiedzy, tworzeniem tekstów reklamowych, rozwiązywaniem problemów na podstawie zdjęć i innych, z kompleksową poprawą w stosunku do poprzedniej generacji."
  },
  "hunyuan-turbos-vision-20250619": {
    "description": "Najnowszy flagowy model wizualno-językowy turbos-vision Hunyuan, z kompleksową poprawą w zadaniach związanych z rozumieniem obrazów i tekstu, w tym rozpoznawaniem obiektów na obrazach, pytaniami i odpowiedziami opartymi na wiedzy, tworzeniem tekstów reklamowych, rozwiązywaniem problemów na podstawie zdjęć, w porównaniu do poprzedniej domyślnej wersji modelu."
  },
  "hunyuan-vision": {
    "description": "Najnowocześniejszy model multimodalny Hunyuan, wspierający generowanie treści tekstowych na podstawie obrazów i tekstu."
  },
  "image-01": {
    "description": "Nowy model generowania obrazów o delikatnej jakości wizualnej, wspierający generację obrazów na podstawie tekstu oraz obrazów na podstawie obrazów."
  },
  "image-01-live": {
    "description": "Model generowania obrazów o delikatnej jakości wizualnej, wspierający generację obrazów na podstawie tekstu z możliwością ustawienia stylu."
  },
  "imagen-4.0-fast-generate-001": {
    "description": "Imagen — czwarta generacja serii modeli tekst-na-obraz, wersja Fast"
  },
  "imagen-4.0-generate-001": {
    "description": "Seria modeli Imagen czwartej generacji do tworzenia obrazów na podstawie tekstu"
  },
  "imagen-4.0-generate-preview-06-06": {
    "description": "Seria modeli tekst-na-obraz Imagen czwartej generacji"
  },
  "imagen-4.0-ultra-generate-001": {
    "description": "Imagen — seria modeli przekształcających tekst w obraz czwartej generacji, wersja Ultra"
  },
  "imagen-4.0-ultra-generate-preview-06-06": {
    "description": "Seria modeli tekst-na-obraz Imagen czwartej generacji, wersja Ultra"
  },
  "inception/mercury-coder-small": {
    "description": "Mercury Coder Small to idealny wybór do generowania, debugowania i refaktoryzacji kodu, oferujący minimalne opóźnienia."
  },
  "inclusionAI/Ling-flash-2.0": {
    "description": "Ling-flash-2.0 to trzeci model z serii architektury Ling 2.0 wydany przez zespół Bailing z Ant Group. Jest to model hybrydowy ekspertów (MoE) o łącznej liczbie parametrów 100 miliardów, z aktywacją jedynie 6,1 miliarda parametrów na token (48 miliardów bez uwzględnienia wektorów osadzeń). Jako lekka konfiguracja modelu, Ling-flash-2.0 wykazuje w wielu autorytatywnych testach wydajność porównywalną lub przewyższającą modele gęste (Dense) o wielkości 40 miliardów parametrów oraz większe modele MoE. Model ten ma na celu eksplorację efektywnych ścieżek w kontekście powszechnego przekonania, że „duży model to duża liczba parametrów”, poprzez zaawansowany projekt architektury i strategię treningową."
  },
  "inclusionAI/Ling-mini-2.0": {
    "description": "Ling-mini-2.0 to mały, wysokowydajny duży model językowy oparty na architekturze MoE. Posiada 16 miliardów parametrów, ale aktywuje tylko 1,4 miliarda na token (789 milionów bez osadzeń), co zapewnia bardzo wysoką szybkość generowania. Dzięki efektywnemu projektowi MoE i dużej, wysokiej jakości bazie treningowej, mimo niskiej liczby aktywowanych parametrów, Ling-mini-2.0 osiąga w zadaniach downstream wydajność porównywalną z najlepszymi modelami gęstymi poniżej 10 miliardów parametrów oraz większymi modelami MoE."
  },
  "inclusionAI/Ring-flash-2.0": {
    "description": "Ring-flash-2.0 to wysoko wydajny model myślenia głęboko zoptymalizowany na bazie Ling-flash-2.0-base. Wykorzystuje architekturę hybrydowych ekspertów (MoE) z łączną liczbą parametrów 100 miliardów, aktywując podczas inferencji tylko 6,1 miliarda parametrów. Model rozwiązuje problem niestabilności treningu MoE w uczeniu ze wzmocnieniem (RL) dzięki autorskiej metodzie icepop, co pozwala na ciągłe zwiększanie zdolności do złożonego wnioskowania podczas długotrwałego treningu. Ring-flash-2.0 osiągnął znaczące przełomy w trudnych benchmarkach, takich jak konkursy matematyczne, generowanie kodu i rozumowanie logiczne. Jego wydajność przewyższa najlepsze modele gęste poniżej 40 miliardów parametrów i jest porównywalna z większymi otwartoźródłowymi modelami MoE oraz zamkniętymi modelami myślenia o wysokiej wydajności. Choć skupiony na złożonym wnioskowaniu, model dobrze radzi sobie także z zadaniami kreatywnego pisania. Dzięki efektywnej architekturze Ring-flash-2.0 oferuje wysoką wydajność przy szybkim inferowaniu, co znacząco obniża koszty wdrożenia modeli myślenia w środowiskach o dużej równoczesności."
  },
  "internlm/internlm2_5-7b-chat": {
    "description": "InternLM2.5 oferuje inteligentne rozwiązania dialogowe w różnych scenariuszach."
  },
  "internlm2.5-latest": {
    "description": "Nasza najnowsza seria modeli, charakteryzująca się doskonałymi osiągami wnioskowania, obsługująca długość kontekstu do 1M oraz lepsze możliwości śledzenia instrukcji i wywoływania narzędzi."
  },
  "internlm3-latest": {
    "description": "Nasza najnowsza seria modeli, charakteryzująca się doskonałą wydajnością wnioskowania, prowadzi wśród modeli open-source o podobnej skali. Domyślnie wskazuje na naszą najnowszą wersję modelu InternLM3."
  },
  "internvl2.5-latest": {
    "description": "Wersja InternVL2.5, którą nadal utrzymujemy, charakteryzuje się doskonałą i stabilną wydajnością. Domyślnie wskazuje na nasz najnowszy model z serii InternVL2.5, obecnie wskazuje na internvl2.5-78b."
  },
  "internvl3-latest": {
    "description": "Nasz najnowszy model multimodalny, który ma silniejsze zdolności rozumienia tekstu i obrazów oraz długoterminowego rozumienia obrazów, osiągający wyniki porównywalne z najlepszymi modelami zamkniętymi. Domyślnie wskazuje na nasz najnowszy model z serii InternVL, obecnie wskazuje na internvl3-78b."
  },
  "irag-1.0": {
    "description": "Opracowana przez Baidu technologia iRAG (image based RAG) to wzmacniana wyszukiwaniem generacja obrazów na podstawie tekstu, łącząca miliardowe zasoby obrazów Baidu z potężnymi możliwościami modelu bazowego. Pozwala generować niezwykle realistyczne obrazy, znacznie przewyższając natywne systemy generacji tekst-na-obraz, eliminując sztuczny efekt AI i przy niskich kosztach. iRAG cechuje się brakiem halucynacji, ultra-realistycznym wyglądem i natychmiastową dostępnością."
  },
  "jamba-large": {
    "description": "Nasz najsilniejszy i najbardziej zaawansowany model, zaprojektowany do obsługi złożonych zadań na poziomie przedsiębiorstw, oferujący doskonałą wydajność."
  },
  "jamba-mini": {
    "description": "Najbardziej efektywny model w swojej klasie, łączący szybkość z jakością, o mniejszych rozmiarach."
  },
  "jina-deepsearch-v1": {
    "description": "Głębokie wyszukiwanie łączy wyszukiwanie w sieci, czytanie i wnioskowanie, umożliwiając kompleksowe badania. Możesz to traktować jako agenta, który przyjmuje Twoje zadania badawcze - przeprowadza szerokie poszukiwania i wielokrotne iteracje, zanim poda odpowiedź. Proces ten obejmuje ciągłe badania, wnioskowanie i rozwiązywanie problemów z różnych perspektyw. To zasadniczo różni się od standardowych dużych modeli, które generują odpowiedzi bezpośrednio z wstępnie wytrenowanych danych oraz od tradycyjnych systemów RAG, które polegają na jednorazowym powierzchownym wyszukiwaniu."
  },
  "kimi-k2": {
    "description": "Kimi-K2 to podstawowy model architektury MoE opracowany przez Moonshot AI, wyposażony w potężne zdolności kodowania i agenta, z łączną liczbą parametrów 1 biliona i 32 miliardami aktywowanych parametrów. W testach wydajności w zakresie ogólnej wiedzy, programowania, matematyki i zadań agenta model K2 przewyższa inne popularne otwarte modele."
  },
  "kimi-k2-0711-preview": {
    "description": "kimi-k2 to podstawowy model architektury MoE o potężnych zdolnościach kodowania i agenta, z łączną liczbą parametrów 1T i 32B aktywowanych parametrów. W testach wydajności na benchmarkach obejmujących ogólne rozumowanie, programowanie, matematykę i agentów model K2 przewyższa inne popularne modele open source."
  },
  "kimi-k2-0905-preview": {
    "description": "Model kimi-k2-0905-preview obsługuje długość kontekstu do 256k, oferując silniejsze zdolności Agentic Coding, bardziej estetyczny i praktyczny kod frontendowy oraz lepsze rozumienie kontekstu."
  },
  "kimi-k2-turbo-preview": {
    "description": "kimi-k2 to bazowy model z architekturą MoE, dysponujący wyjątkowymi możliwościami w zakresie kodowania i agentów, z łączną liczbą parametrów 1T oraz 32B parametrów aktywacyjnych. W standardowych testach wydajności (benchmarkach) dla głównych kategorii takich jak wnioskowanie z wiedzy ogólnej, programowanie, matematyka i agenty, model K2 przewyższa inne popularne otwarte modele."
  },
  "kimi-k2:1t": {
    "description": "Kimi K2 to duży, hybrydowy model ekspertowy (MoE) języka opracowany przez AI z Ciemnej Strony Księżyca, posiadający 1 bilion parametrów ogółem oraz 32 miliardy aktywowanych parametrów na pojedyncze przejście w przód. Model jest zoptymalizowany pod kątem zdolności agentowych, w tym zaawansowanego korzystania z narzędzi, wnioskowania i syntezy kodu."
  },
  "kimi-latest": {
    "description": "Produkt Kimi Smart Assistant korzysta z najnowszego modelu Kimi, który może zawierać cechy jeszcze niestabilne. Obsługuje zrozumienie obrazów i automatycznie wybiera model 8k/32k/128k jako model rozliczeniowy w zależności od długości kontekstu żądania."
  },
  "kimi-thinking-preview": {
    "description": "Model kimi-thinking-preview dostarczany przez Moon’s Dark Side to multimodalny model myślenia z umiejętnościami ogólnego i głębokiego rozumowania, który pomaga rozwiązywać bardziej złożone i trudniejsze problemy."
  },
  "learnlm-1.5-pro-experimental": {
    "description": "LearnLM to eksperymentalny model językowy, specyficzny dla zadań, przeszkolony zgodnie z zasadami nauki o uczeniu się, który może przestrzegać systemowych instrukcji w scenariuszach nauczania i uczenia się, pełniąc rolę eksperta mentora."
  },
  "learnlm-2.0-flash-experimental": {
    "description": "LearnLM to eksperymentalny model językowy, specyficzny dla zadań, przeszkolony zgodnie z zasadami nauki o uczeniu się, który może przestrzegać systemowych instrukcji w scenariuszach nauczania i uczenia się, pełniąc rolę eksperta mentora."
  },
  "lite": {
    "description": "Spark Lite to lekki model językowy o dużej skali, charakteryzujący się niezwykle niskim opóźnieniem i wysoką wydajnością przetwarzania, całkowicie darmowy i otwarty, wspierający funkcje wyszukiwania w czasie rzeczywistym. Jego cechy szybkiej reakcji sprawiają, że doskonale sprawdza się w zastosowaniach inferencyjnych na urządzeniach o niskiej mocy obliczeniowej oraz w dostosowywaniu modeli, oferując użytkownikom znakomity stosunek kosztów do korzyści oraz inteligentne doświadczenie, szczególnie w kontekście pytań i odpowiedzi, generowania treści oraz wyszukiwania."
  },
  "llama-2-7b-chat": {
    "description": "Llama2 to seria modeli językowych (LLM) opracowanych i udostępnionych przez Meta, obejmująca modele generujące tekst o różnej skali, od 7 miliardów do 70 miliardów parametrów, które przeszły wstępną naukę i dostrajanie. Na poziomie architektury, Llama2 jest modelem językowym optymalizowanym za pomocą architektury transformerowej. Zdolność do dostosowywania modeli do preferencji ludzi pod względem użyteczności i bezpieczeństwa została osiągnięta poprzez nadzorowane dostrajanie (SFT) i uczenie wzmacnianie z uwzględnieniem opinii ludzi (RLHF). Llama2 osiąga lepsze wyniki niż poprzednia seria Llama na wielu zbiorach danych akademickich, co dostarcza inspiracji dla projektowania i tworzenia wielu innych modeli."
  },
  "llama-3.1-70b-versatile": {
    "description": "Llama 3.1 70B oferuje potężne możliwości wnioskowania AI, odpowiednie do złożonych zastosowań, wspierające ogromne przetwarzanie obliczeniowe przy zachowaniu efektywności i dokładności."
  },
  "llama-3.1-8b-instant": {
    "description": "Llama 3.1 8B to model o wysokiej wydajności, oferujący szybkie możliwości generowania tekstu, idealny do zastosowań wymagających dużej efektywności i opłacalności."
  },
  "llama-3.1-instruct": {
    "description": "Model Llama 3.1 zoptymalizowany do rozmów przewyższa wiele istniejących open-source modeli czatowych w standardowych testach branżowych."
  },
  "llama-3.2-11b-vision-instruct": {
    "description": "Wyjątkowe zdolności wnioskowania wizualnego na obrazach o wysokiej rozdzielczości, idealne do zastosowań związanych ze zrozumieniem wizualnym."
  },
  "llama-3.2-11b-vision-preview": {
    "description": "Llama 3.2 jest zaprojektowana do obsługi zadań łączących dane wizualne i tekstowe. Wykazuje doskonałe wyniki w zadaniach takich jak opisywanie obrazów i wizualne pytania i odpowiedzi, przekraczając przepaść między generowaniem języka a wnioskowaniem wizualnym."
  },
  "llama-3.2-90b-vision-instruct": {
    "description": "Zaawansowane zdolności wnioskowania obrazów dla zastosowań w agentach zrozumienia wizualnego."
  },
  "llama-3.2-90b-vision-preview": {
    "description": "Llama 3.2 jest zaprojektowana do obsługi zadań łączących dane wizualne i tekstowe. Wykazuje doskonałe wyniki w zadaniach takich jak opisywanie obrazów i wizualne pytania i odpowiedzi, przekraczając przepaść między generowaniem języka a wnioskowaniem wizualnym."
  },
  "llama-3.2-vision-instruct": {
    "description": "Model Llama 3.2-Vision zoptymalizowany jest do rozpoznawania wizualnego, wnioskowania na podstawie obrazów, opisywania obrazów oraz odpowiadania na typowe pytania związane z obrazami."
  },
  "llama-3.3-70b-instruct": {
    "description": "Llama 3.3 to najnowocześniejszy wielojęzyczny, otwarty model językowy z serii Llama, który oferuje wydajność porównywalną z modelem 405B przy bardzo niskich kosztach. Opiera się na strukturze Transformer i poprawia użyteczność oraz bezpieczeństwo dzięki nadzorowanemu dostrajaniu (SFT) i uczeniu ze wzmocnieniem na podstawie ludzkich opinii (RLHF). Jego wersja dostosowana do instrukcji jest zoptymalizowana do wielojęzycznych rozmów i w wielu branżowych benchmarkach przewyższa wiele otwartych i zamkniętych modeli czatu. Data graniczna wiedzy to grudzień 2023."
  },
  "llama-3.3-70b-versatile": {
    "description": "Meta Llama 3.3 to wielojęzyczny model językowy (LLM) 70B, pretrenowany i dostosowany do poleceń. Model Llama 3.3, dostosowany do poleceń, jest zoptymalizowany do zastosowań w dialogach wielojęzycznych i przewyższa wiele dostępnych modeli czatu, zarówno open source, jak i zamkniętych, w popularnych branżowych benchmarkach."
  },
  "llama-3.3-instruct": {
    "description": "Model Llama 3.3 zoptymalizowany do rozmów, który w standardowych testach branżowych przewyższa wiele istniejących modeli czatowych o otwartym kodzie."
  },
  "llama3-70b-8192": {
    "description": "Meta Llama 3 70B oferuje niezrównane możliwości przetwarzania złożoności, dostosowane do projektów o wysokich wymaganiach."
  },
  "llama3-8b-8192": {
    "description": "Meta Llama 3 8B zapewnia wysoką jakość wydajności wnioskowania, odpowiednią do różnych zastosowań."
  },
  "llama3-groq-70b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 70B Tool Use oferuje potężne możliwości wywoływania narzędzi, wspierając efektywne przetwarzanie złożonych zadań."
  },
  "llama3-groq-8b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 8B Tool Use to model zoptymalizowany do efektywnego korzystania z narzędzi, wspierający szybkie obliczenia równoległe."
  },
  "llama3.1": {
    "description": "Llama 3.1 to wiodący model wydany przez Meta, obsługujący do 405B parametrów, mogący być stosowany w złożonych dialogach, tłumaczeniach wielojęzycznych i analizie danych."
  },
  "llama3.1:405b": {
    "description": "Llama 3.1 to wiodący model wydany przez Meta, obsługujący do 405B parametrów, mogący być stosowany w złożonych dialogach, tłumaczeniach wielojęzycznych i analizie danych."
  },
  "llama3.1:70b": {
    "description": "Llama 3.1 to wiodący model wydany przez Meta, obsługujący do 405B parametrów, mogący być stosowany w złożonych dialogach, tłumaczeniach wielojęzycznych i analizie danych."
  },
  "llava": {
    "description": "LLaVA to multimodalny model łączący kodery wizualne i Vicunę, przeznaczony do silnego rozumienia wizualnego i językowego."
  },
  "llava-v1.5-7b-4096-preview": {
    "description": "LLaVA 1.5 7B oferuje zintegrowane możliwości przetwarzania wizualnego, generując złożone wyjścia na podstawie informacji wizualnych."
  },
  "llava:13b": {
    "description": "LLaVA to multimodalny model łączący kodery wizualne i Vicunę, przeznaczony do silnego rozumienia wizualnego i językowego."
  },
  "llava:34b": {
    "description": "LLaVA to multimodalny model łączący kodery wizualne i Vicunę, przeznaczony do silnego rozumienia wizualnego i językowego."
  },
  "magistral-medium-latest": {
    "description": "Magistral Medium 1.2 to zaawansowany model inferencyjny z obsługą wizualną, wydany przez Mistral AI we wrześniu 2025 roku."
  },
  "magistral-small-2509": {
    "description": "Magistral Small 1.2 to otwartoźródłowy, kompaktowy model inferencyjny z obsługą wizualną, wydany przez Mistral AI we wrześniu 2025 roku."
  },
  "mathstral": {
    "description": "MathΣtral zaprojektowany do badań naukowych i wnioskowania matematycznego, oferujący efektywne możliwości obliczeniowe i interpretację wyników."
  },
  "max-32k": {
    "description": "Spark Max 32K jest wyposażony w dużą zdolność przetwarzania kontekstu, oferując silniejsze zrozumienie kontekstu i zdolności logicznego wnioskowania, obsługując teksty o długości do 32K tokenów, co czyni go odpowiednim do czytania długich dokumentów, prywatnych pytań i odpowiedzi oraz innych scenariuszy."
  },
  "megrez-3b-instruct": {
    "description": "Megrez-3B-Instruct to model językowy duży skali, w pełni samodzielnie wytrenowany przez Qwen. Megrez-3B-Instruct ma na celu stworzenie szybkiego, kompaktowego i łatwego w użyciu rozwiązania inteligentnego na urządzeniach klienckich, opartego na koncepcji integracji oprogramowania i sprzętu."
  },
  "meta-llama-3-70b-instruct": {
    "description": "Potężny model z 70 miliardami parametrów, doskonały w rozumowaniu, kodowaniu i szerokich zastosowaniach językowych."
  },
  "meta-llama-3-8b-instruct": {
    "description": "Wszechstronny model z 8 miliardami parametrów, zoptymalizowany do zadań dialogowych i generacji tekstu."
  },
  "meta-llama-3.1-405b-instruct": {
    "description": "Modele tekstowe Llama 3.1 dostosowane do instrukcji, zoptymalizowane do wielojęzycznych przypadków użycia dialogowego, przewyższają wiele dostępnych modeli open source i zamkniętych w powszechnych benchmarkach branżowych."
  },
  "meta-llama-3.1-70b-instruct": {
    "description": "Modele tekstowe Llama 3.1 dostosowane do instrukcji, zoptymalizowane do wielojęzycznych przypadków użycia dialogowego, przewyższają wiele dostępnych modeli open source i zamkniętych w powszechnych benchmarkach branżowych."
  },
  "meta-llama-3.1-8b-instruct": {
    "description": "Modele tekstowe Llama 3.1 dostosowane do instrukcji, zoptymalizowane do wielojęzycznych przypadków użycia dialogowego, przewyższają wiele dostępnych modeli open source i zamkniętych w powszechnych benchmarkach branżowych."
  },
  "meta-llama/Llama-2-13b-chat-hf": {
    "description": "LLaMA-2 Chat (13B) oferuje doskonałe możliwości przetwarzania języka i znakomite doświadczenie interakcji."
  },
  "meta-llama/Llama-2-70b-hf": {
    "description": "LLaMA-2 oferuje doskonałe zdolności przetwarzania języka i znakomite doświadczenie interakcyjne."
  },
  "meta-llama/Llama-3-70b-chat-hf": {
    "description": "LLaMA-3 Chat (70B) to potężny model czatu, wspierający złożone potrzeby dialogowe."
  },
  "meta-llama/Llama-3-8b-chat-hf": {
    "description": "LLaMA-3 Chat (8B) oferuje wsparcie dla wielu języków, obejmując bogatą wiedzę z różnych dziedzin."
  },
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 zaprojektowana do przetwarzania zadań łączących dane wizualne i tekstowe. Doskonała w zadaniach takich jak opisywanie obrazów i wizualne pytania odpowiedzi, przekracza granice między generowaniem języka a wnioskowaniem wizualnym."
  },
  "meta-llama/Llama-3.2-3B-Instruct-Turbo": {
    "description": "LLaMA 3.2 zaprojektowana do przetwarzania zadań łączących dane wizualne i tekstowe. Doskonała w zadaniach takich jak opisywanie obrazów i wizualne pytania odpowiedzi, przekracza granice między generowaniem języka a wnioskowaniem wizualnym."
  },
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 zaprojektowana do przetwarzania zadań łączących dane wizualne i tekstowe. Doskonała w zadaniach takich jak opisywanie obrazów i wizualne pytania odpowiedzi, przekracza granice między generowaniem języka a wnioskowaniem wizualnym."
  },
  "meta-llama/Llama-3.3-70B-Instruct-Turbo": {
    "description": "Meta Llama 3.3 to wielojęzyczny model językowy (LLM) o skali 70B (wejście/wyjście tekstowe), będący modelem generacyjnym wstępnie wytrenowanym i dostosowanym do instrukcji. Model Llama 3.3 dostosowany do instrukcji jest zoptymalizowany pod kątem zastosowań w dialogach wielojęzycznych i przewyższa wiele dostępnych modeli open-source i zamkniętych w popularnych testach branżowych."
  },
  "meta-llama/Llama-Vision-Free": {
    "description": "LLaMA 3.2 zaprojektowana do przetwarzania zadań łączących dane wizualne i tekstowe. Doskonała w zadaniach takich jak opisywanie obrazów i wizualne pytania odpowiedzi, przekracza granice między generowaniem języka a wnioskowaniem wizualnym."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
    "description": "Llama 3 70B Instruct Lite jest idealny do środowisk wymagających wysokiej wydajności i niskiego opóźnienia."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": {
    "description": "Llama 3 70B Instruct Turbo oferuje doskonałe możliwości rozumienia i generowania języka, idealny do najbardziej wymagających zadań obliczeniowych."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite": {
    "description": "Llama 3 8B Instruct Lite jest dostosowany do środowisk z ograniczonymi zasobami, oferując doskonałą równowagę wydajności."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": {
    "description": "Llama 3 8B Instruct Turbo to wydajny model językowy, wspierający szeroki zakres zastosowań."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "description": "LLaMA 3.1 405B to potężny model do wstępnego uczenia się i dostosowywania instrukcji."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
    "description": "Model Llama 3.1 Turbo 405B oferuje ogromną pojemność kontekstową dla przetwarzania dużych danych, wyróżniając się w zastosowaniach sztucznej inteligencji o dużej skali."
  },
  "meta-llama/Meta-Llama-3.1-70B": {
    "description": "Llama 3.1 to wiodący model wydany przez Meta, wspierający do 405B parametrów, mogący być stosowany w złożonych rozmowach, tłumaczeniach wielojęzycznych i analizie danych."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
    "description": "Model Llama 3.1 70B został starannie dostosowany do aplikacji o dużym obciążeniu, kwantyzowany do FP8, co zapewnia wyższą wydajność obliczeniową i dokładność, gwarantując doskonałe osiągi w złożonych scenariuszach."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
    "description": "Model Llama 3.1 8B wykorzystuje kwantyzację FP8, obsługując do 131,072 kontekstowych tokenów, wyróżniając się wśród modeli open source, idealny do złożonych zadań, przewyższający wiele branżowych standardów."
  },
  "meta-llama/llama-3-70b-instruct": {
    "description": "Llama 3 70B Instruct zoptymalizowano do wysokiej jakości dialogów, osiągając znakomite wyniki w różnych ocenach ludzkich."
  },
  "meta-llama/llama-3-8b-instruct": {
    "description": "Llama 3 8B Instruct zoptymalizowano do wysokiej jakości scenariuszy dialogowych, osiągając lepsze wyniki niż wiele modeli zamkniętych."
  },
  "meta-llama/llama-3.1-70b-instruct": {
    "description": "Llama 3.1 70B Instruct zaprojektowano z myślą o wysokiej jakości dialogach, osiągając znakomite wyniki w ocenach ludzkich, szczególnie w scenariuszach o wysokiej interakcji."
  },
  "meta-llama/llama-3.1-8b-instruct": {
    "description": "Llama 3.1 8B Instruct to najnowsza wersja wydana przez Meta, zoptymalizowana do wysokiej jakości scenariuszy dialogowych, przewyższająca wiele wiodących modeli zamkniętych."
  },
  "meta-llama/llama-3.1-8b-instruct:free": {
    "description": "LLaMA 3.1 oferuje wsparcie dla wielu języków i jest jednym z wiodących modeli generacyjnych w branży."
  },
  "meta-llama/llama-3.2-11b-vision-instruct": {
    "description": "LLaMA 3.2 jest zaprojektowana do przetwarzania zadań łączących dane wizualne i tekstowe. Wykazuje doskonałe wyniki w zadaniach takich jak opisywanie obrazów i wizualne pytania i odpowiedzi, przekraczając granice między generowaniem języka a wnioskowaniem wizualnym."
  },
  "meta-llama/llama-3.2-3b-instruct": {
    "description": "meta-llama/llama-3.2-3b-instruct"
  },
  "meta-llama/llama-3.2-90b-vision-instruct": {
    "description": "LLaMA 3.2 jest zaprojektowana do przetwarzania zadań łączących dane wizualne i tekstowe. Wykazuje doskonałe wyniki w zadaniach takich jak opisywanie obrazów i wizualne pytania i odpowiedzi, przekraczając granice między generowaniem języka a wnioskowaniem wizualnym."
  },
  "meta-llama/llama-3.3-70b-instruct": {
    "description": "Llama 3.3 to najnowocześniejszy wielojęzyczny, otwarty model językowy z serii Llama, który oferuje wydajność porównywalną z modelem 405B przy bardzo niskich kosztach. Opiera się na strukturze Transformer i poprawia użyteczność oraz bezpieczeństwo dzięki nadzorowanemu dostrajaniu (SFT) i uczeniu ze wzmocnieniem na podstawie ludzkich opinii (RLHF). Jego wersja dostosowana do instrukcji jest zoptymalizowana do wielojęzycznych rozmów i w wielu branżowych benchmarkach przewyższa wiele otwartych i zamkniętych modeli czatu. Data graniczna wiedzy to grudzień 2023."
  },
  "meta-llama/llama-3.3-70b-instruct:free": {
    "description": "Llama 3.3 to najnowocześniejszy wielojęzyczny, otwarty model językowy z serii Llama, który oferuje wydajność porównywalną z modelem 405B przy bardzo niskich kosztach. Opiera się na strukturze Transformer i poprawia użyteczność oraz bezpieczeństwo dzięki nadzorowanemu dostrajaniu (SFT) i uczeniu ze wzmocnieniem na podstawie ludzkich opinii (RLHF). Jego wersja dostosowana do instrukcji jest zoptymalizowana do wielojęzycznych rozmów i w wielu branżowych benchmarkach przewyższa wiele otwartych i zamkniętych modeli czatu. Data graniczna wiedzy to grudzień 2023."
  },
  "meta.llama3-1-405b-instruct-v1:0": {
    "description": "Meta Llama 3.1 405B Instruct to największy i najpotężniejszy model w rodzinie modeli Llama 3.1 Instruct. Jest to wysoko zaawansowany model do dialogów, wnioskowania i generowania danych, który może być również używany jako podstawa do specjalistycznego, ciągłego wstępnego szkolenia lub dostosowywania w określonych dziedzinach. Llama 3.1 oferuje wielojęzyczne duże modele językowe (LLM), które są zestawem wstępnie wytrenowanych, dostosowanych do instrukcji modeli generacyjnych, obejmujących rozmiary 8B, 70B i 405B (wejście/wyjście tekstowe). Modele tekstowe Llama 3.1 dostosowane do instrukcji (8B, 70B, 405B) zostały zoptymalizowane do zastosowań w wielojęzycznych dialogach i przewyższają wiele dostępnych modeli czatu open source w powszechnych testach branżowych. Llama 3.1 jest zaprojektowana do użytku komercyjnego i badawczego w wielu językach. Modele tekstowe dostosowane do instrukcji nadają się do czatu w stylu asystenta, podczas gdy modele wstępnie wytrenowane mogą być dostosowane do różnych zadań generowania języka naturalnego. Modele Llama 3.1 wspierają również wykorzystanie ich wyjść do poprawy innych modeli, w tym generowania danych syntetycznych i udoskonalania. Llama 3.1 jest modelem językowym autoregresywnym opartym na zoptymalizowanej architekturze transformatora. Dostosowane wersje wykorzystują nadzorowane dostosowywanie (SFT) oraz uczenie się ze wzmocnieniem z ludzkim feedbackiem (RLHF), aby odpowiadać ludzkim preferencjom dotyczącym pomocności i bezpieczeństwa."
  },
  "meta.llama3-1-70b-instruct-v1:0": {
    "description": "Zaktualizowana wersja Meta Llama 3.1 70B Instruct, obejmująca rozszerzone 128K długości kontekstu, wielojęzyczność i poprawione zdolności wnioskowania. Llama 3.1 oferuje wielojęzyczne modele językowe (LLMs) jako zestaw wstępnie wytrenowanych, dostosowanych do instrukcji modeli generacyjnych, w tym rozmiarów 8B, 70B i 405B (wejście/wyjście tekstowe). Modele tekstowe Llama 3.1 dostosowane do instrukcji (8B, 70B, 405B) są zoptymalizowane do zastosowań w dialogach wielojęzycznych i przewyższają wiele dostępnych modeli czatu w powszechnych testach branżowych. Llama 3.1 jest przeznaczona do zastosowań komercyjnych i badawczych w wielu językach. Modele tekstowe dostosowane do instrukcji są odpowiednie do czatu podobnego do asystenta, podczas gdy modele wstępnie wytrenowane mogą być dostosowane do różnych zadań generowania języka naturalnego. Modele Llama 3.1 wspierają również wykorzystanie wyników ich modeli do poprawy innych modeli, w tym generowania danych syntetycznych i rafinacji. Llama 3.1 jest modelem językowym autoregresywnym, wykorzystującym zoptymalizowaną architekturę transformatora. Wersje dostosowane wykorzystują nadzorowane dostrajanie (SFT) i uczenie się ze wzmocnieniem z ludzkim feedbackiem (RLHF), aby dostosować się do ludzkich preferencji dotyczących pomocności i bezpieczeństwa."
  },
  "meta.llama3-1-8b-instruct-v1:0": {
    "description": "Zaktualizowana wersja Meta Llama 3.1 8B Instruct, obejmująca rozszerzone 128K długości kontekstu, wielojęzyczność i poprawione zdolności wnioskowania. Llama 3.1 oferuje wielojęzyczne modele językowe (LLMs) jako zestaw wstępnie wytrenowanych, dostosowanych do instrukcji modeli generacyjnych, w tym rozmiarów 8B, 70B i 405B (wejście/wyjście tekstowe). Modele tekstowe Llama 3.1 dostosowane do instrukcji (8B, 70B, 405B) są zoptymalizowane do zastosowań w dialogach wielojęzycznych i przewyższają wiele dostępnych modeli czatu w powszechnych testach branżowych. Llama 3.1 jest przeznaczona do zastosowań komercyjnych i badawczych w wielu językach. Modele tekstowe dostosowane do instrukcji są odpowiednie do czatu podobnego do asystenta, podczas gdy modele wstępnie wytrenowane mogą być dostosowane do różnych zadań generowania języka naturalnego. Modele Llama 3.1 wspierają również wykorzystanie wyników ich modeli do poprawy innych modeli, w tym generowania danych syntetycznych i rafinacji. Llama 3.1 jest modelem językowym autoregresywnym, wykorzystującym zoptymalizowaną architekturę transformatora. Wersje dostosowane wykorzystują nadzorowane dostrajanie (SFT) i uczenie się ze wzmocnieniem z ludzkim feedbackiem (RLHF), aby dostosować się do ludzkich preferencji dotyczących pomocności i bezpieczeństwa."
  },
  "meta.llama3-70b-instruct-v1:0": {
    "description": "Meta Llama 3 to otwarty duży model językowy (LLM) skierowany do deweloperów, badaczy i przedsiębiorstw, mający na celu pomoc w budowaniu, eksperymentowaniu i odpowiedzialnym rozwijaniu ich pomysłów na generatywną sztuczną inteligencję. Jako część podstawowego systemu innowacji globalnej społeczności, jest idealny do tworzenia treści, AI do dialogów, rozumienia języka, badań i zastosowań biznesowych."
  },
  "meta.llama3-8b-instruct-v1:0": {
    "description": "Meta Llama 3 to otwarty duży model językowy (LLM) skierowany do deweloperów, badaczy i przedsiębiorstw, mający na celu pomoc w budowaniu, eksperymentowaniu i odpowiedzialnym rozwijaniu ich pomysłów na generatywną sztuczną inteligencję. Jako część podstawowego systemu innowacji globalnej społeczności, jest idealny dla urządzeń o ograniczonej mocy obliczeniowej i zasobach, a także dla szybszego czasu szkolenia."
  },
  "meta/Llama-3.2-11B-Vision-Instruct": {
    "description": "Wysokiej jakości zdolności wnioskowania obrazowego na obrazach o wysokiej rozdzielczości, idealne do zastosowań związanych z rozumieniem wizualnym."
  },
  "meta/Llama-3.2-90B-Vision-Instruct": {
    "description": "Zaawansowane zdolności wnioskowania obrazowego przeznaczone do zastosowań agentów rozumienia wizualnego."
  },
  "meta/Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 to najnowocześniejszy wielojęzyczny, otwarty model językowy z serii Llama, oferujący wydajność porównywalną z modelem 405B przy bardzo niskich kosztach. Opiera się na architekturze Transformer i jest ulepszony przez nadzorowane dostrajanie (SFT) oraz uczenie ze wzmocnieniem na podstawie opinii ludzi (RLHF). Wersja dostrojona pod kątem instrukcji jest zoptymalizowana do wielojęzycznych dialogów i przewyższa wiele otwartych i zamkniętych modeli czatu w licznych branżowych benchmarkach. Data odcięcia wiedzy: grudzień 2023."
  },
  "meta/Meta-Llama-3-70B-Instruct": {
    "description": "Potężny model o 70 miliardach parametrów, wyróżniający się wnioskowaniem, kodowaniem i szerokim zastosowaniem językowym."
  },
  "meta/Meta-Llama-3-8B-Instruct": {
    "description": "Wszechstronny model o 8 miliardach parametrów, zoptymalizowany do zadań dialogowych i generowania tekstu."
  },
  "meta/Meta-Llama-3.1-405B-Instruct": {
    "description": "Model tekstowy Llama 3.1 dostrojony pod kątem instrukcji, zoptymalizowany do wielojęzycznych zastosowań dialogowych, osiągający doskonałe wyniki w wielu dostępnych otwartych i zamkniętych modelach czatu na popularnych branżowych benchmarkach."
  },
  "meta/Meta-Llama-3.1-70B-Instruct": {
    "description": "Model tekstowy Llama 3.1 dostrojony pod kątem instrukcji, zoptymalizowany do wielojęzycznych zastosowań dialogowych, osiągający doskonałe wyniki w wielu dostępnych otwartych i zamkniętych modelach czatu na popularnych branżowych benchmarkach."
  },
  "meta/Meta-Llama-3.1-8B-Instruct": {
    "description": "Model tekstowy Llama 3.1 dostrojony pod kątem instrukcji, zoptymalizowany do wielojęzycznych zastosowań dialogowych, osiągający doskonałe wyniki w wielu dostępnych otwartych i zamkniętych modelach czatu na popularnych branżowych benchmarkach."
  },
  "meta/llama-3-70b": {
    "description": "Model open source o 70 miliardach parametrów, starannie dostrojony przez Meta do celów przestrzegania instrukcji. Obsługiwany przez Groq na ich niestandardowym sprzęcie LPU, zapewnia szybkie i wydajne wnioskowanie."
  },
  "meta/llama-3-8b": {
    "description": "Model open source o 8 miliardach parametrów, starannie dostrojony przez Meta do celów przestrzegania instrukcji. Obsługiwany przez Groq na ich niestandardowym sprzęcie LPU, zapewnia szybkie i wydajne wnioskowanie."
  },
  "meta/llama-3.1-405b-instruct": {
    "description": "Zaawansowany LLM, wspierający generowanie danych syntetycznych, destylację wiedzy i wnioskowanie, odpowiedni do chatbotów, programowania i zadań w określonych dziedzinach."
  },
  "meta/llama-3.1-70b": {
    "description": "Zaktualizowana wersja Meta Llama 3 70B Instruct, obejmująca rozszerzoną długość kontekstu 128K, wsparcie wielojęzyczne i ulepszone zdolności wnioskowania."
  },
  "meta/llama-3.1-70b-instruct": {
    "description": "Umożliwia złożone rozmowy, posiadając doskonałe zrozumienie kontekstu, zdolności wnioskowania i generowania tekstu."
  },
  "meta/llama-3.1-8b": {
    "description": "Llama 3.1 8B obsługuje okno kontekstu 128K, co czyni go idealnym wyborem do interfejsów rozmów na żywo i analizy danych, oferując jednocześnie znaczące oszczędności kosztów w porównaniu z większymi modelami. Obsługiwany przez Groq na ich niestandardowym sprzęcie LPU, zapewnia szybkie i wydajne wnioskowanie."
  },
  "meta/llama-3.1-8b-instruct": {
    "description": "Zaawansowany, nowoczesny model, posiadający zrozumienie języka, doskonałe zdolności wnioskowania i generowania tekstu."
  },
  "meta/llama-3.2-11b": {
    "description": "Model generujący wnioskowania obrazowe dostosowany do instrukcji (wejście tekst + obraz / wyjście tekst), zoptymalizowany pod kątem rozpoznawania wizualnego, wnioskowania obrazowego, generowania podpisów i odpowiadania na ogólne pytania dotyczące obrazów."
  },
  "meta/llama-3.2-11b-vision-instruct": {
    "description": "Nowoczesny model wizualno-językowy, specjalizujący się w wysokiej jakości wnioskowaniu z obrazów."
  },
  "meta/llama-3.2-1b": {
    "description": "Model tylko tekstowy, wspierający zastosowania na urządzeniach, takie jak wielojęzyczne lokalne wyszukiwanie wiedzy, streszczanie i przepisywanie."
  },
  "meta/llama-3.2-1b-instruct": {
    "description": "Zaawansowany, nowoczesny mały model językowy, posiadający zrozumienie języka, doskonałe zdolności wnioskowania i generowania tekstu."
  },
  "meta/llama-3.2-3b": {
    "description": "Model tylko tekstowy, starannie dostrojony do wspierania zastosowań na urządzeniach, takich jak wielojęzyczne lokalne wyszukiwanie wiedzy, streszczanie i przepisywanie."
  },
  "meta/llama-3.2-3b-instruct": {
    "description": "Zaawansowany, nowoczesny mały model językowy, posiadający zrozumienie języka, doskonałe zdolności wnioskowania i generowania tekstu."
  },
  "meta/llama-3.2-90b": {
    "description": "Model generujący wnioskowania obrazowe dostosowany do instrukcji (wejście tekst + obraz / wyjście tekst), zoptymalizowany pod kątem rozpoznawania wizualnego, wnioskowania obrazowego, generowania podpisów i odpowiadania na ogólne pytania dotyczące obrazów."
  },
  "meta/llama-3.2-90b-vision-instruct": {
    "description": "Nowoczesny model wizualno-językowy, specjalizujący się w wysokiej jakości wnioskowaniu z obrazów."
  },
  "meta/llama-3.3-70b": {
    "description": "Idealne połączenie wydajności i efektywności. Model wspiera wysokowydajne AI konwersacyjne, zaprojektowany do tworzenia treści, zastosowań korporacyjnych i badań, oferując zaawansowane zdolności rozumienia języka, w tym streszczanie tekstu, klasyfikację, analizę sentymentu i generowanie kodu."
  },
  "meta/llama-3.3-70b-instruct": {
    "description": "Zaawansowany LLM, specjalizujący się w wnioskowaniu, matematyce, zdrowym rozsądku i wywoływaniu funkcji."
  },
  "meta/llama-4-maverick": {
    "description": "Zestaw modeli Llama 4 to natywne modele AI multimodalne, wspierające tekst i doświadczenia multimodalne. Modele te wykorzystują architekturę hybrydowych ekspertów, oferując wiodącą w branży wydajność w rozumieniu tekstu i obrazów. Llama 4 Maverick to model o 17 miliardach parametrów z 128 ekspertami. Dostarczany przez DeepInfra."
  },
  "meta/llama-4-scout": {
    "description": "Zestaw modeli Llama 4 to natywne modele AI multimodalne, wspierające tekst i doświadczenia multimodalne. Modele te wykorzystują architekturę hybrydowych ekspertów, oferując wiodącą w branży wydajność w rozumieniu tekstu i obrazów. Llama 4 Scout to model o 17 miliardach parametrów z 16 ekspertami. Dostarczany przez DeepInfra."
  },
  "microsoft/Phi-3-medium-128k-instruct": {
    "description": "Ten sam model Phi-3-medium, ale z większym rozmiarem kontekstu, odpowiedni do RAG lub nielicznych podpowiedzi."
  },
  "microsoft/Phi-3-medium-4k-instruct": {
    "description": "Model o 14 miliardach parametrów, lepszej jakości niż Phi-3-mini, skoncentrowany na wysokiej jakości i danych wymagających intensywnego wnioskowania."
  },
  "microsoft/Phi-3-mini-128k-instruct": {
    "description": "Ten sam model Phi-3-mini, ale z większym rozmiarem kontekstu, odpowiedni do RAG lub nielicznych podpowiedzi."
  },
  "microsoft/Phi-3-mini-4k-instruct": {
    "description": "Najmniejszy członek rodziny Phi-3, zoptymalizowany pod kątem jakości i niskich opóźnień."
  },
  "microsoft/Phi-3-small-128k-instruct": {
    "description": "Ten sam model Phi-3-small, ale z większym rozmiarem kontekstu, odpowiedni do RAG lub nielicznych podpowiedzi."
  },
  "microsoft/Phi-3-small-8k-instruct": {
    "description": "Model o 7 miliardach parametrów, lepszej jakości niż Phi-3-mini, skoncentrowany na wysokiej jakości i danych wymagających intensywnego wnioskowania."
  },
  "microsoft/Phi-3.5-mini-instruct": {
    "description": "Zaktualizowana wersja modelu Phi-3-mini."
  },
  "microsoft/Phi-3.5-vision-instruct": {
    "description": "Zaktualizowana wersja modelu Phi-3-vision."
  },
  "microsoft/WizardLM-2-8x22B": {
    "description": "WizardLM 2 to model językowy oferowany przez Microsoft AI, który wyróżnia się w złożonych rozmowach, wielojęzyczności, wnioskowaniu i jako inteligentny asystent."
  },
  "microsoft/wizardlm-2-8x22b": {
    "description": "WizardLM-2 8x22B to najnowocześniejszy model Wizard od Microsoftu, wykazujący niezwykle konkurencyjne osiągi."
  },
  "minicpm-v": {
    "description": "MiniCPM-V to nowa generacja multimodalnego dużego modelu wydanego przez OpenBMB, który posiada doskonałe zdolności rozpoznawania OCR oraz zrozumienia multimodalnego, wspierając szeroki zakres zastosowań."
  },
  "ministral-3b-latest": {
    "description": "Ministral 3B to czołowy model brzegowy Mistrala."
  },
  "ministral-8b-latest": {
    "description": "Ministral 8B to opłacalny model brzegowy Mistrala."
  },
  "mistral": {
    "description": "Mistral to model 7B wydany przez Mistral AI, odpowiedni do zmiennych potrzeb przetwarzania języka."
  },
  "mistral-ai/Mistral-Large-2411": {
    "description": "Flagowy model Mistral, odpowiedni do zadań wymagających dużej mocy obliczeniowej lub wysoko wyspecjalizowanych, takich jak generowanie tekstu syntetycznego, generowanie kodu, RAG lub agentów."
  },
  "mistral-ai/Mistral-Nemo": {
    "description": "Mistral Nemo to nowoczesny model językowy (LLM) oferujący najlepsze w swojej klasie zdolności wnioskowania, wiedzy o świecie i kodowania."
  },
  "mistral-ai/mistral-small-2503": {
    "description": "Mistral Small jest przeznaczony do wszelkich zadań językowych wymagających wysokiej wydajności i niskich opóźnień."
  },
  "mistral-large": {
    "description": "Mixtral Large to flagowy model Mistral, łączący zdolności generowania kodu, matematyki i wnioskowania, wspierający kontekst o długości 128k."
  },
  "mistral-large-instruct": {
    "description": "Mistral-Large-Instruct-2407 to zaawansowany gęsty model językowy o dużym rozmiarze (LLM) z 123 miliardami parametrów, posiadający najnowocześniejsze zdolności wnioskowania, wiedzy i kodowania."
  },
  "mistral-large-latest": {
    "description": "Mistral Large to flagowy model, doskonały w zadaniach wielojęzycznych, złożonym wnioskowaniu i generowaniu kodu, idealny do zaawansowanych zastosowań."
  },
  "mistral-medium-latest": {
    "description": "Mistral Medium 3 oferuje najnowocześniejszą wydajność przy kosztach 8 razy niższych, a także zasadniczo upraszcza wdrożenia w przedsiębiorstwach."
  },
  "mistral-nemo": {
    "description": "Mistral Nemo, opracowany przez Mistral AI i NVIDIA, to model 12B o wysokiej wydajności."
  },
  "mistral-nemo-instruct": {
    "description": "Duży model językowy (LLM) Mistral-Nemo-Instruct-2407 to wersja dostosowana do poleceń modelu Mistral-Nemo-Base-2407."
  },
  "mistral-small": {
    "description": "Mistral Small może być używany w każdym zadaniu opartym na języku, które wymaga wysokiej wydajności i niskiej latencji."
  },
  "mistral-small-latest": {
    "description": "Mistral Small to opcja o wysokiej efektywności kosztowej, szybka i niezawodna, odpowiednia do tłumaczeń, podsumowań i analizy sentymentu."
  },
  "mistral/codestral": {
    "description": "Mistral Codestral 25.01 to najnowocześniejszy model kodowania, zoptymalizowany pod kątem niskich opóźnień i wysokiej częstotliwości zastosowań. Obsługuje ponad 80 języków programowania i wyróżnia się w zadaniach takich jak wypełnianie środkowe (FIM), korekta kodu i generowanie testów."
  },
  "mistral/codestral-embed": {
    "description": "Model osadzeń kodu, który można osadzić w bazach danych i repozytoriach kodu, wspierający asystentów kodowania."
  },
  "mistral/devstral-small": {
    "description": "Devstral to agentowy duży model językowy do zadań inżynierii oprogramowania, idealny jako agent inżynierii oprogramowania."
  },
  "mistral/magistral-medium": {
    "description": "Złożone myślenie wspierane głębokim zrozumieniem, z przejrzystym rozumowaniem, które można śledzić i weryfikować. Model utrzymuje wysoką wierność rozumowania w wielu językach, nawet przy zmianie języka w trakcie zadania."
  },
  "mistral/magistral-small": {
    "description": "Złożone myślenie wspierane głębokim zrozumieniem, z przejrzystym rozumowaniem, które można śledzić i weryfikować. Model utrzymuje wysoką wierność rozumowania w wielu językach, nawet przy zmianie języka w trakcie zadania."
  },
  "mistral/ministral-3b": {
    "description": "Kompaktowy, wydajny model do zadań na urządzeniach, takich jak inteligentni asystenci i lokalna analiza, oferujący niskie opóźnienia."
  },
  "mistral/ministral-8b": {
    "description": "Mocniejszy model z szybszym i bardziej pamięciooszczędnym wnioskowaniem, idealny do złożonych przepływów pracy i wymagających zastosowań brzegowych."
  },
  "mistral/mistral-embed": {
    "description": "Uniwersalny model osadzeń tekstowych do wyszukiwania semantycznego, podobieństwa, klastrowania i przepływów pracy RAG."
  },
  "mistral/mistral-large": {
    "description": "Mistral Large to idealny wybór do złożonych zadań wymagających dużej mocy wnioskowania lub wysokiej specjalizacji — takich jak generowanie tekstu syntetycznego, generowanie kodu, RAG lub agenci."
  },
  "mistral/mistral-small": {
    "description": "Mistral Small to idealny wybór do prostych zadań, które można przetwarzać hurtowo — takich jak klasyfikacja, obsługa klienta czy generowanie tekstu. Oferuje doskonałą wydajność w przystępnej cenie."
  },
  "mistral/mixtral-8x22b-instruct": {
    "description": "Model 8x22b Instruct. 8x22b to otwarty model hybrydowych ekspertów obsługiwany przez Mistral."
  },
  "mistral/pixtral-12b": {
    "description": "Model 12B z umiejętnościami rozumienia obrazów oraz tekstu."
  },
  "mistral/pixtral-large": {
    "description": "Pixtral Large to drugi model w naszej rodzinie multimodalnej, prezentujący zaawansowany poziom rozumienia obrazów. Model potrafi rozumieć dokumenty, wykresy i obrazy naturalne, zachowując jednocześnie wiodące zdolności rozumienia tekstu Mistral Large 2."
  },
  "mistralai/Mistral-7B-Instruct-v0.1": {
    "description": "Mistral (7B) Instruct jest znany z wysokiej wydajności, idealny do różnorodnych zadań językowych."
  },
  "mistralai/Mistral-7B-Instruct-v0.2": {
    "description": "Mistral 7B to model dostosowany na żądanie, oferujący zoptymalizowane odpowiedzi na zadania."
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "description": "Mistral (7B) Instruct v0.3 oferuje efektywne możliwości obliczeniowe i rozumienia języka naturalnego, idealne do szerokiego zakresu zastosowań."
  },
  "mistralai/Mistral-7B-v0.1": {
    "description": "Mistral 7B to kompaktowy, ale wysokowydajny model, dobrze radzący sobie z przetwarzaniem wsadowym i prostymi zadaniami, takimi jak klasyfikacja i generowanie tekstu, z dobrą zdolnością wnioskowania."
  },
  "mistralai/Mixtral-8x22B-Instruct-v0.1": {
    "description": "Mixtral-8x22B Instruct (141B) to super duży model językowy, wspierający ekstremalne wymagania przetwarzania."
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "description": "Mixtral 8x7B to wstępnie wytrenowany model rzadkiego mieszania ekspertów, przeznaczony do ogólnych zadań tekstowych."
  },
  "mistralai/Mixtral-8x7B-v0.1": {
    "description": "Mixtral 8x7B to model sparsity expert, który korzysta z wielu parametrów, aby zwiększyć prędkość wnioskowania, idealny do przetwarzania zadań wielojęzycznych i generowania kodu."
  },
  "mistralai/mistral-7b-instruct": {
    "description": "Mistral 7B Instruct to model o wysokiej wydajności, który łączy optymalizację prędkości z obsługą długiego kontekstu."
  },
  "mistralai/mistral-nemo": {
    "description": "Mistral Nemo to model z 7,3 miliardami parametrów, wspierający wiele języków i wysoką wydajność programowania."
  },
  "mixtral": {
    "description": "Mixtral to model ekspercki Mistral AI, z otwartymi wagami, oferujący wsparcie w generowaniu kodu i rozumieniu języka."
  },
  "mixtral-8x7b-32768": {
    "description": "Mixtral 8x7B oferuje wysoką tolerancję na błędy w obliczeniach równoległych, odpowiednią do złożonych zadań."
  },
  "mixtral:8x22b": {
    "description": "Mixtral to model ekspercki Mistral AI, z otwartymi wagami, oferujący wsparcie w generowaniu kodu i rozumieniu języka."
  },
  "moonshot-v1-128k": {
    "description": "Moonshot V1 128K to model o zdolności przetwarzania kontekstu o ultra-długiej długości, odpowiedni do generowania bardzo długich tekstów, spełniający wymagania złożonych zadań generacyjnych, zdolny do przetwarzania treści do 128 000 tokenów, idealny do zastosowań w badaniach, akademickich i generowaniu dużych dokumentów."
  },
  "moonshot-v1-128k-vision-preview": {
    "description": "Model wizualny Kimi (w tym moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview itp.) potrafi rozumieć treść obrazów, w tym teksty na obrazach, kolory obrazów i kształty obiektów."
  },
  "moonshot-v1-32k": {
    "description": "Moonshot V1 32K oferuje zdolność przetwarzania kontekstu o średniej długości, zdolną do przetwarzania 32 768 tokenów, szczególnie odpowiednią do generowania różnych długich dokumentów i złożonych dialogów, stosowaną w tworzeniu treści, generowaniu raportów i systemach dialogowych."
  },
  "moonshot-v1-32k-vision-preview": {
    "description": "Model wizualny Kimi (w tym moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview itp.) potrafi rozumieć treść obrazów, w tym teksty na obrazach, kolory obrazów i kształty obiektów."
  },
  "moonshot-v1-8k": {
    "description": "Moonshot V1 8K zaprojektowany do generowania krótkich tekstów, charakteryzuje się wydajnością przetwarzania, zdolny do przetwarzania 8 192 tokenów, idealny do krótkich dialogów, notatek i szybkiego generowania treści."
  },
  "moonshot-v1-8k-vision-preview": {
    "description": "Model wizualny Kimi (w tym moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview itp.) potrafi rozumieć treść obrazów, w tym teksty na obrazach, kolory obrazów i kształty obiektów."
  },
  "moonshot-v1-auto": {
    "description": "Moonshot V1 Auto może wybierać odpowiedni model w zależności od liczby tokenów zajmowanych przez bieżący kontekst."
  },
  "moonshotai/Kimi-Dev-72B": {
    "description": "Kimi-Dev-72B to otwarty model kodu źródłowego, zoptymalizowany za pomocą zaawansowanego uczenia ze wzmocnieniem, zdolny do generowania stabilnych, gotowych do produkcji poprawek. Model osiągnął nowy rekord 60,4% na SWE-bench Verified, ustanawiając nowy standard w zadaniach automatyzacji inżynierii oprogramowania, takich jak naprawa błędów i przegląd kodu."
  },
  "moonshotai/Kimi-K2-Instruct-0905": {
    "description": "Kimi K2-Instruct-0905 to najnowsza i najpotężniejsza wersja Kimi K2. Jest to zaawansowany model językowy typu Mixture of Experts (MoE) z 1 bilionem parametrów ogółem i 32 miliardami aktywowanych parametrów. Główne cechy modelu to: wzmocniona inteligencja kodowania agentów, która wykazuje znaczącą poprawę wydajności w publicznych testach porównawczych oraz w rzeczywistych zadaniach kodowania agentów; ulepszone doświadczenie kodowania front-end, z postępami zarówno w estetyce, jak i funkcjonalności programowania front-endowego."
  },
  "moonshotai/kimi-k2": {
    "description": "Kimi K2 to duży model językowy hybrydowych ekspertów (MoE) opracowany przez Moonshot AI, z 1 bilionem parametrów łącznie i 32 miliardami aktywnych parametrów na pojedyncze przejście. Model jest zoptymalizowany pod kątem zdolności agentowych, w tym zaawansowanego użycia narzędzi, wnioskowania i syntezy kodu."
  },
  "moonshotai/kimi-k2-0905": {
    "description": "Model kimi-k2-0905-preview obsługuje długość kontekstu do 256k, oferując silniejsze zdolności Agentic Coding, bardziej estetyczny i praktyczny kod frontendowy oraz lepsze rozumienie kontekstu."
  },
  "moonshotai/kimi-k2-instruct-0905": {
    "description": "Model kimi-k2-0905-preview obsługuje długość kontekstu do 256k, oferując silniejsze zdolności Agentic Coding, bardziej estetyczny i praktyczny kod frontendowy oraz lepsze rozumienie kontekstu."
  },
  "morph/morph-v3-fast": {
    "description": "Morph oferuje specjalistyczny model AI, który szybko stosuje zmiany kodu sugerowane przez najnowocześniejsze modele, takie jak Claude czy GPT-4o, do istniejących plików kodu — SZYBKOŚĆ ponad 4500 tokenów/sekundę. Działa jako ostatni krok w przepływie pracy kodowania AI. Obsługuje 16k tokenów wejściowych i 16k tokenów wyjściowych."
  },
  "morph/morph-v3-large": {
    "description": "Morph oferuje specjalistyczny model AI, który stosuje zmiany kodu sugerowane przez najnowocześniejsze modele, takie jak Claude czy GPT-4o, do istniejących plików kodu — SZYBKOŚĆ ponad 2500 tokenów/sekundę. Działa jako ostatni krok w przepływie pracy kodowania AI. Obsługuje 16k tokenów wejściowych i 16k tokenów wyjściowych."
  },
  "nousresearch/hermes-2-pro-llama-3-8b": {
    "description": "Hermes 2 Pro Llama 3 8B to ulepszona wersja Nous Hermes 2, zawierająca najnowsze wewnętrznie opracowane zbiory danych."
  },
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
    "description": "Llama 3.1 Nemotron 70B to dostosowany przez NVIDIA duży model językowy, mający na celu zwiększenie użyteczności odpowiedzi generowanych przez LLM w odpowiedzi na zapytania użytkowników. Model ten osiągnął doskonałe wyniki w testach benchmarkowych, takich jak Arena Hard, AlpacaEval 2 LC i GPT-4-Turbo MT-Bench, zajmując pierwsze miejsce we wszystkich trzech automatycznych testach do 1 października 2024 roku. Model został przeszkolony przy użyciu RLHF (szczególnie REINFORCE), Llama-3.1-Nemotron-70B-Reward i HelpSteer2-Preference na bazie modelu Llama-3.1-70B-Instruct."
  },
  "nvidia/llama-3.1-nemotron-51b-instruct": {
    "description": "Unikalny model językowy, oferujący niezrównaną dokładność i wydajność."
  },
  "nvidia/llama-3.1-nemotron-70b-instruct": {
    "description": "Llama-3.1-Nemotron-70B-Instruct to dostosowany przez NVIDIA duży model językowy, zaprojektowany w celu zwiększenia użyteczności odpowiedzi generowanych przez LLM."
  },
  "o1": {
    "description": "Skupia się na zaawansowanym wnioskowaniu i rozwiązywaniu złożonych problemów, w tym zadań matematycznych i naukowych. Doskonale nadaje się do aplikacji wymagających głębokiego zrozumienia kontekstu i zarządzania procesami."
  },
  "o1-mini": {
    "description": "o1-mini to szybki i ekonomiczny model wnioskowania zaprojektowany z myślą o programowaniu, matematyce i zastosowaniach naukowych. Model ten ma kontekst 128K i datę graniczną wiedzy z października 2023 roku."
  },
  "o1-preview": {
    "description": "Skoncentrowany na zaawansowanym wnioskowaniu i rozwiązywaniu złożonych problemów, w tym zadań matematycznych i naukowych. Doskonale nadaje się do zastosowań wymagających głębokiego zrozumienia kontekstu i autonomicznych przepływów pracy."
  },
  "o1-pro": {
    "description": "Modele z serii o1 są trenowane z wykorzystaniem uczenia ze wzmocnieniem, potrafią myśleć przed udzieleniem odpowiedzi i wykonywać złożone zadania rozumowania. Model o1-pro wykorzystuje więcej zasobów obliczeniowych, aby prowadzić głębsze rozważania i stale dostarczać lepsze odpowiedzi."
  },
  "o3": {
    "description": "o3 to wszechstronny i potężny model, który doskonale sprawdza się w wielu dziedzinach. Ustanawia nowe standardy w zadaniach matematycznych, naukowych, programistycznych i wizualnych. Jest również biegły w pisaniu technicznym i przestrzeganiu instrukcji. Użytkownicy mogą go wykorzystać do analizy tekstów, kodów i obrazów, rozwiązując złożone problemy wieloetapowe."
  },
  "o3-2025-04-16": {
    "description": "o3 to nowy model rozumowania OpenAI, obsługujący wejścia tekstowo-obrazowe i generujący tekst, przeznaczony do złożonych zadań wymagających szerokiej wiedzy ogólnej."
  },
  "o3-deep-research": {
    "description": "o3-deep-research to nasz najbardziej zaawansowany model głębokich badań, zaprojektowany specjalnie do obsługi złożonych, wieloetapowych zadań badawczych. Potrafi wyszukiwać i integrować informacje z internetu, a także uzyskiwać dostęp do Twoich własnych danych i wykorzystywać je za pośrednictwem łącznika MCP."
  },
  "o3-mini": {
    "description": "o3-mini to nasz najnowszy mały model wnioskowania, który oferuje wysoką inteligencję przy tych samych kosztach i celach opóźnienia co o1-mini."
  },
  "o3-pro": {
    "description": "Model o3-pro wykorzystuje większą moc obliczeniową do głębszego myślenia i zawsze dostarcza lepsze odpowiedzi, dostępny wyłącznie przez API Responses."
  },
  "o3-pro-2025-06-10": {
    "description": "o3 Pro to nowy model rozumowania OpenAI, obsługujący wejścia tekstowo-obrazowe i generujący tekst, przeznaczony do złożonych zadań wymagających szerokiej wiedzy ogólnej."
  },
  "o4-mini": {
    "description": "o4-mini to nasz najnowszy mały model z serii o. Został zoptymalizowany do szybkiego i efektywnego wnioskowania, osiągając wysoką wydajność i efektywność w zadaniach kodowania i wizualnych."
  },
  "o4-mini-2025-04-16": {
    "description": "o4-mini to model rozumowania OpenAI, obsługujący wejścia tekstowo-obrazowe i generujący tekst, przeznaczony do złożonych zadań wymagających szerokiej wiedzy ogólnej. Model posiada kontekst o długości 200 tys. tokenów."
  },
  "o4-mini-deep-research": {
    "description": "o4-mini-deep-research to nasz szybszy i bardziej przystępny cenowo model głębokich badań — idealny do obsługi złożonych, wieloetapowych zadań badawczych. Potrafi wyszukiwać i integrować informacje z internetu, a także uzyskiwać dostęp do Twoich własnych danych i wykorzystywać je za pośrednictwem łącznika MCP."
  },
  "open-codestral-mamba": {
    "description": "Codestral Mamba to model językowy Mamba 2 skoncentrowany na generowaniu kodu, oferujący silne wsparcie dla zaawansowanych zadań kodowania i wnioskowania."
  },
  "open-mistral-7b": {
    "description": "Mistral 7B to kompaktowy, ale wydajny model, doskonały do przetwarzania wsadowego i prostych zadań, takich jak klasyfikacja i generowanie tekstu, z dobrą wydajnością wnioskowania."
  },
  "open-mistral-nemo": {
    "description": "Mistral Nemo to model 12B opracowany we współpracy z Nvidia, oferujący doskonałe możliwości wnioskowania i kodowania, łatwy do integracji i zastąpienia."
  },
  "open-mixtral-8x22b": {
    "description": "Mixtral 8x22B to większy model eksperta, skoncentrowany na złożonych zadaniach, oferujący doskonałe możliwości wnioskowania i wyższą przepustowość."
  },
  "open-mixtral-8x7b": {
    "description": "Mixtral 8x7B to model rzadkiego eksperta, który wykorzystuje wiele parametrów do zwiększenia prędkości wnioskowania, odpowiedni do przetwarzania zadań wielojęzycznych i generowania kodu."
  },
  "openai/gpt-3.5-turbo": {
    "description": "Najbardziej kompetentny i opłacalny model z serii GPT-3.5 od OpenAI, zoptymalizowany pod kątem czatu, ale również dobrze radzący sobie z tradycyjnymi zadaniami uzupełniania."
  },
  "openai/gpt-3.5-turbo-instruct": {
    "description": "Model o zdolnościach podobnych do modeli z ery GPT-3, kompatybilny z tradycyjnymi punktami końcowymi uzupełniania, a nie czatu."
  },
  "openai/gpt-4-turbo": {
    "description": "gpt-4-turbo od OpenAI posiada szeroką wiedzę ogólną i specjalistyczną, umożliwiającą wykonywanie złożonych instrukcji w języku naturalnym i precyzyjne rozwiązywanie trudnych problemów. Data zakończenia wiedzy to kwiecień 2023, a okno kontekstu wynosi 128 000 tokenów."
  },
  "openai/gpt-4.1": {
    "description": "GPT 4.1 to flagowy model OpenAI, przeznaczony do złożonych zadań. Doskonale nadaje się do rozwiązywania problemów międzydziedzinowych."
  },
  "openai/gpt-4.1-mini": {
    "description": "GPT 4.1 mini osiąga równowagę między inteligencją, szybkością i kosztami, czyniąc go atrakcyjnym modelem dla wielu zastosowań."
  },
  "openai/gpt-4.1-nano": {
    "description": "GPT-4.1 nano to najszybszy i najbardziej opłacalny model GPT 4.1."
  },
  "openai/gpt-4o": {
    "description": "GPT-4o od OpenAI posiada szeroką wiedzę ogólną i specjalistyczną, umożliwiającą wykonywanie złożonych instrukcji w języku naturalnym i precyzyjne rozwiązywanie trudnych problemów. Oferuje wydajność porównywalną z GPT-4 Turbo, ale z szybszym i tańszym API."
  },
  "openai/gpt-4o-mini": {
    "description": "GPT-4o mini od OpenAI to ich najbardziej zaawansowany i opłacalny mały model. Jest multimodalny (przyjmuje tekst lub obrazy i generuje tekst) oraz inteligentniejszy niż gpt-3.5-turbo, zachowując podobną szybkość."
  },
  "openai/gpt-5": {
    "description": "GPT-5 to flagowy model językowy OpenAI, wyróżniający się w złożonym wnioskowaniu, szerokiej wiedzy o świecie, zadaniach intensywnie kodujących i wieloetapowych zadaniach agentowych."
  },
  "openai/gpt-5-mini": {
    "description": "GPT-5 mini to model zoptymalizowany pod kątem kosztów, oferujący doskonałą wydajność w zadaniach wnioskowania i czatu. Zapewnia najlepszą równowagę między szybkością, kosztami i możliwościami."
  },
  "openai/gpt-5-nano": {
    "description": "GPT-5 nano to model o wysokiej przepustowości, doskonały w prostych zadaniach instrukcyjnych lub klasyfikacyjnych."
  },
  "openai/gpt-oss-120b": {
    "description": "Niezwykle kompetentny, uniwersalny duży model językowy z potężnymi i kontrolowanymi zdolnościami wnioskowania."
  },
  "openai/gpt-oss-20b": {
    "description": "Kompaktowy model językowy z otwartym kodem, zoptymalizowany pod kątem niskich opóźnień i środowisk o ograniczonych zasobach, w tym wdrożeń lokalnych i brzegowych."
  },
  "openai/o1": {
    "description": "o1 od OpenAI to flagowy model wnioskowania, zaprojektowany do złożonych problemów wymagających głębokiego myślenia. Zapewnia potężne zdolności wnioskowania i wyższą dokładność w złożonych, wieloetapowych zadaniach."
  },
  "openai/o1-mini": {
    "description": "o1-mini to szybki i ekonomiczny model wnioskowania zaprojektowany z myślą o programowaniu, matematyce i zastosowaniach naukowych. Model ten ma kontekst 128K i datę graniczną wiedzy z października 2023 roku."
  },
  "openai/o1-preview": {
    "description": "o1 to nowy model wnioskowania OpenAI, odpowiedni do złożonych zadań wymagających szerokiej wiedzy ogólnej. Model ten ma kontekst 128K i datę graniczną wiedzy z października 2023 roku."
  },
  "openai/o3": {
    "description": "o3 od OpenAI to najsilniejszy model wnioskowania, ustanawiający nowe standardy w kodowaniu, matematyce, nauce i percepcji wizualnej. Doskonale radzi sobie ze złożonymi zapytaniami wymagającymi wieloaspektowej analizy, z wyjątkowymi zdolnościami w analizie obrazów, wykresów i grafów."
  },
  "openai/o3-mini": {
    "description": "o3-mini to najnowszy mały model wnioskowania OpenAI, oferujący wysoką inteligencję przy tych samych celach kosztowych i opóźnieniowych co o1-mini."
  },
  "openai/o3-mini-high": {
    "description": "o3-mini w wersji o wysokim poziomie rozumowania, oferujący wysoką inteligencję przy tych samych kosztach i celach opóźnienia co o1-mini."
  },
  "openai/o4-mini": {
    "description": "o4-mini od OpenAI oferuje szybkie i opłacalne wnioskowanie z doskonałą wydajnością w swojej klasie, szczególnie w zadaniach matematycznych (najlepsze wyniki w benchmarku AIME), kodowaniu i zadaniach wizualnych."
  },
  "openai/o4-mini-high": {
    "description": "o4-mini w wersji o wysokim poziomie wnioskowania, zoptymalizowany do szybkiego i efektywnego wnioskowania, osiągający wysoką wydajność i efektywność w zadaniach kodowania i wizualnych."
  },
  "openai/text-embedding-3-large": {
    "description": "Najbardziej kompetentny model osadzeń OpenAI, odpowiedni do zadań w języku angielskim i innych językach."
  },
  "openai/text-embedding-3-small": {
    "description": "Ulepszona, bardziej wydajna wersja modelu osadzeń ada od OpenAI."
  },
  "openai/text-embedding-ada-002": {
    "description": "Tradycyjny model osadzeń tekstowych od OpenAI."
  },
  "openrouter/auto": {
    "description": "W zależności od długości kontekstu, tematu i złożoności, Twoje zapytanie zostanie wysłane do Llama 3 70B Instruct, Claude 3.5 Sonnet (samoregulacja) lub GPT-4o."
  },
  "perplexity/sonar": {
    "description": "Lekki produkt Perplexity z funkcją wyszukiwania, szybszy i tańszy niż Sonar Pro."
  },
  "perplexity/sonar-pro": {
    "description": "Flagowy produkt Perplexity z funkcją wyszukiwania, obsługujący zaawansowane zapytania i działania następcze."
  },
  "perplexity/sonar-reasoning": {
    "description": "Model skoncentrowany na wnioskowaniu, generujący łańcuchy myślowe (CoT) w odpowiedziach, oferujący szczegółowe wyjaśnienia z funkcją wyszukiwania."
  },
  "perplexity/sonar-reasoning-pro": {
    "description": "Zaawansowany model skoncentrowany na wnioskowaniu, generujący łańcuchy myślowe (CoT) w odpowiedziach, oferujący ulepszone możliwości wyszukiwania i wielokrotne zapytania wyszukiwania na każde żądanie."
  },
  "phi3": {
    "description": "Phi-3 to lekki model otwarty wydany przez Microsoft, odpowiedni do efektywnej integracji i dużej skali wnioskowania wiedzy."
  },
  "phi3:14b": {
    "description": "Phi-3 to lekki model otwarty wydany przez Microsoft, odpowiedni do efektywnej integracji i dużej skali wnioskowania wiedzy."
  },
  "pixtral-12b-2409": {
    "description": "Model Pixtral wykazuje silne zdolności w zadaniach związanych z analizą wykresów i zrozumieniem obrazów, pytaniami dokumentowymi, wielomodalnym rozumowaniem i przestrzeganiem instrukcji, zdolny do przyjmowania obrazów w naturalnej rozdzielczości i proporcjach, a także do przetwarzania dowolnej liczby obrazów w długim oknie kontekstowym o długości do 128K tokenów."
  },
  "pixtral-large-latest": {
    "description": "Pixtral Large to otwarty model wielomodalny z 124 miliardami parametrów, zbudowany na bazie Mistral Large 2. To nasz drugi model w rodzinie wielomodalnej, który wykazuje zaawansowane zdolności rozumienia obrazów."
  },
  "pro-128k": {
    "description": "Spark Pro 128K jest wyposażony w wyjątkową zdolność przetwarzania kontekstu, mogąc obsługiwać do 128K informacji kontekstowych, co czyni go idealnym do analizy całościowej i długoterminowego przetwarzania logicznych powiązań w długich treściach, zapewniając płynność i spójność logiczną oraz różnorodne wsparcie cytatów w złożonej komunikacji tekstowej."
  },
  "qvq-72b-preview": {
    "description": "Model QVQ jest eksperymentalnym modelem badawczym opracowanym przez zespół Qwen, skoncentrowanym na zwiększeniu zdolności w zakresie rozumowania wizualnego, szczególnie w dziedzinie rozumowania matematycznego."
  },
  "qvq-max": {
    "description": "Model wizualnego wnioskowania Tongyi Qianwen QVQ, obsługujący wejścia wizualne i generujący łańcuchy myślowe, wykazujący silne zdolności w matematyce, programowaniu, analizie wizualnej, twórczości oraz zadaniach ogólnych."
  },
  "qvq-plus": {
    "description": "Model wnioskowania wizualnego. Obsługuje wejścia wizualne oraz generowanie łańcuchów myślowych. Wersja plus po modelu qvq-max, charakteryzuje się szybszym wnioskowaniem oraz lepszą równowagą między efektywnością a kosztami w porównaniu do qvq-max."
  },
  "qwen-coder-plus": {
    "description": "Model kodowania Tongyi Qianwen."
  },
  "qwen-coder-turbo": {
    "description": "Model kodowania Tongyi Qianwen."
  },
  "qwen-coder-turbo-latest": {
    "description": "Model kodowania Qwen."
  },
  "qwen-flash": {
    "description": "Seria Tongyi Qianwen to najszybsze i najtańsze modele, odpowiednie do prostych zadań."
  },
  "qwen-image": {
    "description": "Qwen-Image jest uniwersalnym modelem generowania obrazów, obsługującym wiele stylów artystycznych, a w szczególności znakomicie radzącym sobie z renderowaniem złożonego tekstu, zwłaszcza tekstu w języku chińskim i angielskim. Model obsługuje układy wielowierszowe, generowanie tekstu na poziomie akapitu oraz odwzorowywanie drobnych detali, co pozwala na tworzenie złożonych projektów łączących obraz i tekst."
  },
  "qwen-image-edit": {
    "description": "Qwen Image Edit to model generujący obrazy na podstawie obrazów i tekstu, umożliwiający edycję i modyfikację obrazów zgodnie z podanymi wskazówkami. Potrafi precyzyjnie dostosować i kreatywnie przekształcić oryginalny obraz zgodnie z potrzebami użytkownika."
  },
  "qwen-long": {
    "description": "Qwen to ultra-duży model językowy, który obsługuje długie konteksty tekstowe oraz funkcje dialogowe oparte na długich dokumentach i wielu dokumentach."
  },
  "qwen-math-plus": {
    "description": "Model matematyczny Tongyi Qianwen, specjalnie zaprojektowany do rozwiązywania zadań matematycznych."
  },
  "qwen-math-plus-latest": {
    "description": "Model matematyczny Qwen, stworzony specjalnie do rozwiązywania problemów matematycznych."
  },
  "qwen-math-turbo": {
    "description": "Model matematyczny Tongyi Qianwen, specjalnie zaprojektowany do rozwiązywania zadań matematycznych."
  },
  "qwen-math-turbo-latest": {
    "description": "Model matematyczny Qwen, stworzony specjalnie do rozwiązywania problemów matematycznych."
  },
  "qwen-max": {
    "description": "Qwen Max to model językowy o skali miliardowej, obsługujący chiński, angielski i inne języki. Aktualna wersja API modelu na bazie Qwen 2.5."
  },
  "qwen-omni-turbo": {
    "description": "Modele z serii Qwen-Omni obsługują dane wejściowe w różnych modalnościach, w tym wideo, audio, obrazy i tekst, oraz generują wyjścia w postaci audio i tekstu."
  },
  "qwen-plus": {
    "description": "Qwen Plus to ulepszona wersja ogromnego modelu językowego, wspierająca różne języki, w tym chiński i angielski."
  },
  "qwen-turbo": {
    "description": "通义千问 Turbo nie będzie już aktualizowany; zaleca się zastąpienie go modelem 通义千问 Flash. 通义千问 to model językowy o bardzo dużej skali, obsługujący wejścia w języku chińskim, angielskim i innych językach."
  },
  "qwen-vl-chat-v1": {
    "description": "Qwen VL obsługuje elastyczne interakcje, w tym wiele obrazów, wielokrotne pytania i odpowiedzi oraz zdolności twórcze."
  },
  "qwen-vl-max": {
    "description": "Nadzwyczajny, bardzo duży model wizualno-językowy Tongyi Qianwen. W porównaniu z wersją wzmocnioną, ponownie poprawia zdolności wnioskowania wizualnego i przestrzegania instrukcji, oferując wyższy poziom percepcji i poznania wizualnego."
  },
  "qwen-vl-max-latest": {
    "description": "Model wizualno-językowy Qwen o ultra dużej skali. W porównaniu do wersji rozszerzonej, ponownie zwiększa zdolności wnioskowania wizualnego i przestrzegania instrukcji, oferując wyższy poziom percepcji wizualnej i poznawczej."
  },
  "qwen-vl-ocr": {
    "description": "Tongyi Qianwen OCR to specjalistyczny model do ekstrakcji tekstu, skoncentrowany na rozpoznawaniu tekstu w dokumentach, tabelach, zadaniach testowych i pismach odręcznych. Potrafi rozpoznawać wiele języków, w tym chiński, angielski, francuski, japoński, koreański, niemiecki, rosyjski, włoski, wietnamski i arabski."
  },
  "qwen-vl-plus": {
    "description": "Wzmocniona wersja dużego modelu wizualno-językowego Tongyi Qianwen. Znacząco poprawia zdolność rozpoznawania detali i tekstu, obsługuje obrazy o rozdzielczości przekraczającej milion pikseli oraz dowolnych proporcjach."
  },
  "qwen-vl-plus-latest": {
    "description": "Wersja rozszerzona modelu wizualno-językowego Qwen. Znacząco poprawia zdolność rozpoznawania szczegółów i tekstu, obsługuje obrazy o rozdzielczości przekraczającej milion pikseli oraz dowolnych proporcjach."
  },
  "qwen-vl-v1": {
    "description": "Model wstępnie wytrenowany, zainicjowany przez model językowy Qwen-7B, dodający model obrazowy, z rozdzielczością wejściową obrazu wynoszącą 448."
  },
  "qwen/qwen-2-7b-instruct": {
    "description": "Qwen2 to nowa seria dużych modeli językowych Qwen. Qwen2 7B to model oparty na transformatorze, który wykazuje doskonałe wyniki w zakresie rozumienia języka, zdolności wielojęzycznych, programowania, matematyki i wnioskowania."
  },
  "qwen/qwen-2-7b-instruct:free": {
    "description": "Qwen2 to nowa seria dużych modeli językowych, charakteryzująca się silniejszymi zdolnościami rozumienia i generowania."
  },
  "qwen/qwen-2-vl-72b-instruct": {
    "description": "Qwen2-VL to najnowsza iteracja modelu Qwen-VL, która osiągnęła najnowocześniejsze wyniki w testach benchmarkowych dotyczących rozumienia wizualnego, w tym MathVista, DocVQA, RealWorldQA i MTVQA. Qwen2-VL potrafi rozumieć filmy trwające ponad 20 minut, umożliwiając wysokiej jakości pytania i odpowiedzi, dialogi oraz tworzenie treści oparte na wideo. Posiada również zdolności do złożonego wnioskowania i podejmowania decyzji, co pozwala na integrację z urządzeniami mobilnymi, robotami itp., aby automatycznie działać na podstawie środowiska wizualnego i instrukcji tekstowych. Oprócz angielskiego i chińskiego, Qwen2-VL teraz wspiera również rozumienie tekstu w różnych językach w obrazach, w tym większości języków europejskich, japońskiego, koreańskiego, arabskiego i wietnamskiego."
  },
  "qwen/qwen-2.5-72b-instruct": {
    "description": "Qwen2.5-72B-Instruct to jeden z najnowszych modeli dużych języków wydanych przez Alibaba Cloud. Model 72B wykazuje znaczną poprawę w obszarach kodowania i matematyki. Model ten oferuje wsparcie dla wielu języków, obejmując ponad 29 języków, w tym chiński i angielski. Model znacząco poprawił zdolność do podążania za instrukcjami, rozumienia danych strukturalnych oraz generowania strukturalnych wyników (szczególnie JSON)."
  },
  "qwen/qwen2.5-32b-instruct": {
    "description": "Qwen2.5-32B-Instruct to jeden z najnowszych modeli dużych języków wydanych przez Alibaba Cloud. Model 32B wykazuje znaczną poprawę w obszarach kodowania i matematyki. Model ten oferuje wsparcie dla wielu języków, obejmując ponad 29 języków, w tym chiński i angielski. Model znacząco poprawił zdolność do podążania za instrukcjami, rozumienia danych strukturalnych oraz generowania strukturalnych wyników (szczególnie JSON)."
  },
  "qwen/qwen2.5-7b-instruct": {
    "description": "LLM skierowany na język chiński i angielski, skoncentrowany na języku, programowaniu, matematyce, wnioskowaniu i innych dziedzinach."
  },
  "qwen/qwen2.5-coder-32b-instruct": {
    "description": "Zaawansowany LLM, wspierający generowanie kodu, wnioskowanie i naprawę, obejmujący główne języki programowania."
  },
  "qwen/qwen2.5-coder-7b-instruct": {
    "description": "Potężny średniej wielkości model kodu, wspierający długość kontekstu 32K, specjalizujący się w programowaniu wielojęzycznym."
  },
  "qwen/qwen3-14b": {
    "description": "Qwen3-14B to gęsty model językowy o 14 miliardach parametrów w serii Qwen3, zaprojektowany z myślą o złożonym wnioskowaniu i efektywnych dialogach. Obsługuje płynne przełączanie między trybem 'myślenia' używanym do matematyki, programowania i wnioskowania logicznego a trybem 'nie-myślenia' stosowanym w ogólnych rozmowach. Model został dostosowany do przestrzegania instrukcji, użycia narzędzi agenta, twórczego pisania oraz wielojęzycznych zadań w ponad 100 językach i dialektach. Obsługuje natywnie 32K tokenów kontekstu i może być rozszerzany do 131K tokenów za pomocą YaRN."
  },
  "qwen/qwen3-14b:free": {
    "description": "Qwen3-14B to gęsty model językowy o 14 miliardach parametrów w serii Qwen3, zaprojektowany z myślą o złożonym wnioskowaniu i efektywnych dialogach. Obsługuje płynne przełączanie między trybem 'myślenia' używanym do matematyki, programowania i wnioskowania logicznego a trybem 'nie-myślenia' stosowanym w ogólnych rozmowach. Model został dostosowany do przestrzegania instrukcji, użycia narzędzi agenta, twórczego pisania oraz wielojęzycznych zadań w ponad 100 językach i dialektach. Obsługuje natywnie 32K tokenów kontekstu i może być rozszerzany do 131K tokenów za pomocą YaRN."
  },
  "qwen/qwen3-235b-a22b": {
    "description": "Qwen3-235B-A22B to model mieszanki ekspertów (MoE) o 235 miliardach parametrów opracowany przez Qwen, aktywujący 22 miliardy parametrów przy każdym przejściu do przodu. Obsługuje płynne przełączanie między trybem 'myślenia' używanym do złożonego wnioskowania, matematyki i zadań kodowania a trybem 'nie-myślenia' stosowanym w ogólnych rozmowach. Model wykazuje silne zdolności w zakresie wnioskowania, wsparcia wielojęzycznego (ponad 100 języków i dialektów), zaawansowanego przestrzegania instrukcji oraz wywoływania narzędzi agenta. Obsługuje natywnie okno kontekstu 32K tokenów i może być rozszerzany do 131K tokenów za pomocą YaRN."
  },
  "qwen/qwen3-235b-a22b:free": {
    "description": "Qwen3-235B-A22B to model mieszanki ekspertów (MoE) o 235 miliardach parametrów opracowany przez Qwen, aktywujący 22 miliardy parametrów przy każdym przejściu do przodu. Obsługuje płynne przełączanie między trybem 'myślenia' używanym do złożonego wnioskowania, matematyki i zadań kodowania a trybem 'nie-myślenia' stosowanym w ogólnych rozmowach. Model wykazuje silne zdolności w zakresie wnioskowania, wsparcia wielojęzycznego (ponad 100 języków i dialektów), zaawansowanego przestrzegania instrukcji oraz wywoływania narzędzi agenta. Obsługuje natywnie okno kontekstu 32K tokenów i może być rozszerzany do 131K tokenów za pomocą YaRN."
  },
  "qwen/qwen3-30b-a3b": {
    "description": "Qwen3 to najnowsza generacja serii dużych modeli językowych Qwen, charakteryzująca się architekturą gęstą i mieszanką ekspertów (MoE), która doskonale radzi sobie w zakresie wnioskowania, wsparcia wielojęzycznego i zaawansowanych zadań agenta. Jego unikalna zdolność do płynnego przełączania się między trybem myślenia w złożonym wnioskowaniu a trybem nie-myślenia w efektywnych dialogach zapewnia wszechstronność i wysoką jakość wydajności.\n\nQwen3 znacząco przewyższa wcześniejsze modele, takie jak QwQ i Qwen2.5, oferując doskonałe umiejętności w zakresie matematyki, kodowania, wnioskowania ogólnego, twórczego pisania i interaktywnych dialogów. Wariant Qwen3-30B-A3B zawiera 30,5 miliarda parametrów (3,3 miliarda aktywowanych parametrów), 48 warstw, 128 ekspertów (aktywowano 8 dla każdego zadania) i obsługuje kontekst do 131K tokenów (z użyciem YaRN), ustanawiając nowy standard dla modeli open source."
  },
  "qwen/qwen3-30b-a3b:free": {
    "description": "Qwen3 to najnowsza generacja serii dużych modeli językowych Qwen, charakteryzująca się architekturą gęstą i mieszanką ekspertów (MoE), która doskonale radzi sobie w zakresie wnioskowania, wsparcia wielojęzycznego i zaawansowanych zadań agenta. Jego unikalna zdolność do płynnego przełączania się między trybem myślenia w złożonym wnioskowaniu a trybem nie-myślenia w efektywnych dialogach zapewnia wszechstronność i wysoką jakość wydajności.\n\nQwen3 znacząco przewyższa wcześniejsze modele, takie jak QwQ i Qwen2.5, oferując doskonałe umiejętności w zakresie matematyki, kodowania, wnioskowania ogólnego, twórczego pisania i interaktywnych dialogów. Wariant Qwen3-30B-A3B zawiera 30,5 miliarda parametrów (3,3 miliarda aktywowanych parametrów), 48 warstw, 128 ekspertów (aktywowano 8 dla każdego zadania) i obsługuje kontekst do 131K tokenów (z użyciem YaRN), ustanawiając nowy standard dla modeli open source."
  },
  "qwen/qwen3-32b": {
    "description": "Qwen3-32B to gęsty model językowy o 32 miliardach parametrów w serii Qwen3, zoptymalizowany pod kątem złożonego wnioskowania i efektywnych dialogów. Obsługuje płynne przełączanie między trybem 'myślenia' używanym do matematyki, kodowania i wnioskowania logicznego a trybem 'nie-myślenia' stosowanym w szybszych, ogólnych rozmowach. Model wykazuje silną wydajność w przestrzeganiu instrukcji, użyciu narzędzi agenta, twórczym pisaniu oraz wielojęzycznych zadań w ponad 100 językach i dialektach. Obsługuje natywnie 32K tokenów kontekstu i może być rozszerzany do 131K tokenów za pomocą YaRN."
  },
  "qwen/qwen3-32b:free": {
    "description": "Qwen3-32B to gęsty model językowy o 32 miliardach parametrów w serii Qwen3, zoptymalizowany pod kątem złożonego wnioskowania i efektywnych dialogów. Obsługuje płynne przełączanie między trybem 'myślenia' używanym do matematyki, kodowania i wnioskowania logicznego a trybem 'nie-myślenia' stosowanym w szybszych, ogólnych rozmowach. Model wykazuje silną wydajność w przestrzeganiu instrukcji, użyciu narzędzi agenta, twórczym pisaniu oraz wielojęzycznych zadań w ponad 100 językach i dialektach. Obsługuje natywnie 32K tokenów kontekstu i może być rozszerzany do 131K tokenów za pomocą YaRN."
  },
  "qwen/qwen3-8b:free": {
    "description": "Qwen3-8B to gęsty model językowy o 8 miliardach parametrów w serii Qwen3, zaprojektowany z myślą o zadaniach wymagających intensywnego wnioskowania i efektywnych dialogach. Obsługuje płynne przełączanie między trybem 'myślenia' używanym do matematyki, kodowania i wnioskowania logicznego a trybem 'nie-myślenia' stosowanym w ogólnych rozmowach. Model został dostosowany do przestrzegania instrukcji, integracji agenta, twórczego pisania oraz wielojęzycznego użycia w ponad 100 językach i dialektach. Obsługuje natywnie okno kontekstu 32K tokenów i może być rozszerzany do 131K tokenów za pomocą YaRN."
  },
  "qwen2": {
    "description": "Qwen2 to nowa generacja dużego modelu językowego Alibaba, wspierająca różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen2-72b-instruct": {
    "description": "Qwen2 to nowa generacja modeli językowych stworzona przez zespół Qwen. Opiera się na architekturze Transformer i wykorzystuje funkcję aktywacji SwiGLU, obciążenie QKV uwagi (attention QKV bias), grupowe zapytanie uwagi (group query attention), mieszankę uwagi z oknem przesuwnym (mixture of sliding window attention) i pełną uwagą. Ponadto, zespół Qwen wprowadził ulepszony tokenizator dostosowany do wielu języków naturalnych i kodu."
  },
  "qwen2-7b-instruct": {
    "description": "Qwen2 to nowa seria modeli językowych stworzona przez zespół Qwen. Opiera się na architekturze Transformer i wykorzystuje funkcję aktywacji SwiGLU, obciążenie QKV uwagi (attention QKV bias), grupowe zapytanie uwagi (group query attention), mieszankę uwagi okna suwającego się (mixture of sliding window attention) i pełnej uwagi. Ponadto, zespół Qwen wprowadził ulepszone tokenizery dostosowane do wielu języków naturalnych i kodu."
  },
  "qwen2.5": {
    "description": "Qwen2.5 to nowa generacja dużego modelu językowego Alibaba, który wspiera różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen2.5-14b-instruct": {
    "description": "Model Qwen 2.5 o skali 14B, udostępniony na zasadzie open source."
  },
  "qwen2.5-14b-instruct-1m": {
    "description": "Model o skali 72B, udostępniony przez Tongyi Qianwen 2.5."
  },
  "qwen2.5-32b-instruct": {
    "description": "Model Qwen 2.5 o skali 32B, udostępniony na zasadzie open source."
  },
  "qwen2.5-72b-instruct": {
    "description": "Model Qwen 2.5 o skali 72B, udostępniony na zasadzie open source."
  },
  "qwen2.5-7b-instruct": {
    "description": "Model Qwen 2.5 o skali 7B, udostępniony na zasadzie open source."
  },
  "qwen2.5-coder-1.5b-instruct": {
    "description": "Otwarta wersja modelu kodowania Qwen."
  },
  "qwen2.5-coder-14b-instruct": {
    "description": "Otwarta wersja modelu kodowania Tongyi Qianwen."
  },
  "qwen2.5-coder-32b-instruct": {
    "description": "Otwarta wersja modelu kodowania Qwen."
  },
  "qwen2.5-coder-7b-instruct": {
    "description": "Otwarta wersja modelu kodowania Qwen."
  },
  "qwen2.5-coder-instruct": {
    "description": "Qwen2.5-Coder to najnowszy model językowy o dużym rozmiarze z serii Qwen, specjalnie przeznaczony do obsługi kodu (wcześniej znany jako CodeQwen)."
  },
  "qwen2.5-instruct": {
    "description": "Qwen2.5 to najnowsza seria modeli językowych Qwen. W przypadku Qwen2.5 wydaliśmy wiele podstawowych modeli językowych oraz modeli językowych dostosowanych do instrukcji, z zakresem parametrów od 500 milionów do 7,2 miliarda."
  },
  "qwen2.5-math-1.5b-instruct": {
    "description": "Model Qwen-Math ma silne umiejętności rozwiązywania problemów matematycznych."
  },
  "qwen2.5-math-72b-instruct": {
    "description": "Model Qwen-Math, który ma silne zdolności rozwiązywania problemów matematycznych."
  },
  "qwen2.5-math-7b-instruct": {
    "description": "Model Qwen-Math, który ma silne zdolności rozwiązywania problemów matematycznych."
  },
  "qwen2.5-omni-7b": {
    "description": "Model serii Qwen-Omni obsługuje wprowadzanie danych w różnych modalnościach, w tym wideo, audio, obrazy i tekst, oraz generuje audio i tekst."
  },
  "qwen2.5-vl-32b-instruct": {
    "description": "Seria modeli Qwen2.5-VL poprawia poziom inteligencji, praktyczności i zastosowania modelu, co pozwala mu lepiej radzić sobie w naturalnej konwersacji, tworzeniu treści, usługach wiedzy specjalistycznej i programowaniu. Wersja 32B została zoptymalizowana za pomocą technologii uczenia wzmacniającego, co w porównaniu z innymi modelami serii Qwen2.5 VL, zapewnia bardziej zgodny z preferencjami ludzi styl wyjściowy, zdolność wnioskowania w złożonych problemach matematycznych oraz zdolność szczegółowej interpretacji i wnioskowania na podstawie obrazów."
  },
  "qwen2.5-vl-72b-instruct": {
    "description": "Zwiększona zdolność do podążania za instrukcjami, matematyki, rozwiązywania problemów i kodowania, poprawiona zdolność do rozpoznawania obiektów, wsparcie dla różnych formatów do precyzyjnego lokalizowania elementów wizualnych, zdolność do rozumienia długich plików wideo (do 10 minut) oraz lokalizowania momentów zdarzeń w czasie rzeczywistym, zdolność do rozumienia kolejności czasowej i szybkości, wsparcie dla operacji na systemach OS lub Mobile, silna zdolność do ekstrakcji kluczowych informacji i generowania wyjścia w formacie JSON. Ta wersja to wersja 72B, najsilniejsza w tej serii."
  },
  "qwen2.5-vl-7b-instruct": {
    "description": "Zwiększona zdolność do podążania za instrukcjami, matematyki, rozwiązywania problemów i kodowania, poprawiona zdolność do rozpoznawania obiektów, wsparcie dla różnych formatów do precyzyjnego lokalizowania elementów wizualnych, zdolność do rozumienia długich plików wideo (do 10 minut) oraz lokalizowania momentów zdarzeń w czasie rzeczywistym, zdolność do rozumienia kolejności czasowej i szybkości, wsparcie dla operacji na systemach OS lub Mobile, silna zdolność do ekstrakcji kluczowych informacji i generowania wyjścia w formacie JSON. Ta wersja to wersja 72B, najsilniejsza w tej serii."
  },
  "qwen2.5-vl-instruct": {
    "description": "Qwen2.5-VL to najnowsza wersja modelu wizualno-lingwistycznego rodziny Qwen."
  },
  "qwen2.5:0.5b": {
    "description": "Qwen2.5 to nowa generacja dużego modelu językowego Alibaba, który wspiera różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen2.5:1.5b": {
    "description": "Qwen2.5 to nowa generacja dużego modelu językowego Alibaba, który wspiera różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen2.5:72b": {
    "description": "Qwen2.5 to nowa generacja dużego modelu językowego Alibaba, który wspiera różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen2:0.5b": {
    "description": "Qwen2 to nowa generacja dużego modelu językowego Alibaba, wspierająca różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen2:1.5b": {
    "description": "Qwen2 to nowa generacja dużego modelu językowego Alibaba, wspierająca różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen2:72b": {
    "description": "Qwen2 to nowa generacja dużego modelu językowego Alibaba, wspierająca różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen3": {
    "description": "Qwen3 to nowa generacja dużego modelu językowego od Alibaba, który wspiera różnorodne potrzeby aplikacyjne dzięki doskonałej wydajności."
  },
  "qwen3-0.6b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-1.7b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-14b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-235b-a22b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-235b-a22b-instruct-2507": {
    "description": "Otwartoźródłowy model trybu nie myślącego oparty na Qwen3, z niewielką poprawą w zakresie kreatywności subiektywnej i bezpieczeństwa modelu w porównaniu do poprzedniej wersji (Tongyi Qianwen 3-235B-A22B)."
  },
  "qwen3-235b-a22b-thinking-2507": {
    "description": "Otwartoźródłowy model trybu myślącego oparty na Qwen3, z dużymi ulepszeniami w zakresie zdolności logicznych, ogólnych, wzbogacenia wiedzy i kreatywności w porównaniu do poprzedniej wersji (Tongyi Qianwen 3-235B-A22B), odpowiedni do zadań wymagających zaawansowanego wnioskowania."
  },
  "qwen3-30b-a3b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-30b-a3b-instruct-2507": {
    "description": "W porównaniu z poprzednią wersją (Qwen3-30B-A3B) ogólne zdolności w języku chińskim, angielskim i wielojęzyczne zostały znacznie poprawione. Specjalna optymalizacja dla zadań subiektywnych i otwartych sprawia, że model lepiej odpowiada preferencjom użytkowników i potrafi dostarczać bardziej pomocne odpowiedzi."
  },
  "qwen3-30b-a3b-thinking-2507": {
    "description": "Model open source w trybie myślenia oparty na Qwen3, który w porównaniu z poprzednią wersją (Tongyi Qianwen 3-30B-A3B) wykazuje znaczne ulepszenia w zakresie zdolności logicznych, ogólnych, wzbogacenia wiedzy oraz kreatywności. Nadaje się do trudnych scenariuszy wymagających zaawansowanego rozumowania."
  },
  "qwen3-32b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-4b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-8b": {
    "description": "Qwen3 to nowa generacja modelu Qwen, który znacznie zwiększa możliwości w zakresie wnioskowania, ogólności, agenta i wielojęzyczności, osiągając wiodące w branży wyniki w wielu kluczowych obszarach i wspierając przełączanie trybów myślenia."
  },
  "qwen3-coder-480b-a35b-instruct": {
    "description": "Otwartoźródłowa wersja modelu kodowania Tongyi Qianwen. Najnowszy qwen3-coder-480b-a35b-instruct to model generowania kodu oparty na Qwen3, posiadający potężne zdolności agenta kodującego, specjalizujący się w wywoływaniu narzędzi i interakcji środowiskowej, umożliwiający autonomiczne programowanie z doskonałymi zdolnościami kodowania i ogólnymi."
  },
  "qwen3-coder-flash": {
    "description": "Model kodowania Tongyi Qianwen. Najnowsza seria modeli Qwen3-Coder oparta na Qwen3 to modele generujące kod, posiadające potężne zdolności agenta kodującego, biegłe w wywoływaniu narzędzi i interakcji ze środowiskiem, umożliwiające autonomiczne programowanie, łącząc doskonałe umiejętności kodowania z uniwersalnymi zdolnościami."
  },
  "qwen3-coder-plus": {
    "description": "Model kodowania Tongyi Qianwen. Najnowsza seria modeli Qwen3-Coder oparta na Qwen3 to modele generujące kod, posiadające potężne zdolności agenta kodującego, biegłe w wywoływaniu narzędzi i interakcji ze środowiskiem, umożliwiające autonomiczne programowanie, łącząc doskonałe umiejętności kodowania z uniwersalnymi zdolnościami."
  },
  "qwen3-coder:480b": {
    "description": "Wysokowydajny model długiego kontekstu od Alibaba, zoptymalizowany pod kątem zadań agenta i kodowania."
  },
  "qwen3-max": {
    "description": "Model serii Tongyi Qianwen 3 Max, który w porównaniu do serii 2.5 oferuje znacznie ulepszone zdolności ogólne, w tym rozumienie tekstu w języku chińskim i angielskim, zdolność do wykonywania złożonych instrukcji, zadania otwarte o charakterze subiektywnym, wielojęzyczność oraz wywoływanie narzędzi; model cechuje się mniejszą halucynacją wiedzy. Najnowsza wersja qwen3-max, w porównaniu do wersji podglądowej qwen3-max-preview, została specjalnie ulepszona w zakresie programowania agentów i wywoływania narzędzi. Wydany oficjalny model osiąga poziom SOTA w swojej dziedzinie i jest dostosowany do bardziej złożonych scenariuszy zastosowań agentów."
  },
  "qwen3-next-80b-a3b-instruct": {
    "description": "Nowa generacja otwartego modelu bez trybu myślenia oparta na Qwen3, która w porównaniu z poprzednią wersją (Tongyi Qianwen 3-235B-A22B-Instruct-2507) cechuje się lepszym rozumieniem tekstu w języku chińskim, wzmocnionymi zdolnościami wnioskowania logicznego oraz lepszą wydajnością w zadaniach generowania tekstu."
  },
  "qwen3-next-80b-a3b-thinking": {
    "description": "Nowa generacja otwartego modelu z trybem myślenia oparta na Qwen3, która w porównaniu z poprzednią wersją (Tongyi Qianwen 3-235B-A22B-Thinking-2507) wykazuje poprawę w przestrzeganiu instrukcji oraz bardziej zwięzłe podsumowania w odpowiedziach modelu."
  },
  "qwen3-vl-plus": {
    "description": "Tongyi Qianwen VL to model generujący tekst z umiejętnością rozumienia wizualnego (obrazów). Potrafi nie tylko wykonywać OCR (rozpoznawanie tekstu na obrazach), ale także podsumowywać i wnioskować, na przykład wyodrębniać atrybuty z fotografii produktów czy rozwiązywać zadania na podstawie ilustracji."
  },
  "qwq": {
    "description": "QwQ to eksperymentalny model badawczy, skoncentrowany na zwiększeniu zdolności wnioskowania AI."
  },
  "qwq-32b": {
    "description": "Model inferency QwQ, oparty na modelu Qwen2.5-32B, został znacznie ulepszony dzięki uczeniu przez wzmocnienie, co zwiększa jego zdolności inferencyjne. Kluczowe wskaźniki modelu, takie jak matematyczny kod i inne (AIME 24/25, LiveCodeBench), oraz niektóre ogólne wskaźniki (IFEval, LiveBench itp.) osiągają poziom pełnej wersji DeepSeek-R1, a wszystkie wskaźniki znacznie przewyższają te, które są oparte na Qwen2.5-32B, w tym DeepSeek-R1-Distill-Qwen-32B."
  },
  "qwq-32b-preview": {
    "description": "Model QwQ to eksperymentalny model badawczy opracowany przez zespół Qwen, skoncentrowany na zwiększeniu zdolności wnioskowania AI."
  },
  "qwq-plus": {
    "description": "Model wnioskowania QwQ oparty na modelu Qwen2.5, znacznie poprawiony dzięki uczeniu ze wzmocnieniem. Kluczowe wskaźniki modelu w matematyce i kodowaniu (AIME 24/25, LiveCodeBench) oraz niektóre wskaźniki ogólne (IFEval, LiveBench itp.) osiągają poziom pełnej wersji DeepSeek-R1."
  },
  "qwq_32b": {
    "description": "Model wnioskowania średniej wielkości z serii Qwen. W porównaniu do tradycyjnych modeli dostosowanych do instrukcji, QwQ, posiadający zdolności myślenia i wnioskowania, może znacznie poprawić wydajność w zadaniach końcowych, zwłaszcza w rozwiązywaniu trudnych problemów."
  },
  "r1-1776": {
    "description": "R1-1776 to wersja modelu DeepSeek R1, która została poddana dalszemu treningowi, aby dostarczać nieocenzurowane, bezstronne informacje faktograficzne."
  },
  "solar-mini": {
    "description": "Solar Mini to kompaktowy LLM, który przewyższa GPT-3.5, posiadając potężne zdolności wielojęzyczne, wspierając angielski i koreański, oferując efektywne i zgrabne rozwiązania."
  },
  "solar-mini-ja": {
    "description": "Solar Mini (Ja) rozszerza możliwości Solar Mini, koncentrując się na języku japońskim, jednocześnie zachowując wysoką efektywność i doskonałe osiągi w użyciu angielskiego i koreańskiego."
  },
  "solar-pro": {
    "description": "Solar Pro to model LLM o wysokiej inteligencji wydany przez Upstage, koncentrujący się na zdolności do przestrzegania instrukcji na pojedynczym GPU, osiągając wynik IFEval powyżej 80. Obecnie wspiera język angielski, a wersja oficjalna planowana jest na listopad 2024, z rozszerzeniem wsparcia językowego i długości kontekstu."
  },
  "sonar": {
    "description": "Lekki produkt wyszukiwania oparty na kontekście, szybszy i tańszy niż Sonar Pro."
  },
  "sonar-deep-research": {
    "description": "Deep Research przeprowadza kompleksowe badania na poziomie eksperckim i łączy je w dostępne, praktyczne raporty."
  },
  "sonar-pro": {
    "description": "Zaawansowany produkt wyszukiwania wspierający kontekst wyszukiwania, oferujący zaawansowane zapytania i śledzenie."
  },
  "sonar-reasoning": {
    "description": "Nowy produkt API wspierany przez model wnioskowania DeepSeek."
  },
  "sonar-reasoning-pro": {
    "description": "Nowy produkt API wspierany przez model wnioskowania DeepSeek."
  },
  "stable-diffusion-3-medium": {
    "description": "Najnowszy duży model generowania obrazów na podstawie tekstu wydany przez Stability AI. Ta wersja zachowuje zalety poprzednich generacji, jednocześnie znacząco poprawiając jakość obrazu, rozumienie tekstu i różnorodność stylów. Potrafi dokładniej interpretować złożone naturalne polecenia i generować bardziej precyzyjne oraz zróżnicowane obrazy."
  },
  "stable-diffusion-3.5-large": {
    "description": "stable-diffusion-3.5-large to model multimodalnego dyfuzyjnego transformera (MMDiT) do generowania obrazów na podstawie tekstu, wyposażony w 800 milionów parametrów. Charakteryzuje się doskonałą jakością obrazu i zgodnością z poleceniami, wspiera generowanie obrazów o rozdzielczości do 1 miliona pikseli i działa efektywnie na standardowym sprzęcie konsumenckim."
  },
  "stable-diffusion-3.5-large-turbo": {
    "description": "stable-diffusion-3.5-large-turbo to model oparty na stable-diffusion-3.5-large, wykorzystujący technikę destylacji dyfuzji przeciwstawnej (ADD), oferujący wyższą szybkość działania."
  },
  "stable-diffusion-v1.5": {
    "description": "stable-diffusion-v1.5 to model zainicjowany wagami ze stable-diffusion-v1.2 i dostrojony przez 595 tysięcy kroków na zbiorze \"laion-aesthetics v2 5+\" w rozdzielczości 512x512, z redukcją warunkowania tekstowego o 10% w celu poprawy próbkowania bez klasyfikatora."
  },
  "stable-diffusion-xl": {
    "description": "stable-diffusion-xl wprowadza znaczące ulepszenia w porównaniu do wersji v1.5 i osiąga efekty porównywalne z najlepszymi otwartymi modelami generacji obrazów, takimi jak midjourney. Kluczowe ulepszenia obejmują: trzykrotnie większy unet backbone, dodanie modułu refinacji poprawiającego jakość generowanych obrazów oraz bardziej efektywne techniki treningowe."
  },
  "stable-diffusion-xl-base-1.0": {
    "description": "Duży model generowania obrazów na podstawie tekstu opracowany i udostępniony przez Stability AI, wyróżniający się czołowymi zdolnościami twórczymi. Posiada doskonałe zdolności rozumienia instrukcji i wspiera definiowanie treści za pomocą odwrotnych promptów."
  },
  "step-1-128k": {
    "description": "Równoważy wydajność i koszty, odpowiedni do ogólnych scenariuszy."
  },
  "step-1-256k": {
    "description": "Posiada zdolność przetwarzania ultra długiego kontekstu, szczególnie odpowiedni do analizy długich dokumentów."
  },
  "step-1-32k": {
    "description": "Obsługuje średniej długości dialogi, odpowiedni do różnych zastosowań."
  },
  "step-1-8k": {
    "description": "Mały model, odpowiedni do lekkich zadań."
  },
  "step-1-flash": {
    "description": "Model o wysokiej prędkości, odpowiedni do dialogów w czasie rzeczywistym."
  },
  "step-1.5v-mini": {
    "description": "Ten model ma potężne zdolności rozumienia wideo."
  },
  "step-1o-turbo-vision": {
    "description": "Model ten ma potężne zdolności rozumienia obrazów, w dziedzinie matematyki i kodowania przewyższa 1o. Model jest mniejszy niż 1o, a prędkość wyjścia jest szybsza."
  },
  "step-1o-vision-32k": {
    "description": "Ten model ma potężne zdolności rozumienia obrazów. W porównaniu do modeli z serii step-1v, oferuje lepsze osiągi wizualne."
  },
  "step-1v-32k": {
    "description": "Obsługuje wejścia wizualne, wzmacniając doświadczenie interakcji multimodalnych."
  },
  "step-1v-8k": {
    "description": "Mały model wizualny, odpowiedni do podstawowych zadań związanych z tekstem i obrazem."
  },
  "step-1x-edit": {
    "description": "Model skoncentrowany na zadaniach edycji obrazów, potrafiący modyfikować i wzmacniać obrazy na podstawie dostarczonych przez użytkownika obrazów i opisów tekstowych. Obsługuje różne formaty wejściowe, w tym opisy tekstowe i obrazy przykładowe. Model rozumie intencje użytkownika i generuje zgodne z nimi wyniki edycji obrazów."
  },
  "step-1x-medium": {
    "description": "Model o silnych zdolnościach generowania obrazów, obsługujący wejścia w postaci opisów tekstowych. Posiada natywną obsługę języka chińskiego, co pozwala lepiej rozumieć i przetwarzać chińskie opisy tekstowe, dokładniej uchwycić ich znaczenie i przekształcić je w cechy obrazu, umożliwiając precyzyjne generowanie obrazów. Model generuje obrazy o wysokiej rozdzielczości i jakości oraz posiada pewne zdolności transferu stylu."
  },
  "step-2-16k": {
    "description": "Obsługuje interakcje z dużą ilością kontekstu, idealny do złożonych scenariuszy dialogowych."
  },
  "step-2-16k-exp": {
    "description": "Eksperymentalna wersja modelu step-2, zawierająca najnowsze funkcje, aktualizacje w trybie ciągłym. Nie zaleca się używania w produkcji."
  },
  "step-2-mini": {
    "description": "Model oparty na nowej generacji własnej architektury Attention MFA, osiągający podobne wyniki jak step1 przy bardzo niskich kosztach, jednocześnie zapewniając wyższą przepustowość i szybszy czas reakcji. Potrafi obsługiwać ogólne zadania, a w zakresie umiejętności kodowania ma szczególne zdolności."
  },
  "step-2x-large": {
    "description": "Nowa generacja modelu Step Star, skoncentrowana na generowaniu obrazów na podstawie tekstu. Model tworzy obrazy o bardziej realistycznej fakturze i lepszych zdolnościach generowania tekstu w języku chińskim i angielskim."
  },
  "step-3": {
    "description": "Model posiada zaawansowane zdolności percepcji wzrokowej i złożonego wnioskowania. Potrafi z wysoką precyzją realizować międzydziedzinowe zrozumienie skomplikowanej wiedzy, przeprowadzać analizę łączącą informacje matematyczne i wizualne oraz rozwiązywać różnorodne problemy związane z analizą wizualną w życiu codziennym."
  },
  "step-r1-v-mini": {
    "description": "Model ten to potężny model wnioskowania z zdolnościami rozumienia obrazów, zdolny do przetwarzania informacji wizualnych i tekstowych, generując tekst po głębokim przemyśleniu. Model ten wyróżnia się w dziedzinie wnioskowania wizualnego, a także posiada pierwszorzędne zdolności wnioskowania matematycznego, kodowania i tekstu. Długość kontekstu wynosi 100k."
  },
  "stepfun-ai/step3": {
    "description": "Step3 to zaawansowany multimodalny model wnioskowania wydany przez StepFun (阶跃星辰). Został zbudowany na architekturze Mixture of Experts (MoE) z łączną liczbą 321 mld parametrów i 38 mld parametrów aktywacji. Model ma konstrukcję end-to-end, zaprojektowaną tak, aby minimalizować koszty dekodowania, jednocześnie zapewniając najwyższą wydajność w zadaniach wnioskowania wizualno-językowego. Dzięki współdziałaniu mechanizmów Multi-Matrix Factorized Attention (MFA) i Attention-FFN Decoupling (AFD), Step3 zachowuje znakomitą efektywność zarówno na akceleratorach klasy flagowej, jak i na urządzeniach o niższej wydajności. W fazie pretrenowania Step3 przetworzył ponad 20 bilionów tokenów tekstowych oraz 4 biliony tokenów mieszanych tekstowo-obrazowych, obejmujących ponad dziesięć języków. Model osiągnął czołowe wyniki wśród modeli open-source na wielu benchmarkach, w tym w zadaniach z zakresu matematyki, programowania i multimodalu."
  },
  "taichu_llm": {
    "description": "Model językowy TaiChu charakteryzuje się wyjątkową zdolnością rozumienia języka oraz umiejętnościami w zakresie tworzenia tekstów, odpowiadania na pytania, programowania, obliczeń matematycznych, wnioskowania logicznego, analizy emocji i streszczenia tekstu. Innowacyjnie łączy wstępne uczenie się na dużych zbiorach danych z bogatą wiedzą z wielu źródeł, stale doskonaląc technologię algorytmiczną i nieustannie przyswajając nową wiedzę z zakresu słownictwa, struktury, gramatyki i semantyki z ogromnych zbiorów danych tekstowych, co prowadzi do ciągłej ewolucji modelu. Umożliwia użytkownikom łatwiejszy dostęp do informacji i usług oraz bardziej inteligentne doświadczenia."
  },
  "taichu_o1": {
    "description": "taichu_o1 to nowa generacja modelu wnioskowania, która poprzez interakcje multimodalne i uczenie przez wzmocnienie realizuje łańcuchy myślenia przypominające ludzkie, wspierając złożone symulacje decyzji, jednocześnie prezentując ścieżki myślenia modelu przy zachowaniu wysokiej precyzji wyników, odpowiednia do analizy strategii i głębokiego myślenia."
  },
  "taichu_vl": {
    "description": "Łączy zdolności rozumienia obrazów, transferu wiedzy i logicznego wnioskowania, wyróżniając się w dziedzinie pytań i odpowiedzi na podstawie tekstu i obrazów."
  },
  "tencent/Hunyuan-A13B-Instruct": {
    "description": "Hunyuan-A13B-Instruct ma 80 miliardów parametrów, a aktywacja 13 miliardów parametrów pozwala mu konkurować z większymi modelami. Wspiera hybrydowe wnioskowanie „szybkiego myślenia/powolnego myślenia”; stabilne rozumienie długich tekstów; potwierdzona przewaga zdolności agenta w testach BFCL-v3 i τ-Bench; dzięki połączeniu GQA i wielu formatów kwantyzacji zapewnia efektywne wnioskowanie."
  },
  "text-embedding-3-large": {
    "description": "Najpotężniejszy model wektoryzacji, odpowiedni do zadań w języku angielskim i innych językach."
  },
  "text-embedding-3-small": {
    "description": "Nowej generacji model Embedding, efektywny i ekonomiczny, odpowiedni do wyszukiwania wiedzy, aplikacji RAG i innych scenariuszy."
  },
  "thudm/glm-4-32b": {
    "description": "GLM-4-32B-0414 to dwujęzyczny (chińsko-angielski) model językowy o otwartych wagach 32B, zoptymalizowany do generowania kodu, wywołań funkcji i zadań agentowych. Został wstępnie wytrenowany na 15T wysokiej jakości danych i danych do ponownego wnioskowania, a następnie udoskonalony przy użyciu dostosowania do preferencji ludzkich, próbkowania odrzucającego i uczenia przez wzmocnienie. Model wykazuje doskonałe wyniki w złożonym wnioskowaniu, generowaniu artefaktów i zadaniach związanych z wyjściem strukturalnym, osiągając wyniki porównywalne z GPT-4o i DeepSeek-V3-0324 w wielu testach porównawczych."
  },
  "thudm/glm-4-32b:free": {
    "description": "GLM-4-32B-0414 to dwujęzyczny (chińsko-angielski) model językowy o otwartych wagach 32B, zoptymalizowany do generowania kodu, wywołań funkcji i zadań agentowych. Został wstępnie wytrenowany na 15T wysokiej jakości danych i danych do ponownego wnioskowania, a następnie udoskonalony przy użyciu dostosowania do preferencji ludzkich, próbkowania odrzucającego i uczenia przez wzmocnienie. Model wykazuje doskonałe wyniki w złożonym wnioskowaniu, generowaniu artefaktów i zadaniach związanych z wyjściem strukturalnym, osiągając wyniki porównywalne z GPT-4o i DeepSeek-V3-0324 w wielu testach porównawczych."
  },
  "thudm/glm-4-9b-chat": {
    "description": "Otwarta wersja najnowszej generacji modelu pretrenowanego GLM-4 wydanego przez Zhipu AI."
  },
  "thudm/glm-z1-32b": {
    "description": "GLM-Z1-32B-0414 to wzmocniona wariant wnioskowania GLM-4-32B, zaprojektowana do rozwiązywania głębokich problemów matematycznych, logicznych i związanych z kodem. Wykorzystuje rozszerzone uczenie przez wzmocnienie (specyficzne dla zadań i oparte na ogólnych preferencjach par) w celu poprawy wydajności w złożonych zadaniach wieloetapowych. W porównaniu do podstawowego modelu GLM-4-32B, Z1 znacznie poprawia zdolności w zakresie wnioskowania strukturalnego i formalnego.\n\nModel wspiera wymuszanie kroków 'myślenia' poprzez inżynierię podpowiedzi i zapewnia poprawioną spójność dla długich formatów wyjściowych. Jest zoptymalizowany pod kątem przepływów pracy agentów i wspiera długi kontekst (przez YaRN), wywołania narzędzi JSON oraz konfiguracje drobnoziarnistego próbkowania dla stabilnego wnioskowania. Idealny do przypadków użycia wymagających przemyślanego, wieloetapowego wnioskowania lub formalnych dedukcji."
  },
  "thudm/glm-z1-rumination-32b": {
    "description": "THUDM: GLM Z1 Rumination 32B to model głębokiego wnioskowania o 32 miliardach parametrów w serii GLM-4-Z1, zoptymalizowany do złożonych, otwartych zadań wymagających długotrwałego myślenia. Opiera się na glm-4-32b-0414, dodając dodatkowe etapy uczenia przez wzmocnienie i strategie wieloetapowego dostosowania, wprowadzając zdolność 'refleksji' mającą na celu symulację rozszerzonego przetwarzania poznawczego. Obejmuje to iteracyjne wnioskowanie, analizy wielokrokowe i wzbogacone narzędziami przepływy pracy, takie jak wyszukiwanie, pobieranie i syntezę z uwzględnieniem cytatów.\n\nModel doskonale sprawdza się w pisaniu badawczym, analizie porównawczej i złożonych pytaniach i odpowiedziach. Obsługuje wywołania funkcji dla prymitywów wyszukiwania i nawigacji (`search`, `click`, `open`, `finish`), co umożliwia jego użycie w agentowych przepływach pracy. Zachowanie refleksyjne kształtowane jest przez wieloetapową kontrolę cykliczną z nagrodami opartymi na regułach i mechanizmem opóźnionych decyzji, a także na głębokich ramach badawczych, takich jak wewnętrzny stos dostosowujący OpenAI. Ten wariant jest odpowiedni dla scenariuszy wymagających głębokości, a nie szybkości."
  },
  "tngtech/deepseek-r1t-chimera:free": {
    "description": "DeepSeek-R1T-Chimera powstał poprzez połączenie DeepSeek-R1 i DeepSeek-V3 (0324), łącząc zdolności wnioskowania R1 z poprawą efektywności tokenów V3. Opiera się na architekturze DeepSeek-MoE Transformer i został zoptymalizowany do ogólnych zadań generowania tekstu.\n\nModel łączy w sobie wagi wstępnie wytrenowane z dwóch źródłowych modeli, aby zrównoważyć wydajność wnioskowania, efektywności i przestrzegania instrukcji. Został wydany na licencji MIT, z zamiarem użycia w badaniach i zastosowaniach komercyjnych."
  },
  "togethercomputer/StripedHyena-Nous-7B": {
    "description": "StripedHyena Nous (7B) oferuje zwiększoną moc obliczeniową dzięki efektywnym strategiom i architekturze modelu."
  },
  "tts-1": {
    "description": "Najnowocześniejszy model tekstu na mowę, zoptymalizowany pod kątem szybkości w scenariuszach w czasie rzeczywistym."
  },
  "tts-1-hd": {
    "description": "Najnowocześniejszy model tekstu na mowę, zoptymalizowany pod kątem jakości."
  },
  "upstage/SOLAR-10.7B-Instruct-v1.0": {
    "description": "Upstage SOLAR Instruct v1 (11B) jest przeznaczony do precyzyjnych zadań poleceniowych, oferując doskonałe możliwości przetwarzania języka."
  },
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet podnosi standardy branżowe, przewyższając modele konkurencji oraz Claude 3 Opus, osiągając doskonałe wyniki w szerokim zakresie ocen, przy zachowaniu prędkości i kosztów naszych modeli średniego poziomu."
  },
  "us.anthropic.claude-3-7-sonnet-20250219-v1:0": {
    "description": "Claude 3.7 sonet to najszybszy model następnej generacji od Anthropic. W porównaniu do Claude 3 Haiku, Claude 3.7 Sonet wykazuje poprawę w różnych umiejętnościach i przewyższa największy model poprzedniej generacji, Claude 3 Opus, w wielu testach inteligencji."
  },
  "v0-1.0-md": {
    "description": "Model v0-1.0-md to starsza wersja modelu udostępniana przez API v0"
  },
  "v0-1.5-lg": {
    "description": "Model v0-1.5-lg jest przeznaczony do zaawansowanych zadań myślenia lub rozumowania"
  },
  "v0-1.5-md": {
    "description": "Model v0-1.5-md jest odpowiedni do codziennych zadań i generowania interfejsu użytkownika (UI)"
  },
  "vercel/v0-1.0-md": {
    "description": "Dostęp do modelu stojącego za v0 do generowania, naprawiania i optymalizacji nowoczesnych aplikacji webowych, z rozumowaniem specyficznym dla frameworków i aktualną wiedzą."
  },
  "vercel/v0-1.5-md": {
    "description": "Dostęp do modelu stojącego za v0 do generowania, naprawiania i optymalizacji nowoczesnych aplikacji webowych, z rozumowaniem specyficznym dla frameworków i aktualną wiedzą."
  },
  "wan2.2-t2i-flash": {
    "description": "Wersja ekspresowa Wanxiang 2.2, najnowszy model. Kompleksowo ulepszony pod względem kreatywności, stabilności i realizmu, generuje szybko i oferuje wysoką opłacalność."
  },
  "wan2.2-t2i-plus": {
    "description": "Profesjonalna wersja Wanxiang 2.2, najnowszy model. Kompleksowo ulepszony pod względem kreatywności, stabilności i realizmu, generuje obrazy o bogatych detalach."
  },
  "wanx-v1": {
    "description": "Podstawowy model generowania obrazów na podstawie tekstu. Odpowiada uniwersalnemu modelowi 1.0 na oficjalnej stronie Tongyi Wanxiang."
  },
  "wanx2.0-t2i-turbo": {
    "description": "Specjalizuje się w realistycznych portretach, oferuje średnią prędkość i niskie koszty. Odpowiada ekspresowemu modelowi 2.0 na oficjalnej stronie Tongyi Wanxiang."
  },
  "wanx2.1-t2i-plus": {
    "description": "Wersja z kompleksowymi ulepszeniami. Generuje obrazy o bogatszych detalach, z nieco wolniejszą prędkością. Odpowiada profesjonalnemu modelowi 2.1 na oficjalnej stronie Tongyi Wanxiang."
  },
  "wanx2.1-t2i-turbo": {
    "description": "Wersja z kompleksowymi ulepszeniami. Generuje szybko, oferuje wszechstronne efekty i wysoką opłacalność. Odpowiada ekspresowemu modelowi 2.1 na oficjalnej stronie Tongyi Wanxiang."
  },
  "whisper-1": {
    "description": "Uniwersalny model rozpoznawania mowy, obsługujący wielojęzyczne rozpoznawanie mowy, tłumaczenie mowy oraz identyfikację języka."
  },
  "wizardlm2": {
    "description": "WizardLM 2 to model językowy dostarczany przez Microsoft AI, który wyróżnia się w złożonych dialogach, wielojęzyczności, wnioskowaniu i inteligentnych asystentach."
  },
  "wizardlm2:8x22b": {
    "description": "WizardLM 2 to model językowy dostarczany przez Microsoft AI, który wyróżnia się w złożonych dialogach, wielojęzyczności, wnioskowaniu i inteligentnych asystentach."
  },
  "x1": {
    "description": "Model Spark X1 zostanie dalej ulepszony, osiągając wyniki w zadaniach ogólnych, takich jak rozumowanie, generowanie tekstu i rozumienie języka, które będą porównywalne z OpenAI o1 i DeepSeek R1."
  },
  "xai/grok-2": {
    "description": "Grok 2 to nowoczesny model językowy o zaawansowanych zdolnościach wnioskowania. Wyróżnia się w czacie, kodowaniu i wnioskowaniu, przewyższając Claude 3.5 Sonnet i GPT-4-Turbo na liście LMSYS."
  },
  "xai/grok-2-vision": {
    "description": "Model wizualny Grok 2 doskonale radzi sobie z zadaniami opartymi na wizji, oferując najnowocześniejszą wydajność w wizualnym wnioskowaniu matematycznym (MathVista) i pytaniach opartych na dokumentach (DocVQA). Potrafi przetwarzać różnorodne informacje wizualne, w tym dokumenty, wykresy, diagramy, zrzuty ekranu i zdjęcia."
  },
  "xai/grok-3": {
    "description": "Flagowy model xAI, doskonały w zastosowaniach korporacyjnych, takich jak ekstrakcja danych, kodowanie i streszczanie tekstu. Posiada głęboką wiedzę dziedzinową w finansach, opiece zdrowotnej, prawie i nauce."
  },
  "xai/grok-3-fast": {
    "description": "Flagowy model xAI, doskonały w zastosowaniach korporacyjnych, takich jak ekstrakcja danych, kodowanie i streszczanie tekstu. Wersja szybka działa na szybszej infrastrukturze, oferując znacznie krótsze czasy odpowiedzi. Zwiększona szybkość wiąże się z wyższym kosztem na token wyjściowy."
  },
  "xai/grok-3-mini": {
    "description": "Lekki model xAI, który myśli przed odpowiedzią. Idealny do prostych lub logicznych zadań, które nie wymagają głębokiej wiedzy dziedzinowej. Dostępne są surowe ścieżki myślowe."
  },
  "xai/grok-3-mini-fast": {
    "description": "Lekki model xAI, który myśli przed odpowiedzią. Idealny do prostych lub logicznych zadań, które nie wymagają głębokiej wiedzy dziedzinowej. Dostępne są surowe ścieżki myślowe. Wersja szybka działa na szybszej infrastrukturze, oferując znacznie krótsze czasy odpowiedzi. Zwiększona szybkość wiąże się z wyższym kosztem na token wyjściowy."
  },
  "xai/grok-4": {
    "description": "Najnowszy i najlepszy flagowy model xAI, oferujący niezrównaną wydajność w języku naturalnym, matematyce i wnioskowaniu — idealny wszechstronny zawodnik."
  },
  "yi-1.5-34b-chat": {
    "description": "Yi-1.5 to ulepszona wersja Yi. Używa ona wysokiej jakości korpusu danych o rozmiarze 500B tokenów do dalszego wstępnego treningu Yi, a także do dopasowywania na 3M różnorodnych próbkach dopasowujących."
  },
  "yi-large": {
    "description": "Nowy model z miliardami parametrów, oferujący niezwykłe możliwości w zakresie pytań i generowania tekstu."
  },
  "yi-large-fc": {
    "description": "Model yi-large z wzmocnioną zdolnością do wywołań narzędzi, odpowiedni do różnych scenariuszy biznesowych wymagających budowy agentów lub workflow."
  },
  "yi-large-preview": {
    "description": "Wersja wstępna, zaleca się korzystanie z yi-large (nowa wersja)."
  },
  "yi-large-rag": {
    "description": "Zaawansowana usługa oparta na modelu yi-large, łącząca techniki wyszukiwania i generowania, oferująca precyzyjne odpowiedzi oraz usługi wyszukiwania informacji w czasie rzeczywistym."
  },
  "yi-large-turbo": {
    "description": "Model o doskonałym stosunku jakości do ceny, z doskonałymi osiągami. Wysokiej precyzji optymalizacja w oparciu o wydajność, szybkość wnioskowania i koszty."
  },
  "yi-lightning": {
    "description": "Najnowocześniejszy model o wysokiej wydajności, zapewniający wysoką jakość wyjściową przy znacznie zwiększonej prędkości wnioskowania."
  },
  "yi-lightning-lite": {
    "description": "Lekka wersja, zaleca się użycie yi-lightning."
  },
  "yi-medium": {
    "description": "Model średniej wielkości, zrównoważony pod względem możliwości i kosztów. Głęboko zoptymalizowana zdolność do przestrzegania poleceń."
  },
  "yi-medium-200k": {
    "description": "Okno kontekstowe o długości 200K, oferujące głębokie zrozumienie i generowanie długich tekstów."
  },
  "yi-spark": {
    "description": "Mały, ale potężny, lekki model o wysokiej prędkości. Oferuje wzmocnione możliwości obliczeń matematycznych i pisania kodu."
  },
  "yi-vision": {
    "description": "Model do złożonych zadań wizualnych, oferujący wysoką wydajność w zakresie rozumienia i analizy obrazów."
  },
  "yi-vision-v2": {
    "description": "Model do złożonych zadań wizualnych, oferujący wysokowydajną zdolność rozumienia i analizy na podstawie wielu obrazów."
  },
  "zai-org/GLM-4.5": {
    "description": "GLM-4.5 to podstawowy model zaprojektowany specjalnie do zastosowań agentowych, wykorzystujący architekturę mieszanych ekspertów (Mixture-of-Experts). Model jest głęboko zoptymalizowany pod kątem wywoływania narzędzi, przeglądania stron internetowych, inżynierii oprogramowania i programowania frontendowego, wspierając bezproblemową integrację z inteligentnymi agentami kodu takimi jak Claude Code i Roo Code. GLM-4.5 stosuje hybrydowy tryb wnioskowania, dostosowując się do złożonych i codziennych scenariuszy użycia."
  },
  "zai-org/GLM-4.5-Air": {
    "description": "GLM-4.5-Air to podstawowy model zaprojektowany specjalnie do zastosowań agentowych, wykorzystujący architekturę mieszanych ekspertów (Mixture-of-Experts). Model jest głęboko zoptymalizowany pod kątem wywoływania narzędzi, przeglądania stron internetowych, inżynierii oprogramowania i programowania frontendowego, wspierając bezproblemową integrację z inteligentnymi agentami kodu takimi jak Claude Code i Roo Code. GLM-4.5 stosuje hybrydowy tryb wnioskowania, dostosowując się do złożonych i codziennych scenariuszy użycia."
  },
  "zai-org/GLM-4.5V": {
    "description": "GLM-4.5V to najnowszej generacji model wizualno‑językowy (VLM) wydany przez Zhipu AI. Model zbudowano na flagowym modelu tekstowym GLM-4.5-Air, który dysponuje 106 mld parametrów łącznie oraz 12 mld parametrów aktywacyjnych. Wykorzystuje architekturę Mixture-of-Experts (MoE) i został zaprojektowany, by przy niższych kosztach inferencji osiągać znakomitą wydajność. GLM-4.5V technicznie kontynuuje podejście GLM-4.1V-Thinking i wprowadza innowacje takie jak trójwymiarowe obrotowe kodowanie pozycji (3D‑RoPE), co znacząco poprawia postrzeganie i wnioskowanie dotyczące relacji przestrzennych w 3D. Dzięki optymalizacjom w fazach pretrenowania, nadzorowanego dostrajania i uczenia przez wzmocnienie model potrafi przetwarzać obrazy, filmy i długie dokumenty, osiągając czołowe wyniki wśród otwartoźródłowych modeli w 41 publicznych benchmarkach multimodalnych. Dodatkowo model zyskał przełącznik „trybu myślenia”, który pozwala użytkownikom elastycznie wybierać między szybką odpowiedzią a głębokim rozumowaniem, aby zrównoważyć efektywność i skuteczność."
  },
  "zai-org/GLM-4.6": {
    "description": "W porównaniu do GLM-4.5, GLM-4.6 wprowadza wiele kluczowych ulepszeń. Okno kontekstowe zostało rozszerzone z 128K do 200K tokenów, co pozwala modelowi radzić sobie z bardziej złożonymi zadaniami agenta. Model osiągnął wyższe wyniki w benchmarkach kodu oraz wykazał lepszą wydajność w rzeczywistych zastosowaniach takich jak Claude Code, Cline, Roo Code i Kilo Code, w tym poprawę w generowaniu wizualnie dopracowanych stron frontendowych. GLM-4.6 wykazuje wyraźny wzrost wydajności inferencyjnej i wspiera użycie narzędzi podczas inferencji, co przekłada się na silniejsze zdolności ogólne. Model lepiej radzi sobie z użyciem narzędzi i agentami opartymi na wyszukiwaniu oraz jest bardziej efektywnie integrowany w ramach architektury agentów. W pisaniu model lepiej odpowiada ludzkim preferencjom pod względem stylu i czytelności oraz zachowuje się bardziej naturalnie w scenariuszach odgrywania ról."
  },
  "zai/glm-4.5": {
    "description": "Seria modeli GLM-4.5 to podstawowe modele zaprojektowane specjalnie dla agentów. Flagowy GLM-4.5 integruje 355 miliardów parametrów łącznie (32 miliardy aktywnych), łącząc zdolności wnioskowania, kodowania i agentów do rozwiązywania złożonych wymagań aplikacji. Jako system hybrydowego wnioskowania oferuje podwójne tryby operacyjne."
  },
  "zai/glm-4.5-air": {
    "description": "GLM-4.5 i GLM-4.5-Air to nasze najnowsze flagowe modele, zaprojektowane jako podstawowe modele dla zastosowań agentowych. Oba wykorzystują architekturę hybrydowych ekspertów (MoE). GLM-4.5 ma 355 miliardów parametrów łącznie i 32 miliardy aktywnych na pojedyncze przejście, podczas gdy GLM-4.5-Air ma uproszczoną konstrukcję z 106 miliardami parametrów łącznie i 12 miliardami aktywnych."
  },
  "zai/glm-4.5v": {
    "description": "GLM-4.5V zbudowany jest na bazie GLM-4.5-Air, dziedzicząc zweryfikowane technologie GLM-4.1V-Thinking, jednocześnie skutecznie skalując się dzięki potężnej architekturze MoE z 106 miliardami parametrów."
  }
}
