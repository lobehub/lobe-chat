# Agent Runtime E2E 测试指南

本文档描述 Agent Runtime 端到端测试的核心原则和实施方法。

## 核心原则

### 1. 最小化 Mock 原则

E2E 测试的目标是尽可能接近真实运行环境。因此，我们只 Mock **三个外部依赖**：

| 依赖 | Mock 方式 | 说明 |
| --- | --- | --- |
| **Database** | PGLite | 使用 `@lobechat/database/test-utils` 提供的内存数据库 |
| **Redis** | InMemoryAgentStateManager | Mock `AgentStateManager` 使用内存实现 |
| **Redis** | InMemoryStreamEventManager | Mock `StreamEventManager` 使用内存实现 |

**不 Mock 的部分：**

- `model-bank` - 使用真实的模型配置数据
- `Mecha` (AgentToolsEngine, ContextEngineering) - 使用真实逻辑
- `AgentRuntimeService` - 使用真实逻辑
- `AgentRuntimeCoordinator` - 使用真实逻辑

### 2. 使用 vi.spyOn 而非 vi.mock

不同测试场景需要不同的 LLM 响应。使用 `vi.spyOn` 可以：

- 在每个测试中灵活控制返回值
- 便于测试不同场景（纯文本、tool calls、错误等）
- 避免全局 mock 导致的测试隔离问题

### 3. 默认模型使用 gpt-5

- `model-bank` 中肯定有该模型的数据
- 避免短期内因模型更新需要修改测试

## 技术实现

### 数据库设置

```typescript
import { LobeChatDatabase } from '@lobechat/database';
import { getTestDB } from '@lobechat/database/test-utils';

let testDB: LobeChatDatabase;

beforeEach(async () => {
  testDB = await getTestDB();
});
```

### OpenAI Response Mock Helper

创建一个 helper 函数来生成 OpenAI 格式的流式响应：

```typescript
/**
 * 创建 OpenAI 格式的流式响应
 */
export const createOpenAIStreamResponse = (options: {
  content?: string;
  toolCalls?: Array<{
    id: string;
    name: string;
    arguments: string;
  }>;
  finishReason?: 'stop' | 'tool_calls';
}) => {
  const { content, toolCalls, finishReason = 'stop' } = options;

  return new Response(
    new ReadableStream({
      start(controller) {
        const encoder = new TextEncoder();

        // 发送内容 chunk
        if (content) {
          const chunk = {
            id: 'chatcmpl-mock',
            object: 'chat.completion.chunk',
            model: 'gpt-5',
            choices: [
              {
                index: 0,
                delta: { content },
                finish_reason: null,
              },
            ],
          };
          controller.enqueue(encoder.encode(`data: ${JSON.stringify(chunk)}\n\n`));
        }

        // 发送 tool_calls chunk
        if (toolCalls) {
          for (const tool of toolCalls) {
            const chunk = {
              id: 'chatcmpl-mock',
              object: 'chat.completion.chunk',
              model: 'gpt-5',
              choices: [
                {
                  index: 0,
                  delta: {
                    tool_calls: [
                      {
                        index: 0,
                        id: tool.id,
                        type: 'function',
                        function: {
                          name: tool.name,
                          arguments: tool.arguments,
                        },
                      },
                    ],
                  },
                  finish_reason: null,
                },
              ],
            };
            controller.enqueue(encoder.encode(`data: ${JSON.stringify(chunk)}\n\n`));
          }
        }

        // 发送完成 chunk
        const finishChunk = {
          id: 'chatcmpl-mock',
          object: 'chat.completion.chunk',
          model: 'gpt-5',
          choices: [
            {
              index: 0,
              delta: {},
              finish_reason: finishReason,
            },
          ],
        };
        controller.enqueue(encoder.encode(`data: ${JSON.stringify(finishChunk)}\n\n`));
        controller.enqueue(encoder.encode('data: [DONE]\n\n'));
        controller.close();
      },
    }),
    { headers: { 'content-type': 'text/event-stream' } },
  );
};
```

### 内存状态管理

使用依赖注入替代 Redis：

```typescript
import {
  InMemoryAgentStateManager,
  InMemoryStreamEventManager,
} from '@/server/modules/AgentRuntime';
import { AgentRuntimeService } from '@/server/services/agentRuntime';

const stateManager = new InMemoryAgentStateManager();
const streamEventManager = new InMemoryStreamEventManager();

const service = new AgentRuntimeService(serverDB, userId, {
  coordinatorOptions: {
    stateManager,
    streamEventManager,
  },
  queueService: null, // 禁用 QStash 队列，使用 executeSync
  streamEventManager,
});
```

### Mock OpenAI API

在测试中使用 `vi.spyOn` mock fetch：

```typescript
import { vi } from 'vitest';

// 在测试文件顶部或 beforeEach 中
const fetchSpy = vi.spyOn(globalThis, 'fetch');

// 在具体测试中设置返回值
it('should handle text response', async () => {
  fetchSpy.mockResolvedValueOnce(createOpenAIStreamResponse({ content: '杭州今天天气晴朗' }));

  // ... 执行测试
});

it('should handle tool calls', async () => {
  fetchSpy.mockResolvedValueOnce(
    createOpenAIStreamResponse({
      toolCalls: [
        {
          id: 'call_123',
          name: 'lobe-web-browsing____search____builtin',
          arguments: JSON.stringify({ query: '杭州天气' }),
        },
      ],
      finishReason: 'tool_calls',
    }),
  );

  // ... 执行测试
});
```

## 测试场景

### 1. 基本对话测试

```typescript
describe('Basic Chat', () => {
  it('should complete a simple conversation', async () => {
    fetchSpy.mockResolvedValueOnce(
      createOpenAIStreamResponse({ content: 'Hello! How can I help you?' }),
    );

    const result = await service.createOperation({
      agentConfig: { model: 'gpt-5', provider: 'openai' },
      initialMessages: [{ role: 'user', content: 'Hi' }],
      // ...
    });

    const finalState = await service.executeSync(result.operationId);
    expect(finalState.status).toBe('done');
  });
});
```

### 2. Tool 调用测试

```typescript
describe('Tool Calls', () => {
  it('should execute web-browsing tool', async () => {
    // 第一次调用：LLM 返回 tool_calls
    fetchSpy.mockResolvedValueOnce(
      createOpenAIStreamResponse({
        toolCalls: [
          {
            id: 'call_123',
            name: 'lobe-web-browsing____search____builtin',
            arguments: JSON.stringify({ query: '杭州天气' }),
          },
        ],
        finishReason: 'tool_calls',
      }),
    );

    // 第二次调用：处理 tool 结果后的响应
    fetchSpy.mockResolvedValueOnce(
      createOpenAIStreamResponse({ content: '根据搜索结果，杭州今天...' }),
    );

    // ... 执行测试
  });
});
```

### 3. 错误处理测试

```typescript
describe('Error Handling', () => {
  it('should handle API errors gracefully', async () => {
    fetchSpy.mockRejectedValueOnce(new Error('API rate limit exceeded'));

    // ... 执行测试并验证错误处理
  });
});
```

## 文件组织

```
src/server/routers/lambda/__tests__/integration/
├── setup.ts                    # 测试设置工具
├── aiAgent.integration.test.ts # 现有集成测试
├── aiAgent.e2e.test.ts         # E2E 测试
└── helpers/
    └── openaiMock.ts           # OpenAI mock helper
```

## 注意事项

1. **测试隔离**：每个测试后清理 `InMemoryAgentStateManager` 和 `InMemoryStreamEventManager`
2. **超时设置**：E2E 测试可能需要更长的超时时间
3. **调试**：使用 `DEBUG=lobe-server:*` 环境变量查看详细日志
